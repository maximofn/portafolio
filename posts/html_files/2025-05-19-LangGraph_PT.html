<section class="section-block-markdown-cell">
<h1 id="LangGraph">LangGraph<a class="anchor-link" href="#LangGraph">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p><code>LangGraph</code> é um framework de orquestração de baixo nível para construir agentes controláveis</p>
</section>
<section class="section-block-markdown-cell">
<p>Enquanto o <code>LangChain</code> fornece integrações e componentes para acelerar o desenvolvimento de aplicações LLM, a biblioteca <code>LangGraph</code> permite a orquestração de agentes, oferecendo arquiteturas personalizáveis, memória de longo prazo e <code>human in the loop</code> para lidar com tarefas complexas de forma confiável.</p>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Neste post, vamos desabilitar o <code>LangSmith</code>, que é uma ferramenta de depuração de grafos. Vamos desabilitá-lo para não adicionar mais complexidade ao post e nos concentrarmos apenas no <code>LangGraph</code>.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<h2 id="Como funciona LangGraph?">Como funciona <code>LangGraph</code>?<a class="anchor-link" href="#Como funciona LangGraph?">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p><code>LangGraph</code> baseia-se em três componentes:</p>
<ul>
  <li><strong>Nós</strong>: Representam as unidades de processamento da aplicação, como chamar um LLM ou uma ferramenta. São funções de Python que são executadas quando o nó é chamado.</li>
  <li>Tomar o estado como entrada</li>
  <li>Realizam alguma operação</li>
  <li>Retornam o estado atualizado</li>
  <li><strong>Arestas</strong>: Representam as transições entre os nós. Definem a lógica de como o grafo será executado, ou seja, qual nó será executado após outro. Podem ser:</li>
  <li>Diretos: Vão de um nó para outro</li>
  <li>Condicional: Dependem de uma condição</li>
  <li><strong>Estado</strong>: Representa o estado da aplicação, ou seja, contém todas as informações necessárias para a aplicação. É mantido durante a execução da aplicação. É definido pelo usuário, então é preciso pensar muito bem no que será salvo nele.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LangGraph_concept.webp" alt="Conceito LangGraph">
</section>
<section class="section-block-markdown-cell">
<p>Todos os grafos de <code>LangGraph</code> começam a partir de um nó <code>START</code> e terminam em um nó <code>END</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Instalacao do LangGraph">Instalação do LangGraph<a class="anchor-link" href="#Instalacao do LangGraph">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar <code>LangGraph</code> pode-se usar pip:</p>
<div class='highlight'><pre><code class="language-bash">pip install -U langgraph
</code></pre></div>
<p>ou instalar a partir do Conda:</p>
<div class='highlight'><pre><code class="language-bash">conda install langgraph
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Instalacao do modulo da Hugging Face e Anthropic">Instalação do módulo da Hugging Face e Anthropic<a class="anchor-link" href="#Instalacao do modulo da Hugging Face e Anthropic">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a usar um modelo de linguagem da <code>Hugging Face</code>, por isso precisamos instalar seu pacote de langgraph.</p>
<div class='highlight'><pre><code class="language-bash">pip install langchain-huggingface
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Para uma parte vamos usar <code>Sonnet 3.7</code>, depois explicaremos por quê. Então também instalamos o pacote de <code>Anthropic</code>.</p>
<div class='highlight'><pre><code class="language-bash">pip install langchain_anthropic
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<h2 id="CHAVE DE API do Hugging Face">CHAVE DE API do Hugging Face<a class="anchor-link" href="#CHAVE DE API do Hugging Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a usar <code>Qwen/Qwen2.5-72B-Instruct</code> através de <code>Hugging Face Inference Endpoints</code>, por isso precisamos de uma API KEY.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para poder usar o <code>Inference Endpoints</code> da HuggingFace, o primeiro que você precisa é ter uma conta na HuggingFace. Uma vez que você tenha, é necessário ir até <a href="https://huggingface.co/settings/keys">Access tokens</a> nas configurações do seu perfil e gerar um novo token.</p>
<p>Tem que dar um nome. No meu caso, vou chamá-lo de <code>langgraph</code> e ativar a permissão <code>Make calls to inference providers</code>. Isso criará um token que teremos que copiar.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para gerenciar o token, vamos a criar um arquivo no mesmo caminho em que estamos trabalhando chamado <code>.env</code> e vamos colocar o token que copiamos no arquivo da seguinte maneira:</p>
<div class='highlight'><pre><code class="language-bash">HUGGINGFACE_LANGGRAPH="hf_...."
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, para poder obter o token, precisamos ter instalado <code>dotenv</code>, que instalamos através de</p>
<div class='highlight'><pre><code class="language-bash">pip install python-dotenv
</code></pre></div>
<p>Executamos o seguinte</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos um token, criamos um cliente. Para isso, precisamos ter a biblioteca <code>huggingface_hub</code> instalada. A instalamos através do conda ou pip.</p>
<div class='highlight'><pre><code class="language-bash">pip install --upgrade huggingface_hub
</code></pre></div>
<p>o</p>
<div class='highlight'><pre><code class="language-bash">conda install -c conda-forge huggingface_hub
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Agora temos que escolher qual modelo vamos usar. Você pode ver os modelos disponíveis na página de <a href="https://huggingface.co/docs/api-inference/supported-models">Supported models</a> da documentação de <code>Inference Endpoints</code> do Hugging Face.</p>
<p>Vamos a usar <code>Qwen2.5-72B-Instruct</code> que é um modelo muito bom.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos criar o cliente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceClient</span>
<span class="w"> </span>
<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>
<span class="n">client</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;InferenceClient(model=&#x27;Qwen/Qwen2.5-72B-Instruct&#x27;, timeout=None)&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos um teste para ver se funciona</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>
<span class="w">	</span><span class="p">{</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, qué tal?&quot;</span> <span class="p">}</span>
<span class="p">]</span>
<span class="w"> </span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
<span class="w">	</span><span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> 
<span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
<span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span class="w">	</span><span class="n">stream</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="CHAVE DE API da Anthropic">CHAVE DE API da Anthropic<a class="anchor-link" href="#CHAVE DE API da Anthropic">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h2 id="Criar um chatbot basico">Criar um chatbot básico<a class="anchor-link" href="#Criar um chatbot basico">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos criar um chatbot simples usando <code>LangGraph</code>. Este chatbot responderá diretamente às mensagens do usuário. Embora seja simples, nos servirá para ver os conceitos básicos da construção de grafos com <code>LangGraph</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como o nome sugere, <code>LangGraph</code> é uma biblioteca para manipular grafos. Então, começamos criando um grafo <a href="https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph">StateGraph</a>.</p>
<p>Um <code>StateGraph</code> define a estrutura do nosso chatbot como uma <code>máquina de estados</code>. Adicionaremos <code>nós</code> ao nosso grafo para representar os <code>llm</code>s, <code>tool</code>s e <code>funções</code>, os <code>llm</code>s poderão fazer uso dessas <code>tool</code>s e <code>funções</code>; e adicionamos <code>arestas</code> para especificar como o bot deve fazer a transição entre esses <code>nós</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Então começamos criando um <code>StateGraph</code> que precisa de uma classe <code>State</code> para gerenciar o estado do grafo. Como agora vamos criar um chatbot simples, precisamos apenas gerenciar uma lista de mensagens no estado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>

<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Messages have the type &quot;list&quot;. The `add_messages` function</span>
<span class="w">    </span><span class="c1"># in the annotation defines how this state key should be updated</span>
<span class="w">    </span><span class="c1"># (in this case, it appends messages to the list, rather than overwriting them)</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>

<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A função <a href="https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages">add_messages</a> une duas listas de mensagens.</p>
<p>Chegarão novas listas de mensagens, portanto, serão adicionadas à lista de mensagens já existente. Cada lista de mensagens contém um <code>ID</code>, portanto, são adicionadas com este <code>ID</code>. Isso garante que as mensagens sejam apenas adicionadas, não substituídas, a menos que uma nova mensagem tenha o mesmo <code>ID</code> que uma já existente, nesse caso, ela será substituída.</p>
<p><code>add_messages</code> é uma <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers">função reducer</a>, é uma função responsável por atualizar o estado.</p>
</section>
<section class="section-block-markdown-cell">
<p>O grafo <code>graph_builder</code> que criamos, recebe um estado <code>State</code> e retorna um novo estado <code>State</code>. Além disso, atualiza a lista de mensagens.</p>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>**Conceito**</p>
</blockquote>
<blockquote>
<p>> Ao definir um grafo, o primeiro passo é definir seu <code>State</code>. O <code>State</code> inclui o esquema do grafo e as <code>reducer functions</code> que manipulam atualizações do estado.</p>
</blockquote>
<blockquote>
<p>> No nosso exemplo, <code>State</code> é do tipo <code>TypedDict</code> (dicionário tipado) com uma chave: <code>messages</code>.</p>
</blockquote>
<blockquote>
<p>> <code>add_messages</code> é uma <code>função reducer</code> que é usada para adicionar novas mensagens à lista em vez de sobrescrevê-las na lista. Se uma chave de um estado não tiver uma <code>função reducer</code>, cada valor que chegar dessa chave sobrescreverá os valores anteriores.</p>
</blockquote>
<blockquote>
<p>> <code>add_messages</code> é uma <code>função reducer</code> do langgraph, mas nós vamos poder criar as nossas</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos adicionar ao grafo o nó <code>chatbot</code>. Os nós representam unidades de trabalho. Geralmente, são funções regulares de <code>Python</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos um nó com o método <code>add_node</code> que recebe o nome do nó e a função que será executada.</p>
</section>
<section class="section-block-markdown-cell">
<p>Então vamos criar um LLM com HuggingFace, depois criaremos um modelo de chat com <code>LangChain</code> que fará referência ao LLM criado. Uma vez definido o modelo de chat, definimos a função que será executada no nó do nosso grafo. Essa função fará uma chamada ao modelo de chat criado e retornará o resultado.</p>
</section>
<section class="section-block-markdown-cell">
<p>Por último, vamos a adicionar um nó com a função do chatbot ao gráfico</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>

<span class="c1"># The first argument is the unique node name</span>
<span class="c1"># The second argument is the function or object that will be called whenever</span>
<span class="c1"># the node is used.</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x130548440&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós usamos <a href="https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html#langchain_huggingface.chat_models.huggingface.ChatHuggingFace">ChatHuggingFace</a> que é um chat do tipo <a href="https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel">BaseChatModel</a> que é um tipo de chat base de <code>LangChain</code>. Uma vez criado o <code>BaseChatModel</code>, nós criamos a função <code>chatbot_function</code> que será executada quando o nó for executado. E por último, criamos o nó <code>chatbot_node</code> e indicamos que ele deve executar a função <code>chatbot_function</code>.</p>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>**Aviso**</p>
</blockquote>
<blockquote>
<p>> A função de nó <code>chatbot_function</code> recebe o estado <code>State</code> como entrada e retorna um dicionário que contém uma atualização da lista <code>messages</code> para a chave <code>mensagens</code>. Este é o padrão básico para todas as funções do nó <code>LangGraph</code>.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>A <code>função reducer</code> do nosso grafo <code>add_messages</code> adicionará as mensagens de resposta do <code>llm</code> a qualquer mensagem que já esteja no estado.</p>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, adicionamos um nó <code>entry</code>. Isso diz ao nosso grafo onde começar seu trabalho sempre que o executamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">START</span>
<span class="w"> </span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x130548440&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Da mesma forma, adicionamos um nó <code>finish</code>. Isso indica ao grafo cada vez que esse nó é executado, ele pode finalizar o trabalho.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">END</span>
<span class="w"> </span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x130548440&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Importamos <code>START</code> e <code>END</code>, que podem ser encontrados em <a href="https://langchain-ai.github.io/langgraph/reference/constants/">constants</a>, e são o primeiro e o último nó do nosso grafo.</p>
<p>Normalmente são nós virtuais</p>
</section>
<section class="section-block-markdown-cell">
<p>Finalmente, temos que compilar nosso grafo. Para fazer isso, usamos o método construtor de grafos <code>compile()</code>. Isso cria um <code>CompiledGraph</code> que podemos usar para executar nossa aplicação.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos visualizar o grafo usando o método <code>get_graph</code> e um dos métodos de "desenho", como <code>draw_ascii</code> ou <code>draw_mermaid_png</code>. O desenho de cada um dos métodos requerir dependências adicionais.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos testar o chatbot!</p>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>**Dica**</p>
</blockquote>
<blockquote>
<p>> No bloco de código seguinte, você pode sair do loop de bate-papo a qualquer momento digitando <code>quit</code>, <code>exit</code> ou <code>q</code>.</p>
</blockquote>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Colors for the terminal</span>
<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[32m&quot;</span>
<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[33m&quot;</span>
<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[0m&quot;</span>
<span class="w"> </span>

<span class="k">def</span><span class="w"> </span><span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]}):</span>
<span class="w">        </span><span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">value</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="w">    </span><span class="k">try</span><span class="p">:</span>
<span class="w">        </span><span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;User: &quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;quit&quot;</span><span class="p">,</span> <span class="s2">&quot;exit&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">]:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Goodbye!&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">        </span>
<span class="w">        </span><span class="n">events</span> <span class="o">=</span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">    </span><span class="k">except</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># fallback if input() is not available</span>
<span class="w">        </span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;What do you know about LangGraph?&quot;</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;User: &quot;</span> <span class="o">+</span> <span class="n">user_input</span><span class="p">)</span>
<span class="w">        </span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">        </span><span class="k">break</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>User: Hello
Assistant: Hello! It&#x27;s nice to meet you. How can I assist you today? Whether you have questions, need information, or just want to chat, I&#x27;m here to help!
User: How are you doing?
Assistant: I&#x27;m just a computer program, so I don&#x27;t have feelings, but I&#x27;m here and ready to help you with any questions or tasks you have! How can I assist you today?
User: Me well, I&#x27;m making a post about LangGraph, what do you think?
Assistant: LangGraph is an intriguing topic, especially if you&#x27;re delving into the realm of graph-based models and their applications in natural language processing (NLP). LangGraph, as I understand, is a framework or tool that leverages graph theory to improve or provide a new perspective on NLP tasks such as text classification, information extraction, and semantic analysis. By representing textual information as graphs (nodes for entities and edges for relationships), it can offer a more nuanced understanding of the context and semantics in language data.

If you&#x27;re making a post about it, here are a few points you might consider:

1. **Introduction to LangGraph**: Start with a brief explanation of what LangGraph is and its core principles. How does it model language or text differently compared to traditional NLP approaches? What unique advantages does it offer by using graph-based methods?

2. **Applications of LangGraph**: Discuss some of the key applications where LangGraph has been or can be applied. This could include improving the accuracy of sentiment analysis, enhancing machine translation, or optimizing chatbot responses to be more contextually aware.

3. **Technical Innovations**: Highlight any technical innovations or advancements that LangGraph brings to the table. This could be about new algorithms, more efficient data structures, or novel ways of training models on graph data.

4. **Challenges and Limitations**: It&#x27;s also important to address the challenges and limitations of using graph-based methods in NLP. Performance, scalability, and the current state of the technology can be discussed here.

5. **Future Prospects**: Wrap up with a look into the future of LangGraph and graph-based NLP in general. What are the upcoming trends, potential areas of growth, and how might these tools start impacting broader technology landscapes?

Each section can help frame your post in a way that&#x27;s informative and engaging for your audience, whether they&#x27;re technical experts or casual readers looking for an introduction to this intriguing area of NLP.
User: q
Assistant: Goodbye!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>**!Parabéns!** Você construiu seu primeiro chatbot usando <code>LangGraph</code>. Este bot pode participar de uma conversa básica, recebendo a entrada do usuário e gerando respostas utilizando o <code>LLM</code> que definimos.</p>
</section>
<section class="section-block-markdown-cell">
<p>Antes fomos escrevendo o código aos poucos e pode ser que não tenha ficado muito claro. Foi feito assim para explicar cada parte do código, mas agora vamos reescrevê-lo, mas organizado de outra forma, que fica mais claro à vista. Ou seja, agora que não precisamos explicar cada parte do código, o agrupamos de outra maneira para que seja mais claro.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># State</span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Start to build the graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes to the graph</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Display the graph</span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Mais">Mais<a class="anchor-link" href="#Mais">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Todos os blocos <code>mais</code> estão lá se você quiser aprofundar mais em <code>LangGraph</code>, se não, pode ler tudo sem ler os blocos <code>mais</code></p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Tipagem do estado">Tipagem do estado<a class="anchor-link" href="#Tipagem do estado">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Vimos como criar um agente com um estado tipado usando <code>TypedDict</code>, mas podemos criá-lo com outro tipo tipado.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Tipagem através de TypeDict">Tipagem através de <code>TypeDict</code><a class="anchor-link" href="#Tipagem através de TypeDict">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>É a forma que vimos anteriormente, tipamos o estado como um dicionário usando o tipado de Python <code>TypeDict</code>. Passamos uma chave e um valor para cada variável do nosso estado.</p>
</section>
<section class="section-block-markdown-cell">
<section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-python">from typing_extensions import TypedDict<br>from typing import Anotado<br>from langgraph.graph.message import adicionar_mensagens<br>from langgraph.graph import StateGraph<br><br>class Estado(TypedDict):<br>mensagens: Anotado[list, add_mensagens]</code></pre></div>
      </section>
</section>
<section class="section-block-markdown-cell">
<p>Para acessar as mensagens, fazemos isso como com qualquer dicionário, através de <code>state[&quot;messages&quot;]</code></p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Tipagem com dataclass">Tipagem com <code>dataclass</code><a class="anchor-link" href="#Tipagem com dataclass">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Outra opção é usar o tipado de python <code>dataclass</code></p>
</section>
<section class="section-block-markdown-cell">
<section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-python">from dataclasses import dataclass<br>from typing import Anotado<br>from langgraph.graph.message import add_messages<br>from langgraph.graph import StateGraph<br><br>@dataclass<br>class Estado:<br>mensagens: Anotado[list, adicionar_mensagens]</code></pre></div>
      </section>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, é semelhante ao tipagem por meio de dicionários, mas agora, sendo o estado uma classe, acessamos as mensagens através de <code>state.messages</code></p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Tipagem com Pydantic">Tipagem com <code>Pydantic</code><a class="anchor-link" href="#Tipagem com Pydantic">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p><code>Pydantic</code> é uma biblioteca muito usada para tipar dados em Python. Nos oferece a possibilidade de adicionar uma verificação do tipado. Vamos verificar que a mensagem comece com <code>&#x27;User&#x27;</code>, <code>&#x27;Assistant&#x27;</code> ou <code>&#x27;System&#x27;</code>.</p>
</section>
<section class="section-block-markdown-cell">
<section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-python">from pydantic import BaseModel, field_validator, ValidationError<br>from typing import Anotado<br>from langgraph.graph.message import adicionar_mensagens<br><br>class Estado(BaseModel):<br>mensagens: Anotado[list, add_messages] # Deve começar com &#39;Usuário&#39;, &#39;Assistente&#39; ou &#39;Sistema&#39;<br><br>@field_validator(&#39;mensagens&#39;)<br>@classmethod<br>def validate_messages(cls, value):<br># Garanta que as mensagens comecem com `User`, `Assistant` ou `System`<br>Se não value.startswith["&#39;User&#39;"] e não value.startswith["&#39;Assistant&#39;"] e não value.startswith["&#39;System&#39;"]:<br>raise ValueError("A mensagem deve começar com &#39;User&#39;, &#39;Assistant&#39; ou &#39;System&#39;")<br>valor de retorno<br><br>tente:<br>state = PydanticState(messages=["Olá"])<br>except ValidationError as e:<br>print("Erro de Validação:", e)</code></pre></div>
      </section>
</section>
<section class="section-block-markdown-cell">
<h4 id="Redutores">Redutores<a class="anchor-link" href="#Redutores">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Como dissemos, precisamos usar uma função do tipo <code>Reducer</code> para indicar como atualizar o estado, pois se não os valores do estado serão sobrescritos.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver um exemplo de um grafo no qual não usamos uma função do tipo <code>Reducer</code> para indicar como atualizar o estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">foo</span><span class="p">:</span> <span class="nb">int</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 1---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 2---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 3---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Build graph</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Logic</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;node_1&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_2&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_3&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># View</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, definimos um grafo no qual o nó 1 é executado primeiro e depois os nós 2 e 3. Vamos executá-lo para ver o que acontece.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.errors</span><span class="w"> </span><span class="kn">import</span> <span class="n">InvalidUpdateError</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
<span class="k">except</span> <span class="n">InvalidUpdateError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;InvalidUpdateError occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>---Node 1---
---Node 2---
---Node 3---
InvalidUpdateError occurred: At key &#x27;foo&#x27;: Can receive only one value per step. Use an Annotated key to handle multiple values.
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtemos um erro porque primeiro o nó 1 modifica o valor de <code>foo</code> e depois os nós 2 e 3 tentam modificar o valor de <code>foo</code> em paralelo, o que dá um erro.</p>
</section>
<section class="section-block-markdown-cell">
<p>Então, para evitar isso, usamos uma função do tipo <code>Reducer</code> para indicar como modificar o estado.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Redutores pre-definidos">Redutores pré-definidos<a class="anchor-link" href="#Redutores pre-definidos">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Usamos o tipo <code>Annotated</code> para especificar que é uma função do tipo <code>Reducer</code>. E usamos o operador <code>add</code> para adicionar um valor a uma lista.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">operator</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">foo</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 1---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 2---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 3---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Build graph</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Logic</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;node_1&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_2&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_3&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># View</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Executamos novamente para ver o que acontece</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>---Node 1---
---Node 2---
---Node 3---
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;foo&#x27;: [1, 2, 3, 3]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos inicializamos o valor de <code>foo</code> a 1, o qual se adiciona em uma lista. Depois o nó 1 soma 1 e o adiciona como novo valor na lista, ou seja, adiciona um 2. Por fim os nós 2 e 3 somam um ao último valor da lista, ou seja, os dois nós obtêm um 3 e ambos o adicionam no final da lista, por isso a lista resultante tem dois 3 no final</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver o caso de que uma branch tenha mais nós que outra</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">operator</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">foo</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 1---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_2_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 2_1---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_2_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 2_2---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 3---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Build graph</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_2_1&quot;</span><span class="p">,</span> <span class="n">node_2_1</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_2_2&quot;</span><span class="p">,</span> <span class="n">node_2_2</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Logic</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;node_1&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_2_1&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_3&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_2_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_2_2&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_2_2&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># View</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se agora executarmos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>---Node 1---
---Node 2_1---
---Node 3---
---Node 2_2---
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;foo&#x27;: [1, 2, 3, 3, 4]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O que aconteceu é que primeiro foi executado o nó 1, em seguida o nó 2_1, depois, em paralelo, os nós 2_2 e 3, e finalmente o nó <code>END</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Como definimos <code>foo</code> como uma lista de inteiros, e está tipada, se inicializarmos o estado com <code>None</code> obtemos um erro</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span> <span class="p">:</span> <span class="kc">None</span><span class="p">})</span>
<span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TypeError occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>TypeError occurred: can only concatenate list (not &quot;NoneType&quot;) to list
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver como arrumar isso com <code>reducidores personalizados</code></p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Redutores personalizados">Redutores personalizados<a class="anchor-link" href="#Redutores personalizados">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Às vezes não podemos usar um <code>Reducer</code> pré-definido e temos que criar o nosso próprio.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">reducer_function</span><span class="p">(</span><span class="n">current_list</span><span class="p">,</span> <span class="n">new_item</span><span class="p">:</span> <span class="nb">list</span> <span class="o">|</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="n">current_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="n">current_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">if</span> <span class="n">new_item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span> <span class="n">current_list</span> <span class="o">+</span> <span class="n">new_item</span>
<span class="w">    </span><span class="k">return</span> <span class="n">current_list</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">foo</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">reducer_function</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 1---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span> <span class="p">{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 2---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 3---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Build graph</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Logic</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;node_1&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_2&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_3&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_3&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># View</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se agora inicializarmos o grafo com um valor <code>None</code>, não recebemos mais um erro.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span> <span class="p">:</span> <span class="kc">None</span><span class="p">})</span>
<span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TypeError occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>---Node 1---
---Node 2---
---Node 3---
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Multiplos estados">Múltiplos estados<a class="anchor-link" href="#Multiplos estados">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<h5 id="Estados privados">Estados privados<a class="anchor-link" href="#Estados privados">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Suponhamos que queremos ocultar variáveis de estado, pela razão que seja, porque algumas variáveis só trazem ruído ou porque queremos manter alguma variável privada.</p>
</section>
<section class="section-block-markdown-cell">
<p>Se quisermos ter um estado privado, simplesmente o criamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">public_var</span><span class="p">:</span> <span class="nb">int</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">PrivateState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">private_var</span><span class="p">:</span> <span class="nb">int</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">PrivateState</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 1---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;private_var&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;public_var&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">PrivateState</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">OverallState</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---Node 2---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;public_var&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;private_var&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Build graph</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Logic</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;node_1&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;node_2&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;node_2&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># View</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, criamos o estado privado <code>PrivateState</code> e o estado público <code>OverallState</code>. Cada um com uma variável privada e uma pública. Primeiro é executado o nó 1, que modifica a variável privada e a retorna. Em seguida, é executado o nó 2, que modifica a variável pública e a retorna. Vamos executar o grafo para ver o que acontece.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;public_var&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>---Node 1---
---Node 2---
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;public_var&#x27;: 3&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos ao executar o grafo, passamos a variável pública <code>public_var</code> e obtemos na saída outra variável pública <code>public_var</code> com o valor modificado, mas nunca se acessou a variável privada <code>private_var</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Estados de entrada e saida">Estados de entrada e saída<a class="anchor-link" href="#Estados de entrada e saida">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Podemos definir as variáveis de entrada e saída do grafo. Embora internamente o estado possa ter mais variáveis, definimos quais variáveis são de entrada para o grafo e quais variáveis são de saída.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">InputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">OutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">notes</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">thinking_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">InputState</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;bye&quot;</span><span class="p">,</span> <span class="s2">&quot;notes&quot;</span><span class="p">:</span> <span class="s2">&quot;... his is name is Lance&quot;</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">answer_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">OutputState</span><span class="p">:</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;bye Lance&quot;</span><span class="p">}</span>
<span class="w"> </span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">InputState</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">OutputState</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;answer_node&quot;</span><span class="p">,</span> <span class="n">answer_node</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;thinking_node&quot;</span><span class="p">,</span> <span class="n">thinking_node</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;thinking_node&quot;</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;thinking_node&quot;</span><span class="p">,</span> <span class="s2">&quot;answer_node&quot;</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;answer_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># View</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Neste caso, o estado tem 3 variáveis: <code>question</code>, <code>answer</code> e <code>notes</code>. No entanto, definimos como entrada do grafo <code>question</code> e como saída do grafo <code>answer</code>. Portanto, o estado interno pode ter mais variáveis, mas elas não são consideradas na hora de invocar o grafo. Vamos executar o grafo para ver o que acontece.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span><span class="s2">&quot;hi&quot;</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;answer&#x27;: &#x27;bye Lance&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, adicionamos <code>question</code> ao grafo e obtivemos <code>answer</code> na saída.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Gerenciamento do contexto">Gerenciamento do contexto<a class="anchor-link" href="#Gerenciamento do contexto">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a revisar novamente o código do chatbot básico</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connect nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a criar um contexto que passaremos ao modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="w"> </span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;So you said you were researching ocean mammals?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">)]</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Yes, I know about whales. But what others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="w"> </span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, I know about whales. But what others should I learn about?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se passarmos para o grafo, obteremos a saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, I know about whales. But what others should I learn about?
================================== Ai Message ==================================

That&#x27;s a great topic! Besides whales, there are several other fascinating ocean mammals you might want to learn about. Here are a few:

1. **Dolphins**: Highly intelligent and social, dolphins are found in all oceans of the world. They are known for their playful behavior and communication skills.

2. **Porpoises**: Similar to dolphins but generally smaller and stouter, porpoises are less social and more elusive. They are found in coastal waters around the world.

3. **Seals and Sea Lions**: These are semi-aquatic mammals that can be found in both Arctic and Antarctic regions, as well as in more temperate waters. They are known for their sleek bodies and flippers, and they differ in their ability to walk on land (sea lions can &quot;walk&quot; on their flippers, while seals can only wriggle or slide).

4. **Walruses**: Known for their large tusks and whiskers, walruses are found in the Arctic. They are well-adapted to cold waters and have a thick layer of blubber to keep them warm.

5. **Manatees and Dugongs**: These gentle, herbivorous mammals are often called &quot;sea cows.&quot; They live in shallow, coastal areas and are found in tropical and subtropical regions. Manatees are found in the Americas, while dugongs are found in the Indo-Pacific region.

6. **Otters**: While not fully aquatic, sea otters spend most of their lives in the water and are excellent swimmers. They are known for their dense fur, which keeps them warm in cold waters.

7. **Polar Bears**: Although primarily considered land animals, polar bears are excellent swimmers and spend a significant amount of time in the water, especially when hunting for seals.

Each of these mammals has unique adaptations and behaviors that make them incredibly interesting to study. If you have any specific questions or topics you&#x27;d like to explore further, feel free to ask!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos agora na saída temos uma mensagem adicional.</p>
<p>Se isso continuar crescendo, chegará um momento em que teremos um contexto muito longo, o que representará um maior gasto de tokens, podendo resultar em um maior custo econômico e também maior latência.</p>
<p>Além disso, com contextos muito longos, os LLMs começam a performar pior.</p>
<p>Nos últimos modelos, à data da escrita deste post, acima de 8k tokens de contexto, começa a decrescer o desempenho do LLM</p>
</section>
<section class="section-block-markdown-cell">
<p>Então vamos ver várias maneiras de gerenciar isso</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Modificar o contexto com funções do tipo Reducer">Modificar o contexto com funções do tipo <code>Reducer</code><a class="anchor-link" href="#Modificar o contexto com funções do tipo Reducer">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Vimos que com as funções do tipo <code>Reducer</code> podemos modificar as mensagens do estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemoveMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">filter_messages</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Delete all but the 2 most recent messages</span>
<span class="w">    </span><span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">delete_messages</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>    
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;filter_messages_node&quot;</span><span class="p">,</span> <span class="n">filter_messages</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;filter_messages_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;filter_messages_node&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos no grafo, primeiro filtramos as mensagens e depois passamos o resultado ao modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a criar um contexto que passaremos ao modelo, mas agora com mais mensagens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="w"> </span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;So you said you were researching ocean mammals?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">)]</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Yes, I know about whales. But what others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;I know about sharks too&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;What others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;I know about dolphins too&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tell me more about dolphins&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="w"> </span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, I know about whales. But what others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about sharks too
================================ Human Message =================================
Name: Lance

What others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about dolphins too
================================ Human Message =================================
Name: Lance

Tell me more about dolphins
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se passarmos para o grafo, obteremos a saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

I know about dolphins too
================================ Human Message =================================
Name: Lance

Tell me more about dolphins
================================== Ai Message ==================================

Dolphins are highly intelligent marine mammals that are part of the family Delphinidae, which includes about 40 species. They are found in oceans worldwide, from tropical to temperate regions, and are known for their agility and playful behavior. Here are some interesting facts about dolphins:

1. **Social Behavior**: Dolphins are highly social animals and often live in groups called pods, which can range from a few individuals to several hundred. Social interactions are complex and include cooperative behaviors, such as hunting and defending against predators.

2. **Communication**: Dolphins communicate using a variety of sounds, including clicks, whistles, and body language. These sounds can be used for navigation (echolocation), communication, and social bonding. Each dolphin has a unique signature whistle that helps identify it to others in the pod.

3. **Intelligence**: Dolphins are considered one of the most intelligent animals on Earth. They have large brains and display behaviors such as problem-solving, mimicry, and even the use of tools. Some studies suggest that dolphins can recognize themselves in mirrors, indicating a level of self-awareness.

4. **Diet**: Dolphins are carnivores and primarily feed on fish and squid. They use echolocation to locate and catch their prey. Some species, like the bottlenose dolphin, have been observed using teamwork to herd fish into tight groups, making them easier to catch.

5. **Reproduction**: Dolphins typically give birth to a single calf after a gestation period of about 10 to 12 months. Calves are born tail-first and are immediately helped to the surface for their first breath by their mother or another dolphin. Calves nurse for up to two years and remain dependent on their mothers for a significant period.

6. **Conservation**: Many dolphin species are threatened by human activities such as pollution, overfishing, and habitat destruction. Some species, like the Indo-Pacific humpback dolphin and the Amazon river dolphin, are endangered. Conservation efforts are crucial to protect these animals and their habitats.

7. **Human Interaction**: Dolphins have a long history of interaction with humans, often appearing in mythology and literature. In some cultures, they are considered sacred or bring good luck. Today, dolphins are popular in marine parks and are often the focus of eco-tourism activities, such as dolphin-watching tours.

Dolphins continue to fascinate scientists and the general public alike, with ongoing research into their behavior, communication, and social structures providing new insights into these remarkable creatures.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se pode ver, a função de filtragem removeu todas as mensagens, exceto as duas últimas, e essas duas mensagens foram passadas como contexto para o LLM.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Cortar mensagens">Cortar mensagens<a class="anchor-link" href="#Cortar mensagens">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Outra solução é recortar cada mensagem da lista de mensagens que tenham muitos tokens, estabelece-se um limite de tokens e elimina-se a mensagem que ultrapassa esse limite.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">trim_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">trim_messages_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Trim the messages based on the specified parameters</span>
<span class="w">    </span><span class="n">trimmed_messages</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
<span class="w">        </span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>       <span class="c1"># Maximum tokens allowed in the trimmed list</span>
<span class="w">        </span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">,</span>     <span class="c1"># Keep the latest messages</span>
<span class="w">        </span><span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>   <span class="c1"># Use the LLM&#39;s tokenizer to count tokens</span>
<span class="w">        </span><span class="n">allow_partial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Allow cutting messages mid-way if needed</span>
<span class="w">    </span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Print the trimmed messages to see the effect of trim_messages</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- trimmed messages (input to LLM) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">:</span>
<span class="w">        </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the LLM with the trimmed messages</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">trimmed_messages</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;trim_messages_node&quot;</span><span class="p">,</span> <span class="n">trim_messages_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;trim_messages_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;trim_messages_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos no grafo, primeiro filtramos as mensagens e depois passamos o resultado ao modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a criar um contexto que passaremos ao modelo, mas agora com mais mensagens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="w"> </span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;So you said you were researching ocean mammals?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">)]</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Yes, I know about whales. But what others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;I know about sharks too. They are very dangerous, but they are also very beautiful.</span>
<span class="s2">Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.</span>
<span class="s2">They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.</span>
<span class="s2">Often they live in groups of 20 or more, but they are not very common.</span>
<span class="s2">They should eat a lot of food. Normally they eat a lot of fish.</span>
<span class="s2">The white shark is the largest of the sharks and is the most dangerous.</span>
<span class="s2">The great white shark is the most famous of the sharks and is the most dangerous.</span>
<span class="s2">The tiger shark is the most aggressive of the sharks and is the most dangerous.</span>
<span class="s2">The hammerhead shark is the most beautiful of the sharks and is the most dangerous.</span>
<span class="s2">The mako shark is the fastest of the sharks and is the most dangerous.</span>
<span class="s2">The bull shark is the most common of the sharks and is the most dangerous.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;What others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;I know about dolphins too&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tell me more about dolphins&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="w"> </span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, I know about whales. But what others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about sharks too. They are very dangerous, but they are also very beautiful.
Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.
They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.
Often they live in groups of 20 or more, but they are not very common.
They should eat a lot of food. Normally they eat a lot of fish.
The white shark is the largest of the sharks and is the most dangerous.
The great white shark is the most famous of the sharks and is the most dangerous.
The tiger shark is the most aggressive of the sharks and is the most dangerous.
The hammerhead shark is the most beautiful of the sharks and is the most dangerous.
The mako shark is the fastest of the sharks and is the most dangerous.
The bull shark is the most common of the sharks and is the most dangerous.
================================ Human Message =================================
Name: Lance

What others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about dolphins too
================================ Human Message =================================
Name: Lance

Tell me more about dolphins
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se passarmos ao grafo, obteremos a saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>--- trimmed messages (input to LLM) ---
================================== Ai Message ==================================
Name: Bot

The tiger shark is the most aggressive of the sharks and is the most dangerous.
The hammerhead shark is the most beautiful of the sharks and is the most dangerous.
The mako shark is the fastest of the sharks and is the most dangerous.
The bull shark is the most common of the sharks and is the most dangerous.
================================ Human Message =================================
Name: Lance

What others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about dolphins too
================================ Human Message =================================
Name: Lance

Tell me more about dolphins
------------------------------------------------
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, o contexto fornecido ao LLM foi truncado. A mensagem, que era muito longa e continha muitos tokens, foi reduzida. Vamos observar a saída do LLM.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, I know about whales. But what others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about sharks too. They are very dangerous, but they are also very beautiful.
Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.
They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.
Often they live in groups of 20 or more, but they are not very common.
They should eat a lot of food. Normally they eat a lot of fish.
The white shark is the largest of the sharks and is the most dangerous.
The great white shark is the most famous of the sharks and is the most dangerous.
The tiger shark is the most aggressive of the sharks and is the most dangerous.
The hammerhead shark is the most beautiful of the sharks and is the most dangerous.
The mako shark is the fastest of the sharks and is the most dangerous.
The bull shark is the most common of the sharks and is the most dangerous.
================================ Human Message =================================
Name: Lance

What others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about dolphins too
================================ Human Message =================================
Name: Lance

Tell me more about dolphins
================================== Ai Message ==================================

Certainly! Dolphins are intelligent marine mammals that are part of the family Delphinidae, which includes nearly 40 species. Here are some interesting facts about dolphins:

1. **Intelligence**: Dolphins are known for their high intelligence and have large brains relative to their body size. They exhibit behaviors that suggest social complexity, self-awareness, and problem-solving skills. For example, they can recognize themselves in mirrors, a trait shared by only a few other species.

2. **Communication**: Dolphins communicate using a variety of clicks, whistles, and body language. Each dolphin has a unique &quot;signature whistle&quot; that helps identify it to others, similar to a human name. They use echolocation to navigate and locate prey by emitting clicks and interpreting the echoes that bounce back.

3. **Social Structure**: Dolphins are highly social animals and often live in groups called pods. These pods can vary in size from a few individuals to several hundred. Within these groups, dolphins form complex social relationships and often cooperate to hunt and protect each other from predators.

4. **Habitat**: Dolphins are found in all the world&#x27;s oceans and in some rivers. Different species have adapted to various environments, from tropical waters to the cooler regions of the open sea. Some species, like the Amazon river dolphin (also known as the boto), live in freshwater rivers.

5. **Diet**: Dolphins are carnivores and primarily eat fish, squid, and crustaceans. Their diet can vary depending on the species and their habitat. Some species, like the killer whale (which is actually a large dolphin), can even hunt larger marine mammals.

6. **Reproduction**: Dolphins have a long gestation period, typically around 10 to 12 months. Calves are born tail-first and are nursed by their mothers for up to two years. Dolphins often form strong bonds with their offspring and other members of their pod.

7. **Conservation**: Many species of dolphins face threats such as pollution, overfishing, and entanglement in fishing nets. Conservation efforts are ongoing to protect these animals and their habitats. Organizations like the International Union for Conservation of Nature (IUCN) and the World Wildlife Fund (WWF) work to raise awareness and implement conservation measures.

8. **Cultural Significance**: Dolphins have been a source of fascination and inspiration for humans for centuries. They appear in myths, legends, and art across many cultures and are often seen as symbols of intelligence, playfulness, and freedom.

Dolphins are truly remarkable creatures with a lot to teach us about social behavior, communication, and the complexities of marine ecosystems. If you have any specific questions or want to know more about a particular species, feel free to ask!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Com um contexto truncado, o LLM continua respondendo</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Modificacao do contexto e corte de mensagens">Modificação do contexto e corte de mensagens<a class="anchor-link" href="#Modificacao do contexto e corte de mensagens">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a juntar as duas técnicas anteriores, modificaremos o contexto e recortaremos os mensagens.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemoveMessage</span><span class="p">,</span> <span class="n">trim_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">filter_messages</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Delete all but the 2 most recent messages</span>
<span class="w">    </span><span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">delete_messages</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">trim_messages_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># print the messages</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- messages (input to trim_messages) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Trim the messages based on the specified parameters</span>
<span class="w">    </span><span class="n">trimmed_messages</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
<span class="w">        </span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>       <span class="c1"># Maximum tokens allowed in the trimmed list</span>
<span class="w">        </span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">,</span>     <span class="c1"># Keep the latest messages</span>
<span class="w">        </span><span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>   <span class="c1"># Use the LLM&#39;s tokenizer to count tokens</span>
<span class="w">        </span><span class="n">allow_partial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Allow cutting messages mid-way if needed</span>
<span class="w">    </span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Print the trimmed messages to see the effect of trim_messages</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- trimmed messages (input to LLM) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">:</span>
<span class="w">        </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the LLM with the trimmed messages</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">trimmed_messages</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>    
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;filter_messages_node&quot;</span><span class="p">,</span> <span class="n">filter_messages</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;trim_messages_node&quot;</span><span class="p">,</span> <span class="n">trim_messages_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;filter_messages_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;filter_messages_node&quot;</span><span class="p">,</span> <span class="s2">&quot;trim_messages_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;trim_messages_node&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora filtramos ficando com as duas últimas mensagens, depois trimamos o contexto para que não gaste muitos tokens e, finalmente, passamos o resultado ao modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um contexto para passá-lo ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="w"> </span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;So you said you were researching ocean mammals?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">)]</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Yes, I know about whales. But what others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;I know about dolphins too&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;What others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;I know about sharks too. They are very dangerous, but they are also very beautiful.</span>
<span class="s2">Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.</span>
<span class="s2">They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.</span>
<span class="s2">Often they live in groups of 20 or more, but they are not very common.</span>
<span class="s2">They should eat a lot of food. Normally they eat a lot of fish.</span>
<span class="s2">The white shark is the largest of the sharks and is the most dangerous.</span>
<span class="s2">The great white shark is the most famous of the sharks and is the most dangerous.</span>
<span class="s2">The tiger shark is the most aggressive of the sharks and is the most dangerous.</span>
<span class="s2">The hammerhead shark is the most beautiful of the sharks and is the most dangerous.</span>
<span class="s2">The mako shark is the fastest of the sharks and is the most dangerous.</span>
<span class="s2">The bull shark is the most common of the sharks and is the most dangerous.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bot&quot;</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;What others should I learn about?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lance&quot;</span><span class="p">))</span>
<span class="w"> </span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, I know about whales. But what others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about dolphins too
================================ Human Message =================================
Name: Lance

What others should I learn about?
================================== Ai Message ==================================
Name: Bot

I know about sharks too. They are very dangerous, but they are also very beautiful.
Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.
They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.
Often they live in groups of 20 or more, but they are not very common.
They should eat a lot of food. Normally they eat a lot of fish.
The white shark is the largest of the sharks and is the most dangerous.
The great white shark is the most famous of the sharks and is the most dangerous.
The tiger shark is the most aggressive of the sharks and is the most dangerous.
The hammerhead shark is the most beautiful of the sharks and is the most dangerous.
The mako shark is the fastest of the sharks and is the most dangerous.
The bull shark is the most common of the sharks and is the most dangerous.
================================ Human Message =================================
Name: Lance

What others should I learn about?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Passamos para o grafo e obtemos a saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>--- messages (input to trim_messages) ---
================================== Ai Message ==================================
Name: Bot

I know about sharks too. They are very dangerous, but they are also very beautiful.
Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.
They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.
Often they live in groups of 20 or more, but they are not very common.
They should eat a lot of food. Normally they eat a lot of fish.
The white shark is the largest of the sharks and is the most dangerous.
The great white shark is the most famous of the sharks and is the most dangerous.
The tiger shark is the most aggressive of the sharks and is the most dangerous.
The hammerhead shark is the most beautiful of the sharks and is the most dangerous.
The mako shark is the fastest of the sharks and is the most dangerous.
The bull shark is the most common of the sharks and is the most dangerous.
================================ Human Message =================================
Name: Lance

What others should I learn about?
------------------------------------------------
--- trimmed messages (input to LLM) ---
================================ Human Message =================================
Name: Lance

What others should I learn about?
------------------------------------------------
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, ficamos apenas com a última mensagem, isso ocorreu porque a função de filtro retornou as duas últimas mensagens, mas a função de trim removceu a penúltima mensagem por ter mais de 100 tokens.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver o que temos na saída do modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Name: Bot

I know about sharks too. They are very dangerous, but they are also very beautiful.
Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.
They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.
Often they live in groups of 20 or more, but they are not very common.
They should eat a lot of food. Normally they eat a lot of fish.
The white shark is the largest of the sharks and is the most dangerous.
The great white shark is the most famous of the sharks and is the most dangerous.
The tiger shark is the most aggressive of the sharks and is the most dangerous.
The hammerhead shark is the most beautiful of the sharks and is the most dangerous.
The mako shark is the fastest of the sharks and is the most dangerous.
The bull shark is the most common of the sharks and is the most dangerous.
================================ Human Message =================================
Name: Lance

What others should I learn about?
================================== Ai Message ==================================

Certainly! To provide a more tailored response, it would be helpful to know what areas or topics you&#x27;re interested in. However, here’s a general list of areas that are often considered valuable for personal and professional development:

1. **Technology &amp;amp; Digital Skills**: 
&#x20;&#x20;&#x20;- Programming languages (Python, JavaScript, etc.)
&#x20;&#x20;&#x20;- Web development (HTML, CSS, React, etc.)
&#x20;&#x20;&#x20;- Data analysis and visualization (SQL, Tableau, Power BI)
&#x20;&#x20;&#x20;- Machine learning and AI
&#x20;&#x20;&#x20;- Cloud computing (AWS, Azure, Google Cloud)

2. **Business &amp;amp; Entrepreneurship**:
&#x20;&#x20;&#x20;- Marketing (digital marketing, SEO, content marketing)
&#x20;&#x20;&#x20;- Project management
&#x20;&#x20;&#x20;- Financial literacy
&#x20;&#x20;&#x20;- Leadership and management
&#x20;&#x20;&#x20;-Startup and venture capital

3. **Science &amp;amp; Engineering**:
&#x20;&#x20;&#x20;- Biology and genetics
&#x20;&#x20;&#x20;- Physics and materials science
&#x20;&#x20;&#x20;- Environmental science and sustainability
&#x20;&#x20;&#x20;- Robotics and automation
&#x20;&#x20;&#x20;- Aerospace engineering

4. **Health &amp;amp; Wellness**:
&#x20;&#x20;&#x20;- Nutrition and dietetics
&#x20;&#x20;&#x20;- Mental health and psychology
&#x20;&#x20;&#x20;- Exercise science
&#x20;&#x20;&#x20;- Yoga and mindfulness
&#x20;&#x20;&#x20;- Traditional and alternative medicine

5. **Arts &amp;amp; Humanities**:
&#x20;&#x20;&#x20;- Creative writing and storytelling
&#x20;&#x20;&#x20;- Music and sound production
&#x20;&#x20;&#x20;- Visual arts and design (graphic design, photography)
&#x20;&#x20;&#x20;- Philosophy and ethics
&#x20;&#x20;&#x20;- History and cultural studies

6. **Communication &amp;amp; Languages**:
&#x20;&#x20;&#x20;- Public speaking and presentation skills
&#x20;&#x20;&#x20;- Conflict resolution and negotiation
&#x20;&#x20;&#x20;- Learning a new language (Spanish, Mandarin, French, etc.)
&#x20;&#x20;&#x20;- Writing and editing

7. **Personal Development**:
&#x20;&#x20;&#x20;- Time management and productivity
&#x20;&#x20;&#x20;- Mindfulness and stress management
&#x20;&#x20;&#x20;- Goal setting and motivation
&#x20;&#x20;&#x20;- Personal finance and budgeting
&#x20;&#x20;&#x20;- Critical thinking and problem solving

8. **Social &amp;amp; Environmental Impact**:
&#x20;&#x20;&#x20;- Social entrepreneurship
&#x20;&#x20;&#x20;- Community organizing and activism
&#x20;&#x20;&#x20;- Sustainable living practices
&#x20;&#x20;&#x20;- Climate change and environmental policy

If you have a specific area of interest or a particular goal in mind, feel free to share, and I can provide more detailed recommendations!
================================== Ai Message ==================================
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Filtramos tanto o estado que o LLM não tem contexto suficiente, mais tarde veremos uma maneira de resolver isso adicionando ao estado um resumo da conversação.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Modos de transmissao">Modos de transmissão<a class="anchor-link" href="#Modos de transmissao">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<h5 id="Streaming sincrono">Streaming síncrono<a class="anchor-link" href="#Streaming sincrono">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Neste caso, vamos receber o resultado do LLM completo assim que ele terminar de gerar o texto.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para explicar os modos de transmissão síncrona, primeiro vamos criar um grafo básico.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora temos duas maneiras de obter o resultado do LLM, uma é através do modo <code>updates</code> e a outra através do modo <code>values</code>.</p>
<img src="https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png" alt="updates vs values">
<p>Enquanto <code>updates</code> nos dá cada novo resultado, <code>values</code> nos dá todo o histórico de resultados.</p>
</section>
<section class="section-block-markdown-cell">
<h6 id="Atualizacoes">Atualizações<a class="anchor-link" href="#Atualizacoes">¶</a></h6>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi! I&#39;m Máximo&quot;</span><span class="p">)]},</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h6 id="Valores">Valores<a class="anchor-link" href="#Valores">¶</a></h6>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi! I&#39;m Máximo&quot;</span><span class="p">)]},</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>hi! I&#x27;m Máximo
Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h5 id="Streaming assincrono">Streaming assíncrono<a class="anchor-link" href="#Streaming assincrono">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos receber o resultado do LLM token a token. Para isso, temos que adicionar <code>streaming=True</code> quando criamos o LLM da HuggingFace e temos que alterar a função do nó do chatbot para que seja assíncrona.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="w">    </span><span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">astream_log</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]):</span>
<span class="w">        </span><span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">token</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, a função foi criada assíncrona e convertida em um gerador, pois o <code>yield</code> retorna um valor e pausa a execução da função até que seja chamada novamente.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a executar o grafo de forma assíncrona e ver os tipos de eventos que são gerados.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream_events</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi! I&#39;m Máximo&quot;</span><span class="p">)]},</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;v2&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;event: </span><span class="si">{</span><span class="n">event</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_start&#x27;, &#x27;data&#x27;: &#x7B;&#x27;input&#x27;: &#x7B;&#x27;messages&#x27;: [HumanMessage(content=&quot;hi! I&#x27;m Máximo&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)]&#x7D;&#x7D;, &#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;tags&#x27;: [], &#x27;run_id&#x27;: &#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;metadata&#x27;: &#x7B;&#x7D;, &#x27;parent_ids&#x27;: []&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_start&#x27;, &#x27;data&#x27;: &#x7B;&#x27;input&#x27;: &#x7B;&#x27;messages&#x27;: [HumanMessage(content=&quot;hi! I&#x27;m Máximo&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;6469501c-07b0-42e4-a3e6-f133ace1860c&#x27;)]&#x7D;&#x7D;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;graph:step:1&#x27;], &#x27;run_id&#x27;: &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_start&#x27;, &#x27;data&#x27;: &#x7B;&#x7D;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;seq:step:1&#x27;], &#x27;run_id&#x27;: &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;, &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_start&#x27;, &#x27;data&#x27;: &#x7B;&#x27;input&#x27;: &#x7B;&#x27;messages&#x27;: [[HumanMessage(content=&quot;hi! I&#x27;m Máximo&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;6469501c-07b0-42e4-a3e6-f133ace1860c&#x27;)]]&#x7D;&#x7D;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_stream&#x27;, &#x27;run_id&#x27;: &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;seq:step:1&#x27;], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: &#x7B;&#x27;messages&#x27;: [RunLogPatch(&#x7B;&#x27;op&#x27;: &#x27;replace&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;&#x27;,
&#x20;&#x20;&#x27;value&#x27;: &#x7B;&#x27;final_output&#x27;: None,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x27;id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x27;logs&#x27;: &#x7B;&#x7D;,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x27;streamed_output&#x27;: [],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x27;type&#x27;: &#x27;llm&#x27;&#x7D;&#x7D;)]&#x7D;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;Hello&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; Má&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;ximo&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;!&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; It&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&quot;&#x27;s&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; nice&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; to&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; meet&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; you&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;.&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; How&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; can&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; I&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; assist&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; you&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; today&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;?&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; Feel&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; free&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; to&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; ask&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; me&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; any&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; questions&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/Users/macm1/miniforge3/envs/langgraph/lib/python3.13/site-packages/huggingface_hub/inference/_generated/_async_client.py:2308: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version &#x27;0.28.0&#x27;. Use `stop` instead.
&#x20;&#x20;warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; or&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; let&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; me&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; know&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; if&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; you&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; need&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; help&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; with&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; anything&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27; specific&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;.&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_stream&#x27;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: AIMessageChunk(content=&#x27;&amp;lt;|im_end|&amp;gt;&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;)&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chat_model_end&#x27;, &#x27;data&#x27;: &#x7B;&#x27;output&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;), &#x27;input&#x27;: &#x7B;&#x27;messages&#x27;: [[HumanMessage(content=&quot;hi! I&#x27;m Máximo&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;6469501c-07b0-42e4-a3e6-f133ace1860c&#x27;)]]&#x7D;&#x7D;, &#x27;run_id&#x27;: &#x27;74dfdbb9-4c2d-4a08-ad7d-795b5953cae3&#x27;, &#x27;name&#x27;: &#x27;ChatHuggingFace&#x27;, &#x27;tags&#x27;: [], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;ls_provider&#x27;: &#x27;huggingface&#x27;, &#x27;ls_model_type&#x27;: &#x27;chat&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_stream&#x27;, &#x27;run_id&#x27;: &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;seq:step:1&#x27;], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: &#x7B;&#x27;messages&#x27;: [RunLogPatch(&#x7B;&#x27;op&#x27;: &#x27;add&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/streamed_output/-&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;,
 &#x7B;&#x27;op&#x27;: &#x27;replace&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/final_output&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;)]&#x7D;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_end&#x27;, &#x27;data&#x27;: &#x7B;&#x27;output&#x27;: &#x7B;&#x27;messages&#x27;: [RunLogPatch(&#x7B;&#x27;op&#x27;: &#x27;add&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/streamed_output/-&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;,
 &#x7B;&#x27;op&#x27;: &#x27;replace&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/final_output&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;)]&#x7D;, &#x27;input&#x27;: &#x7B;&#x27;messages&#x27;: [HumanMessage(content=&quot;hi! I&#x27;m Máximo&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;6469501c-07b0-42e4-a3e6-f133ace1860c&#x27;)]&#x7D;&#x7D;, &#x27;run_id&#x27;: &#x27;15247b1a-1cd6-4863-9402-66499f921244&#x27;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;seq:step:1&#x27;], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;, &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_stream&#x27;, &#x27;run_id&#x27;: &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;graph:step:1&#x27;], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;data&#x27;: &#x7B;&#x27;chunk&#x27;: &#x7B;&#x27;messages&#x27;: [RunLogPatch(&#x7B;&#x27;op&#x27;: &#x27;add&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/streamed_output/-&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;,
 &#x7B;&#x27;op&#x27;: &#x27;replace&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/final_output&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;)]&#x7D;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;]&#x7D;
event: &#x7B;&#x27;event&#x27;: &#x27;on_chain_end&#x27;, &#x27;data&#x27;: &#x7B;&#x27;output&#x27;: &#x7B;&#x27;messages&#x27;: [RunLogPatch(&#x7B;&#x27;op&#x27;: &#x27;add&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/streamed_output/-&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;,
 &#x7B;&#x27;op&#x27;: &#x27;replace&#x27;,
&#x20;&#x20;&#x27;path&#x27;: &#x27;/final_output&#x27;,
&#x20;&#x20;&#x27;value&#x27;: AIMessage(content=&quot;Hello Máximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&amp;lt;|im_end|&amp;gt;&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0&#x27;)&#x7D;)]&#x7D;, &#x27;input&#x27;: &#x7B;&#x27;messages&#x27;: [HumanMessage(content=&quot;hi! I&#x27;m Máximo&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;6469501c-07b0-42e4-a3e6-f133ace1860c&#x27;)]&#x7D;&#x7D;, &#x27;run_id&#x27;: &#x27;638828c0-4add-4141-b6b6-484446100237&#x27;, &#x27;name&#x27;: &#x27;chatbot_node&#x27;, &#x27;tags&#x27;: [&#x27;graph:step:1&#x27;], &#x27;metadata&#x27;: &#x7B;&#x27;langgraph_step&#x27;: 1, &#x27;langgraph_node&#x27;: &#x27;chatbot_node&#x27;, &#x27;langgraph_triggers&#x27;: (&#x27;branch:to:chatbot_node&#x27;,), &#x27;langgraph_path&#x27;: (&#x27;__pregel_pull&#x27;, &#x27;chatbot_node&#x27;), &#x27;langgraph_checkpoint_ns&#x27;: &#x27;chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd&#x27;&#x7D;, &#x27;parent_ids&#x27;: [&#x27;c9c40a00-157a-4229-a0d1-fda00e7bfd34&#x27;]&#x7D;
Error: Unsupported message type: &amp;lt;class &#x27;langchain_core.tracers.log_stream.RunLogPatch&#x27;&amp;gt;
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se pode ver, os tokens chegam com o evento <code>on_chat_model_stream</code>, então vamos capturá-lo e imprimi-lo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream_events</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi! I&#39;m Máximo&quot;</span><span class="p">)]},</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;v2&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="k">if</span> <span class="n">event</span><span class="p">[</span><span class="s2">&quot;event&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;on_chat_model_stream&quot;</span><span class="p">:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="s2">&quot;chunk&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; | &quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="k">pass</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/Users/macm1/miniforge3/envs/langgraph/lib/python3.13/site-packages/huggingface_hub/inference/_generated/_async_client.py:2308: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version &#x27;0.28.0&#x27;. Use `stop` instead.
&#x20;&#x20;warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello |  Má | ximo | ! |  It | &#x27;s |  nice |  to |  meet |  you | . |  How |  can |  I |  assist |  you |  today | ? |  Feel |  free |  to |  ask |  me |  any |  questions |  or |  let |  me |  know |  if |  you |  need |  help |  with |  anything |  specific | . | &amp;lt;|im_end|&amp;gt; |
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Subgrafos">Subgrafos<a class="anchor-link" href="#Subgrafos">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Antes vimos como bifurcar um grafo de forma que os nós sejam executados em paralelo, mas suponha o caso de que agora o que queremos é que o que seja executado em paralelo sejam subgrafos. Então vamos ver como fazer isso.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como fazer um grafo de gestão de logs que vai ter um subgrafo de resumo de logs e outro subgrafo de análise de erros nos logs.</p>
<img src="https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb1abf89f2d847ee6f1ff_sub-graph1.png" alt="grafo com subgrafos">
<p>Então, o que vamos fazer é primeiro definir cada um dos subgráficos separadamente e depois adicioná-los ao grafo principal.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Subgrafico de analise de erros em logs">Subgráfico de análise de erros em logs<a class="anchor-link" href="#Subgrafico de analise de erros em logs">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Importamos as bibliotecas necessárias</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">operator</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Annotated</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma classe com a estrutura dos logs</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The structure of the logs</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Log</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="nb">id</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">docs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span>
<span class="w">    </span><span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">grade</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="n">grader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="n">feedback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos agora duas classes, uma com a estrutura dos erros dos logs e outra com a análise que relatará na saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Failure Analysis Sub-graph</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FailureAnalysisState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">failures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">FailureAnalysisOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos as funções dos nós, uma obterá os erros nos logs, para isso buscará os logs que tenham algum valor no campo <code>grade</code>. Outra gerará um resumo dos erros. Além disso, vamos adicionar <code>print</code>s para poder ver o que está acontecendo internamente.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_failures</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Get logs that contain a failure &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug get_failures: cleaned_logs: </span><span class="si">{</span><span class="n">cleaned_logs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">failures</span> <span class="o">=</span> <span class="p">[</span><span class="n">log</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span> <span class="k">if</span> <span class="s2">&quot;grade&quot;</span> <span class="ow">in</span> <span class="n">log</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug get_failures: failures: </span><span class="si">{</span><span class="n">failures</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;failures&quot;</span><span class="p">:</span> <span class="n">failures</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Generate summary of failures &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">failures</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;failures&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug generate_summary: failures: </span><span class="si">{</span><span class="n">failures</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">fa_summary</span> <span class="o">=</span> <span class="s2">&quot;Poor quality retrieval of documentation.&quot;</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug generate_summary: fa_summary: </span><span class="si">{</span><span class="n">fa_summary</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;failure-analysis-on-log-</span><span class="si">{</span><span class="n">failure</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">failure</span> <span class="ow">in</span> <span class="n">failures</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug generate_summary: processed_logs: </span><span class="si">{</span><span class="n">processed_logs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;fa_summary&quot;</span><span class="p">:</span> <span class="n">fa_summary</span><span class="p">,</span> <span class="s2">&quot;processed_logs&quot;</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Por último, criamos o grafo, adicionamos os nós e os <code>edges</code> e o compilamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">fa_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">FailureAnalysisState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">FailureAnalysisOutputState</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;get_failures&quot;</span><span class="p">,</span> <span class="n">get_failures</span><span class="p">)</span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;get_failures&quot;</span><span class="p">)</span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;get_failures&quot;</span><span class="p">,</span> <span class="s2">&quot;generate_summary&quot;</span><span class="p">)</span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a criar um log de teste</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">failure_log</span> <span class="o">=</span> <span class="p">{</span>
<span class="w">    </span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> 
<span class="w">    </span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span> 
<span class="w">    </span><span class="s2">&quot;docs&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> 
<span class="w">    </span><span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;42&quot;</span><span class="p">,</span> 
<span class="w">    </span><span class="s2">&quot;grade&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
<span class="w">    </span><span class="s2">&quot;grader&quot;</span><span class="p">:</span> <span class="s2">&quot;AI&quot;</span><span class="p">,</span> 
<span class="w">    </span><span class="s2">&quot;feedback&quot;</span><span class="p">:</span> <span class="s2">&quot;Good job!&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Executamos o grafo com o log de teste. Como a função <code>get_failures</code> pega a chave <code>cleaned_logs</code> do estado, temos que passar o log para o grafo na mesma chave.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>	 debug get_failures: cleaned_logs: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;, &#x27;question&#x27;: &#x27;What is the meaning of life?&#x27;, &#x27;docs&#x27;: None, &#x27;answer&#x27;: &#x27;42&#x27;, &#x27;grade&#x27;: 1, &#x27;grader&#x27;: &#x27;AI&#x27;, &#x27;feedback&#x27;: &#x27;Good job!&#x27;&#x7D;]
	 debug get_failures: failures: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;, &#x27;question&#x27;: &#x27;What is the meaning of life?&#x27;, &#x27;docs&#x27;: None, &#x27;answer&#x27;: &#x27;42&#x27;, &#x27;grade&#x27;: 1, &#x27;grader&#x27;: &#x27;AI&#x27;, &#x27;feedback&#x27;: &#x27;Good job!&#x27;&#x7D;]
	 debug generate_summary: failures: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;, &#x27;question&#x27;: &#x27;What is the meaning of life?&#x27;, &#x27;docs&#x27;: None, &#x27;answer&#x27;: &#x27;42&#x27;, &#x27;grade&#x27;: 1, &#x27;grader&#x27;: &#x27;AI&#x27;, &#x27;feedback&#x27;: &#x27;Good job!&#x27;&#x7D;]
	 debug generate_summary: fa_summary: Poor quality retrieval of documentation.
	 debug generate_summary: processed_logs: [&#x27;failure-analysis-on-log-1&#x27;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;fa_summary&#x27;: &#x27;Poor quality retrieval of documentation.&#x27;,
 &#x27;processed_logs&#x27;: [&#x27;failure-analysis-on-log-1&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Pode-se ver que ele encontrou o log de teste, pois tem um valor de <code>1</code> no campo <code>grade</code> e, em seguida, gerou um resumo dos erros.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a definir todo o subgráfico juntos novamente para ficar mais claro e também para remover os <code>print</code>s que colocamos para depuração.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">operator</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="c1"># The structure of the logs</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Log</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="nb">id</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">docs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span>
<span class="w">    </span><span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">grade</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="n">grader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="n">feedback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Failure clases</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FailureAnalysisState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">failures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">FailureAnalysisOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_failures</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Get logs that contain a failure &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="n">failures</span> <span class="o">=</span> <span class="p">[</span><span class="n">log</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span> <span class="k">if</span> <span class="s2">&quot;grade&quot;</span> <span class="ow">in</span> <span class="n">log</span><span class="p">]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;failures&quot;</span><span class="p">:</span> <span class="n">failures</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Generate summary of failures &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">failures</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;failures&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="n">fa_summary</span> <span class="o">=</span> <span class="s2">&quot;Poor quality retrieval of documentation.&quot;</span>
<span class="w">    </span><span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;failure-analysis-on-log-</span><span class="si">{</span><span class="n">failure</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">failure</span> <span class="ow">in</span> <span class="n">failures</span><span class="p">]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;fa_summary&quot;</span><span class="p">:</span> <span class="n">fa_summary</span><span class="p">,</span> <span class="s2">&quot;processed_logs&quot;</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Build the graph</span>
<span class="n">fa_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">FailureAnalysisState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">FailureAnalysisOutputState</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;get_failures&quot;</span><span class="p">,</span> <span class="n">get_failures</span><span class="p">)</span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;get_failures&quot;</span><span class="p">)</span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;get_failures&quot;</span><span class="p">,</span> <span class="s2">&quot;generate_summary&quot;</span><span class="p">)</span>
<span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se nós o executarmos novamente, obteremos o mesmo resultado, mas sem os <code>print</code>s.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;fa_summary&#x27;: &#x27;Poor quality retrieval of documentation.&#x27;,
 &#x27;processed_logs&#x27;: [&#x27;failure-analysis-on-log-1&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h5 id="Subgrafo de resumo de logs">Subgrafo de resumo de logs<a class="anchor-link" href="#Subgrafo de resumo de logs">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos o subgrafo de resumo de logs. Neste caso, não é necessário recriar a classe com a estrutura dos logs, então criamos as classes com a estrutura para os resumos dos logs e com a estrutura da saída.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Summarization subgraph</span>
<span class="k">class</span><span class="w"> </span><span class="nc">QuestionSummarizationState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">qs_summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">report</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">QuestionSummarizationOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">report</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora definimos as funções dos nós, uma gerará o resumo dos logs e outra "enviará o resumo para o Slack".</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug generate_summary: cleaned_logs: </span><span class="si">{</span><span class="n">cleaned_logs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">summary</span> <span class="o">=</span> <span class="s2">&quot;Questions focused on ...&quot;</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug generate_summary: summary: </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;summary-on-log-</span><span class="si">{</span><span class="n">log</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug generate_summary: processed_logs: </span><span class="si">{</span><span class="n">processed_logs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;qs_summary&quot;</span><span class="p">:</span> <span class="n">summary</span><span class="p">,</span> <span class="s2">&quot;processed_logs&quot;</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">send_to_slack</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="n">qs_summary</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;qs_summary&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug send_to_slack: qs_summary: </span><span class="si">{</span><span class="n">qs_summary</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">report</span> <span class="o">=</span> <span class="s2">&quot;foo bar baz&quot;</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> debug send_to_slack: report: </span><span class="si">{</span><span class="n">report</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;report&quot;</span><span class="p">:</span> <span class="n">report</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Por último, criamos o grafo, adicionamos os nós e as <code>edges</code> e o compilamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Build the graph</span>
<span class="n">qs_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">QuestionSummarizationState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">QuestionSummarizationOutputState</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;send_to_slack&quot;</span><span class="p">,</span> <span class="n">send_to_slack</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;generate_summary&quot;</span><span class="p">)</span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="s2">&quot;send_to_slack&quot;</span><span class="p">)</span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;send_to_slack&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a testar com o log que criamos anteriormente.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>	 debug generate_summary: cleaned_logs: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;, &#x27;question&#x27;: &#x27;What is the meaning of life?&#x27;, &#x27;docs&#x27;: None, &#x27;answer&#x27;: &#x27;42&#x27;, &#x27;grade&#x27;: 1, &#x27;grader&#x27;: &#x27;AI&#x27;, &#x27;feedback&#x27;: &#x27;Good job!&#x27;&#x7D;]
	 debug generate_summary: summary: Questions focused on ...
	 debug generate_summary: processed_logs: [&#x27;summary-on-log-1&#x27;]
	 debug send_to_slack: qs_summary: Questions focused on ...
	 debug send_to_slack: report: foo bar baz
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;report&#x27;: &#x27;foo bar baz&#x27;, &#x27;processed_logs&#x27;: [&#x27;summary-on-log-1&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Reescrevemos o subgrafo, tudo junto para ver com maior clareza e sem os <code>print</code>s.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Summarization clases</span>
<span class="k">class</span><span class="w"> </span><span class="nc">QuestionSummarizationState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">qs_summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">report</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">QuestionSummarizationOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">report</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="n">summary</span> <span class="o">=</span> <span class="s2">&quot;Questions focused on ...&quot;</span>
<span class="w">    </span><span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;summary-on-log-</span><span class="si">{</span><span class="n">log</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span><span class="p">]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;qs_summary&quot;</span><span class="p">:</span> <span class="n">summary</span><span class="p">,</span> <span class="s2">&quot;processed_logs&quot;</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">send_to_slack</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="n">qs_summary</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;qs_summary&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="n">report</span> <span class="o">=</span> <span class="s2">&quot;foo bar baz&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;report&quot;</span><span class="p">:</span> <span class="n">report</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Build the graph</span>
<span class="n">qs_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">QuestionSummarizationState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">QuestionSummarizationOutputState</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;send_to_slack&quot;</span><span class="p">,</span> <span class="n">send_to_slack</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;generate_summary&quot;</span><span class="p">)</span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_summary&quot;</span><span class="p">,</span> <span class="s2">&quot;send_to_slack&quot;</span><span class="p">)</span>
<span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;send_to_slack&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Executamos o grafo novamente com o log de teste.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;report&#x27;: &#x27;foo bar baz&#x27;, &#x27;processed_logs&#x27;: [&#x27;summary-on-log-1&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h5 id="Grafo principal">Grafo principal<a class="anchor-link" href="#Grafo principal">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos os dois subgrafos, podemos criar o grafo principal que os utilizará. Para isso, criamos a classe <code>EntryGraphState</code> que terá o estado dos dois subgrafos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Entry Graph</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EntryGraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">raw_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the FA sub-graph</span>
<span class="w">    </span><span class="n">report</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the QS sub-graph</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span>  <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span> <span class="c1"># This will be generated in BOTH sub-graphs</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma função de limpeza de logs, que será um nó que se executará antes dos dois subgrafos e que lhes fornecerá os logs limpos através da chave <code>cleaned_logs</code>, que é a que os dois subgrafos tomam do estado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">clean_logs</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Get logs</span>
<span class="w">    </span><span class="n">raw_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;raw_logs&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="c1"># Data cleaning raw_logs -&amp;gt; docs </span>
<span class="w">    </span><span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">raw_logs</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">:</span> <span class="n">cleaned_logs</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos o grafo principal</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Build the graph</span>
<span class="n">entry_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">EntryGraphState</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos os nós. Para adicionar um subgráfico como nó, o que fazemos é adicionar sua compilação.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Add nodes</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;clean_logs&quot;</span><span class="p">,</span> <span class="n">clean_logs</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;question_summarization&quot;</span><span class="p">,</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;failure_analysis&quot;</span><span class="p">,</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x107985ef0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A partir de aqui já é como se sempre, adicionamos os <code>edges</code> e o compilamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Add edges</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;clean_logs&quot;</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;clean_logs&quot;</span><span class="p">,</span> <span class="s2">&quot;failure_analysis&quot;</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;clean_logs&quot;</span><span class="p">,</span> <span class="s2">&quot;question_summarization&quot;</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;failure_analysis&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;question_summarization&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">entry_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.
Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.
Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.
Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.
Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Por fim, mostramos o grafo. Adicionamos <code>xray=1</code> para que seja visível o estado interno do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Setting xray to 1 will show the internal structure of the nested graph</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se não tivéssemos adicionado <code>xray=1</code>, o gráfico ficaria assim</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos dois logs de teste, em um haverá um erro (um valor em <code>grade</code>) e no outro não.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Dummy logs</span>
<span class="n">question_answer</span> <span class="o">=</span> <span class="n">Log</span><span class="p">(</span>
<span class="w">    </span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">question</span><span class="o">=</span><span class="s2">&quot;How can I import ChatOllama?&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">answer</span><span class="o">=</span><span class="s2">&quot;To import ChatOllama, use: &#39;from langchain_community.chat_models import ChatOllama.&#39;&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">question_answer_feedback</span> <span class="o">=</span> <span class="n">Log</span><span class="p">(</span>
<span class="w">    </span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;2&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">question</span><span class="o">=</span><span class="s2">&quot;How can I use Chroma vector store?&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">answer</span><span class="o">=</span><span class="s2">&quot;To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">grade</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="n">grader</span><span class="o">=</span><span class="s2">&quot;Document Relevance Recall&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">feedback</span><span class="o">=</span><span class="s2">&quot;The retrieved documents discuss vector stores in general, but not Chroma specifically&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">raw_logs</span> <span class="o">=</span> <span class="p">[</span><span class="n">question_answer</span><span class="p">,</span><span class="n">question_answer_feedback</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Passamo-los para o grafo principal</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;raw_logs&quot;</span><span class="p">:</span> <span class="n">raw_logs</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;raw_logs&#x27;: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I import ChatOllama?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &quot;To import ChatOllama, use: &#x27;from langchain_community.chat_models import ChatOllama.&#x27;&quot;&#x7D;,
&#x20;&#x20;&#x7B;&#x27;id&#x27;: &#x27;2&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I use Chroma vector store?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &#x27;To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).&#x27;,
&#x20;&#x20;&#x20;&#x27;grade&#x27;: 0,
&#x20;&#x20;&#x20;&#x27;grader&#x27;: &#x27;Document Relevance Recall&#x27;,
&#x20;&#x20;&#x20;&#x27;feedback&#x27;: &#x27;The retrieved documents discuss vector stores in general, but not Chroma specifically&#x27;&#x7D;],
 &#x27;cleaned_logs&#x27;: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I import ChatOllama?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &quot;To import ChatOllama, use: &#x27;from langchain_community.chat_models import ChatOllama.&#x27;&quot;&#x7D;,
&#x20;&#x20;&#x7B;&#x27;id&#x27;: &#x27;2&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I use Chroma vector store?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &#x27;To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).&#x27;,
&#x20;&#x20;&#x20;&#x27;grade&#x27;: 0,
&#x20;&#x20;&#x20;&#x27;grader&#x27;: &#x27;Document Relevance Recall&#x27;,
&#x20;&#x20;&#x20;&#x27;feedback&#x27;: &#x27;The retrieved documents discuss vector stores in general, but not Chroma specifically&#x27;&#x7D;],
 &#x27;fa_summary&#x27;: &#x27;Poor quality retrieval of documentation.&#x27;,
 &#x27;report&#x27;: &#x27;foo bar baz&#x27;,
 &#x27;processed_logs&#x27;: [&#x27;failure-analysis-on-log-2&#x27;,
&#x20;&#x20;&#x27;summary-on-log-1&#x27;,
&#x20;&#x20;&#x27;summary-on-log-2&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Assim como antes, escrevemos todo o grafo para vê-lo com maior clareza</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Entry Graph</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EntryGraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">raw_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
<span class="w">    </span><span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the FA sub-graph</span>
<span class="w">    </span><span class="n">report</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the QS sub-graph</span>
<span class="w">    </span><span class="n">processed_logs</span><span class="p">:</span>  <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span> <span class="c1"># This will be generated in BOTH sub-graphs</span>
<span class="w"> </span>
<span class="c1"># Functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">clean_logs</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Get logs</span>
<span class="w">    </span><span class="n">raw_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;raw_logs&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="c1"># Data cleaning raw_logs -&amp;gt; docs </span>
<span class="w">    </span><span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">raw_logs</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;cleaned_logs&quot;</span><span class="p">:</span> <span class="n">cleaned_logs</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Build the graph</span>
<span class="n">entry_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">EntryGraphState</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;clean_logs&quot;</span><span class="p">,</span> <span class="n">clean_logs</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;question_summarization&quot;</span><span class="p">,</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;failure_analysis&quot;</span><span class="p">,</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;clean_logs&quot;</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;clean_logs&quot;</span><span class="p">,</span> <span class="s2">&quot;failure_analysis&quot;</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;clean_logs&quot;</span><span class="p">,</span> <span class="s2">&quot;question_summarization&quot;</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;failure_analysis&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;question_summarization&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">entry_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Setting xray to 1 will show the internal structure of the nested graph</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Passamos os logs de teste ao grafo principal</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;raw_logs&quot;</span><span class="p">:</span> <span class="n">raw_logs</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;raw_logs&#x27;: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I import ChatOllama?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &quot;To import ChatOllama, use: &#x27;from langchain_community.chat_models import ChatOllama.&#x27;&quot;&#x7D;,
&#x20;&#x20;&#x7B;&#x27;id&#x27;: &#x27;2&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I use Chroma vector store?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &#x27;To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).&#x27;,
&#x20;&#x20;&#x20;&#x27;grade&#x27;: 0,
&#x20;&#x20;&#x20;&#x27;grader&#x27;: &#x27;Document Relevance Recall&#x27;,
&#x20;&#x20;&#x20;&#x27;feedback&#x27;: &#x27;The retrieved documents discuss vector stores in general, but not Chroma specifically&#x27;&#x7D;],
 &#x27;cleaned_logs&#x27;: [&#x7B;&#x27;id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I import ChatOllama?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &quot;To import ChatOllama, use: &#x27;from langchain_community.chat_models import ChatOllama.&#x27;&quot;&#x7D;,
&#x20;&#x20;&#x7B;&#x27;id&#x27;: &#x27;2&#x27;,
&#x20;&#x20;&#x20;&#x27;question&#x27;: &#x27;How can I use Chroma vector store?&#x27;,
&#x20;&#x20;&#x20;&#x27;answer&#x27;: &#x27;To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).&#x27;,
&#x20;&#x20;&#x20;&#x27;grade&#x27;: 0,
&#x20;&#x20;&#x20;&#x27;grader&#x27;: &#x27;Document Relevance Recall&#x27;,
&#x20;&#x20;&#x20;&#x27;feedback&#x27;: &#x27;The retrieved documents discuss vector stores in general, but not Chroma specifically&#x27;&#x7D;],
 &#x27;fa_summary&#x27;: &#x27;Poor quality retrieval of documentation.&#x27;,
 &#x27;report&#x27;: &#x27;foo bar baz&#x27;,
 &#x27;processed_logs&#x27;: [&#x27;failure-analysis-on-log-2&#x27;,
&#x20;&#x20;&#x27;summary-on-log-1&#x27;,
&#x20;&#x20;&#x27;summary-on-log-2&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Ramas dinamicas">Ramas dinâmicas<a class="anchor-link" href="#Ramas dinamicas">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Até agora criamos nós e <code>edges</code> estáticos, mas há momentos em que não sabemos se vamos precisar de um galho até que o grafo seja executado. Para isso, podemos usar o método <code>SEND</code> do langgraph, que permite criar galhos dinamicamente.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para vê-lo, vamos criar um grafo que gere piadas sobre alguns temas, mas como não sabemos antecipadamente sobre quantos temas queremos gerar piadas, através do método <code>SEND</code> vamos criar ramificações dinamicamente, de forma que se houverem temas restantes para gerar, uma nova ramificação será criada.</p>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Nota: Vamos a fazer esta seção usando o Sonnet 3.7, pois a integração da HuggingFace não possui a funcionalidade de <code>with_structured_output</code> que fornece uma saída estruturada com uma estrutura definida.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro importamos as bibliotecas necessárias.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">operator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos as classes com a estrutura do estado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">topic</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span>
<span class="w">    </span><span class="n">jokes</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">]</span>
<span class="w">    </span><span class="n">best_selected_joke</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">JokeState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">subject</span><span class="p">:</span> <span class="nb">str</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o LLM</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a função que gerará os temas.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a usar <code>with_structured_output</code> para que o LLM gere uma saída com uma estrutura definida por nós, essa estrutura vamos definir com a classe <code>Subjects</code> que é uma classe do tipo <code>BaseModel</code> de <code>Pydantic</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">Subjects</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">subjects_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Generate a list of 3 sub-topics that are all related to this overall topic: </span><span class="si">{topic}</span><span class="s2">.&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_topics</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">subjects_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;topic&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Subjects</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;subjects&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">subjects</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora definimos a função que gerará os piadas.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="n">joke</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span>
<span class="n">joke_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Generate a joke about </span><span class="si">{subject}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">JokeState</span><span class="p">):</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">subject</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;jokes&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">joke</span><span class="p">]}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E por último a função que selecionará o melhor chiste.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BestJoke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>
<span class="w"> </span>
<span class="n">best_joke_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Below are a bunch of jokes about </span><span class="si">{topic}</span><span class="s2">. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: </span><span class="se">\n\n</span><span class="s2">  </span><span class="si">{jokes}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">best_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
<span class="w">    </span><span class="n">jokes</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;jokes&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">best_joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;topic&quot;</span><span class="p">],</span> <span class="n">jokes</span><span class="o">=</span><span class="n">jokes</span><span class="p">)</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">BestJoke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;best_selected_joke&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;jokes&quot;</span><span class="p">][</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos criar uma função que decida se deve ou não criar uma nova branch com <code>SEND</code>, e para decidir isso, ela verificará se há tópicos restantes a serem gerados.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.constants</span><span class="w"> </span><span class="kn">import</span> <span class="n">Send</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">continue_to_jokes</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">[</span><span class="n">Send</span><span class="p">(</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;subject&quot;</span><span class="p">:</span> <span class="n">s</span><span class="p">})</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;subjects&quot;</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Construímos o grafo, adicionamos os nós e os <code>edges</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Build the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_topics&quot;</span><span class="p">,</span> <span class="n">generate_topics</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">,</span> <span class="n">generate_joke</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;best_joke&quot;</span><span class="p">,</span> <span class="n">best_joke</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;generate_topics&quot;</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;generate_topics&quot;</span><span class="p">,</span> <span class="n">continue_to_jokes</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">])</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">,</span> <span class="s2">&quot;best_joke&quot;</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;best_joke&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Display the graph</span>
<span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se pode ver, o <code>edge</code> entre <code>generate_topics</code> e <code>generate_joke</code> é representado por uma linha tracejada, indicando que é um ramo dinâmico.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos agora um dicionário com a chave <code>topic</code> que é necessária pelo nó <code>generate_topics</code> para gerar os tópicos e o passamos ao grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Call the graph: here we call it to generate a list of jokes</span>
<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">app</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;animals&quot;</span><span class="p">}):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;generate_topics&#x27;: &#x7B;&#x27;subjects&#x27;: [&#x27;Marine Animals&#x27;, &#x27;Endangered Species&#x27;, &#x27;Animal Behavior&#x27;]&#x7D;&#x7D;
&#x7B;&#x27;generate_joke&#x27;: &#x7B;&#x27;jokes&#x27;: [&quot;Why don&#x27;t cats play poker in the wild? Too many cheetahs!&quot;]&#x7D;&#x7D;
&#x7B;&#x27;generate_joke&#x27;: &#x7B;&#x27;jokes&#x27;: [&quot;Why don&#x27;t sharks eat clownfish? Because they taste funny!&quot;]&#x7D;&#x7D;
&#x7B;&#x27;generate_joke&#x27;: &#x7B;&#x27;jokes&#x27;: [&quot;Why don&#x27;t endangered species tell jokes? Because they&#x27;re afraid of dying out from laughter!&quot;]&#x7D;&#x7D;
&#x7B;&#x27;best_joke&#x27;: &#x7B;&#x27;best_selected_joke&#x27;: &quot;Why don&#x27;t cats play poker in the wild? Too many cheetahs!&quot;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a criar o grafo com todo o código junta para maior clareza.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">operator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.constants</span><span class="w"> </span><span class="kn">import</span> <span class="n">Send</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="w"> </span>
<span class="c1"># Prompts we will use</span>
<span class="n">subjects_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Generate a list of 3 sub-topics that are all related to this overall topic: </span><span class="si">{topic}</span><span class="s2">.&quot;&quot;&quot;</span>
<span class="n">joke_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Generate a joke about </span><span class="si">{subject}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="n">best_joke_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Below are a bunch of jokes about </span><span class="si">{topic}</span><span class="s2">. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: </span><span class="se">\n\n</span><span class="s2">  </span><span class="si">{jokes}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">Subjects</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">BestJoke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span>
<span class="k">class</span><span class="w"> </span><span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">topic</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span>
<span class="w">    </span><span class="n">jokes</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">]</span>
<span class="w">    </span><span class="n">best_selected_joke</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">JokeState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">subject</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="n">joke</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_topics</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">subjects_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;topic&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Subjects</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;subjects&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">subjects</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">continue_to_jokes</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">[</span><span class="n">Send</span><span class="p">(</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;subject&quot;</span><span class="p">:</span> <span class="n">s</span><span class="p">})</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;subjects&quot;</span><span class="p">]]</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">JokeState</span><span class="p">):</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">subject</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;jokes&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">joke</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">best_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
<span class="w">    </span><span class="n">jokes</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;jokes&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">best_joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;topic&quot;</span><span class="p">],</span> <span class="n">jokes</span><span class="o">=</span><span class="n">jokes</span><span class="p">)</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">BestJoke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;best_selected_joke&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;jokes&quot;</span><span class="p">][</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Build the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_topics&quot;</span><span class="p">,</span> <span class="n">generate_topics</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">,</span> <span class="n">generate_joke</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;best_joke&quot;</span><span class="p">,</span> <span class="n">best_joke</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;generate_topics&quot;</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;generate_topics&quot;</span><span class="p">,</span> <span class="n">continue_to_jokes</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">])</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_joke&quot;</span><span class="p">,</span> <span class="s2">&quot;best_joke&quot;</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;best_joke&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Display the graph</span>
<span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a executá-lo, mas agora, em vez de <code>animais</code>, vamos fazer com <code>carros</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">app</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;cars&quot;</span><span class="p">}):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;generate_topics&#x27;: &#x7B;&#x27;subjects&#x27;: [&#x27;Car Maintenance and Repair&#x27;, &#x27;Electric and Hybrid Vehicles&#x27;, &#x27;Automotive Design and Engineering&#x27;]&#x7D;&#x7D;
&#x7B;&#x27;generate_joke&#x27;: &#x7B;&#x27;jokes&#x27;: [&quot;Why don&#x27;t electric cars tell jokes? They&#x27;re afraid of running out of charge before they get to the punchline!&quot;]&#x7D;&#x7D;
&#x7B;&#x27;generate_joke&#x27;: &#x7B;&#x27;jokes&#x27;: [&quot;Why don&#x27;t automotive engineers play hide and seek? Because good luck hiding when you&#x27;re always making a big noise about torque!&quot;]&#x7D;&#x7D;
&#x7B;&#x27;generate_joke&#x27;: &#x7B;&#x27;jokes&#x27;: [&quot;Why don&#x27;t cars ever tell their own jokes? Because they always exhaust themselves during the delivery! Plus, their timing belts are always a little off.&quot;]&#x7D;&#x7D;
&#x7B;&#x27;best_joke&#x27;: &#x7B;&#x27;best_selected_joke&#x27;: &quot;Why don&#x27;t electric cars tell jokes? They&#x27;re afraid of running out of charge before they get to the punchline!&quot;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Melhorar o chatbot com ferramentas">Melhorar o chatbot com ferramentas<a class="anchor-link" href="#Melhorar o chatbot com ferramentas">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para lidar com algumas consultas, nosso chatbot não pode responder <code>com base em seu conhecimento</code>, então vamos integrar uma ferramenta de busca na web. Nosso bot pode usar essa ferramenta para encontrar informações relevantes e fornecer respostas melhores.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Requisitos">Requisitos<a class="anchor-link" href="#Requisitos">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Antes de começar, temos que instalar o buscador <a href="https://python.langchain.com/docs/integrations/tools/tavily_search/">Tavily</a> que é um buscador web que nos permite buscar informações na web.</p>
<div class='highlight'><pre><code class="language-bash">pip install -U tavily-python langchain_community
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Depois, temos que criar uma <a href="https://app.tavily.com/home">API KEY</a>, escrevemos no nosso arquivo <code>.env</code> e carregamos em uma variável.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Chatbot com ferramentas">Chatbot com ferramentas<a class="anchor-link" href="#Chatbot com ferramentas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro criamos o estado e o LLM</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Create the LLM</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, definimos a ferramenta de busca na web através do <a href="https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html">TavilySearchResults</a></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Testamos a ferramenta, vamos fazer uma busca na internet.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tool</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;What was the result of Real Madrid&#39;s at last match in the Champions League?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;title&#x27;: &#x27;HIGHLIGHTS | Real Madrid 3-2 Leganés | LaLiga 2024/25 - YouTube&#x27;,
&#x20;&#x20;&#x27;url&#x27;: &#x27;https://www.youtube.com/watch?v=Np-Kwz4RDpY&#x27;,
&#x20;&#x20;&#x27;content&#x27;: &quot;20:14 · Go to channel · RONALDO&#x27;S LAST MATCH WITH REAL MADRID: THE MOST THRILLING FINAL EVER! ... Champions League 1/4 Final | PES. Football&quot;,
&#x20;&#x20;&#x27;score&#x27;: 0.65835214&#x7D;,
 &#x7B;&#x27;title&#x27;: &#x27;Real Madrid | History | UEFA Champions League&#x27;,
&#x20;&#x20;&#x27;url&#x27;: &#x27;https://www.uefa.com/uefachampionsleague/history/clubs/50051--real-madrid/&#x27;,
&#x20;&#x20;&#x27;content&#x27;: &#x27;1955/56 P W D L Final 7 5 0 2\nUEFA Champions League [...] 2010/11 P W D L Semi-finals 12 8 3 1\n2009/10 P W D L Round of 16 8 4 2 2\n2000s\n2008/09 P W D L Round of 16 8 4 0 4\n2007/08 P W D L Round of 16 8 3 2 3\n2006/07 P W D L Round of 16 8 4 2 2\n2005/06 P W D L Round of 16 8 3 2 3\n2004/05 P W D L Round of 16 10 6 2 2\n2003/04 P W D L Quarter-finals 10 6 3 1\n2002/03 P W D L Semi-finals 16 7 5 4\n2001/02 P W D L Final 17 12 3 2\n2000/01 P W D L Semi-finals 16 9 2 5\n1990s\n1999/00 P W D L Final 17 10 3 4\n1998/99 P W D L Quarter-finals 8 4 1 3 [...] 1969/70 P W D L Second round 4 2 0 2\n1968/69 P W D L Second round 4 3 0 1\n1967/68 P W D L Semi-finals 8 2 4 2\n1966/67 P W D L Quarter-finals 4 1 0 3\n1965/66 P W D L Final 9 5 2 2\n1964/65 P W D L Quarter-finals 6 4 1 1\n1963/64 P W D L Final 9 7 0 2\n1962/63 P W D L Preliminary round 2 0 1 1\n1961/62 P W D L Final 10 8 0 2\n1960/61 P W D L First round 2 0 1 1\n1950s\n1959/60 P W D L Final 7 6 0 1\n1958/59 P W D L Final 8 5 2 1\n1957/58 P W D L Final 7 5 1 1\n1956/57 P W D L Final 8 6 1 1&#x27;,
&#x20;&#x20;&#x27;score&#x27;: 0.6030211&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Os resultados são resumos de páginas que nosso chatbot pode usar para responder perguntas.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma lista de ferramentas, pois nosso grafo precisa definir as ferramentas por meio de uma lista.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos a lista de <code>tools</code>, criamos um <code>llm_with_tools</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Modification: tell the LLM which tools it can call</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos a função queirá no nó chat bot</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Precisamos criar uma função para executar as <code>tools_list</code> se forem chamadas. Adicionamos as <code>tools_list</code> a um novo nó.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mais tarde faremos isso com o método <a href="https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode">ToolNode</a> de <code>LangGraph</code>, mas primeiro o construiremos nós mesmos para entender como funciona.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a implementar a classe <code>BasicToolNode</code>, que verifica a mensagem mais recente no estado e chama as <code>tools_list</code> se a mensagem contiver <code>tool_calls</code>.</p>
<p>Baseia-se no suporte a <code>tool_calling</code> dos <code>LLMs</code>, que está disponível na <code>Anthropic</code>, <code>HuggingFace</code>, <code>Google Gemini</code>, <code>OpenAI</code> e vários outros provedores de <code>LLM</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolMessage</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">BasicToolNode</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A node that runs the tools requested in the last AIMessage.&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tools</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the tools</span>
<span class="w"> </span>
<span class="sd">        Args:</span>
<span class="sd">            tools (list): The tools to use</span>
<span class="w"> </span>
<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="c1"># Initialize the tools</span>
<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span> <span class="o">=</span> <span class="p">{</span><span class="n">tool</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tool</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">}</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Call the node</span>
<span class="w"> </span>
<span class="sd">        Args:</span>
<span class="sd">            inputs (dict): The inputs to the node</span>
<span class="w"> </span>
<span class="sd">        Returns:</span>
<span class="sd">            dict: The outputs of the node</span>
<span class="sd">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="c1"># Get the last message</span>
<span class="w">        </span><span class="k">if</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">,</span> <span class="p">[]):</span>
<span class="w">            </span><span class="n">message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">        </span><span class="k">else</span><span class="p">:</span>
<span class="w">            </span><span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No message found in input&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">        </span><span class="c1"># Execute the tools</span>
<span class="w">        </span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="w">        </span><span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>
<span class="w">            </span><span class="n">tool_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span><span class="p">[</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
<span class="w">                </span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;args&quot;</span><span class="p">]</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">            </span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
<span class="w">                </span><span class="n">ToolMessage</span><span class="p">(</span>
<span class="w">                    </span><span class="n">content</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_result</span><span class="p">),</span>
<span class="w">                    </span><span class="n">name</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">],</span>
<span class="w">                    </span><span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">],</span>
<span class="w">                </span><span class="p">)</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">outputs</span><span class="p">}</span>
<span class="w"> </span>
<span class="n">basic_tool_node</span> <span class="o">=</span> <span class="n">BasicToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós usamos <a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html">ToolMessage</a> que passa o resultado de executar uma <code>tool</code> de volta para o <code>LLM</code>.</p>
<p><code>ToolMessage</code> contém o resultado de uma invocação de uma <code>tool</code>.</p>
<p>Isso significa que, assim que temos o resultado de usar uma <code>Tool</code>, passamos esse resultado para o LLM processar.</p>
</section>
<section class="section-block-markdown-cell">
<p>Com o objeto <code>basic_tool_node</code> (que é um objeto da classe <code>BasicToolNode</code> que criamos), já podemos fazer com que o LLM execute <code>tool</code>s.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora, assim como fizemos quando construímos um chatbot básico, vamos criar o grafo e adicionar nós a ele.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Create graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add the chatbot node</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools_node&quot;</span><span class="p">,</span> <span class="n">basic_tool_node</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x14996cd70&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando o LLM receber uma mensagem, como conhece as <code>tools</code> que tem à disposição, decidirá se deve responder ou usar uma <code>tool</code>. Então, vamos criar uma função de roteamento, que executará uma <code>tool</code> se o LLM decidir usá-la, ou caso contrário, terminará a execução do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">route_tools_function</span><span class="p">(</span>
<span class="w">    </span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use in the conditional_edge to route to the ToolNode if the last message</span>
<span class="sd">    has tool calls. Otherwise, route to the end.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="c1"># Get last message</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">        </span><span class="n">ai_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="k">elif</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">,</span> <span class="p">[]):</span>
<span class="w">        </span><span class="n">ai_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No messages found in input state to tool_edge: </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Router in function of last message</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ai_message</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ai_message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span> <span class="s2">&quot;tools_node&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">END</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos os <code>edge</code>s.</p>
</section>
<section class="section-block-markdown-cell">
<p>Temos que adicionar uma <code>edge</code> especial através de <code>add_conditional_edges</code>, que criará um nó condicional. Une o nó <code>chatbot_node</code> com a função de roteamento que criamos anteriormente <code>route_tools_function</code>. Com este nó, se obtivermos na saída de <code>route_tools_function</code> a string <code>tools_node</code>, o grafo será roteado para o nó <code>tools_node</code>, mas se recebermos <code>END</code>, o grafo será roteado para o nó <code>END</code> e terminará a execução do grafo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mais tarde, substituiremos isso pelo método pré-construído <code>tools_condition</code>, mas agora o implementamos nós mesmos para ver como funciona.</p>
</section>
<section class="section-block-markdown-cell">
<p>Por último, adiciona-se outro <code>edge</code> que une <code>tools_node</code> com <code>chatbot_node</code>, para que quando termine de executar uma <code>tool</code> o grafo volte ao nó do <code>LLM</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Add edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">route_tools_function</span><span class="p">,</span>
<span class="w">    </span><span class="c1"># The following dictionary lets you tell the graph to interpret the condition&#39;s outputs as a specific node</span>
<span class="w">    </span><span class="c1"># It defaults to the identity function, but if you</span>
<span class="w">    </span><span class="c1"># want to use a node named something else apart from &quot;tools&quot;,</span>
<span class="w">    </span><span class="c1"># You can update the value of the dictionary to something else</span>
<span class="w">    </span><span class="c1"># e.g., &quot;tools&quot;: &quot;my_tools&quot;</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;tools_node&quot;</span><span class="p">:</span> <span class="s2">&quot;tools_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">:</span> <span class="n">END</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools_node&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x14996cd70&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Compilamos o nó e o representamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos fazer perguntas ao bot fora de seus dados de treinamento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Colors for the terminal</span>
<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[32m&quot;</span>
<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[33m&quot;</span>
<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[0m&quot;</span>
<span class="w"> </span>

<span class="k">def</span><span class="w"> </span><span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]}):</span>
<span class="w">        </span><span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">value</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="w">    </span><span class="k">try</span><span class="p">:</span>
<span class="w">        </span><span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;User: &quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;quit&quot;</span><span class="p">,</span> <span class="s2">&quot;exit&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">]:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Goodbye!&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">break</span>
<span class="w"> </span>
<span class="w">        </span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">    </span><span class="k">except</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># fallback if input() is not available</span>
<span class="w">        </span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;What do you know about LangGraph?&quot;</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;User: &quot;</span> <span class="o">+</span> <span class="n">user_input</span><span class="p">)</span>
<span class="w">        </span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">        </span><span class="k">break</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca&quot;, &quot;url&quot;: &quot;https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html&quot;, &quot;content&quot;: &quot;While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday&#x27;s match having picked up 34 points from 13 matches.\n\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. [...] The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\n\nReal Madrid vs Leganes LIVE - Latest Updates\n\nMatch ends, Real Madrid 3, Leganes 2.\n\nSecond Half ends, Real Madrid 3, Leganes 2.\n\nFoul by Vin\u00edcius J\u00fanior (Real Madrid).\n\nSeydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half. [...] Goal! Real Madrid 1, Leganes 1. Diego Garc\u00eda (Leganes) left footed shot from very close range.\n\nAttempt missed. \u00d3scar Rodr\u00edguez (Leganes) left footed shot from the centre of the box.\n\nGoal! Real Madrid 1, Leganes 0. Kylian Mbapp\u00e9 (Real Madrid) converts the penalty with a right footed shot.\n\nPenalty Real Madrid. Arda G\u00fcler draws a foul in the penalty area.\n\nPenalty conceded by \u00d3scar Rodr\u00edguez (Leganes) after a foul in the penalty area.\n\nDelay over. They are ready to continue.&quot;, &quot;score&quot;: 0.8548001&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/report/_/gameId/704946&quot;, &quot;content&quot;: &quot;Real Madrid\n\nLegan\u00e9s\n\nMbapp\u00e9 nets twice to keep Real Madrid&#x27;s title hopes alive\n\nReal Madrid vs. Legan\u00e9s - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Legan\u00e9s, 03/30/2025\n\nReal Madrid&#x27;s Kylian Mbapp\u00e9 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\&quot;We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\&quot; Leganes striker Garcia said.\n\n\&quot;Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\&quot;\n\nGame Information&quot;, &quot;score&quot;: 0.82220376&#x7D;]
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid vs Leganes 3-2 | Highlights &amp;amp; All Goals - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=ngBWsjmeHEk&quot;, &quot;content&quot;: &quot;Real Madrid secured a dramatic 3-2 victory over Leganes in an intense La Liga showdown on 29 March 2025! \u26bd Watch all the goals and&quot;, &quot;score&quot;: 0.5157425&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/report/_/gameId/704946&quot;, &quot;content&quot;: &quot;\&quot;We know what we always have to do: win. We started well, in the opposition half, and we scored a goal. Then we didn&#x27;t play well for 20 minutes and conceded two goals,\&quot; said Mbapp\u00e9.\n\n\&quot;But we know that if we play well we&#x27;ll score and in the second half we scored two goals. We won the game and we&#x27;re very happy.\n\n\&quot;We worked on [the set piece] a few weeks ago with the staff. I knew I could shoot this way, I saw the space. I asked the others to let me shoot and it worked out well.\&quot; [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\&quot;We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\&quot; Leganes striker Garcia said.\n\n\&quot;Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\&quot;\n\nGame Information [...] However, Leganes responded almost immediately as Diego Garcia tapped in a loose ball at the far post to equalise in the following minute before Rodriguez set up Dani Raba to slot past goalkeeper Andriy Lunin in the 41st.\n\nReal midfielder Jude Bellingham brought the scores level two minutes after the break, sliding the ball into the net after a rebound off the crossbar. Mbapp\u00e9 then bagged the winner with a brilliant curled free kick in the 76th minute for his second.&quot;, &quot;score&quot;: 0.50944775&#x7D;]
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/report/_/gameId/704946&quot;, &quot;content&quot;: &quot;Real Madrid\n\nLegan\u00e9s\n\nMbapp\u00e9 nets twice to keep Real Madrid&#x27;s title hopes alive\n\nReal Madrid vs. Legan\u00e9s - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Legan\u00e9s, 03/30/2025\n\nReal Madrid&#x27;s Kylian Mbapp\u00e9 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\&quot;We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\&quot; Leganes striker Garcia said.\n\n\&quot;Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\&quot;\n\nGame Information [...] However, Leganes responded almost immediately as Diego Garcia tapped in a loose ball at the far post to equalise in the following minute before Rodriguez set up Dani Raba to slot past goalkeeper Andriy Lunin in the 41st.\n\nReal midfielder Jude Bellingham brought the scores level two minutes after the break, sliding the ball into the net after a rebound off the crossbar. Mbapp\u00e9 then bagged the winner with a brilliant curled free kick in the 76th minute for his second.&quot;, &quot;score&quot;: 0.93666285&#x7D;, &#x7B;&quot;title&quot;: &quot;MBAPPE BRACE Leganes vs. Real Madrid - ESPN FC - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=0xwUhzx19_4&quot;, &quot;content&quot;: &quot;MBAPPE BRACE \ud83d\udd25 Leganes vs. Real Madrid | LALIGA Highlights | ESPN FC \n ESPN FC \n 6836 likes \n 550646 views \n 29 Mar 2025 \n Watch these highlights as Kylian Mbappe scores 2 goals to give Real Madrid the 3-2 victory over Leganes in their LALIGA matchup.\n\n\u2714 Subscribe to ESPN+: http://espnplus.com/soccer/youtube\n\u2714 Subscribe to ESPN FC on YouTube: http://bit.ly/SUBSCRIBEtoESPNFC \n 790 comments&quot;, &quot;score&quot;: 0.92857105&#x7D;]
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;(VIDEO) All Goals from Real Madrid vs Leganes in La Liga&quot;, &quot;url&quot;: &quot;https://www.beinsports.com/en-us/soccer/la-liga/articles-video/-video-all-goals-from-real-madrid-vs-leganes-in-la-liga-2025-03-29?ess=&quot;, &quot;content&quot;: &quot;Real Madrid will host CD Leganes this Saturday, March 29, 2025, at the Santiago Bernab\u00e9u in a Matchday 29 clash of LaLiga EA Sports.&quot;, &quot;score&quot;: 0.95628047&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid v Leganes | March 29, 2025 | Goal.com US&quot;, &quot;url&quot;: &quot;https://www.goal.com/en-us/match/real-madrid-vs-leganes/sZTw_SnjyKCcntxKHHQI7&quot;, &quot;content&quot;: &quot;Latest news, stats and live commentary for the LaLiga&#x27;s meeting between Real Madrid v Leganes on the March 29, 2025.&quot;, &quot;score&quot;: 0.9522955&#x7D;]
User: How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: Real Madrid faced Leganes in La Liga this weekend and came away with a 3-2 victory at the Santiago Bernabéu. The match was intense, with Kylian Mbappé scoring twice for Real Madrid, including a curled free kick in the 76th minute that proved to be the winner. Leganes managed to take the lead briefly with goals from Diego García and Dani Raba, but Real Madrid leveled through Jude Bellingham before Mbappé&#x27;s second goal secured the win. This result keeps Real Madrid&#x27;s title hopes alive, moving them level on points with leaders Barcelona.
User: Which players played the match?
Assistant: The question is too vague and doesn&#x27;t provide context such as the sport, league, or specific match in question. Could you please provide more details?
User: q
Assistant: Goodbye!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você vê, primeiro perguntei como ficou o Real Madrid em seu último jogo na Liga contra o Leganés</p>
<p>Como é algo atual, ele decidiu usar a ferramenta de busca, com isso obteve o resultado.</p>
</section>
<section class="section-block-markdown-cell">
<p>No entanto, em seguida eu lhe perguntei quais jogadores estavam jogando e ele não sabia do que eu estava falando, isso porque o contexto da conversa não é mantido. Então, a próxima coisa que vamos fazer é adicionar uma memória ao agente para que ele possa manter o contexto da conversa.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a escrever tudo junto para que seja mais legível</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolMessage</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># State</span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># Create the LLM with tools</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># BasicToolNode class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BasicToolNode</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A node that runs the tools requested in the last AIMessage.&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tools</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the tools</span>
<span class="w"> </span>
<span class="sd">        Args:</span>
<span class="sd">            tools (list): The tools to use</span>
<span class="w"> </span>
<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="c1"># Initialize the tools</span>
<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span> <span class="o">=</span> <span class="p">{</span><span class="n">tool</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tool</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">}</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Call the node</span>
<span class="w"> </span>
<span class="sd">        Args:</span>
<span class="sd">            inputs (dict): The inputs to the node</span>
<span class="w"> </span>
<span class="sd">        Returns:</span>
<span class="sd">            dict: The outputs of the node</span>
<span class="sd">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="c1"># Get the last message</span>
<span class="w">        </span><span class="k">if</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">,</span> <span class="p">[]):</span>
<span class="w">            </span><span class="n">message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">        </span><span class="k">else</span><span class="p">:</span>
<span class="w">            </span><span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No message found in input&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">        </span><span class="c1"># Execute the tools</span>
<span class="w">        </span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="w">        </span><span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>
<span class="w">            </span><span class="n">tool_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span><span class="p">[</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
<span class="w">                </span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;args&quot;</span><span class="p">]</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">            </span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
<span class="w">                </span><span class="n">ToolMessage</span><span class="p">(</span>
<span class="w">                    </span><span class="n">content</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_result</span><span class="p">),</span>
<span class="w">                    </span><span class="n">name</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">],</span>
<span class="w">                    </span><span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">],</span>
<span class="w">                </span><span class="p">)</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">outputs</span><span class="p">}</span>
<span class="w"> </span>
<span class="n">basic_tool_node</span> <span class="o">=</span> <span class="n">BasicToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Route function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">route_tools_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use in the conditional_edge to route to the ToolNode if the last message</span>
<span class="sd">    has tool calls. Otherwise, route to the end.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="c1"># Get last message</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">        </span><span class="n">ai_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="k">elif</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">,</span> <span class="p">[]):</span>
<span class="w">        </span><span class="n">ai_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No messages found in input state to tool_edge: </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Router in function of last message</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ai_message</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ai_message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span> <span class="s2">&quot;tools_node&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">END</span>
<span class="w"> </span>
<span class="c1"># Start to build the graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes to the graph</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools_node&quot;</span><span class="p">,</span> <span class="n">basic_tool_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">route_tools_function</span><span class="p">,</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;tools_node&quot;</span><span class="p">:</span> <span class="s2">&quot;tools_node&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="n">END</span><span class="p">:</span> <span class="n">END</span>
<span class="w">    </span><span class="p">},</span>
<span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools_node&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Display the graph</span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:
1. Check your internet connection and try again
2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`
3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Executamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Colors for the terminal</span>
<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[32m&quot;</span>
<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[33m&quot;</span>
<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[0m&quot;</span>
<span class="w"> </span>

<span class="k">def</span><span class="w"> </span><span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]}):</span>
<span class="w">        </span><span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">value</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="w">    </span><span class="k">try</span><span class="p">:</span>
<span class="w">        </span><span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;User: &quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;quit&quot;</span><span class="p">,</span> <span class="s2">&quot;exit&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">]:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Goodbye!&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">break</span>
<span class="w"> </span>
<span class="w">        </span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">    </span><span class="k">except</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># fallback if input() is not available</span>
<span class="w">        </span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;What do you know about LangGraph?&quot;</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;User: &quot;</span> <span class="o">+</span> <span class="n">user_input</span><span class="p">)</span>
<span class="w">        </span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">        </span><span class="k">break</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Leganes: Mbappe, Bellingham inspire comeback to ...&quot;, &quot;url&quot;: &quot;https://www.nbcsports.com/soccer/news/how-to-watch-real-madrid-vs-leganes-live-stream-link-tv-team-news-prediction&quot;, &quot;content&quot;: &quot;Real Madrid fought back to beat struggling Leganes 3-2 at the Santiago Bernabeu on Saturday as Kylian Mbappe scored twice and Jude&quot;, &quot;score&quot;: 0.78749067&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid vs Leganes 3-2: LaLiga \u2013 as it happened - Al Jazeera&quot;, &quot;url&quot;: &quot;https://www.aljazeera.com/sports/liveblog/2025/3/29/live-real-madrid-vs-leganes-laliga&quot;, &quot;content&quot;: &quot;Defending champions Real Madrid beat 3-2 Leganes in Spain&#x27;s LaLiga. The match at Santiago Bernabeu in Madrid, Spain saw Real trail 2-1 at half-&quot;, &quot;score&quot;: 0.7485182&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid vs Legan\u00e9s: Spanish La Liga stats &amp;amp; head-to-head - BBC&quot;, &quot;url&quot;: &quot;https://www.bbc.com/sport/football/live/cm2ndndvdgmt&quot;, &quot;content&quot;: &quot;Mbappe scores winner as Real Madrid survive Leganes scare\nMatch Summary\nSat 29 Mar 2025\n\u2027\nSpanish La Liga\nReal Madrid 3 , Legan\u00e9s 2 at Full time\nReal MadridReal MadridReal Madrid\n\n3\n2\n\nLegan\u00e9sLegan\u00e9sLegan\u00e9s\nFull time\nFT\nHalf Time Real Madrid 1 , Legan\u00e9s 2\nHT 1-2\nKey Events\nReal Madrid\n\nK. Mbapp\u00e9 (32&#x27; pen, 76&#x27;)Penalty 32 minutes, Goal 76 minutes\nJ. Bellingham (47&#x27;)Goal 47 minutes\n\nLegan\u00e9s\n\nDiego Garc\u00eda (34&#x27;)Goal 34 minutes\nDani Raba (41&#x27;)Goal 41 minutes [...] Good nightpublished at 22:14 Greenwich Mean Time 29 March\n22:14 GMT 29 March\nThanks for joining us, that was a great game.\nSee you again soon for more La Liga action.\n13\n2\nShare\nclose panel\nShare page\nCopy link\nAbout sharing\n\n\nPostpublished at 22:10 Greenwich Mean Time 29 March\n22:10 GMT 29 March\nFT: Real Madrid 3-2 Leganes [...] Postpublished at 22:02 Greenwich Mean Time 29 March\n22:02 GMT 29 March\nFT: Real Madrid 3-2 Leganes\nOver to you, Barcelona.\nHansi Flick&#x27;s side face Girona tomorrow (15:15 BST) and have the chance to regain their three point lead if they are victorious.\n18\n6\nShare\nclose panel\nShare page\nCopy link\nAbout sharing&quot;, &quot;score&quot;: 0.86413884&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3 - 2 CD Legan\u00e9s (03/29) - Game Report - 365Scores&quot;, &quot;url&quot;: &quot;https://www.365scores.com/en-us/football/match/laliga-11/cd-leganes-real-madrid-131-9242-11&quot;, &quot;content&quot;: &quot;The game between Real Madrid and CD Legan\u00e9s ended with a score of Real Madrid 3 - 2 CD Legan\u00e9s. On 365Scores, you can check all the head-to-head results between&quot;, &quot;score&quot;: 0.8524574&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid&quot;, &quot;content&quot;: &quot;Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] Match Commentary\n-Match ends, Real Madrid 3, Leganes 2.90&#x27;+9&#x27;Second Half ends, Real Madrid 3, Leganes 2.90&#x27;+7&#x27;Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half.\nFull Commentary\nMatch Stats\nRMALEG\nPossession\n70.7%\n29.3%\n\nShots on Goal\n10\n4\nShot Attempts\n24\n10\nYellow Cards\n1\n4\nCorner Kicks\n8\n3\nSaves\n2\n6\n4-2-3-1\n\n\n13\nLunin\n*   20\nGarc\u00eda\n*   22\nR\u00fcdiger\n*   35\nAsencio\n*   17\nV\u00e1zquez\n\n\n6\nCamavinga\n*   10\nModric\n\n\n21\nD\u00edaz\n\n\n5\nBellingham\n*   15\nG\u00fcler\n\n\n9\nMbapp\u00e9 [...] | Rayo Vallecano | 35 | 12 | 11 | 12 | -5 | 47 |\n| Mallorca | 35 | 13 | 8 | 14 | -7 | 47 |\n| Valencia | 35 | 11 | 12 | 12 | -8 | 45 |\n| Osasuna | 35 | 10 | 15 | 10 | -8 | 45 |\n| Real Sociedad | 35 | 12 | 7 | 16 | -9 | 43 |\n| Getafe | 35 | 10 | 9 | 16 | -3 | 39 |\n| Espanyol | 35 | 10 | 9 | 16 | -9 | 39 |\n| Girona | 35 | 10 | 8 | 17 | -12 | 38 |\n| Sevilla | 35 | 9 | 11 | 15 | -10 | 38 |\n| Alav\u00e9s | 35 | 8 | 11 | 16 | -12 | 35 |\n| Legan\u00e9s | 35 | 7 | 13 | 15 | -18 | 34 |&quot;, &quot;score&quot;: 0.93497354&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid v Leganes | March 29, 2025 | Goal.com US&quot;, &quot;url&quot;: &quot;https://www.goal.com/en-us/match/real-madrid-vs-leganes/sZTw_SnjyKCcntxKHHQI7&quot;, &quot;content&quot;: &quot;Latest news, stats and live commentary for the LaLiga&#x27;s meeting between Real Madrid v Leganes on the March 29, 2025.&quot;, &quot;score&quot;: 0.921929&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid&quot;, &quot;content&quot;: &quot;Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] Match Commentary\n-Match ends, Real Madrid 3, Leganes 2.90&#x27;+9&#x27;Second Half ends, Real Madrid 3, Leganes 2.90&#x27;+7&#x27;Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half.\nFull Commentary\nMatch Stats\nRMALEG\nPossession\n70.7%\n29.3%\n\nShots on Goal\n10\n4\nShot Attempts\n24\n10\nYellow Cards\n1\n4\nCorner Kicks\n8\n3\nSaves\n2\n6\n4-2-3-1\n\n\n13\nLunin\n*   20\nGarc\u00eda\n*   22\nR\u00fcdiger\n*   35\nAsencio\n*   17\nV\u00e1zquez\n\n\n6\nCamavinga\n*   10\nModric\n\n\n21\nD\u00edaz\n\n\n5\nBellingham\n*   15\nG\u00fcler\n\n\n9\nMbapp\u00e9 [...] Mbapp\u00e9 nets twice to maintain Madrid title hopes ------------------------------------------------ Kylian Mbapp\u00e9 struck twice to guide Real Madrid to a 3-2 home win over relegation-threatened Leganes on Saturday. Mar 29, 2025, 10:53 pm - Reuters\nMatch Timeline\nReal Madrid\nLegan\u00e9s\n\nKO\n\n32\n\n\n34\n\n\n41\n\n\nHT\n\n\n47\n\n\n62\n\n\n62\n\n\n62\n\n\n65\n\n\n66\n\n\n72\n\n\n74\n\n\n76\n\n\n81\n\n\n83\n\n\n86\n\n\n89\n\n\nFT&quot;, &quot;score&quot;: 0.96213967&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid&quot;, &quot;content&quot;: &quot;Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] Match Commentary\n-Match ends, Real Madrid 3, Leganes 2.90&#x27;+9&#x27;Second Half ends, Real Madrid 3, Leganes 2.90&#x27;+7&#x27;Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half.\nFull Commentary\nMatch Stats\nRMALEG\nPossession\n70.7%\n29.3%\n\nShots on Goal\n10\n4\nShot Attempts\n24\n10\nYellow Cards\n1\n4\nCorner Kicks\n8\n3\nSaves\n2\n6\n4-2-3-1\n\n\n13\nLunin\n*   20\nGarc\u00eda\n*   22\nR\u00fcdiger\n*   35\nAsencio\n*   17\nV\u00e1zquez\n\n\n6\nCamavinga\n*   10\nModric\n\n\n21\nD\u00edaz\n\n\n5\nBellingham\n*   15\nG\u00fcler\n\n\n9\nMbapp\u00e9 [...] -550\n\no3.5\n+105\n-1.5\n-165\nLEGLegan\u00e9sLegan\u00e9s\n(6-9-14)\n(6-9-14, 27 pts)\nu3.5\n-120\n+950\nu3.5\n-135&quot;, &quot;score&quot;: 0.9635647&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid v Leganes | March 29, 2025 | Goal.com US&quot;, &quot;url&quot;: &quot;https://www.goal.com/en-us/match/real-madrid-vs-leganes/sZTw_SnjyKCcntxKHHQI7&quot;, &quot;content&quot;: &quot;Latest news, stats and live commentary for the LaLiga&#x27;s meeting between Real Madrid v Leganes on the March 29, 2025.&quot;, &quot;score&quot;: 0.95921934&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid&quot;, &quot;content&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN Real Madrid -Match ends, Real Madrid 3, Leganes 2.90&#x27;+9&#x27;Second Half ends, Real Madrid 3, Leganes 2.90&#x27;+7&#x27;Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half. Freedom from Property StressJohn buys bay area houses | [Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;amp;utm_source=espnnetwork-espn&amp;amp;utm_medium=referral&amp;amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;amp;utm_source=espnnetwork-espn&amp;amp;utm_medium=referral&amp;amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:) Get Offer Brand-New 2-Bedroom Senior Apartment in Mountain View: You Won&#x27;t Believe the Price2-Bedroom Senior Apartment | [Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;amp;utm_source=espnnetwork-espn&amp;amp;utm_medium=referral&amp;amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;amp;utm_source=espnnetwork-espn&amp;amp;utm_medium=referral&amp;amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:) Read More | Real Madrid | 35 | 23 | 6 | 6 | +35 | 75 | Real Madrid woes continue as Vin\u00edcius J\u00fanior injury confirmed ------------------------------------------------------------- Injuries to Vin\u00edcius J\u00fanior and Lucas V\u00e1zquez added to Real Madrid&#x27;s problems on Monday. To learn more, visit \&quot;Do Not Sell or Share My Personal Information\&quot; and \&quot;Targeted Advertising\&quot; Opt-Out Rights.&quot;, &quot;score&quot;: 0.98565&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/report/_/gameId/704946&quot;, &quot;content&quot;: &quot;Real Madrid&#x27;s Kylian Mbapp\u00e9 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened&quot;, &quot;score&quot;: 0.98277&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: 
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: [&#x7B;&quot;title&quot;: &quot;Real Madrid 3 - 2 CD Legan\u00e9s (03/29) - Game Report - 365Scores&quot;, &quot;url&quot;: &quot;https://www.365scores.com/en-us/football/match/laliga-11/cd-leganes-real-madrid-131-9242-11&quot;, &quot;content&quot;: &quot;The game between Real Madrid and CD Legan\u00e9s in the Regular Season of LaLiga, held on Saturday, March 29, 2025 at Estadio Santiago Bernab\u00e9u, ended with a score&quot;, &quot;score&quot;: 0.96686727&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid&quot;, &quot;content&quot;: &quot;Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] -550\n\no3.5\n+105\n-1.5\n-165\nLEGLegan\u00e9sLegan\u00e9s\n(6-9-14)\n(6-9-14, 27 pts)\nu3.5\n-120\n+950\nu3.5\n-135 [...] Referees:\nPablo Gonz\u00e1lez Fuertes&quot;, &quot;score&quot;: 0.9595845&#x7D;]
User:  How did Real Madrid fare this weekend against Leganes in La Liga?
Assistant: Real Madrid faced CD Leganés in a La Liga match on Saturday, March 29, 2025, at the Estadio Santiago Bernabéu. The match was a thrilling encounter, with Real Madrid coming from behind to secure a 3-2 victory. 

Key points from the match include:
- **Scoreline**: Real Madrid 3, Leganés 2.
- **Goals**:
&#x20;&#x20;- **Real Madrid**: Kylian Mbappé scored twice, including a penalty, and Jude Bellingham also found the net.
&#x20;&#x20;- **Leganés**: Goals were scored by Diego García and Dani Raba.
- **Attendance**: The match was played in front of 73,641 spectators.
- **Key Moments**:
&#x20;&#x20;- Real Madrid trailed 2-1 at half-time but mounted a comeback in the second half.
&#x20;&#x20;- Mbappé&#x27;s penalty in the 32nd minute and his second goal in the 76th minute were crucial in turning the game around.
&#x20;&#x20;- Bellingham&#x27;s goal in the 47th minute shortly after the break tied the game.

This victory is significant for Real Madrid as they continue their push for the La Liga title, while Leganés remains in a difficult position, fighting against relegation.
User: Which players played the match?
Assistant: I&#x27;m sorry, but I need more information to answer your question. Could you please specify which match you&#x27;re referring to, including the sport, the teams, or any other relevant details? This will help me provide you with the correct information.
User: q
Assistant: Goodbye!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a ver que o problema é que não lembra o contexto da conversação.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Adicionar memoria ao chatbot - memoria de curto prazo, memoria dentro do fio">Adicionar memória ao chatbot - memória de curto prazo, memória dentro do fio<a class="anchor-link" href="#Adicionar memoria ao chatbot - memoria de curto prazo, memoria dentro do fio">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Nosso chatbot agora pode usar ferramentas para responder perguntas dos usuários, mas não lembra o contexto das interações anteriores. Isso limita sua capacidade de ter conversas coerentes e de múltiplos turnos.</p>
</section>
<section class="section-block-markdown-cell">
<p><code>LangGraph</code> resolve este problema através de pontos de verificação persistentes ou <code>checkpoints</code>. Se fornecermos um <code>checkpointer</code> ao compilar o gráfico e um <code>thread_id</code> ao chamar o gráfico, <code>LangGraph</code> salva automaticamente o estado após cada iteração na conversa.</p>
<p>Quando invocarmos o grafo novamente usando o mesmo <code>thread_id</code>, o grafo carregará seu estado salvo, permitindo que o chatbot continue de onde parou.</p>
</section>
<section class="section-block-markdown-cell">
<p>Veremos mais tarde que esse <code>checkpointing</code> é muito mais poderoso do que a simples memória de chat: permite salvar e retomar estados complexos a qualquer momento para recuperação de erros, fluxos de trabalho com <code>human in the loop</code>, interações no tempo e mais. Mas antes de ver tudo isso, vamos adicionar pontos de controle para permitir conversas de várias iterações.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para começar, criamos um <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver">MemorySaver</a>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>**Aviso**</p>
</blockquote>
<blockquote>
<p>> Estamos usando um <code>checkpointer</code> na memória, ou seja, ele é armazenado na RAM e quando a execução do grafo terminar, ele será excluído. Isso nos serve para este caso, pois é um exemplo para aprender a usar <code>LangGraph</code>. Em uma aplicação de produção, provavelmente será necessário alterar isso para usá-lo com <code>SqliteSaver</code> ou <code>PostgresSaver</code> e conectar-se ao nosso próprio banco de dados.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, definimos o grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>

<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>

<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos a <code>ferramenta</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, o <code>LLM</code> com as <code>bind_tools</code> e adicionamos ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Modification: tell the LLM which tools it can call</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Add the chatbot node</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x1173534d0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Antes construímos nosso próprio <code>BasicToolNode</code> para aprender como funciona, agora o substituiremos pelo método de LangGraph <code>ToolNode</code> e <code>tools_condition</code>, pois estes fazem algumas coisas boas como a execução paralela de API. Além disso, o resto é igual ao anterior.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="w"> </span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">tool</span><span class="p">])</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x1173534d0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos o nó de <code>tools_condition</code> ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">tools_condition</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x1173534d0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos o nó de <code>tools</code> ao gráfico</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x1173534d0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos o nódo de <code>START</code> ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x1173534d0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Compilamos o gráfico adicionando o <code>checkpointer</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O representamos graficamente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma configuração com um <code>thread_id</code> de um usuário</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">}}</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Hi there! My name is Maximo.&quot;</span>
<span class="w"> </span>
<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER1</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi there! My name is Maximo.
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query:  does not reside in any location,&#x7D;&#x7D;,
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Determining an individual&#x27;s tax residency status - IRS&quot;, &quot;url&quot;: &quot;https://www.irs.gov/individuals/international-taxpayers/determining-an-individuals-tax-residency-status&quot;, &quot;content&quot;: &quot;If you are not a U.S. citizen, you are considered a nonresident of the United States for U.S. tax purposes unless you meet one of two tests.&quot;, &quot;score&quot;: 0.1508904&#x7D;, &#x7B;&quot;title&quot;: &quot;Fix \&quot;Location Is Not Available\&quot;, C:\\WINDOWS\\system32 ... - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=QFD-Ptp0SJw&quot;, &quot;content&quot;: &quot;Fix Error \&quot;Location is not available\&quot; C:\\WINDOWS\\system32\\config\\systemprofile\\Desktop is unavailable. If the location is on this PC,&quot;, &quot;score&quot;: 0.07777658&#x7D;]
================================== Ai Message ==================================
Invalid Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;&#x7B;&quot;query&quot;: &quot;Arguments[&quot;image=&#x7B;&quot;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Do you remember my name?&quot;</span>
<span class="w"> </span>
<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER1</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Do you remember my name?
================================== Ai Message ==================================

Of course! You mentioned your name is Maximo.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, não passamos uma lista com as mensagens, tudo está sendo gerenciado pelo <code>checkpointer</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Se agora testarmos com outro usuário, ou seja, com outro <code>thread_id</code>, veremos que o grafo não lembra a conversa anterior.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER2_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
<span class="n">config_USER2</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER2_THREAD_ID</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Do you remember my name?&quot;</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER2</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Do you remember my name?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: Do you Remember My Name
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Sam Fender - Remember My Name (Official Video) - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=uaQm48G6IjY&quot;, &quot;content&quot;: &quot;Sam Fender - Remember My Name (Official Video) \n SamFenderVEVO \n 10743 likes \n 862209 views \n 14 Feb 2025 \n Remember My Name is a love song dedicated to my late Grandparents - they were always so fiercely proud of our family so I wrote the song in honour of them, from the perspective of my Grandad who was looking after my Grandma when she was suffering from dementia. This video is a really special one for me and I want to say thank you to everyone involved in making it. I hope you like it ❤️ [...] If I was wanting of anymore\nI’d be as greedy as those men on the hill\nBut I remain forlorn\nIn the memory of what once was\n\nChasing a cross in from the wing\nOur boy’s a whippet, he’s faster than anything\nRemember the pride that we felt\nFor the two of us made him ourselves\n\nHumour me\nMake my day\nI’ll tell you stories\nKiss your face\nAnd I’ll pray\nYou’ll remember\nMy name\n\nI’m not sure of what awaits\nWasn’t a fan of St Peter and his gates\nBut by god I pray\nThat I’ll see you in some way [...] Oh 11 Walk Avenue\nSomething to behold\nTo them it’s a council house\nTo me it’s a home\nAnd a home that you made\nWhere the grandkids could play\nBut it’s never the same without you\n\nHumour me\nMake my day\nI’ll tell you stories\nI’ll kiss your face\nAnd I’ll pray\nYou’ll remember\nMy name\n\nAnd I’ll pray you remember my name\nAnd I’ll pray you remember my name\n\n---&quot;, &quot;score&quot;: 0.6609831&#x7D;, &#x7B;&quot;title&quot;: &quot;Do You Remember My Name? - Novel Updates&quot;, &quot;url&quot;: &quot;https://www.novelupdates.com/series/do-you-remember-my-name/&quot;, &quot;content&quot;: &quot;This is a Cute, Tender, and Heartwarming High School Romance. It&#x27;s not Heavy. It&#x27;s not so Emotional too, but it does have Emotional moments. It&#x27;s story Full of&quot;, &quot;score&quot;: 0.608897&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: do you remember my name
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Sam Fender - Remember My Name (Official Video) - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=uaQm48G6IjY&quot;, &quot;content&quot;: &quot;Sam Fender - Remember My Name (Official Video) \n SamFenderVEVO \n 10743 likes \n 862209 views \n 14 Feb 2025 \n Remember My Name is a love song dedicated to my late Grandparents - they were always so fiercely proud of our family so I wrote the song in honour of them, from the perspective of my Grandad who was looking after my Grandma when she was suffering from dementia. This video is a really special one for me and I want to say thank you to everyone involved in making it. I hope you like it ❤️ [...] Oh 11 Walk Avenue\nSomething to behold\nTo them it’s a council house\nTo me it’s a home\nAnd a home that you made\nWhere the grandkids could play\nBut it’s never the same without you\n\nHumour me\nMake my day\nI’ll tell you stories\nI’ll kiss your face\nAnd I’ll pray\nYou’ll remember\nMy name\n\nAnd I’ll pray you remember my name\nAnd I’ll pray you remember my name\n\n--- [...] If I was wanting of anymore\nI’d be as greedy as those men on the hill\nBut I remain forlorn\nIn the memory of what once was\n\nChasing a cross in from the wing\nOur boy’s a whippet, he’s faster than anything\nRemember the pride that we felt\nFor the two of us made him ourselves\n\nHumour me\nMake my day\nI’ll tell you stories\nKiss your face\nAnd I’ll pray\nYou’ll remember\nMy name\n\nI’m not sure of what awaits\nWasn’t a fan of St Peter and his gates\nBut by god I pray\nThat I’ll see you in some way&quot;, &quot;score&quot;: 0.7123327&#x7D;, &#x7B;&quot;title&quot;: &quot;Do you remember my name? - song and lyrics by Alea, Mama Marjas&quot;, &quot;url&quot;: &quot;https://open.spotify.com/track/3GVBn3rEQLxZl4zJ4dG8UJ&quot;, &quot;content&quot;: &quot;Listen to Do you remember my name? on Spotify. Song · Alea, Mama Marjas · 2023.&quot;, &quot;score&quot;: 0.6506676&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: do you remember my name
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Sam Fender - Remember My Name (Official Video) - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=uaQm48G6IjY&quot;, &quot;content&quot;: &quot;Sam Fender - Remember My Name (Official Video) \n SamFenderVEVO \n 10743 likes \n 862209 views \n 14 Feb 2025 \n Remember My Name is a love song dedicated to my late Grandparents - they were always so fiercely proud of our family so I wrote the song in honour of them, from the perspective of my Grandad who was looking after my Grandma when she was suffering from dementia. This video is a really special one for me and I want to say thank you to everyone involved in making it. I hope you like it ❤️ [...] Oh 11 Walk Avenue\nSomething to behold\nTo them it’s a council house\nTo me it’s a home\nAnd a home that you made\nWhere the grandkids could play\nBut it’s never the same without you\n\nHumour me\nMake my day\nI’ll tell you stories\nI’ll kiss your face\nAnd I’ll pray\nYou’ll remember\nMy name\n\nAnd I’ll pray you remember my name\nAnd I’ll pray you remember my name\n\n--- [...] If I was wanting of anymore\nI’d be as greedy as those men on the hill\nBut I remain forlorn\nIn the memory of what once was\n\nChasing a cross in from the wing\nOur boy’s a whippet, he’s faster than anything\nRemember the pride that we felt\nFor the two of us made him ourselves\n\nHumour me\nMake my day\nI’ll tell you stories\nKiss your face\nAnd I’ll pray\nYou’ll remember\nMy name\n\nI’m not sure of what awaits\nWasn’t a fan of St Peter and his gates\nBut by god I pray\nThat I’ll see you in some way&quot;, &quot;score&quot;: 0.7123327&#x7D;, &#x7B;&quot;title&quot;: &quot;Do you remember my name? - song and lyrics by Alea, Mama Marjas&quot;, &quot;url&quot;: &quot;https://open.spotify.com/track/3GVBn3rEQLxZl4zJ4dG8UJ&quot;, &quot;content&quot;: &quot;Listen to Do you remember my name? on Spotify. Song · Alea, Mama Marjas · 2023.&quot;, &quot;score&quot;: 0.6506676&#x7D;]
================================== Ai Message ==================================

I&#x27;m here to assist you, but I don&#x27;t actually have the ability to remember names or personal information from previous conversations. How can I assist you today?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora que nosso chatbot tem ferramentas de busca e memória, vamos repetir o exemplo anterior, onde pergunto sobre o resultado do último jogo do Real Madrid na Liga e depois quais jogadores atuaram.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER3_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;3&quot;</span>
<span class="n">config_USER3</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER3_THREAD_ID</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;How did Real Madrid fare this weekend against Leganes in La Liga?&quot;</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER3</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

How did Real Madrid fare this weekend against Leganes in La Liga?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: Real Madrid vs Leganes La Liga this weekend
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca&quot;, &quot;url&quot;: &quot;https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html&quot;, &quot;content&quot;: &quot;While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday&#x27;s match having picked up 34 points from 13 matches.\n\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. [...] The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\n\nReal Madrid vs Leganes LIVE - Latest Updates\n\nMatch ends, Real Madrid 3, Leganes 2.\n\nSecond Half ends, Real Madrid 3, Leganes 2.\n\nFoul by Vinícius Júnior (Real Madrid).\n\nSeydouba Cissé (Leganes) wins a free kick in the defensive half. [...] Goal! Real Madrid 1, Leganes 1. Diego García (Leganes) left footed shot from very close range.\n\nAttempt missed. Óscar Rodríguez (Leganes) left footed shot from the centre of the box.\n\nGoal! Real Madrid 1, Leganes 0. Kylian Mbappé (Real Madrid) converts the penalty with a right footed shot.\n\nPenalty Real Madrid. Arda Güler draws a foul in the penalty area.\n\nPenalty conceded by Óscar Rodríguez (Leganes) after a foul in the penalty area.\n\nDelay over. They are ready to continue.&quot;, &quot;score&quot;: 0.8548001&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Leganés (Mar 29, 2025) Game Analysis - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/report/_/gameId/704946&quot;, &quot;content&quot;: &quot;Real Madrid\n\nLeganés\n\nMbappé nets twice to keep Real Madrid&#x27;s title hopes alive\n\nReal Madrid vs. Leganés - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Leganés, 03/30/2025\n\nReal Madrid&#x27;s Kylian Mbappé struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\&quot;We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\&quot; Leganes striker Garcia said.\n\n\&quot;Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\&quot;\n\nGame Information&quot;, &quot;score&quot;: 0.82220376&#x7D;]
================================== Ai Message ==================================

Real Madrid secured a 3-2 victory against Leganes this weekend in their La Liga match. Kylian Mbappé scored twice, including a penalty, to help his team come from behind and claim the win, keeping Real Madrid&#x27;s title hopes alive. Leganes, now sitting 18th in the table, continues to face challenges in their fight against relegation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora perguntamos pelos jogadores que participaram na partida.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Which players played the match?&quot;</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER3</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Which players played the match?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: Real Madrid vs Leganes match report players lineup
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Real Madrid vs. Leganes final score: La Liga result, updates, stats ...&quot;, &quot;url&quot;: &quot;https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-score-result-updates-stats-la-liga/8ecf730cfcb9b6c5f6693a0d&quot;, &quot;content&quot;: &quot;Real Madrid came through a topsy-turvy game with Leganes to claim a 3-2 victory and put pressure back on Barcelona in La Liga&#x27;s title race. Kylian Mbappe scored in each half either side of a Jude Bellingham goal — his first in the league since January 3 — to seal all three points for the champions after Leganes had come from behind to lead at the interval. Rodrygo won back the ball in the Leganes half and earned a free-kick on the edge of the box, and Mbappe found the bottom corner after rolling the ball short to Fran Garcia to work an angle. Leganes lead Real Madrid at the Bernabeu for the very first time! *Real Madrid starting lineup (4-3-3, right to left):* Lunin (GK) — Vazquez, Rudiger, Asencio, Garcia — Modric, Bellingham, Camavinga — B.&quot;, &quot;score&quot;: 0.88372874&#x7D;, &#x7B;&quot;title&quot;: &quot;CONFIRMED lineups: Real Madrid vs Leganés, 2025 La Liga&quot;, &quot;url&quot;: &quot;https://www.managingmadrid.com/2025/3/29/24396638/real-madrid-vs-leganes-2025-la-liga-live-online-stream&quot;, &quot;content&quot;: &quot;Real Madrid starting XI: Lunin, Vazquez, Rudiger, Asencio, Fran Garcia, Camavinga, Guler, Modric, Bellingham, Brahim, Mbappe. Leganes starting&quot;, &quot;score&quot;: 0.83452857&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: Real Madrid vs Leganes players 2025
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Player Ratings: Real Madrid 3-2 Leganes; 2025 La Liga&quot;, &quot;url&quot;: &quot;https://www.managingmadrid.com/2025/3/30/24396688/player-ratings-real-madrid-3-2-leganes-2025-la-liga&quot;, &quot;content&quot;: &quot;Raúl Asencio—7: Applauded by the Bernabeu on multiple occasions with good sweeping up defensively.\n\nFran García—6: Better on the offensive end, getting into the final third and playing some dagger crosses.\n\nEduardo Camavinga—6: Modric and Camavinga struggled to deal with Leganes counter attacks and Diego, playing as a #10 for Leganes, got the better of both of them. [...] Follow Managing Madrid online:\n\nSite search\n\nManaging Madrid main menu\n\nFiled under:\n\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nKylian Mbappe scores a brace to help Madrid secure a nervy 3-2 victory.\n\nShare this story\n\nShare\nAll sharing options for:\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nFull match player ratings below:\n\nAndriy Lunin—7: Not at fault for the goals, was left with the opposition taking a shot from near the six yard box. [...] Lucas Vázquez—4: Exposed in transition and lacking the speed and athleticism to cover the gaps he leaves when venturing forward. Needs a more “pessimistic” attitude when the ball is on the opposite flank, occupying better spots in ““rest defense”.\n\nAntonio Rudiger—5: Several unnecessary long distance shots to hurt Madrid’s rhythm and reinforce Leganes game plan. Playing with too many matches in his legs and it’s beginning to show.&quot;, &quot;score&quot;: 0.8832463&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid vs. Leganés (Mar 29, 2025) Live Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946&quot;, &quot;content&quot;: &quot;Match Formations · 13. Lunin · 20. García · 22. Rüdiger · 35. Asencio · 17. Vázquez · 5. Bellingham · 10. Modric · 6. Camavinga.&quot;, &quot;score&quot;: 0.86413884&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: Real Madrid vs Leganes starting lineup
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Starting lineups of Real Madrid and Leganés&quot;, &quot;url&quot;: &quot;https://www.realmadrid.com/en-US/news/football/first-team/latest-news/once-inicial-del-real-madrid-contra-el-leganes-29-03-2025&quot;, &quot;content&quot;: &quot;Starting lineups of Real Madrid and LeganÃ©s\n\n\n\nThe Whitesâ team is: Lunin, Lucas V., Asencio, RÃ¼diger, Fran GarcÃ­a, Arda GÃ¼ler, ModriÄ, Camavinga, Bellingham, Brahim and MbappÃ©.\n\n\n\n\n\nReal Madrid have named their starting line-up for the game against LeganÃ©s on matchday 29 of LaLiga, which will be played at the Santiago BernabÃ©u (9 pm CET). [...] Real Madrid starting line-up:\n13. Lunin\n17. Lucas V.\n35. Asencio\n22. RÃ¼diger\n20. Fran GarcÃ­a\n15. Arda GÃ¼ler\n10. ModriÄ\n6. Camavinga\n5. Bellingham\n21. Brahim\n9. MbappÃ©.\n\nSubstitutes:\n26. Fran GonzÃ¡lez\n34. Sergio Mestre\n4. Alaba\n7. Vini Jr.\n8. Valverde\n11. Rodrygo\n14. Tchouameni\n16. Endrick\n18. Vallejo\n43. Diego Aguado.\n\nLeganÃ©s starting line-up:\n13. Dmitrovic\n5. Tapia\n6. Sergio G.\n7. Ãscar\n10. Raba\n11. Cruz\n12. V. Rosier\n17. Neyou\n19. Diego G.\n20. Javi HernÃ¡ndez\n22. Nastasic. [...] Suplentes:\n1. Juan Soriano\n36. Abajas\n2. A. Alti\n3. Jorge SÃ¡enz\n8. Cisse\n9. Miguel\n14. Darko\n18. Duk\n21. R. LÃ³pez\n23. Munir\n24. Chicco\n30. I. Diomande.\n\n\n\nDownload Now\n\nOfficial App Fan\n\nReal Madrid Â© 2025 All rights reserved&quot;, &quot;score&quot;: 0.9465623&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid vs. Leganes lineups, confirmed starting 11, team news ...&quot;, &quot;url&quot;: &quot;https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-lineups-starting-11-team-news-injuries/aac757d10cc7b9a084995b4d&quot;, &quot;content&quot;: &quot;Real Madrid starting lineup (4-3-3, right to left): Lunin (GK) — Vazquez, Rudiger, Asencio, Garcia — Modric, Bellingham, Camavinga — B. Diaz,&quot;, &quot;score&quot;: 0.9224337&#x7D;]
================================== Ai Message ==================================

The starting lineup for Real Madrid in their match against Leganés was: Lunin (GK), Vázquez, Rüdiger, Asencio, Fran García, Modric, Bellingham, Camavinga, Brahim, Arda Güler, and Mbappé. Notable players like Vini Jr., Rodrygo, and Valverde were on the bench.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Depois de muito procurar, ele finalmente o encontra. Então agora temos um chatbot com <code>tools</code> e memória.</p>
</section>
<section class="section-block-markdown-cell">
<p>Até agora, criamos alguns <code>checkpoints</code> em três threads diferentes. Mas, o que entra em cada <code>checkpoint</code>? Para inspecionar o estado de um grafo para uma configuração dada, podemos usar o método <code>get_state(config)</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config_USER3</span><span class="p">)</span>
<span class="n">snapshot</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>StateSnapshot(values=&#x7B;&#x27;messages&#x27;: [HumanMessage(content=&#x27;How did Real Madrid fare this weekend against Leganes in La Liga?&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;a33f5825-1ae4-4717-ad17-8e306f35b027&#x27;), AIMessage(content=&#x27;&#x27;, additional_kwargs=&#x7B;&#x27;tool_calls&#x27;: [&#x7B;&#x27;function&#x27;: &#x7B;&#x27;arguments&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes La Liga this weekend&#x27;&#x7D;, &#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;description&#x27;: None&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;function&#x27;&#x7D;]&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 25, &#x27;prompt_tokens&#x27;: 296, &#x27;total_tokens&#x27;: 321&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-7905b5ae-5dee-4641-b012-396affde984c-0&#x27;, tool_calls=[&#x7B;&#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;args&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes La Liga this weekend&#x27;&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;&#x7D;]), ToolMessage(content=&#x27;[&#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca&quot;, &quot;url&quot;: &quot;https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html&quot;, &quot;content&quot;: &quot;While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday\&#x27;s match having picked up 34 points from 13 matches.\\n\\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. [...] The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\\n\\nReal Madrid vs Leganes LIVE - Latest Updates\\n\\nMatch ends, Real Madrid 3, Leganes 2.\\n\\nSecond Half ends, Real Madrid 3, Leganes 2.\\n\\nFoul by Vinícius Júnior (Real Madrid).\\n\\nSeydouba Cissé (Leganes) wins a free kick in the defensive half. [...] Goal! Real Madrid 1, Leganes 1. Diego García (Leganes) left footed shot from very close range.\\n\\nAttempt missed. Óscar Rodríguez (Leganes) left footed shot from the centre of the box.\\n\\nGoal! Real Madrid 1, Leganes 0. Kylian Mbappé (Real Madrid) converts the penalty with a right footed shot.\\n\\nPenalty Real Madrid. Arda Güler draws a foul in the penalty area.\\n\\nPenalty conceded by Óscar Rodríguez (Leganes) after a foul in the penalty area.\\n\\nDelay over. They are ready to continue.&quot;, &quot;score&quot;: 0.8548001&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid 3-2 Leganés (Mar 29, 2025) Game Analysis - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/report/_/gameId/704946&quot;, &quot;content&quot;: &quot;Real Madrid\\n\\nLeganés\\n\\nMbappé nets twice to keep Real Madrid\&#x27;s title hopes alive\\n\\nReal Madrid vs. Leganés - Game Highlights\\n\\nWatch the Game Highlights from Real Madrid vs. Leganés, 03/30/2025\\n\\nReal Madrid\&#x27;s Kylian Mbappé struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\\n\\n\\&quot;We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\\&quot; Leganes striker Garcia said.\\n\\n\\&quot;Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\\&quot;\\n\\nGame Information&quot;, &quot;score&quot;: 0.82220376&#x7D;]&#x27;, name=&#x27;tavily_search_results_json&#x27;, id=&#x27;0e02fce3-a6f0-4cce-9217-04c8c3219265&#x27;, tool_call_id=&#x27;0&#x27;, artifact=&#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes La Liga this weekend&#x27;, &#x27;follow_up_questions&#x27;: None, &#x27;answer&#x27;: None, &#x27;images&#x27;: [], &#x27;results&#x27;: [&#x7B;&#x27;url&#x27;: &#x27;https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html&#x27;, &#x27;title&#x27;: &#x27;Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca&#x27;, &#x27;content&#x27;: &quot;While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday&#x27;s match having picked up 34 points from 13 matches.\n\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. [...] The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\n\nReal Madrid vs Leganes LIVE - Latest Updates\n\nMatch ends, Real Madrid 3, Leganes 2.\n\nSecond Half ends, Real Madrid 3, Leganes 2.\n\nFoul by Vinícius Júnior (Real Madrid).\n\nSeydouba Cissé (Leganes) wins a free kick in the defensive half. [...] Goal! Real Madrid 1, Leganes 1. Diego García (Leganes) left footed shot from very close range.\n\nAttempt missed. Óscar Rodríguez (Leganes) left footed shot from the centre of the box.\n\nGoal! Real Madrid 1, Leganes 0. Kylian Mbappé (Real Madrid) converts the penalty with a right footed shot.\n\nPenalty Real Madrid. Arda Güler draws a foul in the penalty area.\n\nPenalty conceded by Óscar Rodríguez (Leganes) after a foul in the penalty area.\n\nDelay over. They are ready to continue.&quot;, &#x27;score&#x27;: 0.8548001, &#x27;raw_content&#x27;: None&#x7D;, &#x7B;&#x27;url&#x27;: &#x27;https://www.espn.com/soccer/report/_/gameId/704946&#x27;, &#x27;title&#x27;: &#x27;Real Madrid 3-2 Leganés (Mar 29, 2025) Game Analysis - ESPN&#x27;, &#x27;content&#x27;: &#x27;Real Madrid\n\nLeganés\n\nMbappé nets twice to keep Real Madrid\&#x27;s title hopes alive\n\nReal Madrid vs. Leganés - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Leganés, 03/30/2025\n\nReal Madrid\&#x27;s Kylian Mbappé struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n&quot;We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,&quot; Leganes striker Garcia said.\n\n&quot;Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.&quot;\n\nGame Information&#x27;, &#x27;score&#x27;: 0.82220376, &#x27;raw_content&#x27;: None&#x7D;], &#x27;response_time&#x27;: 1.47&#x7D;), AIMessage(content=&quot;Real Madrid secured a 3-2 victory against Leganes this weekend in their La Liga match. Kylian Mbappé scored twice, including a penalty, to help his team come from behind and claim the win, keeping Real Madrid&#x27;s title hopes alive. Leganes, now sitting 18th in the table, continues to face challenges in their fight against relegation.&quot;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 92, &#x27;prompt_tokens&#x27;: 1086, &#x27;total_tokens&#x27;: 1178&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-22226dda-0475-49b7-882f-fe7bd63ef025-0&#x27;), HumanMessage(content=&#x27;Which players played the match?&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;3e6d9f84-06a2-4148-8f2b-d8ef42c3bea1&#x27;), AIMessage(content=&#x27;&#x27;, additional_kwargs=&#x7B;&#x27;tool_calls&#x27;: [&#x7B;&#x27;function&#x27;: &#x7B;&#x27;arguments&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes match report players lineup&#x27;&#x7D;, &#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;description&#x27;: None&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;function&#x27;&#x7D;]&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 29, &#x27;prompt_tokens&#x27;: 1178, &#x27;total_tokens&#x27;: 1207&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-025d3235-61b9-4add-8e1b-5b1bc795a9d3-0&#x27;, tool_calls=[&#x7B;&#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;args&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes match report players lineup&#x27;&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;&#x7D;]), ToolMessage(content=&#x27;[&#x7B;&quot;title&quot;: &quot;Real Madrid vs. Leganes final score: La Liga result, updates, stats ...&quot;, &quot;url&quot;: &quot;https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-score-result-updates-stats-la-liga/8ecf730cfcb9b6c5f6693a0d&quot;, &quot;content&quot;: &quot;Real Madrid came through a topsy-turvy game with Leganes to claim a 3-2 victory and put pressure back on Barcelona in La Liga\&#x27;s title race. Kylian Mbappe scored in each half either side of a Jude Bellingham goal — his first in the league since January 3 — to seal all three points for the champions after Leganes had come from behind to lead at the interval. Rodrygo won back the ball in the Leganes half and earned a free-kick on the edge of the box, and Mbappe found the bottom corner after rolling the ball short to Fran Garcia to work an angle. Leganes lead Real Madrid at the Bernabeu for the very first time! *Real Madrid starting lineup (4-3-3, right to left):*\xa0Lunin (GK) — Vazquez, Rudiger, Asencio, Garcia — Modric, Bellingham, Camavinga — B.&quot;, &quot;score&quot;: 0.88372874&#x7D;, &#x7B;&quot;title&quot;: &quot;CONFIRMED lineups: Real Madrid vs Leganés, 2025 La Liga&quot;, &quot;url&quot;: &quot;https://www.managingmadrid.com/2025/3/29/24396638/real-madrid-vs-leganes-2025-la-liga-live-online-stream&quot;, &quot;content&quot;: &quot;Real Madrid starting XI: Lunin, Vazquez, Rudiger, Asencio, Fran Garcia, Camavinga, Guler, Modric, Bellingham, Brahim, Mbappe. Leganes starting&quot;, &quot;score&quot;: 0.83452857&#x7D;]&#x27;, name=&#x27;tavily_search_results_json&#x27;, id=&#x27;2dbc1324-2c20-406a-b2d7-a3d6fc609537&#x27;, tool_call_id=&#x27;0&#x27;, artifact=&#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes match report players lineup&#x27;, &#x27;follow_up_questions&#x27;: None, &#x27;answer&#x27;: None, &#x27;images&#x27;: [], &#x27;results&#x27;: [&#x7B;&#x27;url&#x27;: &#x27;https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-score-result-updates-stats-la-liga/8ecf730cfcb9b6c5f6693a0d&#x27;, &#x27;title&#x27;: &#x27;Real Madrid vs. Leganes final score: La Liga result, updates, stats ...&#x27;, &#x27;content&#x27;: &quot;Real Madrid came through a topsy-turvy game with Leganes to claim a 3-2 victory and put pressure back on Barcelona in La Liga&#x27;s title race. Kylian Mbappe scored in each half either side of a Jude Bellingham goal — his first in the league since January 3 — to seal all three points for the champions after Leganes had come from behind to lead at the interval. Rodrygo won back the ball in the Leganes half and earned a free-kick on the edge of the box, and Mbappe found the bottom corner after rolling the ball short to Fran Garcia to work an angle. Leganes lead Real Madrid at the Bernabeu for the very first time! *Real Madrid starting lineup (4-3-3, right to left):*\xa0Lunin (GK) — Vazquez, Rudiger, Asencio, Garcia — Modric, Bellingham, Camavinga — B.&quot;, &#x27;score&#x27;: 0.88372874, &#x27;raw_content&#x27;: None&#x7D;, &#x7B;&#x27;url&#x27;: &#x27;https://www.managingmadrid.com/2025/3/29/24396638/real-madrid-vs-leganes-2025-la-liga-live-online-stream&#x27;, &#x27;title&#x27;: &#x27;CONFIRMED lineups: Real Madrid vs Leganés, 2025 La Liga&#x27;, &#x27;content&#x27;: &#x27;Real Madrid starting XI: Lunin, Vazquez, Rudiger, Asencio, Fran Garcia, Camavinga, Guler, Modric, Bellingham, Brahim, Mbappe. Leganes starting&#x27;, &#x27;score&#x27;: 0.83452857, &#x27;raw_content&#x27;: None&#x7D;], &#x27;response_time&#x27;: 3.36&#x7D;), AIMessage(content=&#x27;&#x27;, additional_kwargs=&#x7B;&#x27;tool_calls&#x27;: [&#x7B;&#x27;function&#x27;: &#x7B;&#x27;arguments&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes players 2025&#x27;&#x7D;, &#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;description&#x27;: None&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;function&#x27;&#x7D;]&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 31, &#x27;prompt_tokens&#x27;: 1630, &#x27;total_tokens&#x27;: 1661&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-d6b4c4ff-0923-4082-9dea-7c51b2a4fc60-0&#x27;, tool_calls=[&#x7B;&#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;args&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes players 2025&#x27;&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;&#x7D;]), ToolMessage(content=&#x27;[&#x7B;&quot;title&quot;: &quot;Player Ratings: Real Madrid 3-2 Leganes; 2025 La Liga&quot;, &quot;url&quot;: &quot;https://www.managingmadrid.com/2025/3/30/24396688/player-ratings-real-madrid-3-2-leganes-2025-la-liga&quot;, &quot;content&quot;: &quot;Raúl Asencio—7: Applauded by the Bernabeu on multiple occasions with good sweeping up defensively.\\n\\nFran García—6: Better on the offensive end, getting into the final third and playing some dagger crosses.\\n\\nEduardo Camavinga—6: Modric and Camavinga struggled to deal with Leganes counter attacks and Diego, playing as a #10 for Leganes, got the better of both of them. [...] Follow Managing Madrid online:\\n\\nSite search\\n\\nManaging Madrid main menu\\n\\nFiled under:\\n\\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\\n\\nKylian Mbappe scores a brace to help Madrid secure a nervy 3-2 victory.\\n\\nShare this story\\n\\nShare\\nAll sharing options for:\\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\\n\\nFull match player ratings below:\\n\\nAndriy Lunin—7: Not at fault for the goals, was left with the opposition taking a shot from near the six yard box. [...] Lucas Vázquez—4: Exposed in transition and lacking the speed and athleticism to cover the gaps he leaves when venturing forward. Needs a more “pessimistic” attitude when the ball is on the opposite flank, occupying better spots in ““rest defense”.\\n\\nAntonio Rudiger—5: Several unnecessary long distance shots to hurt Madrid’s rhythm and reinforce Leganes game plan. Playing with too many matches in his legs and it’s beginning to show.&quot;, &quot;score&quot;: 0.8832463&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid vs. Leganés (Mar 29, 2025) Live Score - ESPN&quot;, &quot;url&quot;: &quot;https://www.espn.com/soccer/match/_/gameId/704946&quot;, &quot;content&quot;: &quot;Match Formations · 13. Lunin · 20. García · 22. Rüdiger · 35. Asencio · 17. Vázquez · 5. Bellingham · 10. Modric · 6. Camavinga.&quot;, &quot;score&quot;: 0.86413884&#x7D;]&#x27;, name=&#x27;tavily_search_results_json&#x27;, id=&#x27;ac15dd6e-09b1-4075-834e-d869f4079285&#x27;, tool_call_id=&#x27;0&#x27;, artifact=&#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes players 2025&#x27;, &#x27;follow_up_questions&#x27;: None, &#x27;answer&#x27;: None, &#x27;images&#x27;: [], &#x27;results&#x27;: [&#x7B;&#x27;url&#x27;: &#x27;https://www.managingmadrid.com/2025/3/30/24396688/player-ratings-real-madrid-3-2-leganes-2025-la-liga&#x27;, &#x27;title&#x27;: &#x27;Player Ratings: Real Madrid 3-2 Leganes; 2025 La Liga&#x27;, &#x27;content&#x27;: &#x27;Raúl Asencio—7: Applauded by the Bernabeu on multiple occasions with good sweeping up defensively.\n\nFran García—6: Better on the offensive end, getting into the final third and playing some dagger crosses.\n\nEduardo Camavinga—6: Modric and Camavinga struggled to deal with Leganes counter attacks and Diego, playing as a #10 for Leganes, got the better of both of them. [...] Follow Managing Madrid online:\n\nSite search\n\nManaging Madrid main menu\n\nFiled under:\n\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nKylian Mbappe scores a brace to help Madrid secure a nervy 3-2 victory.\n\nShare this story\n\nShare\nAll sharing options for:\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nFull match player ratings below:\n\nAndriy Lunin—7: Not at fault for the goals, was left with the opposition taking a shot from near the six yard box. [...] Lucas Vázquez—4: Exposed in transition and lacking the speed and athleticism to cover the gaps he leaves when venturing forward. Needs a more “pessimistic” attitude when the ball is on the opposite flank, occupying better spots in ““rest defense”.\n\nAntonio Rudiger—5: Several unnecessary long distance shots to hurt Madrid’s rhythm and reinforce Leganes game plan. Playing with too many matches in his legs and it’s beginning to show.&#x27;, &#x27;score&#x27;: 0.8832463, &#x27;raw_content&#x27;: None&#x7D;, &#x7B;&#x27;url&#x27;: &#x27;https://www.espn.com/soccer/match/_/gameId/704946&#x27;, &#x27;title&#x27;: &#x27;Real Madrid vs. Leganés (Mar 29, 2025) Live Score - ESPN&#x27;, &#x27;content&#x27;: &#x27;Match Formations · 13. Lunin · 20. García · 22. Rüdiger · 35. Asencio · 17. Vázquez · 5. Bellingham · 10. Modric · 6. Camavinga.&#x27;, &#x27;score&#x27;: 0.86413884, &#x27;raw_content&#x27;: None&#x7D;], &#x27;response_time&#x27;: 0.89&#x7D;), AIMessage(content=&#x27;&#x27;, additional_kwargs=&#x7B;&#x27;tool_calls&#x27;: [&#x7B;&#x27;function&#x27;: &#x7B;&#x27;arguments&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes starting lineup&#x27;&#x7D;, &#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;description&#x27;: None&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;function&#x27;&#x7D;]&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 27, &#x27;prompt_tokens&#x27;: 2212, &#x27;total_tokens&#x27;: 2239&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-68867df1-2012-47ac-9f01-42b071ef3a1f-0&#x27;, tool_calls=[&#x7B;&#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;args&#x27;: &#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes starting lineup&#x27;&#x7D;, &#x27;id&#x27;: &#x27;0&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;&#x7D;]), ToolMessage(content=&#x27;[&#x7B;&quot;title&quot;: &quot;Starting lineups of Real Madrid and Leganés&quot;, &quot;url&quot;: &quot;https://www.realmadrid.com/en-US/news/football/first-team/latest-news/once-inicial-del-real-madrid-contra-el-leganes-29-03-2025&quot;, &quot;content&quot;: &quot;Starting lineups of Real Madrid and LeganÃ©s\\n\\n\\n\\nThe Whitesâ\x80\x99 team is: Lunin, Lucas V., Asencio, RÃ¼diger, Fran GarcÃ\xada, Arda GÃ¼ler, ModriÄ\x87, Camavinga, Bellingham, Brahim and MbappÃ©.\\n\\n\\n\\n\\n\\nReal Madrid\xa0have named their starting line-up for the game against LeganÃ©s on matchday 29 of\xa0LaLiga, which will be played at the\xa0Santiago BernabÃ©u\xa0(9 pm CET). [...] Real Madrid starting line-up:\\n13. Lunin\\n17. Lucas V.\\n35. Asencio\\n22. RÃ¼diger\\n20. Fran GarcÃ\xada\\n15. Arda GÃ¼ler\\n10. ModriÄ\x87\\n6. Camavinga\\n5. Bellingham\\n21. Brahim\\n9. MbappÃ©.\\n\\nSubstitutes:\\n26. Fran GonzÃ¡lez\\n34. Sergio Mestre\\n4. Alaba\\n7. Vini Jr.\\n8. Valverde\\n11. Rodrygo\\n14. Tchouameni\\n16. Endrick\\n18. Vallejo\\n43. Diego Aguado.\\n\\nLeganÃ©s starting line-up:\\n13. Dmitrovic\\n5. Tapia\\n6. Sergio G.\\n7. Ã\x93scar\\n10. Raba\\n11. Cruz\\n12. V. Rosier\\n17. Neyou\\n19. Diego G.\\n20. Javi HernÃ¡ndez\\n22. Nastasic. [...] Suplentes:\\n1. Juan Soriano\\n36. Abajas\\n2. A. Alti\\n3. Jorge SÃ¡enz\\n8. Cisse\\n9. Miguel\\n14. Darko\\n18. Duk\\n21. R. LÃ³pez\\n23. Munir\\n24. Chicco\\n30. I. Diomande.\\n\\n\\n\\nDownload Now\\n\\nOfficial App Fan\\n\\nReal Madrid Â© 2025 All rights reserved&quot;, &quot;score&quot;: 0.9465623&#x7D;, &#x7B;&quot;title&quot;: &quot;Real Madrid vs. Leganes lineups, confirmed starting 11, team news ...&quot;, &quot;url&quot;: &quot;https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-lineups-starting-11-team-news-injuries/aac757d10cc7b9a084995b4d&quot;, &quot;content&quot;: &quot;Real Madrid starting lineup (4-3-3, right to left): Lunin (GK) — Vazquez, Rudiger, Asencio, Garcia — Modric, Bellingham, Camavinga — B. Diaz,&quot;, &quot;score&quot;: 0.9224337&#x7D;]&#x27;, name=&#x27;tavily_search_results_json&#x27;, id=&#x27;46721f2b-2df2-4da2-831a-ce94f6b4ff8f&#x27;, tool_call_id=&#x27;0&#x27;, artifact=&#x7B;&#x27;query&#x27;: &#x27;Real Madrid vs Leganes starting lineup&#x27;, &#x27;follow_up_questions&#x27;: None, &#x27;answer&#x27;: None, &#x27;images&#x27;: [], &#x27;results&#x27;: [&#x7B;&#x27;url&#x27;: &#x27;https://www.realmadrid.com/en-US/news/football/first-team/latest-news/once-inicial-del-real-madrid-contra-el-leganes-29-03-2025&#x27;, &#x27;title&#x27;: &#x27;Starting lineups of Real Madrid and Leganés&#x27;, &#x27;content&#x27;: &#x27;Starting lineups of Real Madrid and LeganÃ©s\n\n\n\nThe Whitesâ\x80\x99 team is: Lunin, Lucas V., Asencio, RÃ¼diger, Fran GarcÃ\xada, Arda GÃ¼ler, ModriÄ\x87, Camavinga, Bellingham, Brahim and MbappÃ©.\n\n\n\n\n\nReal Madrid\xa0have named their starting line-up for the game against LeganÃ©s on matchday 29 of\xa0LaLiga, which will be played at the\xa0Santiago BernabÃ©u\xa0(9 pm CET). [...] Real Madrid starting line-up:\n13. Lunin\n17. Lucas V.\n35. Asencio\n22. RÃ¼diger\n20. Fran GarcÃ\xada\n15. Arda GÃ¼ler\n10. ModriÄ\x87\n6. Camavinga\n5. Bellingham\n21. Brahim\n9. MbappÃ©.\n\nSubstitutes:\n26. Fran GonzÃ¡lez\n34. Sergio Mestre\n4. Alaba\n7. Vini Jr.\n8. Valverde\n11. Rodrygo\n14. Tchouameni\n16. Endrick\n18. Vallejo\n43. Diego Aguado.\n\nLeganÃ©s starting line-up:\n13. Dmitrovic\n5. Tapia\n6. Sergio G.\n7. Ã\x93scar\n10. Raba\n11. Cruz\n12. V. Rosier\n17. Neyou\n19. Diego G.\n20. Javi HernÃ¡ndez\n22. Nastasic. [...] Suplentes:\n1. Juan Soriano\n36. Abajas\n2. A. Alti\n3. Jorge SÃ¡enz\n8. Cisse\n9. Miguel\n14. Darko\n18. Duk\n21. R. LÃ³pez\n23. Munir\n24. Chicco\n30. I. Diomande.\n\n\n\nDownload Now\n\nOfficial App Fan\n\nReal Madrid Â© 2025 All rights reserved&#x27;, &#x27;score&#x27;: 0.9465623, &#x27;raw_content&#x27;: None&#x7D;, &#x7B;&#x27;url&#x27;: &#x27;https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-lineups-starting-11-team-news-injuries/aac757d10cc7b9a084995b4d&#x27;, &#x27;title&#x27;: &#x27;Real Madrid vs. Leganes lineups, confirmed starting 11, team news ...&#x27;, &#x27;content&#x27;: &#x27;Real Madrid starting lineup (4-3-3, right to left): Lunin (GK) — Vazquez, Rudiger, Asencio, Garcia — Modric, Bellingham, Camavinga — B. Diaz,&#x27;, &#x27;score&#x27;: 0.9224337, &#x27;raw_content&#x27;: None&#x7D;], &#x27;response_time&#x27;: 2.3&#x7D;), AIMessage(content=&#x27;The starting lineup for Real Madrid in their match against Leganés was: Lunin (GK), Vázquez, Rüdiger, Asencio, Fran García, Modric, Bellingham, Camavinga, Brahim, Arda Güler, and Mbappé. Notable players like Vini Jr., Rodrygo, and Valverde were on the bench.&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 98, &#x27;prompt_tokens&#x27;: 2954, &#x27;total_tokens&#x27;: 3052&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-0bd921c6-1d94-4a4c-9d9c-d255d301e2d5-0&#x27;)]&#x7D;, next=(), config=&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;3&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;&#x27;, &#x27;checkpoint_id&#x27;: &#x27;1f010a50-49f2-6904-800c-ec8d67fe5b92&#x27;&#x7D;&#x7D;, metadata=&#x7B;&#x27;source&#x27;: &#x27;loop&#x27;, &#x27;writes&#x27;: &#x7B;&#x27;chatbot_node&#x27;: &#x7B;&#x27;messages&#x27;: [AIMessage(content=&#x27;The starting lineup for Real Madrid in their match against Leganés was: Lunin (GK), Vázquez, Rüdiger, Asencio, Fran García, Modric, Bellingham, Camavinga, Brahim, Arda Güler, and Mbappé. Notable players like Vini Jr., Rodrygo, and Valverde were on the bench.&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x27;token_usage&#x27;: &#x7B;&#x27;completion_tokens&#x27;: 98, &#x27;prompt_tokens&#x27;: 2954, &#x27;total_tokens&#x27;: 3052&#x7D;, &#x27;model&#x27;: &#x27;&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;&#x7D;, id=&#x27;run-0bd921c6-1d94-4a4c-9d9c-d255d301e2d5-0&#x27;)]&#x7D;&#x7D;, &#x27;thread_id&#x27;: &#x27;3&#x27;, &#x27;step&#x27;: 12, &#x27;parents&#x27;: &#x7B;&#x7D;&#x7D;, created_at=&#x27;2025-04-03T16:02:18.167222+00:00&#x27;, parent_config=&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;3&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;&#x27;, &#x27;checkpoint_id&#x27;: &#x27;1f010a50-1feb-6534-800b-079c102aaa71&#x27;&#x7D;&#x7D;, tasks=())
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se quisermos ver o próximo nódo a ser processado, podemos usar o atributo <code>next</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>()
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Dado que o grafo foi concluído, <code>next</code> está vazio. Se você obtém um estado a partir de uma invocação do grafo, <code>next</code> indica qual nó será executado em seguida.</p>
</section>
<section class="section-block-markdown-cell">
<p>A captura anterior (<code>snapshot</code>) contém os valores de estado atuais, a configuração correspondente e o próximo nó (<code>next</code>) a ser processado. No nosso caso, o gráfico alcançou o estado <code>END</code>, por isso <code>next</code> está vazio.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a reescrever todo o código para que seja mais legível.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># State</span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># Create the LLM with tools</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Tool node</span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Start to build the graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes to the graph</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Display the graph</span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:
1. Check your internet connection and try again
2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`
3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Hi there! My name is Maximo.&quot;</span>
<span class="w"> </span>
<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER1</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi there! My name is Maximo.
================================== Ai Message ==================================

Hello Maximo! It&#x27;s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Do you remember my name?&quot;</span>
<span class="w"> </span>
<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config_USER1</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Do you remember my name?
================================== Ai Message ==================================

Yes, I remember your name! You mentioned it&#x27;s Maximo. It&#x27;s nice to chat with you, Maximo. How can I assist you today?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>**Parabéns!** Nosso chatbot agora pode manter o estado da conversa em todas as sessões graças ao sistema de pontos de controle (<code>checkpoints</code>) do <code>LangGraph</code>. Isso abre possibilidades para interações mais naturais e contextuais. O controle do <code>LangGraph</code> até mesmo lida com estados de grafos complexos.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Mais">Mais<a class="anchor-link" href="#Mais">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<h4 id="Chatbot com mensagem de resumo">Chatbot com mensagem de resumo<a class="anchor-link" href="#Chatbot com mensagem de resumo">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Se vamos gerenciar o contexto da conversação para não gastar muitos tokens, uma coisa que podemos fazer para melhorar a conversação é adicionar uma mensagem com o resumo da conversação. Isso pode ser útil para o exemplo anterior, no qual filtramos tanto o estado que o LLM não tem contexto suficiente.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemoveMessage</span><span class="p">,</span> <span class="n">trim_messages</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">RemoveMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">memory_saver</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w">    </span><span class="n">summary</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Print functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
<span class="w">        </span><span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
<span class="w">        </span><span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">[HumanMessage]: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">):</span>
<span class="w">        </span><span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
<span class="w">        </span><span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">[SystemMessage]: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
<span class="w">        </span><span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
<span class="w">        </span><span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">[AIMessage]: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">RemoveMessage</span><span class="p">):</span>
<span class="w">        </span><span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
<span class="w">        </span><span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">[RemoveMessage]: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
<span class="w">        </span><span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">[</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span><span class="si">}</span><span class="s2">]: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;summary&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="n">summary_lines</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;summary&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summary_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">Summary of the conversation: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">No summary of the conversation&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="n">summary</span><span class="p">:</span>
<span class="w">        </span><span class="n">summary_lines</span> <span class="o">=</span> <span class="n">summary</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summary_lines</span><span class="p">):</span>
<span class="w">            </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">Summary of the conversation: </span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">No summary of the conversation&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">filter_messages</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 1 messages (input to filter_messages) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Delete all but the 2 most recent messages if there are more than 2</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="mi">2</span><span class="p">:</span>
<span class="w">        </span><span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="w"> </span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 1 messages (output of filter_messages) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">delete_messages</span><span class="p">:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">delete_messages</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">trim_messages_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># print the messages received from filter_messages_node</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n\t</span><span class="s2">--- 2 messages (input to trim_messages) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Trim the messages based on the specified parameters</span>
<span class="w">    </span><span class="n">trimmed_messages</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
<span class="w">        </span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>       <span class="c1"># Maximum tokens allowed in the trimmed list</span>
<span class="w">        </span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">,</span>     <span class="c1"># Keep the latest messages</span>
<span class="w">        </span><span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>   <span class="c1"># Use the LLM&#39;s tokenizer to count tokens</span>
<span class="w">        </span><span class="n">allow_partial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Allow cutting messages mid-way if needed</span>
<span class="w">    </span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Identify the messages that must be removed</span>
<span class="w">    </span><span class="c1"># This is crucial: determine which messages are in &#39;state[&quot;messages&quot;]&#39; but not in &#39;trimmed_messages&#39;</span>
<span class="w">    </span><span class="n">original_ids</span> <span class="o">=</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">id</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]}</span>
<span class="w">    </span><span class="n">trimmed_ids</span> <span class="o">=</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">id</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">}</span>
<span class="w">    </span><span class="n">ids_to_remove</span> <span class="o">=</span> <span class="n">original_ids</span> <span class="o">-</span> <span class="n">trimmed_ids</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Create a RemoveMessage for each message that must be removed</span>
<span class="w">    </span><span class="n">messages_to_remove</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">msg_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">msg_id</span> <span class="ow">in</span> <span class="n">ids_to_remove</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Print the result of the trimming</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 2 messages (output of trim_messages - after trimming) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span> <span class="n">trimmed_messages</span><span class="p">:</span>
<span class="w">        </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">:</span>
<span class="w">            </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[Empty list - No messages after trimming]&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages_to_remove</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Get summary of the conversation if it exists</span>
<span class="w">    </span><span class="n">summary</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;summary&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n\t</span><span class="s2">--- 3 messages (input to chat_model_node) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># If there is a summary, add it to the system message</span>
<span class="w">    </span><span class="k">if</span> <span class="n">summary</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># Add the summary to the system message</span>
<span class="w">        </span><span class="n">system_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Summary of the conversation earlier: </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Add the system message to the messages at the beginning</span>
<span class="w">        </span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># If there is no summary, just return the messages</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 3 messages (input to chat_model_node) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the LLM with the messages</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 3 messages (output of chat_model_node) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_message</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">summarize_conversation</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Get summary of the conversation if it exists</span>
<span class="w">    </span><span class="n">summary</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;summary&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n\t</span><span class="s2">--- 4 messages (input to summarize_conversation) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># If there is a summary, add it to the system message</span>
<span class="w">    </span><span class="k">if</span> <span class="n">summary</span><span class="p">:</span>
<span class="w">        </span><span class="n">summary_message</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;This is a summary of the conversation to date: </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
<span class="w">            </span><span class="s2">&quot;Extend the summary by taking into account the new messages above.&quot;</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># If there is no summary, create a new one</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">summary_message</span> <span class="o">=</span> <span class="s2">&quot;Create a summary of the conversation above.&quot;</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 4 summary message ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">summary_lines</span> <span class="o">=</span> <span class="n">summary_message</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summary_lines</span><span class="p">):</span>
<span class="w">        </span><span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span><span class="p">:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Add prompt to the messages</span>
<span class="w">    </span><span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">summary_message</span><span class="p">)]</span>
<span class="w"> </span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 4 messages (input to summarize_conversation with summary) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">        </span><span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the LLM with the messages</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">--- 4 messages (output of summarize_conversation) ---&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">print_message</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">------------------------------------------------&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Return the summary message in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;summary&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;filter_messages_node&quot;</span><span class="p">,</span> <span class="n">filter_messages</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;trim_messages_node&quot;</span><span class="p">,</span> <span class="n">trim_messages_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;summarize_conversation_node&quot;</span><span class="p">,</span> <span class="n">summarize_conversation</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;filter_messages_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;filter_messages_node&quot;</span><span class="p">,</span> <span class="s2">&quot;trim_messages_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;trim_messages_node&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="s2">&quot;summarize_conversation_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;summarize_conversation_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory_saver</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, temos:</p>
<ul>
  <li>Função de filtro de mensagens: Se houver mais de 2 mensagens no estado, todas as mensagens são removidas, exceto as 2 últimas.</li>
  <li>Função de trimagem de mensagens: São removidas as mensagens que excedem 100 tokens.</li>
  <li>Função do chatbot: O modelo é executado com as mensagens filtradas e cortadas. Além disso, se houver um resumo, ele é adicionado à mensagem do sistema.</li>
  <li>Função de resumo: Cria um resumo da conversa.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma função para imprimir as mensagens do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Colors for the terminal</span>
<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[32m&quot;</span>
<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[33m&quot;</span>
<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[0m&quot;</span>
<span class="w"> </span>

<span class="k">def</span><span class="w"> </span><span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Initialize a flag to track if an assistant response has been printed</span>
<span class="w">    </span><span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Print the user&#39;s input immediately</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Create the user&#39;s message with the HumanMessage class</span>
<span class="w">    </span><span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Stream events from the graph execution</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">]},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="c1"># event is a dictionary mapping node names to their output</span>
<span class="w">        </span><span class="c1"># Example: {&#39;chatbot_node&#39;: {&#39;messages&#39;: [...]}} or {&#39;summarize_conversation_node&#39;: {&#39;summary&#39;: &#39;...&#39;}}</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Iterate through node name and its output</span>
<span class="w">        </span><span class="k">for</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span class="w">            </span><span class="c1"># Check if this event is from the chatbot node which should contain the assistant&#39;s reply</span>
<span class="w">            </span><span class="k">if</span> <span class="n">node_name</span> <span class="o">==</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span>
<span class="w">                </span><span class="c1"># Ensure the output format is as expected (list of messages)</span>
<span class="w">                </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">                    </span><span class="c1"># Get the messages from the event</span>
<span class="w">                    </span><span class="n">messages</span> <span class="o">=</span> <span class="n">value</span>
<span class="w">                    </span><span class="c1"># Ensure &#39;messages&#39; is a non-empty list</span>
<span class="w">                    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">                        </span><span class="c1"># Get the last message (presumably the assistant&#39;s reply)</span>
<span class="w">                        </span><span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">                        </span><span class="c1"># Ensure the message is an instance of AIMessage</span>
<span class="w">                        </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
<span class="w">                            </span><span class="c1"># Ensure the message has content to display</span>
<span class="w">                            </span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">):</span>
<span class="w">                                </span><span class="c1"># Print the assistant&#39;s message content</span>
<span class="w">                                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">last_message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">                                </span><span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Mark that we&#39;ve printed the response</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Fallback if no assistant response was printed (e.g., graph error before chatbot_node)</span>
<span class="w">    </span><span class="k">if</span> <span class="ow">not</span> <span class="n">assistant_response_printed</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">[No response generated or error occurred]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora executamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">}}</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="w">    </span><span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">User: &quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;quit&quot;</span><span class="p">,</span> <span class="s2">&quot;exit&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Exiting...&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Goodbye!&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">break</span>
<span class="w">    </span>
<span class="w">    </span><span class="n">events</span> <span class="o">=</span> <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">config_USER1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>

User: Hello
	--- 1 messages (input to filter_messages) ---
		[HumanMessage]: Hello
		No summary of the conversation
	------------------------------------------------
	--- 1 messages (output of filter_messages) ---
		No summary of the conversation
	------------------------------------------------


	--- 2 messages (input to trim_messages) ---
		[HumanMessage]: Hello
		No summary of the conversation
	------------------------------------------------
	--- 2 messages (output of trim_messages - after trimming) ---
		[HumanMessage]: Hello
		No summary of the conversation
	------------------------------------------------


	--- 3 messages (input to chat_model_node) ---
		[HumanMessage]: Hello
		No summary of the conversation
	------------------------------------------------
	--- 3 messages (input to chat_model_node) ---
		[HumanMessage]: Hello
		No summary of the conversation
	------------------------------------------------
	--- 3 messages (output of chat_model_node) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		No summary of the conversation
	------------------------------------------------
Assistant: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.


	--- 4 messages (input to summarize_conversation) ---
		[HumanMessage]: Hello
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		No summary of the conversation
	------------------------------------------------
	--- 4 summary message ---
		Create a summary of the conversation above.
		No summary of the conversation
	------------------------------------------------
	--- 4 messages (input to summarize_conversation with summary) ---
		[HumanMessage]: Hello
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: Create a summary of the conversation above.
	------------------------------------------------
	--- 4 messages (output of summarize_conversation) ---
		[AIMessage]: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
Assistant: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.


User: I am studying about langgraph, do you know it?
	--- 1 messages (input to filter_messages) ---
		[HumanMessage]: Hello
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 1 messages (output of filter_messages) ---
		[RemoveMessage]: 
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------


	--- 2 messages (input to trim_messages) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 2 messages (output of trim_messages - after trimming) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------


	--- 3 messages (input to chat_model_node) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 3 messages (input to chat_model_node) ---
		[SystemMessage]: Summary of the conversation earlier: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 3 messages (output of chat_model_node) ---
		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.
		
		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		
		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
Assistant: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.

LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:

1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.

If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!


	--- 4 messages (input to summarize_conversation) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.
		
		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		
		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 4 summary message ---
		This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		Extend the summary by taking into account the new messages above.
		Summary of the conversation: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 4 messages (input to summarize_conversation with summary) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.
		
		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		
		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		[HumanMessage]: This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		Extend the summary by taking into account the new messages above.
	------------------------------------------------
	--- 4 messages (output of summarize_conversation) ---
		[AIMessage]: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
Assistant: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.

LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:

1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.

If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!


User: I would like to know about using langsmith with huggingface llms, the integration of huggingface
	--- 1 messages (input to filter_messages) ---
		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		[HumanMessage]: I am studying about langgraph, do you know it?
		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.
		
		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		
		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 1 messages (output of filter_messages) ---
		[RemoveMessage]: 
		[RemoveMessage]: 
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------


	--- 2 messages (input to trim_messages) ---
		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.
		
		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		
		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.
		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.
		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.
		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 2 messages (output of trim_messages - after trimming) ---
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------


	--- 3 messages (input to chat_model_node) ---
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 3 messages (input to chat_model_node) ---
		[SystemMessage]: Summary of the conversation earlier: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 3 messages (output of chat_model_node) ---
		[AIMessage]: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Here’s a detailed look at how you can use LangSmith with Hugging Face models:
		
		### What is LangSmith?
		LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:
		- **Model Management**: Manage and version control your language models.
		- **Data Labeling**: Annotate and label data for training and evaluation.
		- **Model Evaluation**: Evaluate and compare different models and versions.
		- **API Integration**: Integrate with various NLP tools and platforms.
		
		### What is Hugging Face?
		Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:
		- **Pre-trained Models**: Access to a vast library of pre-trained models.
		- **Transformers Library**: A powerful library for working with transformer models.
		- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.
		
		### Integrating LangSmith with Hugging Face Models
		
		#### Step-by-Step Guide
		
		1. **Install Required Libraries**
		   Ensure you have the necessary libraries installed:
		   ```bash
		   pip install transformers datasets langsmith
		   ```
		
		2. **Load a Hugging Face Model**
		   Use the `transformers` library to load a pre-trained model:
		   ```python
		   from transformers import AutoModelForSequenceClassification, AutoTokenizer
		
		   model_name = &quot;distilbert-base-uncased&quot;
		   tokenizer = AutoTokenizer.from_pretrained(model_name)
		   model = AutoModelForSequenceClassification.from_pretrained(model_name)
		   ```
		
		3. **Integrate with LangSmith**
		   - **Initialize LangSmith Client**:
		     ```python
		     from langsmith import Client
		
		     client = Client()
		     ```
		
		   - **Create or Load a Dataset**:
		     ```python
		     from datasets import Dataset
		
		     # Example dataset
		     data = &#x7B;
		         &quot;text&quot;: [&quot;This is a positive review.&quot;, &quot;This is a negative review.&quot;],
		         &quot;label&quot;: [1, 0]
		     &#x7D;
		     dataset = Dataset.from_dict(data)
		
		     # Save dataset to LangSmith
		     dataset_id = client.create_dataset(name=&quot;my_dataset&quot;, data=dataset)
		     ```
		
		   - **Evaluate the Model**:
		     ```python
		     from langsmith import EvaluationResult
		
		     def evaluate_model(dataset, tokenizer, model):
		         results = []
		         for example in dataset:
		             inputs = tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)
		             outputs = model(**inputs)
		             predicted_label = outputs.logits.argmax().item()
		             result = EvaluationResult(
		                 example_id=example[&quot;id&quot;],
		                 predicted_label=predicted_label,
		                 ground_truth_label=example[&quot;label&quot;]
		             )
		             results.append(result)
		         return results
		
		     evaluation_results = evaluate_model(dataset, tokenizer, model)
		     ```
		
		   - **Upload Evaluation Results to LangSmith**:
		     ```python
		     client.log_results(dataset_id, evaluation_results)
		     ```
		
		4. **Visualize and Analyze Results**
		   - Use LangSmith’s web interface to visualize the evaluation results.
		   - Compare different models and versions to identify the best performing model.
		
		### Additional Tips
		- **Model Tuning**: Use Hugging Face’s `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.
		- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.
		- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.
		
		By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.
		
		If you have any specific questions or need further assistance, feel free to ask!
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
Assistant: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Here’s a detailed look at how you can use LangSmith with Hugging Face models:

### What is LangSmith?
LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:
- **Model Management**: Manage and version control your language models.
- **Data Labeling**: Annotate and label data for training and evaluation.
- **Model Evaluation**: Evaluate and compare different models and versions.
- **API Integration**: Integrate with various NLP tools and platforms.

### What is Hugging Face?
Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:
- **Pre-trained Models**: Access to a vast library of pre-trained models.
- **Transformers Library**: A powerful library for working with transformer models.
- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.

### Integrating LangSmith with Hugging Face Models

#### Step-by-Step Guide

1. **Install Required Libraries**
&#x20;&#x20;&#x20;Ensure you have the necessary libraries installed:
&#x20;&#x20;&#x20;```bash
&#x20;&#x20;&#x20;pip install transformers datasets langsmith
&#x20;&#x20;&#x20;```

2. **Load a Hugging Face Model**
&#x20;&#x20;&#x20;Use the `transformers` library to load a pre-trained model:
&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;from transformers import AutoModelForSequenceClassification, AutoTokenizer

&#x20;&#x20;&#x20;model_name = &quot;distilbert-base-uncased&quot;
&#x20;&#x20;&#x20;tokenizer = AutoTokenizer.from_pretrained(model_name)
&#x20;&#x20;&#x20;model = AutoModelForSequenceClassification.from_pretrained(model_name)
&#x20;&#x20;&#x20;```

3. **Integrate with LangSmith**
&#x20;&#x20;&#x20;- **Initialize LangSmith Client**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;from langsmith import Client

&#x20;&#x20;&#x20;&#x20;&#x20;client = Client()
&#x20;&#x20;&#x20;&#x20;&#x20;```

&#x20;&#x20;&#x20;- **Create or Load a Dataset**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;from datasets import Dataset

&#x20;&#x20;&#x20;&#x20;&#x20;# Example dataset
&#x20;&#x20;&#x20;&#x20;&#x20;data = &#x7B;
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&quot;text&quot;: [&quot;This is a positive review.&quot;, &quot;This is a negative review.&quot;],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&quot;label&quot;: [1, 0]
&#x20;&#x20;&#x20;&#x20;&#x20;&#x7D;
&#x20;&#x20;&#x20;&#x20;&#x20;dataset = Dataset.from_dict(data)

&#x20;&#x20;&#x20;&#x20;&#x20;# Save dataset to LangSmith
&#x20;&#x20;&#x20;&#x20;&#x20;dataset_id = client.create_dataset(name=&quot;my_dataset&quot;, data=dataset)
&#x20;&#x20;&#x20;&#x20;&#x20;```

&#x20;&#x20;&#x20;- **Evaluate the Model**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;from langsmith import EvaluationResult

&#x20;&#x20;&#x20;&#x20;&#x20;def evaluate_model(dataset, tokenizer, model):
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;results = []
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;for example in dataset:
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;inputs = tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;outputs = model(**inputs)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;predicted_label = outputs.logits.argmax().item()
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;result = EvaluationResult(
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;example_id=example[&quot;id&quot;],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;predicted_label=predicted_label,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;ground_truth_label=example[&quot;label&quot;]
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;results.append(result)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;return results

&#x20;&#x20;&#x20;&#x20;&#x20;evaluation_results = evaluate_model(dataset, tokenizer, model)
&#x20;&#x20;&#x20;&#x20;&#x20;```

&#x20;&#x20;&#x20;- **Upload Evaluation Results to LangSmith**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;client.log_results(dataset_id, evaluation_results)
&#x20;&#x20;&#x20;&#x20;&#x20;```

4. **Visualize and Analyze Results**
&#x20;&#x20;&#x20;- Use LangSmith’s web interface to visualize the evaluation results.
&#x20;&#x20;&#x20;- Compare different models and versions to identify the best performing model.

### Additional Tips
- **Model Tuning**: Use Hugging Face’s `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.
- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.
- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.

By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.

If you have any specific questions or need further assistance, feel free to ask!


	--- 4 messages (input to summarize_conversation) ---
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		[AIMessage]: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Here’s a detailed look at how you can use LangSmith with Hugging Face models:
		
		### What is LangSmith?
		LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:
		- **Model Management**: Manage and version control your language models.
		- **Data Labeling**: Annotate and label data for training and evaluation.
		- **Model Evaluation**: Evaluate and compare different models and versions.
		- **API Integration**: Integrate with various NLP tools and platforms.
		
		### What is Hugging Face?
		Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:
		- **Pre-trained Models**: Access to a vast library of pre-trained models.
		- **Transformers Library**: A powerful library for working with transformer models.
		- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.
		
		### Integrating LangSmith with Hugging Face Models
		
		#### Step-by-Step Guide
		
		1. **Install Required Libraries**
		   Ensure you have the necessary libraries installed:
		   ```bash
		   pip install transformers datasets langsmith
		   ```
		
		2. **Load a Hugging Face Model**
		   Use the `transformers` library to load a pre-trained model:
		   ```python
		   from transformers import AutoModelForSequenceClassification, AutoTokenizer
		
		   model_name = &quot;distilbert-base-uncased&quot;
		   tokenizer = AutoTokenizer.from_pretrained(model_name)
		   model = AutoModelForSequenceClassification.from_pretrained(model_name)
		   ```
		
		3. **Integrate with LangSmith**
		   - **Initialize LangSmith Client**:
		     ```python
		     from langsmith import Client
		
		     client = Client()
		     ```
		
		   - **Create or Load a Dataset**:
		     ```python
		     from datasets import Dataset
		
		     # Example dataset
		     data = &#x7B;
		         &quot;text&quot;: [&quot;This is a positive review.&quot;, &quot;This is a negative review.&quot;],
		         &quot;label&quot;: [1, 0]
		     &#x7D;
		     dataset = Dataset.from_dict(data)
		
		     # Save dataset to LangSmith
		     dataset_id = client.create_dataset(name=&quot;my_dataset&quot;, data=dataset)
		     ```
		
		   - **Evaluate the Model**:
		     ```python
		     from langsmith import EvaluationResult
		
		     def evaluate_model(dataset, tokenizer, model):
		         results = []
		         for example in dataset:
		             inputs = tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)
		             outputs = model(**inputs)
		             predicted_label = outputs.logits.argmax().item()
		             result = EvaluationResult(
		                 example_id=example[&quot;id&quot;],
		                 predicted_label=predicted_label,
		                 ground_truth_label=example[&quot;label&quot;]
		             )
		             results.append(result)
		         return results
		
		     evaluation_results = evaluate_model(dataset, tokenizer, model)
		     ```
		
		   - **Upload Evaluation Results to LangSmith**:
		     ```python
		     client.log_results(dataset_id, evaluation_results)
		     ```
		
		4. **Visualize and Analyze Results**
		   - Use LangSmith’s web interface to visualize the evaluation results.
		   - Compare different models and versions to identify the best performing model.
		
		### Additional Tips
		- **Model Tuning**: Use Hugging Face’s `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.
		- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.
		- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.
		
		By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.
		
		If you have any specific questions or need further assistance, feel free to ask!
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 4 summary message ---
		This is a summary of the conversation to date: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
		
		Extend the summary by taking into account the new messages above.
		Summary of the conversation: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
	--- 4 messages (input to summarize_conversation with summary) ---
		[HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface
		[AIMessage]: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Here’s a detailed look at how you can use LangSmith with Hugging Face models:
		
		### What is LangSmith?
		LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:
		- **Model Management**: Manage and version control your language models.
		- **Data Labeling**: Annotate and label data for training and evaluation.
		- **Model Evaluation**: Evaluate and compare different models and versions.
		- **API Integration**: Integrate with various NLP tools and platforms.
		
		### What is Hugging Face?
		Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:
		- **Pre-trained Models**: Access to a vast library of pre-trained models.
		- **Transformers Library**: A powerful library for working with transformer models.
		- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.
		
		### Integrating LangSmith with Hugging Face Models
		
		#### Step-by-Step Guide
		
		1. **Install Required Libraries**
		   Ensure you have the necessary libraries installed:
		   ```bash
		   pip install transformers datasets langsmith
		   ```
		
		2. **Load a Hugging Face Model**
		   Use the `transformers` library to load a pre-trained model:
		   ```python
		   from transformers import AutoModelForSequenceClassification, AutoTokenizer
		
		   model_name = &quot;distilbert-base-uncased&quot;
		   tokenizer = AutoTokenizer.from_pretrained(model_name)
		   model = AutoModelForSequenceClassification.from_pretrained(model_name)
		   ```
		
		3. **Integrate with LangSmith**
		   - **Initialize LangSmith Client**:
		     ```python
		     from langsmith import Client
		
		     client = Client()
		     ```
		
		   - **Create or Load a Dataset**:
		     ```python
		     from datasets import Dataset
		
		     # Example dataset
		     data = &#x7B;
		         &quot;text&quot;: [&quot;This is a positive review.&quot;, &quot;This is a negative review.&quot;],
		         &quot;label&quot;: [1, 0]
		     &#x7D;
		     dataset = Dataset.from_dict(data)
		
		     # Save dataset to LangSmith
		     dataset_id = client.create_dataset(name=&quot;my_dataset&quot;, data=dataset)
		     ```
		
		   - **Evaluate the Model**:
		     ```python
		     from langsmith import EvaluationResult
		
		     def evaluate_model(dataset, tokenizer, model):
		         results = []
		         for example in dataset:
		             inputs = tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)
		             outputs = model(**inputs)
		             predicted_label = outputs.logits.argmax().item()
		             result = EvaluationResult(
		                 example_id=example[&quot;id&quot;],
		                 predicted_label=predicted_label,
		                 ground_truth_label=example[&quot;label&quot;]
		             )
		             results.append(result)
		         return results
		
		     evaluation_results = evaluate_model(dataset, tokenizer, model)
		     ```
		
		   - **Upload Evaluation Results to LangSmith**:
		     ```python
		     client.log_results(dataset_id, evaluation_results)
		     ```
		
		4. **Visualize and Analyze Results**
		   - Use LangSmith’s web interface to visualize the evaluation results.
		   - Compare different models and versions to identify the best performing model.
		
		### Additional Tips
		- **Model Tuning**: Use Hugging Face’s `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.
		- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.
		- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.
		
		By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.
		
		If you have any specific questions or need further assistance, feel free to ask!
		[HumanMessage]: This is a summary of the conversation to date: Sure! Here&#x27;s an extended summary of the conversation:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		---
		
		Is there anything else you need assistance with?
		
		Extend the summary by taking into account the new messages above.
	------------------------------------------------
	--- 4 messages (output of summarize_conversation) ---
		[AIMessage]: Sure! Here&#x27;s the extended summary of the conversation to date:
		
		---
		
		**User:** Hello
		
		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		
		**User:** I am studying about langgraph, do you know it?
		
		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:
		1. Visualizing Model Architecture
		2. Comparing Models
		3. Understanding Training Processes
		4. Identifying Bottlenecks
		
		If you have specific questions or aspects of LangGraph you&#x27;re interested in, feel free to let me know!
		
		**User:** This is a summary of the conversation to date: Sure! Here&#x27;s a summary of the conversation above:
		User: Hello
		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I&#x27;m here to help.
		User: Create a summary of the conversation above.
		Qwen: [Provided the summary you are now reading.]
		
		Is there anything else you need assistance with?
		
		**Qwen:** [Extended the summary you are now reading.]
		
		**User:** Is there anything else you need assistance with?
		
		**Qwen:** If you have any more questions about LangGraph or any other topic, feel free to ask! I&#x27;m here to help with any information or assistance you need.
		
		---
		
		Is there anything else you need assistance with?
	------------------------------------------------
Assistant: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Here’s a detailed look at how you can use LangSmith with Hugging Face models:

### What is LangSmith?
LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:
- **Model Management**: Manage and version control your language models.
- **Data Labeling**: Annotate and label data for training and evaluation.
- **Model Evaluation**: Evaluate and compare different models and versions.
- **API Integration**: Integrate with various NLP tools and platforms.

### What is Hugging Face?
Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:
- **Pre-trained Models**: Access to a vast library of pre-trained models.
- **Transformers Library**: A powerful library for working with transformer models.
- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.

### Integrating LangSmith with Hugging Face Models

#### Step-by-Step Guide

1. **Install Required Libraries**
&#x20;&#x20;&#x20;Ensure you have the necessary libraries installed:
&#x20;&#x20;&#x20;```bash
&#x20;&#x20;&#x20;pip install transformers datasets langsmith
&#x20;&#x20;&#x20;```

2. **Load a Hugging Face Model**
&#x20;&#x20;&#x20;Use the `transformers` library to load a pre-trained model:
&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;from transformers import AutoModelForSequenceClassification, AutoTokenizer

&#x20;&#x20;&#x20;model_name = &quot;distilbert-base-uncased&quot;
&#x20;&#x20;&#x20;tokenizer = AutoTokenizer.from_pretrained(model_name)
&#x20;&#x20;&#x20;model = AutoModelForSequenceClassification.from_pretrained(model_name)
&#x20;&#x20;&#x20;```

3. **Integrate with LangSmith**
&#x20;&#x20;&#x20;- **Initialize LangSmith Client**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;from langsmith import Client

&#x20;&#x20;&#x20;&#x20;&#x20;client = Client()
&#x20;&#x20;&#x20;&#x20;&#x20;```

&#x20;&#x20;&#x20;- **Create or Load a Dataset**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;from datasets import Dataset

&#x20;&#x20;&#x20;&#x20;&#x20;# Example dataset
&#x20;&#x20;&#x20;&#x20;&#x20;data = &#x7B;
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&quot;text&quot;: [&quot;This is a positive review.&quot;, &quot;This is a negative review.&quot;],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&quot;label&quot;: [1, 0]
&#x20;&#x20;&#x20;&#x20;&#x20;&#x7D;
&#x20;&#x20;&#x20;&#x20;&#x20;dataset = Dataset.from_dict(data)

&#x20;&#x20;&#x20;&#x20;&#x20;# Save dataset to LangSmith
&#x20;&#x20;&#x20;&#x20;&#x20;dataset_id = client.create_dataset(name=&quot;my_dataset&quot;, data=dataset)
&#x20;&#x20;&#x20;&#x20;&#x20;```

&#x20;&#x20;&#x20;- **Evaluate the Model**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;from langsmith import EvaluationResult

&#x20;&#x20;&#x20;&#x20;&#x20;def evaluate_model(dataset, tokenizer, model):
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;results = []
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;for example in dataset:
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;inputs = tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;outputs = model(**inputs)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;predicted_label = outputs.logits.argmax().item()
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;result = EvaluationResult(
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;example_id=example[&quot;id&quot;],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;predicted_label=predicted_label,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;ground_truth_label=example[&quot;label&quot;]
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;results.append(result)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;return results

&#x20;&#x20;&#x20;&#x20;&#x20;evaluation_results = evaluate_model(dataset, tokenizer, model)
&#x20;&#x20;&#x20;&#x20;&#x20;```

&#x20;&#x20;&#x20;- **Upload Evaluation Results to LangSmith**:
&#x20;&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;&#x20;client.log_results(dataset_id, evaluation_results)
&#x20;&#x20;&#x20;&#x20;&#x20;```

4. **Visualize and Analyze Results**
&#x20;&#x20;&#x20;- Use LangSmith’s web interface to visualize the evaluation results.
&#x20;&#x20;&#x20;- Compare different models and versions to identify the best performing model.

### Additional Tips
- **Model Tuning**: Use Hugging Face’s `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.
- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.
- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.

By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.

If you have any specific questions or need further assistance, feel free to ask!
User: Exiting...
Assistant: Goodbye!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se formos até o final da conversa, podemos ver</p>
<div class='highlight'><pre><code class="language-markdown">Entendido. Estou pronto para traduzir o texto markdown para o português, mantendo a estrutura e estilo originais. Vou apenas traduzir o conteúdo textual, sem alterar enlaces, imagens, códigos de programação ou comandos de terminal. Por favor, forneça o texto que deseja traduzir.
Sim, posso ajudar com informações sobre o LangGraph! O LangGraph é um grafo de modelo de linguagem que representa as relações e conexões entre diferentes modelos de linguagem e seus componentes. Ele pode ser usado para visualizar e entender a arquitetura, os processos de treinamento e as características de desempenho de vários modelos de linguagem.
		
LangGraph pode ser particularmente útil para pesquisadores e desenvolvedores que estão trabalhando em tarefas de processamento de linguagem natural (PLN). Ele ajuda em:
		
1. **Visualização da Arquitetura do Modelo**: Fornece uma visão clara e detalhada de como diferentes componentes de um modelo de linguagem estão conectados.
2. **Comparação de Modelos**: Permite uma fácil comparação de diferentes modelos de linguagem em termos de sua estrutura, dados de treinamento e métricas de desempenho.
3. **Compreendendo Processos de Treinamento**: Ajuda a entender as dinâmicas do treinamento e o fluxo de dados através do modelo.
4. **Identificação de gargalos**: Pode ajudar na identificação de potenciais gargalos ou áreas para melhoria no modelo.
		
Se você tiver perguntas específicas ou aspectos do LangGraph que lhe interessem, sinta-se à vontade para me informar!
Gostaria de saber sobre o uso do langsmith com os modelos de linguagem do Hugging Face, a integração do Hugging Face.
Resumo da conversa: Claro! Aqui está um resumo estendido da conversa:
		
---
		
**Olá**
		
**Qwen:** Olá! Como posso ajudar você hoje? Seja para obter informações, realizar uma tarefa específica ou apenas conversar, estou aqui para ajudar.
		
**Usuário:** Estou estudando sobre langgraph, você conhece?
		
**Qwen:** Sim, posso ajudar com informações sobre o LangGraph! O LangGraph é um grafo de modelo de linguagem que representa as relações e conexões entre diferentes modelos de linguagem e seus componentes. Ele pode ser usado para visualizar e entender a arquitetura, os processos de treinamento e as características de desempenho de vários modelos de linguagem. O LangGraph pode ser particularmente útil para pesquisadores e desenvolvedores que estão trabalhando em tarefas de processamento de linguagem natural (PLN). Ele ajuda em:
1. Visualizando a Arquitetura do Modelo
2. Comparando Modelos
3. Compreendendo os Processos de Treinamento
4. Identificando Engarrafamentos
		
Se você tiver perguntas específicas ou aspectos do LangGraph que lhe interessem, sinta-se à vontade para me informar!
		
**Usuário:** Este é um resumo da conversa até o momento: Claro! Aqui está um resumo da conversa acima:
Olá
Qwen: Olá! Como posso ajudar você hoje? Seja para obter informações, realizar uma tarefa específica ou apenas conversar, estou aqui para ajudar.
Crie um resumo da conversa acima.
Qwen: [Fornecido o resumo que você está lendo agora.]
		
Há algo mais com o qual você precise de ajuda?
		
**Qwen:** [Estendeu o resumo que você está lendo agora.]
		
---
		
Há algo mais com o que você precisa de ajuda?
------------------------------------------------
</code></pre></div>
<p>Vemos que nas mensagens de estado apenas se conservam</p>
<div class='highlight'><pre><code class="language-markdown">Sim, posso ajudar com informações sobre o LangGraph! O LangGraph é um gráfico de modelo de linguagem que representa as relações e conexões entre diferentes modelos de linguagem e seus componentes. Ele pode ser usado para visualizar e entender a arquitetura, os processos de treinamento e as características de desempenho de diversos modelos de linguagem.
		
LangGraph pode ser particularmente útil para pesquisadores e desenvolvedores que estão trabalhando em tarefas de processamento de linguagem natural (PLN). Ele ajuda em:
		
1. **Visualizando a Arquitetura do Modelo**: Fornece uma visão clara e detalhada de como diferentes componentes de um modelo de linguagem estão conectados.
2. **Comparação de Modelos**: Permite uma comparação fácil de diferentes modelos de linguagem em termos de sua estrutura, dados de treinamento e métricas de desempenho.
3. **Compreendendo Processos de Treinamento**: Ajuda a compreender as dinâmicas do treinamento e o fluxo de dados através do modelo.
4. **Identificação de gargalos**: Pode ajudar na identificação de potenciais gargalos ou áreas para melhoria no modelo.
		
Se você tiver perguntas específicas ou aspectos do LangGraph que lhe interessem, sinta-se à vontade para me informar!
Gostaria de saber sobre o uso do langsmith com modelos de linguagem do Hugging Face, a integração do Hugging Face.
</code></pre></div>
<p>Isto é, a função de filtragem mantém apenas as 2 últimas mensagens.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mas tarde podemos ver</p>
<div class='highlight'><pre><code class="language-markdown">--- 2 mensagens (resultado de trim_messages - após a poda) ---
Eu gostaria de saber sobre o uso do langsmith com modelos de linguagem do Hugging Face, a integração do Hugging Face.
Resumo da conversa: Claro! Aqui está um resumo estendido da conversa:
		
---
		
**Olá**
		
**Qwen:** Olá! Como posso ajudá-lo hoje? Seja para obter informações, realizar uma tarefa específica ou apenas conversar, estou aqui para ajudar.
		
**Usuário:** Estou estudando sobre langgraph, você conhece?
		
**Qwen:** Sim, posso ajudar com informações sobre o LangGraph! O LangGraph é um gráfico de modelo de linguagem que representa as relações e conexões entre diferentes modelos de linguagem e seus componentes. Ele pode ser usado para visualizar e entender a arquitetura, os processos de treinamento e as características de desempenho de diversos modelos de linguagem. O LangGraph pode ser particularmente útil para pesquisadores e desenvolvedores que estão trabalhando em tarefas de processamento de linguagem natural (PLN). Ele ajuda em:
1. Visualizando a Arquitetura do Modelo
2. Comparando Modelos
3. Compreendendo os Processos de Treinamento
4. Identificando Engarrafamentos
		
Se você tiver perguntas específicas ou aspectos do LangGraph que lhe interessem, sinta-se à vontade para me informar!
		
**Usuário:** Este é um resumo da conversa até o momento: Claro! Aqui está um resumo da conversa acima:
Olá
Qwen: Olá! Como posso ajudar você hoje? Seja para obter informações, realizar uma tarefa específica ou apenas conversar, estou aqui para ajudar.
Crie um resumo da conversa acima.
Qwen: [Fornecido o resumo que você está lendo agora.]
		
Há algo mais com o qual você precisa de ajuda?
		
**Qwen:** [Estendeu o resumo que você está lendo agora.]
		
---
		
Há algo mais com o que você precisa de ajuda?
------------------------------------------------
</code></pre></div>
<p>Isto é, a função de trimagem remove a mensagem do assistente porque excede os 100 tokens.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mesmo eliminando mensagens, o que significa que o LLM não as tem como contexto, ainda podemos ter uma conversa graças ao resumo da conversa que estamos gerando.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Salvar estado em SQlite">Salvar estado em SQlite<a class="anchor-link" href="#Salvar estado em SQlite">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Vimos como salvar o estado do grafo na memória, mas assim que terminamos o processo, essa memória se perde, então vamos ver como salvá-la no SQLite</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro precisamos instalar o pacote de <code>sqlite</code> para LangGraph.</p>
<div class='highlight'><pre><code class="language-bash">pip install langgraph-checkpoint-sqlite
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Importamos as bibliotecas de <code>sqlite</code> e <code>langgraph-checkpoint-sqlite</code>. Antes, quando salvávamos o estado na memória usávamos <code>memory_saver</code>, agora usaremos <code>SqliteSaver</code> para salvar o estado em um banco de dados SQLite.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sqlite3</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.sqlite</span><span class="w"> </span><span class="kn">import</span> <span class="n">SqliteSaver</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="w"> </span>
<span class="c1"># Create the directory if it doesn&#39;t exist</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">&quot;state_db&quot;</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">db_path</span> <span class="o">=</span> <span class="s2">&quot;state_db/langgraph_sqlite.db&quot;</span>
<span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">db_path</span><span class="p">,</span> <span class="n">check_same_thread</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">SqliteSaver</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a criar um chatbot básico para não adicionar complexidade além da funcionalidade que queremos testar.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos a função para imprimir as mensagens do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Colors for the terminal</span>
<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[32m&quot;</span>
<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[33m&quot;</span>
<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[0m&quot;</span>
<span class="w"> </span>

<span class="k">def</span><span class="w"> </span><span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Initialize a flag to track if an assistant response has been printed</span>
<span class="w">    </span><span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Print the user&#39;s input immediately</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Create the user&#39;s message with the HumanMessage class</span>
<span class="w">    </span><span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Stream events from the graph execution</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">]},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="c1"># event is a dictionary mapping node names to their output</span>
<span class="w">        </span><span class="c1"># Example: {&#39;chatbot_node&#39;: {&#39;messages&#39;: [...]}} or {&#39;summarize_conversation_node&#39;: {&#39;summary&#39;: &#39;...&#39;}}</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Iterate through node name and its output</span>
<span class="w">        </span><span class="k">for</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span class="w">            </span><span class="c1"># Check if this event is from the chatbot node which should contain the assistant&#39;s reply</span>
<span class="w">            </span><span class="k">if</span> <span class="n">node_name</span> <span class="o">==</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span>
<span class="w">                </span><span class="c1"># Ensure the output format is as expected (list of messages)</span>
<span class="w">                </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">                    </span><span class="c1"># Get the messages from the event</span>
<span class="w">                    </span><span class="n">messages</span> <span class="o">=</span> <span class="n">value</span>
<span class="w">                    </span><span class="c1"># Ensure &#39;messages&#39; is a non-empty list</span>
<span class="w">                    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">                        </span><span class="c1"># Get the last message (presumably the assistant&#39;s reply)</span>
<span class="w">                        </span><span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">                        </span><span class="c1"># Ensure the message is an instance of AIMessage</span>
<span class="w">                        </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
<span class="w">                            </span><span class="c1"># Ensure the message has content to display</span>
<span class="w">                            </span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">):</span>
<span class="w">                                </span><span class="c1"># Print the assistant&#39;s message content</span>
<span class="w">                                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">last_message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">                                </span><span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Mark that we&#39;ve printed the response</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Fallback if no assistant response was printed (e.g., graph error before chatbot_node)</span>
<span class="w">    </span><span class="k">if</span> <span class="ow">not</span> <span class="n">assistant_response_printed</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">[No response generated or error occurred]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Executamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;USER1&quot;</span>
<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">}}</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="w">    </span><span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">User: &quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;quit&quot;</span><span class="p">,</span> <span class="s2">&quot;exit&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Exiting...&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Goodbye!&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">break</span>
<span class="w">    </span>
<span class="w">    </span><span class="n">events</span> <span class="o">=</span> <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">config_USER1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>

User: Hello, my name is Máximo
Assistant: Hello Máximo! It&#x27;s a pleasure to meet you. How can I assist you today?
User: Exiting...
Assistant: Goodbye!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se pode ver, só lhe disse como me chamo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora reiniciamos o notebook para que se eliminem todos os dados salvos na RAM do notebook e voltamos a executar o código anterior.</p>
</section>
<section class="section-block-markdown-cell">
<p>Recriamos a memória de <code>sqlite</code> com <code>SqliteSaver</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sqlite3</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.sqlite</span><span class="w"> </span><span class="kn">import</span> <span class="n">SqliteSaver</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="w"> </span>
<span class="c1"># Create the directory if it doesn&#39;t exist</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">&quot;state_db&quot;</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">db_path</span> <span class="o">=</span> <span class="s2">&quot;state_db/langgraph_sqlite.db&quot;</span>
<span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">db_path</span><span class="p">,</span> <span class="n">check_same_thread</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">SqliteSaver</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a criar o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Re definimos a função para imprimir as mensagens do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Colors for the terminal</span>
<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[32m&quot;</span>
<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[33m&quot;</span>
<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\\033</span><span class="s2">[0m&quot;</span>
<span class="w"> </span>

<span class="k">def</span><span class="w"> </span><span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># Initialize a flag to track if an assistant response has been printed</span>
<span class="w">    </span><span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Print the user&#39;s input immediately</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">user_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Create the user&#39;s message with the HumanMessage class</span>
<span class="w">    </span><span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Stream events from the graph execution</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">]},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="c1"># event is a dictionary mapping node names to their output</span>
<span class="w">        </span><span class="c1"># Example: {&#39;chatbot_node&#39;: {&#39;messages&#39;: [...]}} or {&#39;summarize_conversation_node&#39;: {&#39;summary&#39;: &#39;...&#39;}}</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Iterate through node name and its output</span>
<span class="w">        </span><span class="k">for</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span class="w">            </span><span class="c1"># Check if this event is from the chatbot node which should contain the assistant&#39;s reply</span>
<span class="w">            </span><span class="k">if</span> <span class="n">node_name</span> <span class="o">==</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span>
<span class="w">                </span><span class="c1"># Ensure the output format is as expected (list of messages)</span>
<span class="w">                </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">                    </span><span class="c1"># Get the messages from the event</span>
<span class="w">                    </span><span class="n">messages</span> <span class="o">=</span> <span class="n">value</span>
<span class="w">                    </span><span class="c1"># Ensure &#39;messages&#39; is a non-empty list</span>
<span class="w">                    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">messages</span><span class="p">:</span>
<span class="w">                        </span><span class="c1"># Get the last message (presumably the assistant&#39;s reply)</span>
<span class="w">                        </span><span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w">                        </span><span class="c1"># Ensure the message is an instance of AIMessage</span>
<span class="w">                        </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
<span class="w">                            </span><span class="c1"># Ensure the message has content to display</span>
<span class="w">                            </span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">):</span>
<span class="w">                                </span><span class="c1"># Print the assistant&#39;s message content</span>
<span class="w">                                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}{</span><span class="n">last_message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">                                </span><span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Mark that we&#39;ve printed the response</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Fallback if no assistant response was printed (e.g., graph error before chatbot_node)</span>
<span class="w">    </span><span class="k">if</span> <span class="ow">not</span> <span class="n">assistant_response_printed</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">[No response generated or error occurred]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E o executamos novamente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">&quot;USER1&quot;</span>
<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">}}</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="w">    </span><span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">User: &quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;quit&quot;</span><span class="p">,</span> <span class="s2">&quot;exit&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_GREEN</span><span class="si">}</span><span class="s2">User: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Exiting...&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">COLOR_YELLOW</span><span class="si">}</span><span class="s2">Assistant: </span><span class="si">{</span><span class="n">COLOR_RESET</span><span class="si">}</span><span class="s2">Goodbye!&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">break</span>
<span class="w">    </span>
<span class="w">    </span><span class="n">events</span> <span class="o">=</span> <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">config_USER1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>

User: What&#x27;s my name?
Assistant: Your name is Máximo. It&#x27;s nice to know and use your name as we chat. How can I assist you today, Máximo?
User: Exiting...
Assistant: Goodbye!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, conseguimos recuperar o estado do grafo do banco de dados SQLite.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Memoria de longo prazo, memoria entre threads">Memória de longo prazo, memória entre threads<a class="anchor-link" href="#Memoria de longo prazo, memoria entre threads">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>A memória é uma função cognitiva que permite às pessoas armazenar, recuperar e utilizar informações para compreender, a partir do seu passado, o seu presente e o seu futuro.</p>
<p>Existem vários tipos de <a href="https://langchain-ai.github.io/langgraph/concepts/memory">memória</a> de longo prazo que podem ser utilizados em aplicações de IA.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Introducao ao LangGraph Memory Store">Introdução ao LangGraph Memory Store<a class="anchor-link" href="#Introducao ao LangGraph Memory Store">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>LangGraph fornece o <a href="https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore">LangGraph Memory Store</a>, que é uma forma de salvar e recuperar memória a longo prazo entre diferentes threads. Dessa maneira, em uma conversa, um usuário pode indicar que gosta de algo, e em outra conversa, o chatbot pode recuperar essa informação para gerar uma resposta mais personalizada.</p>
</section>
<section class="section-block-markdown-cell">
<p>Trata-se de uma classe para armazenamentos persistentes de chave-valor (<code>key</code>-<code>value</code>).</p>
</section>
<section class="section-block-markdown-cell">
<p>Quando objetos são armazenados na memória, três coisas são necessárias:</p>
<ul>
  <li>Um <code>namespace</code> para o objeto, é feito através de uma <code>tupla</code></li>
  <li>Uma <code>key</code> única</li>
  <li>O <code>valor</code> do objeto</li>
</ul>
<p>Vamos dar uma olhada em um exemplo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="w"> </span>
<span class="n">in_memory_store</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace_for_memory</span> <span class="o">=</span> <span class="p">(</span><span class="n">user_id</span><span class="p">,</span> <span class="s2">&quot;memories&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Save a memory to namespace as key and value</span>
<span class="n">key</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>
<span class="w"> </span>
<span class="c1"># The value needs to be a dictionary  </span>
<span class="n">value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;food_preference&quot;</span> <span class="p">:</span> <span class="s2">&quot;I like pizza&quot;</span><span class="p">}</span>
<span class="w"> </span>
<span class="c1"># Save the memory</span>
<span class="n">in_memory_store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace_for_memory</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O objeto <code>in_memory_store</code> que criamos tem vários <a href="https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore">métodos</a> e um deles é <code>search</code>, que nos permite buscar por <code>namespace</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Search </span>
<span class="n">memories</span> <span class="o">=</span> <span class="n">in_memory_store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace_for_memory</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">memories</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">memories</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(list, 1)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>É uma lista de um único valor, o que faz sentido, pois armazenamos apenas um valor, então vamos vê-lo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">value</span> <span class="o">=</span> <span class="n">memories</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">value</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;namespace&#x27;: [&#x27;1&#x27;, &#x27;memories&#x27;],
 &#x27;key&#x27;: &#x27;70006131-948a-4d7a-bdce-78351c44fc4d&#x27;,
 &#x27;value&#x27;: &#x7B;&#x27;food_preference&#x27;: &#x27;I like pizza&#x27;&#x7D;,
 &#x27;created_at&#x27;: &#x27;2025-05-11T07:24:31.462465+00:00&#x27;,
 &#x27;updated_at&#x27;: &#x27;2025-05-11T07:24:31.462468+00:00&#x27;,
 &#x27;score&#x27;: None&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver sua <code>key</code> e seu <code>value</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The key, value</span>
<span class="n">memories</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">memories</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;70006131-948a-4d7a-bdce-78351c44fc4d&#x27;, &#x7B;&#x27;food_preference&#x27;: &#x27;I like pizza&#x27;&#x7D;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Também podemos usar o método <code>get</code> para obter um objeto da memória a partir de seu <code>namespace</code> e sua <code>key</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Get the memory by namespace and key</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">in_memory_store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace_for_memory</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">memory</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;namespace&#x27;: [&#x27;1&#x27;, &#x27;memories&#x27;],
 &#x27;key&#x27;: &#x27;70006131-948a-4d7a-bdce-78351c44fc4d&#x27;,
 &#x27;value&#x27;: &#x7B;&#x27;food_preference&#x27;: &#x27;I like pizza&#x27;&#x7D;,
 &#x27;created_at&#x27;: &#x27;2025-05-11T07:24:31.462465+00:00&#x27;,
 &#x27;updated_at&#x27;: &#x27;2025-05-11T07:24:31.462468+00:00&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Assim como usamos os checkpoints para a memória de curto prazo, para a memória de longo prazo vamos usar <code>LangGraph Store</code></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Chatbot com memoria de longo prazo">Chatbot com memória de longo prazo<a class="anchor-link" href="#Chatbot com memoria de longo prazo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um chatbot básico, com memória de longo prazo e memória de curto prazo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Chatbot instruction</span>
<span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a helpful assistant that can answer questions and help with tasks.</span>
<span class="s2">You have access to a long-term memory that you can use to answer questions and help with tasks.</span>
<span class="s2">Here is the memory (it may be empty): </span><span class="si">{memory}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Create new memory from the chat history and any existing memory</span>
<span class="n">CREATE_MEMORY_INSTRUCTION</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a helpful assistant that gets information from the user to personalize your responses.</span>
<span class="w"> </span>
<span class="s2"># INFORMATION FROM THE USER:</span>
<span class="si">{memory}</span>
<span class="w"> </span>
<span class="s2"># INSTRUCTIONS:</span>
<span class="s2">1. Carefully review the chat history</span>
<span class="s2">2. Identify new information from the user, such as:</span>
<span class="s2">   - Personal details (name, location)</span>
<span class="s2">   - Preferences (likes, dislikes)</span>
<span class="s2">   - Interests and hobbies</span>
<span class="s2">   - Past experiences</span>
<span class="s2">   - Goals or future plans</span>
<span class="s2">3. Combine any new information with the existing memory</span>
<span class="s2">4. Format the memory as a clear, bulleted list</span>
<span class="s2">5. If new information conflicts with existing memory, keep the most recent version</span>
<span class="w"> </span>
<span class="s2">Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.</span>
<span class="w"> </span>
<span class="s2">Based on the chat history below, please update the user information:&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load memory from the store and use it to personalize the chatbot&#39;s response.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;user_memory&quot;</span>
<span class="w">    </span><span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Extract the actual memory content if it exists and add a prefix</span>
<span class="w">    </span><span class="k">if</span> <span class="n">existing_memory</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># Value is a dictionary with a memory key</span>
<span class="w">        </span><span class="n">existing_memory_content</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">existing_memory_content</span> <span class="o">=</span> <span class="s2">&quot;No existing memory found.&quot;</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_memory_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{</span><span class="n">existing_memory_content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{</span><span class="n">existing_memory_content</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memory in the system prompt</span>
<span class="w">    </span><span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">existing_memory_content</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Respond using memory as well as the chat history</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reflect on the chat history and save a memory to the store.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve existing memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">    </span><span class="c1"># Extract the memory</span>
<span class="w">    </span><span class="k">if</span> <span class="n">existing_memory</span><span class="p">:</span>
<span class="w">        </span><span class="n">existing_memory_content</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">existing_memory_content</span> <span class="o">=</span> <span class="s2">&quot;No existing memory found.&quot;</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_memory_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memory: </span><span class="si">{</span><span class="n">existing_memory_content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memory: </span><span class="si">{</span><span class="n">existing_memory_content</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memory in the system prompt</span>
<span class="w">    </span><span class="n">system_msg</span> <span class="o">=</span> <span class="n">CREATE_MEMORY_INSTRUCTION</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">existing_memory_content</span><span class="p">)</span>
<span class="w">    </span><span class="n">new_memory</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">])</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_memory</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">[Write memory debug] New memory: </span><span class="si">{</span><span class="n">new_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">[Write memory debug] New memory: </span><span class="si">{</span><span class="n">new_memory</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Overwrite the existing memory in the store </span>
<span class="w">    </span><span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;user_memory&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Write value as a dictionary with a memory key</span>
<span class="w">    </span><span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;memory&quot;</span><span class="p">:</span> <span class="n">new_memory</span><span class="o">.</span><span class="n">content</span><span class="p">})</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connect nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;call_model&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="s2">&quot;write_memory&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Store for long-term (across-thread) memory</span>
<span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Checkpointer for short-term (within-thread) memory</span>
<span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a testá-lo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>
<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi, my name is Maximo&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi, my name is Maximo
	[Call model debug] Existing memory: No existing memory found.
================================== Ai Message ==================================

Hello Maximo! It&#x27;s nice to meet you. How can I assist you today?
	[Write memory debug] Existing memory: No existing memory found.

	[Write memory debug] New memory:  

Here&#x27;s the updated information I have about you:
- Name: Maximo
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I like to bike around San Francisco&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I like to bike around San Francisco
	[Call model debug] Existing memory:  

Here&#x27;s the updated information I have about you:
- Name: Maximo
================================== Ai Message ==================================

That sounds like a great way to explore the city! San Francisco has some fantastic biking routes. Are there any specific areas or routes you enjoy biking the most, or are you looking for some new recommendations?
	[Write memory debug] Existing memory:  

Here&#x27;s the updated information I have about you:
- Name: Maximo

	[Write memory debug] New memory:  

Here&#x27;s the updated information about you:
- Name: Maximo
- Location: San Francisco
- Interest: Biking around San Francisco
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se recuperarmos a memória de longo prazo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="n">existing_memory</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;namespace&#x27;: [&#x27;memory&#x27;, &#x27;1&#x27;],
 &#x27;key&#x27;: &#x27;user_memory&#x27;,
 &#x27;value&#x27;: &#x7B;&#x27;memory&#x27;: &quot; \n\nHere&#x27;s the updated information about you:\n- Name: Maximo\n- Location: San Francisco\n- Interest: Biking around San Francisco&quot;&#x7D;,
 &#x27;created_at&#x27;: &#x27;2025-05-11T09:41:26.739207+00:00&#x27;,
 &#x27;updated_at&#x27;: &#x27;2025-05-11T09:41:26.739211+00:00&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtemos seu valor</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre> 

Here&#x27;s the updated information about you:
- Name: Maximo
- Location: San Francisco
- Interest: Biking around San Francisco
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos começar um novo fio de conversação, mas com a mesma memória de longo prazo. Veremos que o chatbot lembra as informações do usuário.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a user ID for across-thread memory as well as a new thread ID</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi! Where would you recommend that I go biking?&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi! Where would you recommend that I go biking?
	[Call model debug] Existing memory:  

Here&#x27;s the updated information about you:
- Name: Maximo
- Location: San Francisco
- Interest: Biking around San Francisco
================================== Ai Message ==================================

Hi there! Given my interest in biking around San Francisco, I&#x27;d recommend a few great routes:

1. **Golden Gate Park**: This is a fantastic place to bike, with wide paths that are separated from vehicle traffic. You can start at the eastern end near Stow Lake and bike all the way to the western end at Ocean Beach. There are plenty of scenic spots to stop and enjoy along the way.

2. **The Embarcadero**: This route follows the waterfront from Fisherman’s Wharf to the Bay Bridge. It’s relatively flat and offers beautiful views of the San Francisco Bay and the city skyline. You can also stop by the Ferry Building for some delicious food and drinks.

3. **Presidio**: The Presidio is a large park with numerous trails that offer diverse landscapes, from forests to coastal bluffs. The Crissy Field area is especially popular for its views of the Golden Gate Bridge.

4. **Golden Gate Bridge**: Riding across the Golden Gate Bridge is a must-do experience. You can start from the San Francisco side, bike across the bridge, and then continue into Marin County for a longer ride with stunning views.

5. **Lombard Street**: While not a long ride, biking down the famous crooked section of Lombard Street can be a fun and memorable experience. Just be prepared for the steep hill on the way back up!

Each of these routes offers a unique experience, so you can choose based on your interests and the type of scenery you enjoy. Happy biking!
	[Write memory debug] Existing memory:  

Here&#x27;s the updated information about you:
- Name: Maximo
- Location: San Francisco
- Interest: Biking around San Francisco

	[Write memory debug] New memory:  😊

Let me know if you have any other questions or if you need more recommendations!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Abri um novo fio de conversa, perguntei onde poderia ir andar de bicicleta, ele se lembrou que eu tinha dito que gosto de andar de bicicleta em São Francisco e respondeu com lugares em São Francisco para os quais eu poderia ir.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Chatbot com perfil de usuario">Chatbot com perfil de usuário<a class="anchor-link" href="#Chatbot com perfil de usuario">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Nota: Vamos a fazer esta seção usando o Sonnet 3.7, pois a integração da HuggingFace não possui a funcionalidade de <code>with_structured_output</code> que fornece uma saída estruturada com uma estrutura definida.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Podemos criar tipagens para que o LLM gere uma saída com uma estrutura definida por nós.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a criar um tipagem para o perfil do usuário.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span><span class="p">,</span> <span class="n">List</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">UserProfile</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;User profile schema with typed fields&quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">user_name</span><span class="p">:</span> <span class="nb">str</span>  <span class="c1"># The user&#39;s preferred name</span>
<span class="w">    </span><span class="n">interests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>  <span class="c1"># A list of the user&#39;s interests</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora recriamos o grafo, mas agora com o tipo <code>UserProfile</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a usar <code>with_structured_output</code> para que o LLM gere uma saída com uma estrutura definida por nós, essa estrutura vamos definir com a classe <code>Subjects</code> que é uma classe do tipo <code>BaseModel</code> de <code>Pydantic</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="n">llm_with_structured_output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">UserProfile</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Chatbot instruction</span>
<span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a helpful assistant with memory that provides information about the user. </span>
<span class="s2">If you have memory for this user, use it to personalize your responses.</span>
<span class="s2">Here is the memory (it may be empty): </span><span class="si">{memory}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Create new memory from the chat history and any existing memory</span>
<span class="n">CREATE_MEMORY_INSTRUCTION</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Create or update a user profile memory based on the user&#39;s chat history. </span>
<span class="s2">This will be saved for long-term memory. If there is an existing memory, simply update it. </span>
<span class="s2">Here is the existing memory (it may be empty): </span><span class="si">{memory}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load memory from the store and use it to personalize the chatbot&#39;s response.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memories for the system prompt</span>
<span class="w">    </span><span class="k">if</span> <span class="n">existing_memory</span> <span class="ow">and</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
<span class="w">        </span><span class="n">memory_dict</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
<span class="w">        </span><span class="n">formatted_memory</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Name: </span><span class="si">{</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;user_name&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Unknown&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Interests: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;interests&#39;</span><span class="p">,</span><span class="w"> </span><span class="p">[]))</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">formatted_memory</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="c1"># if isinstance(existing_memory_content, str):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{</span><span class="n">formatted_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># else:</span>
<span class="w">    </span><span class="c1">#     print(f&quot;\t[Call model debug] Existing memory: {existing_memory_content.content}&quot;)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memory in the system prompt</span>
<span class="w">    </span><span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">formatted_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Respond using memory as well as the chat history</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reflect on the chat history and save a memory to the store.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve existing memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memories for the system prompt</span>
<span class="w">    </span><span class="k">if</span> <span class="n">existing_memory</span> <span class="ow">and</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
<span class="w">        </span><span class="n">memory_dict</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
<span class="w">        </span><span class="n">formatted_memory</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Name: </span><span class="si">{</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;user_name&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Unknown&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Interests: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;interests&#39;</span><span class="p">,</span><span class="w"> </span><span class="p">[]))</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">formatted_memory</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memory: </span><span class="si">{</span><span class="n">formatted_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">    </span><span class="c1"># Format the existing memory in the instruction</span>
<span class="w">    </span><span class="n">system_msg</span> <span class="o">=</span> <span class="n">CREATE_MEMORY_INSTRUCTION</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">formatted_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the model to produce structured output that matches the schema</span>
<span class="w">    </span><span class="n">new_memory</span> <span class="o">=</span> <span class="n">llm_with_structured_output</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">])</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] New memory: </span><span class="si">{</span><span class="n">new_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Overwrite the existing use profile memory</span>
<span class="w">    </span><span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;user_memory&quot;</span>
<span class="w">    </span><span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">new_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">MessagesState</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connect nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;call_model&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="s2">&quot;write_memory&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Store for long-term (across-thread) memory</span>
<span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Checkpointer for short-term (within-thread) memory</span>
<span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Executamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>
<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi, my name is Maximo and I like to bike around Madrid and eat salads.&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi, my name is Maximo and I like to bike around Madrid and eat salads.
	[Call model debug] Existing memory: None
================================== Ai Message ==================================

Hello Maximo! It&#x27;s nice to meet you. I see you enjoy biking around Madrid and eating salads - those are great healthy habits! Madrid has some beautiful areas to explore by bike, and the city has been improving its cycling infrastructure in recent years. 

Is there anything specific about Madrid&#x27;s cycling routes or perhaps some good places to find delicious salads in the city that you&#x27;d like to know more about? I&#x27;d be happy to help with any questions you might have.
	[Write memory debug] Existing memory: None
	[Write memory debug] New memory: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;biking&#x27;, &#x27;Madrid&#x27;, &#x27;salads&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, o LLM gerou uma saída com a estrutura definida por nós.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver como foi armazenada a memória de longo prazo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;biking&#x27;, &#x27;Madrid&#x27;, &#x27;salads&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Mais">Mais<a class="anchor-link" href="#Mais">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<h4 id="Atualizar esquemas estruturados com Trustcall">Atualizar esquemas estruturados com Trustcall<a class="anchor-link" href="#Atualizar esquemas estruturados com Trustcall">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>No exemplo anterior, criamos perfis de usuário com dados estruturados</p>
<p>Na realidade, o que acontece por baixo dos panos é a regeneração do perfil do usuário a cada interação. Isso gera um gasto desnecessário de tokens e pode fazer com que informações importantes do perfil do usuário sejam perdidas.</p>
</section>
<section class="section-block-markdown-cell">
<p>Então, para resolver isso, vamos usar a biblioteca <a href="https://github.com/hinthornw/trustcall">TrustCall</a>, que é uma biblioteca open source para atualizar esquemas JSON. Quando precisa atualizar um esquema JSON, faz isso de forma incremental, ou seja, não apaga o esquema anterior, mas vai adicionando os novos campos.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a criar um exemplo de conversação para ver como funciona.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="w"> </span>
<span class="c1"># Conversation</span>
<span class="n">conversation</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi, I&#39;m Maximo.&quot;</span><span class="p">),</span> 
<span class="w">                </span><span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Nice to meet you, Maximo.&quot;</span><span class="p">),</span> 
<span class="w">                </span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I really like playing soccer.&quot;</span><span class="p">)]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um esquema estruturado e um modelo de LLM</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span>
<span class="w"> </span>
<span class="c1"># Schema</span>
<span class="k">class</span><span class="w"> </span><span class="nc">UserProfile</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;User profile schema with typed fields&quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">user_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;The user&#39;s preferred name&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">interests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;A list of the user&#39;s interests&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Utilizamos a função <code>create_extractor</code> de <code>trustcall</code> para criar um extrator de dados estruturados</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">trustcall</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_extractor</span>
<span class="w"> </span>
<span class="c1"># Create the extractor</span>
<span class="n">trustcall_extractor</span> <span class="o">=</span> <span class="n">create_extractor</span><span class="p">(</span>
<span class="w">    </span><span class="n">llm</span><span class="p">,</span>
<span class="w">    </span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">UserProfile</span><span class="p">],</span>
<span class="w">    </span><span class="n">tool_choice</span><span class="o">=</span><span class="s2">&quot;UserProfile&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se pode ver, ao método <code>trustcall_extractor</code> é dado um llm, que será usado como motor de busca.</p>
</section>
<section class="section-block-markdown-cell">
<p>Extraímos os dados estruturados</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span>
<span class="w"> </span>
<span class="c1"># Instruction</span>
<span class="n">system_msg</span> <span class="o">=</span> <span class="s2">&quot;Extract the user profile from the following conversation&quot;</span>
<span class="w"> </span>
<span class="c1"># Invoke the extractor</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">conversation</span><span class="p">})</span>
<span class="n">result</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;messages&#x27;: [AIMessage(content=[&#x7B;&#x27;id&#x27;: &#x27;toolu_01WfgbD1fG3rJYAXGrjqjfVY&#x27;, &#x27;input&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;, &#x27;name&#x27;: &#x27;UserProfile&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;], additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x27;id&#x27;: &#x27;msg_01TEB3FeDKLAeHJtbKo5noyW&#x27;, &#x27;model&#x27;: &#x27;claude-3-7-sonnet-20250219&#x27;, &#x27;stop_reason&#x27;: &#x27;tool_use&#x27;, &#x27;stop_sequence&#x27;: None, &#x27;usage&#x27;: &#x7B;&#x27;cache_creation_input_tokens&#x27;: 0, &#x27;cache_read_input_tokens&#x27;: 0, &#x27;input_tokens&#x27;: 497, &#x27;output_tokens&#x27;: 56&#x7D;, &#x27;model_name&#x27;: &#x27;claude-3-7-sonnet-20250219&#x27;&#x7D;, id=&#x27;run-8a15289b-fd39-4a2d-878a-fa6feaa805c5-0&#x27;, tool_calls=[&#x7B;&#x27;name&#x27;: &#x27;UserProfile&#x27;, &#x27;args&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;, &#x27;id&#x27;: &#x27;toolu_01WfgbD1fG3rJYAXGrjqjfVY&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;&#x7D;], usage_metadata=&#x7B;&#x27;input_tokens&#x27;: 497, &#x27;output_tokens&#x27;: 56, &#x27;total_tokens&#x27;: 553, &#x27;input_token_details&#x27;: &#x7B;&#x27;cache_read&#x27;: 0, &#x27;cache_creation&#x27;: 0&#x7D;&#x7D;)],
 &#x27;responses&#x27;: [UserProfile(user_name=&#x27;Maximo&#x27;, interests=[&#x27;soccer&#x27;])],
 &#x27;response_metadata&#x27;: [&#x7B;&#x27;id&#x27;: &#x27;toolu_01WfgbD1fG3rJYAXGrjqjfVY&#x27;&#x7D;],
 &#x27;attempts&#x27;: 1&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver os mensagens que foram geradas para extrair os dados estruturados</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span> 
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;id&#x27;: &#x27;toolu_01WfgbD1fG3rJYAXGrjqjfVY&#x27;, &#x27;input&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;, &#x27;name&#x27;: &#x27;UserProfile&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;UserProfile (toolu_01WfgbD1fG3rJYAXGrjqjfVY)
 Call ID: toolu_01WfgbD1fG3rJYAXGrjqjfVY
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;user_name: Maximo
&#x20;&#x20;&#x20;&#x20;interests: [&#x27;soccer&#x27;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O esquema de <code>UserProfile</code> foi atualizado com o novo dado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">schema</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;responses&quot;</span><span class="p">]</span>
<span class="n">schema</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[UserProfile(user_name=&#x27;Maximo&#x27;, interests=[&#x27;soccer&#x27;])]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, o esquema é uma lista, vamos ver o tipo de dado do seu único elemento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">schema</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>__main__.UserProfile
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos convertê-lo em um dicionário com <code>model_dump</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">schema</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Graças a ter dado um LLM a <code>trustcall_extractor</code>, podemos pedir-lhe o que queremos que extraia</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a simular que continua a conversação para ver como se atualiza o esquema</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Update the conversation</span>
<span class="n">updated_conversation</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi, I&#39;m Maximo.&quot;</span><span class="p">),</span> 
<span class="w">                </span><span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Nice to meet you, Maximo.&quot;</span><span class="p">),</span> 
<span class="w">                </span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I really like playing soccer.&quot;</span><span class="p">),</span>
<span class="w">                </span><span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;It is great to play soccer! Where do you go after playing soccer?&quot;</span><span class="p">),</span>
<span class="w">                </span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I really like to go to a bakery after playing soccer.&quot;</span><span class="p">),]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Pedimos ao modelo que atualize o esquema (um <code>JSON</code>) por meio da biblioteca <code>trustcall</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Update the instruction</span>
<span class="n">system_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Update the memory (JSON doc) to incorporate new information from the following conversation&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">updated_conversation</span><span class="p">},</span> 
<span class="w">                                    </span><span class="p">{</span><span class="s2">&quot;existing&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;UserProfile&quot;</span><span class="p">:</span> <span class="n">schema</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()}})</span>
<span class="n">result</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;messages&#x27;: [AIMessage(content=[&#x7B;&#x27;id&#x27;: &#x27;toolu_01K1zTh33kXDAw1h18Yh2HBb&#x27;, &#x27;input&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;bakeries&#x27;]&#x7D;, &#x27;name&#x27;: &#x27;UserProfile&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;], additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x27;id&#x27;: &#x27;msg_01RYUJvCdzL4b8kBYKo4BtQf&#x27;, &#x27;model&#x27;: &#x27;claude-3-7-sonnet-20250219&#x27;, &#x27;stop_reason&#x27;: &#x27;tool_use&#x27;, &#x27;stop_sequence&#x27;: None, &#x27;usage&#x27;: &#x7B;&#x27;cache_creation_input_tokens&#x27;: 0, &#x27;cache_read_input_tokens&#x27;: 0, &#x27;input_tokens&#x27;: 538, &#x27;output_tokens&#x27;: 60&#x7D;, &#x27;model_name&#x27;: &#x27;claude-3-7-sonnet-20250219&#x27;&#x7D;, id=&#x27;run-06994472-5ba0-46cc-a512-5fcacce283fc-0&#x27;, tool_calls=[&#x7B;&#x27;name&#x27;: &#x27;UserProfile&#x27;, &#x27;args&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;bakeries&#x27;]&#x7D;, &#x27;id&#x27;: &#x27;toolu_01K1zTh33kXDAw1h18Yh2HBb&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;&#x7D;], usage_metadata=&#x7B;&#x27;input_tokens&#x27;: 538, &#x27;output_tokens&#x27;: 60, &#x27;total_tokens&#x27;: 598, &#x27;input_token_details&#x27;: &#x7B;&#x27;cache_read&#x27;: 0, &#x27;cache_creation&#x27;: 0&#x7D;&#x7D;)],
 &#x27;responses&#x27;: [UserProfile(user_name=&#x27;Maximo&#x27;, interests=[&#x27;soccer&#x27;, &#x27;bakeries&#x27;])],
 &#x27;response_metadata&#x27;: [&#x7B;&#x27;id&#x27;: &#x27;toolu_01K1zTh33kXDAw1h18Yh2HBb&#x27;&#x7D;],
 &#x27;attempts&#x27;: 1&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver os mensagens que foram geradas para atualizar o esquema</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span> 
<span class="w">    </span><span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;id&#x27;: &#x27;toolu_01K1zTh33kXDAw1h18Yh2HBb&#x27;, &#x27;input&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;bakeries&#x27;]&#x7D;, &#x27;name&#x27;: &#x27;UserProfile&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;UserProfile (toolu_01K1zTh33kXDAw1h18Yh2HBb)
 Call ID: toolu_01K1zTh33kXDAw1h18Yh2HBb
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;user_name: Maximo
&#x20;&#x20;&#x20;&#x20;interests: [&#x27;soccer&#x27;, &#x27;bakeries&#x27;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos o esquema atualizado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">updated_schema</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;responses&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">updated_schema</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;bakeries&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Chatbot com perfil de usuario atualizado com Trustcall">Chatbot com perfil de usuário atualizado com Trustcall<a class="anchor-link" href="#Chatbot com perfil de usuario atualizado com Trustcall">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a criar o grafo que atualiza o perfil do usuário, mas agora com a biblioteca <code>trustcall</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trustcall</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_extractor</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Schema </span>
<span class="k">class</span><span class="w"> </span><span class="nc">UserProfile</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Profile of a user &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">user_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;The user&#39;s preferred name&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">user_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;The user&#39;s location&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">interests</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;A list of the user&#39;s interests&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create the extractor</span>
<span class="n">trustcall_extractor</span> <span class="o">=</span> <span class="n">create_extractor</span><span class="p">(</span>
<span class="w">    </span><span class="n">llm</span><span class="p">,</span>
<span class="w">    </span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">UserProfile</span><span class="p">],</span>
<span class="w">    </span><span class="n">tool_choice</span><span class="o">=</span><span class="s2">&quot;UserProfile&quot;</span><span class="p">,</span> <span class="c1"># Enforces use of the UserProfile tool</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Chatbot instruction</span>
<span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a helpful assistant with memory that provides information about the user. </span>
<span class="s2">If you have memory for this user, use it to personalize your responses.</span>
<span class="s2">Here is the memory (it may be empty): </span><span class="si">{memory}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Create new memory from the chat history and any existing memory</span>
<span class="n">TRUSTCALL_INSTRUCTION</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Create or update the memory (JSON doc) to incorporate information from the following conversation:&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load memory from the store and use it to personalize the chatbot&#39;s response.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load memory from the store and use it to personalize the chatbot&#39;s response.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memories for the system prompt</span>
<span class="w">    </span><span class="k">if</span> <span class="n">existing_memory</span> <span class="ow">and</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
<span class="w">        </span><span class="n">memory_dict</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
<span class="w">        </span><span class="n">formatted_memory</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Name: </span><span class="si">{</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;user_name&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Unknown&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Location: </span><span class="si">{</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;user_location&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Unknown&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="w">            </span><span class="sa">f</span><span class="s2">&quot;Interests: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;interests&#39;</span><span class="p">,</span><span class="w"> </span><span class="p">[]))</span><span class="si">}</span><span class="s2">&quot;</span>      
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">formatted_memory</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{</span><span class="n">formatted_memory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memory in the system prompt</span>
<span class="w">    </span><span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">formatted_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Respond using memory as well as the chat history</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reflect on the chat history and save a memory to the store.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve existing memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">    </span><span class="c1"># Get the profile as the value from the list, and convert it to a JSON doc</span>
<span class="w">    </span><span class="n">existing_profile</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;UserProfile&quot;</span><span class="p">:</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">}</span> <span class="k">if</span> <span class="n">existing_memory</span> <span class="k">else</span> <span class="kc">None</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing profile: </span><span class="si">{</span><span class="n">existing_profile</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Invoke the extractor</span>
<span class="w">    </span><span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">TRUSTCALL_INSTRUCTION</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span> <span class="s2">&quot;existing&quot;</span><span class="p">:</span> <span class="n">existing_profile</span><span class="p">})</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the updated profile as a JSON object</span>
<span class="w">    </span><span class="n">updated_profile</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;responses&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Updated profile: </span><span class="si">{</span><span class="n">updated_profile</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Save the updated profile</span>
<span class="w">    </span><span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;user_memory&quot;</span>
<span class="w">    </span><span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">updated_profile</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">MessagesState</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connect nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;call_model&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="s2">&quot;write_memory&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Store for long-term (across-thread) memory</span>
<span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Checkpointer for short-term (within-thread) memory</span>
<span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Iniciamos a conversação</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>
<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi, my name is Maximo&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi, my name is Maximo
	[Call model debug] Existing memory: None
================================== Ai Message ==================================

Hello Maximo! It&#x27;s nice to meet you. How can I help you today? Whether you have questions, need information, or just want to chat, I&#x27;m here to assist you. Is there something specific you&#x27;d like to talk about?
	[Write memory debug] Existing profile: None
	[Write memory debug] Updated profile: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;&amp;lt;UNKNOWN&amp;gt;&#x27;, &#x27;interests&#x27;: []&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, o perfil do usuário não possui nem localização nem interesses definidos. Vamos atualizar o perfil do usuário.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I like to play soccer and I live in Madrid&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I like to play soccer and I live in Madrid
	[Call model debug] Existing memory: Name: Maximo
Location: &amp;lt;UNKNOWN&amp;gt;
Interests: 
================================== Ai Message ==================================

Hello Maximo! It&#x27;s great to learn that you live in Madrid and enjoy playing soccer. Madrid is a fantastic city with a rich soccer culture, being home to world-famous clubs like Real Madrid and Atlético Madrid.

Soccer is truly a way of life in Spain, so you&#x27;re in a perfect location for your interest. Do you support any particular team in Madrid? Or perhaps you enjoy playing soccer recreationally in the city&#x27;s parks and facilities?

Is there anything specific about Madrid or soccer you&#x27;d like to discuss further?
	[Write memory debug] Existing profile: &#x7B;&#x27;UserProfile&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;&amp;lt;UNKNOWN&amp;gt;&#x27;, &#x27;interests&#x27;: []&#x7D;&#x7D;
	[Write memory debug] Updated profile: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;Madrid&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Atualizou o perfil com a localização e os interesses do usuário</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória atualizada</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="n">existing_memory</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;namespace&#x27;: [&#x27;memory&#x27;, &#x27;1&#x27;],
 &#x27;key&#x27;: &#x27;user_memory&#x27;,
 &#x27;value&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;,
&#x20;&#x20;&#x27;user_location&#x27;: &#x27;Madrid&#x27;,
&#x20;&#x20;&#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;,
 &#x27;created_at&#x27;: &#x27;2025-05-12T17:35:03.583258+00:00&#x27;,
 &#x27;updated_at&#x27;: &#x27;2025-05-12T17:35:03.583259+00:00&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos o esquema com o perfil do usuário atualizado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The user profile saved as a JSON object</span>
<span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;Madrid&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a adicionar um novo interesse do usuário</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I also like to play basketball&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I also like to play basketball
	[Call model debug] Existing memory: Name: Maximo
Location: Madrid
Interests: soccer
================================== Ai Message ==================================

That&#x27;s great to know, Maximo! It&#x27;s nice that you enjoy both soccer and basketball. Basketball is also quite popular in Spain, with Liga ACB being one of the strongest basketball leagues in Europe. 

In Madrid, you have the opportunity to follow Real Madrid&#x27;s basketball section, which is one of the most successful basketball teams in Europe. The city offers plenty of courts and facilities where you can play basketball too.

Do you play basketball casually with friends, or are you part of any local leagues in Madrid? And how do you balance your time between soccer and basketball?
	[Write memory debug] Existing profile: &#x7B;&#x27;UserProfile&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;Madrid&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;]&#x7D;&#x7D;
	[Write memory debug] Updated profile: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;Madrid&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;basketball&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a ver a memória atualizada</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;user_memory&quot;</span><span class="p">)</span>
<span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;,
 &#x27;user_location&#x27;: &#x27;Madrid&#x27;,
 &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;basketball&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionou corretamente o novo interesse do usuário.</p>
</section>
<section class="section-block-markdown-cell">
<p>Com essa memória de longo prazo armazenada, podemos iniciar uma nova thread e o chatbot terá acesso ao nosso perfil atualizado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>
<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;What soccer players do you recommend for me?&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

What soccer players do you recommend for me?
	[Call model debug] Existing memory: Name: Maximo
Location: Madrid
Interests: soccer, basketball
================================== Ai Message ==================================

Based on your interest in soccer, I can recommend some players who might appeal to you. Since you&#x27;re from Madrid, you might already follow Real Madrid or Atlético Madrid players, but here are some recommendations:

From La Liga:
- Vinícius Júnior and Jude Bellingham (Real Madrid)
- Antoine Griezmann (Atlético Madrid)
- Robert Lewandowski (Barcelona)
- Lamine Yamal (Barcelona&#x27;s young talent)

International stars:
- Kylian Mbappé
- Erling Haaland
- Mohamed Salah
- Kevin De Bruyne

You might also enjoy watching players with creative playing styles since you&#x27;re interested in basketball as well, which is a sport that values creativity and flair - players like Rodrigo De Paul or João Félix.

Is there a particular league or playing style you prefer in soccer?
	[Write memory debug] Existing profile: &#x7B;&#x27;UserProfile&#x27;: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;Madrid&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;basketball&#x27;]&#x7D;&#x7D;
	[Write memory debug] Updated profile: &#x7B;&#x27;user_name&#x27;: &#x27;Maximo&#x27;, &#x27;user_location&#x27;: &#x27;Madrid&#x27;, &#x27;interests&#x27;: [&#x27;soccer&#x27;, &#x27;basketball&#x27;]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como sabe que eu moro em Madrid, primeiro me sugeriu jogadores de futebol da LaLiga espanhola. E depois me sugeriu jogadores de outras ligas.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Chatbot com colecoes de documentos de usuario atualizadas com Trustcall">Chatbot com coleções de documentos de usuário atualizadas com Trustcall<a class="anchor-link" href="#Chatbot com colecoes de documentos de usuario atualizadas com Trustcall">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Outra abordagem é, em vez de salvar o perfil do usuário em um único documento, salvar uma coleção de documentos, desta forma não estamos presos a um único esquema fechado.</p>
<p>Vamos a ver como fazer isso.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">merge_message_runs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.store.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trustcall</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_extractor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Memory schema</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Memory</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A memory item representing a piece of information learned about the user.&quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">content</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;The main content of the memory. For example: User expressed interest in learning about French.&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create the extractor</span>
<span class="n">trustcall_extractor</span> <span class="o">=</span> <span class="n">create_extractor</span><span class="p">(</span>
<span class="w">    </span><span class="n">llm</span><span class="p">,</span>
<span class="w">    </span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">Memory</span><span class="p">],</span>
<span class="w">    </span><span class="n">tool_choice</span><span class="o">=</span><span class="s2">&quot;Memory&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="c1"># This allows the extractor to insert new memories</span>
<span class="w">    </span><span class="n">enable_inserts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Chatbot instruction</span>
<span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a helpful chatbot. You are designed to be a companion to a user. </span>
<span class="s2">You have a long term memory which keeps track of information you learn about the user over time.</span>
<span class="s2">Current Memory (may include updated memories from this conversation): </span>
<span class="si">{memory}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Create new memory from the chat history and any existing memory</span>
<span class="n">TRUSTCALL_INSTRUCTION</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Reflect on following interaction. </span>
<span class="s2">Use the provided tools to retain any necessary memories about the user. </span>
<span class="s2">Use parallel tool calling to handle updates and insertions simultaneously:&quot;&quot;&quot;</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load memory from the store and use it to personalize the chatbot&#39;s response.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve memory from the store</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memories&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w">    </span><span class="n">memories</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Call model debug] Memories: </span><span class="si">{</span><span class="n">memories</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the memories for the system prompt</span>
<span class="w">    </span><span class="n">info</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">mem</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">mem</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">)</span>
<span class="w">    </span><span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">info</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Respond using memory as well as the chat history</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">}</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reflect on the chat history and save a memory to the store.&quot;&quot;&quot;</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># Get the user ID from the config</span>
<span class="w">    </span><span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s2">&quot;user_id&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Define the namespace for the memories</span>
<span class="w">    </span><span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memories&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Retrieve the most recent memories for context</span>
<span class="w">    </span><span class="n">existing_items</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Format the existing memories for the Trustcall extractor</span>
<span class="w">    </span><span class="n">tool_name</span> <span class="o">=</span> <span class="s2">&quot;Memory&quot;</span>
<span class="w">    </span><span class="n">existing_memories</span> <span class="o">=</span> <span class="p">([(</span><span class="n">existing_item</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">tool_name</span><span class="p">,</span> <span class="n">existing_item</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="w">                          </span><span class="k">for</span> <span class="n">existing_item</span> <span class="ow">in</span> <span class="n">existing_items</span><span class="p">]</span>
<span class="w">                          </span><span class="k">if</span> <span class="n">existing_items</span>
<span class="w">                          </span><span class="k">else</span> <span class="kc">None</span>
<span class="w">                        </span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memories: </span><span class="si">{</span><span class="n">existing_memories</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Merge the chat history and the instruction</span>
<span class="w">    </span><span class="n">updated_messages</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">merge_message_runs</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">TRUSTCALL_INSTRUCTION</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]))</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the extractor</span>
<span class="w">    </span><span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">updated_messages</span><span class="p">,</span> 
<span class="w">                                        </span><span class="s2">&quot;existing&quot;</span><span class="p">:</span> <span class="n">existing_memories</span><span class="p">})</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Save the memories from Trustcall to the store</span>
<span class="w">    </span><span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">rmeta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;responses&quot;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;response_metadata&quot;</span><span class="p">]):</span>
<span class="w">        </span><span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span>
<span class="w">                  </span><span class="n">rmeta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;json_doc_id&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())),</span>
<span class="w">                  </span><span class="n">r</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">),</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">[Write memory debug] Saved memories: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;responses&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">MessagesState</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connect nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;call_model&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;call_model&quot;</span><span class="p">,</span> <span class="s2">&quot;write_memory&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;write_memory&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Store for long-term (across-thread) memory</span>
<span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Checkpointer for short-term (within-thread) memory</span>
<span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Começamos uma nova conversa</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>
<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hi, my name is Maximo&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Hi, my name is Maximo
	[Call model debug] Memories: []
================================== Ai Message ==================================

Hello Maximo! It&#x27;s nice to meet you. I&#x27;m your companion chatbot, here to chat, help answer questions, or just be someone to talk to. 

I&#x27;ll remember your name is Maximo for our future conversations. What would you like to talk about today? How are you doing?
	[Write memory debug] Existing memories: None
	[Write memory debug] Saved memories: [Memory(content=&quot;User&#x27;s name is Maximo.&quot;)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos um novo interesse do usuário</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I like to play soccer&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I like to play soccer
	[Call model debug] Memories: [Item(namespace=[&#x27;memories&#x27;, &#x27;1&#x27;], key=&#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, value=&#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;, created_at=&#x27;2025-05-12T18:32:38.070902+00:00&#x27;, updated_at=&#x27;2025-05-12T18:32:38.070903+00:00&#x27;, score=None)]
================================== Ai Message ==================================

That&#x27;s great to know, Maximo! Soccer is such a wonderful sport. Do you play on a team, or more casually with friends? I&#x27;d also be curious to know what position you typically play, or if you have a favorite professional team you follow. I&#x27;ll remember that you enjoy soccer for our future conversations.
	[Write memory debug] Existing memories: [(&#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, &#x27;Memory&#x27;, &#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;)]
	[Write memory debug] Saved memories: [Memory(content=&#x27;User enjoys playing soccer.&#x27;)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, o novo interesse do usuário foi adicionado à memória.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória atualizada</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memories&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="n">memories</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">dict</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;namespace&#x27;: [&#x27;memories&#x27;, &#x27;1&#x27;], &#x27;key&#x27;: &#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, &#x27;value&#x27;: &#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;, &#x27;created_at&#x27;: &#x27;2025-05-12T18:32:38.070902+00:00&#x27;, &#x27;updated_at&#x27;: &#x27;2025-05-12T18:32:38.070903+00:00&#x27;, &#x27;score&#x27;: None&#x7D;
&#x7B;&#x27;namespace&#x27;: [&#x27;memories&#x27;, &#x27;1&#x27;], &#x27;key&#x27;: &#x27;25d2ee8c-5890-415b-85e0-d9fb0ea4cd43&#x27;, &#x27;value&#x27;: &#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;, &#x27;created_at&#x27;: &#x27;2025-05-12T18:32:42.558787+00:00&#x27;, &#x27;updated_at&#x27;: &#x27;2025-05-12T18:32:42.558789+00:00&#x27;, &#x27;score&#x27;: None&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;
&#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que são salvos documentos de memória, não um perfil do usuário.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a adicionar um novo interesse do usuário</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I also like to play basketball&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I also like to play basketball
	[Call model debug] Memories: [Item(namespace=[&#x27;memories&#x27;, &#x27;1&#x27;], key=&#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, value=&#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;, created_at=&#x27;2025-05-12T18:32:38.070902+00:00&#x27;, updated_at=&#x27;2025-05-12T18:32:38.070903+00:00&#x27;, score=None), Item(namespace=[&#x27;memories&#x27;, &#x27;1&#x27;], key=&#x27;25d2ee8c-5890-415b-85e0-d9fb0ea4cd43&#x27;, value=&#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;, created_at=&#x27;2025-05-12T18:32:42.558787+00:00&#x27;, updated_at=&#x27;2025-05-12T18:32:42.558789+00:00&#x27;, score=None)]
================================== Ai Message ==================================

That&#x27;s awesome, Maximo! Both soccer and basketball are fantastic sports. I&#x27;ll remember that you enjoy basketball as well. Do you find yourself playing one more than the other? And similar to soccer, do you play basketball with a team or more casually? Many people enjoy the different skills and dynamics each sport offers - soccer with its continuous flow and footwork, and basketball with its fast pace and shooting precision. Any favorite basketball teams you follow?
	[Write memory debug] Existing memories: [(&#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, &#x27;Memory&#x27;, &#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;), (&#x27;25d2ee8c-5890-415b-85e0-d9fb0ea4cd43&#x27;, &#x27;Memory&#x27;, &#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;)]
	[Write memory debug] Saved memories: [Memory(content=&#x27;User enjoys playing basketball.&#x27;)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a ver a memória atualizada</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Namespace for the memory to save</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;memories&quot;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
<span class="n">memories</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;
&#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;
&#x7B;&#x27;content&#x27;: &#x27;User enjoys playing basketball.&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Iniciamos uma nova conversa com um novo fio</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>
<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># User input </span>
<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;What soccer players do you recommend for me?&quot;</span><span class="p">)]</span>
<span class="w"> </span>
<span class="c1"># Run the graph</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

What soccer players do you recommend for me?
	[Call model debug] Memories: [Item(namespace=[&#x27;memories&#x27;, &#x27;1&#x27;], key=&#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, value=&#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;, created_at=&#x27;2025-05-12T18:32:38.070902+00:00&#x27;, updated_at=&#x27;2025-05-12T18:32:38.070903+00:00&#x27;, score=None), Item(namespace=[&#x27;memories&#x27;, &#x27;1&#x27;], key=&#x27;25d2ee8c-5890-415b-85e0-d9fb0ea4cd43&#x27;, value=&#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;, created_at=&#x27;2025-05-12T18:32:42.558787+00:00&#x27;, updated_at=&#x27;2025-05-12T18:32:42.558789+00:00&#x27;, score=None), Item(namespace=[&#x27;memories&#x27;, &#x27;1&#x27;], key=&#x27;965f2e52-bea0-44d4-8534-4fce2bbc1c4b&#x27;, value=&#x7B;&#x27;content&#x27;: &#x27;User enjoys playing basketball.&#x27;&#x7D;, created_at=&#x27;2025-05-12T18:33:38.613626+00:00&#x27;, updated_at=&#x27;2025-05-12T18:33:38.613629+00:00&#x27;, score=None)]
================================== Ai Message ==================================

Hi Maximo! Since you enjoy soccer, I&#x27;d be happy to recommend some players you might find interesting to follow or learn from.

Based on your interests in both soccer and basketball, I might suggest players who are known for their athleticism and skill:

1. Lionel Messi - Widely considered one of the greatest players of all time
2. Cristiano Ronaldo - Known for incredible athleticism and dedication
3. Kylian Mbappé - Young talent with amazing speed and technical ability
4. Kevin De Bruyne - Master of passing and vision
5. Erling Haaland - Goal-scoring phenomenon

Is there a particular position or playing style you&#x27;re most interested in? That would help me refine my recommendations further. I could also suggest players from specific leagues or teams if you have preferences!
	[Write memory debug] Existing memories: [(&#x27;6d06c4f5-3a74-46b2-92b4-1e29ba128c90&#x27;, &#x27;Memory&#x27;, &#x7B;&#x27;content&#x27;: &quot;User&#x27;s name is Maximo.&quot;&#x7D;), (&#x27;25d2ee8c-5890-415b-85e0-d9fb0ea4cd43&#x27;, &#x27;Memory&#x27;, &#x7B;&#x27;content&#x27;: &#x27;User enjoys playing soccer.&#x27;&#x7D;), (&#x27;965f2e52-bea0-44d4-8534-4fce2bbc1c4b&#x27;, &#x27;Memory&#x27;, &#x7B;&#x27;content&#x27;: &#x27;User enjoys playing basketball.&#x27;&#x7D;)]
	[Write memory debug] Saved memories: [Memory(content=&#x27;User asked for soccer player recommendations, suggesting an active interest in following professional soccer beyond just playing it.&#x27;)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que se lembrava que gostávamos de futebol e basquete.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Humano no loop">Humano no loop<a class="anchor-link" href="#Humano no loop">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Embora um agente possa realizar tarefas, para certas tarefas, é necessário que haja uma supervisão humana. Isso é chamado de <code>human in the loop</code>. Vamos ver como isso pode ser feito com <code>LangGraph</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>A camada de <a href="https://langchain-ai.github.io/langgraph/concepts/persistence/">persistência</a> de <code>LangGraph</code> suporta fluxos de trabalho com humanos no loop, permitindo que a execução seja interrompida e retomada com base nos comentários dos usuários. A interface principal desta funcionalidade é a função <a href="https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#interrupt">interrupt</a>. Chamando <code>interrupt</code> dentro de um nó, a execução será interrompida. A execução pode ser retomada, junto com a nova contribuição humana, passada em uma primitiva <a href="https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#the-command-primitive">Command</a>. <code>interrupt</code> é similar ao comando de Python <code>input()</code>, mas com algumas considerações extras.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos adicionar ao chatbot que tem memória a curto prazo e acesso a ferramentas, mas faremos uma mudança, que é adicionar uma simples ferramenta <code>human_assistance</code>. Esta ferramenta utiliza <code>interrupt</code> para receber informações de um ser humano.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro carregamos os valores das chaves API.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>

<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos a <code>ferramenta</code> de busca</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos a <code>tool</code> de ajuda humana</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">human_assistance</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>
<span class="sd">    After receiving the expert&#39;s response, you should provide an elaborated response to the user based on the information received</span>
<span class="sd">    based on the information received, without calling this tool again.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        query: The query to ask the human expert.</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The response from the human expert.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
<span class="w">    </span><span class="k">return</span> <span class="n">human_response</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p><code>LangGraph</code> obtém informações das ferramentas através da documentação da ferramenta, ou seja, o <code>docstring</code> da função. Portanto, é muito importante gerar um bom <code>docstring</code> para a ferramenta.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma lista de <code>tools</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, o <code>LLM</code> com as <code>bind_tools</code> e adicionamos ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Modification: tell the LLM which tools it can call</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Add the chatbot node</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x10764b380&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se você reparar, mudamos a forma de definir a função <code>chatbot_function</code>, pois agora ela precisa lidar com a interrupção.</p>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos a <code>tool_node</code> ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="w"> </span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x10764b380&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos o nódo de <code>START</code> ao gráfico</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x10764b380&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver">MemorySaver</a>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Compilamos o gráfico com o <code>checkpointer</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O representamos graficamente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos solicitar ao chatbot com uma pergunta que envolverá a nova ferramenta <code>human_assistance</code>:</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;I need some expert guidance for building an AI agent. Could you request assistance for me?&quot;</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I need some expert guidance for building an AI agent. Could you request assistance for me?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;human_assistance (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: I need some expert guidance for building an AI agent. Could you provide me with some advice?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se pode ver, o chatbot gerou uma chamada para a ferramenta de assistência humana.</p>
<div class='highlight'><pre><code class="language-markdown">Chamadas de Ferramentas:
human_assistance (0)
ID da Chamada: 0
Argumentos:
Eu preciso de algumas orientações de especialistas para construir um agente de IA. Você poderia fornecer conselhos sobre considerações-chave, melhores práticas e armadilhas potenciais a serem evitadas?
</code></pre></div>
<p>Mas tarde a execução foi interrompida. Vamos verificar o estado do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">snapshot</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;tools&#x27;,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que se deteveu no nó de <code>tools</code>. Analisamos como se definiu a ferramenta <code>human_assistance</code>.</p>
<section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-python">from langgraph.types import Command, interrupt<br>from langchain_core.tools import tool<br><br>@tool<br>def human_assistance(query: str) -&gt; str:<br>&#x20;&#x20;"""<br>&#x20;&#x20;Request assistance from a human expert. Use this tool ONLY ONCE per conversation.<br>&#x20;&#x20;After receiving the expert&#39;s response, you should provide an elaborated response to the user based on the information received<br>&#x20;&#x20;based on the information received, without calling this tool again.<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;query: The query to ask the human expert.<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;The response from the human expert.<br>&#x20;&#x20;"""<br>&#x20;&#x20;human_response = interrupt(&#123;"query": query&#125;)<br>&#x20;&#x20;return human_response["data"]</code></pre></div>
      </section>
</section>
<section class="section-block-markdown-cell">
<p>Chamando a ferramenta <code>interrupt</code>, a execução será interrompida, semelhante à função do Python <code>input()</code>.</p>
<p>O progresso é mantido com base em nossa escolha de <a href="https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries">checkpointer</a>. Ou seja, a escolha de onde o estado do grafo é salvo. Portanto, se estamos persistindo (salvando o estado do grafo) com um banco de dados como <code>SQLite</code>, <code>Postgres</code>, etc, podemos retomar a execução a qualquer momento, desde que o banco de dados esteja ativo.</p>
<p>Aqui estamos persistindo (salvando o estado do grafo) com o ponteiro de verificação na memória RAM, portanto podemos retomar a qualquer momento enquanto nosso kernel Python estiver em execução. No meu caso, enquanto não reiniciar o kernel do meu Jupyter Notebook.</p>
<p>Para retomar a execução, passamos um objeto <a href="https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#the-command-primitive">Command</a> que contém os dados esperados pela ferramenta. O formato desses dados pode ser personalizado de acordo com nossas necessidades. Aqui, apenas precisamos de um dicionário com uma chave <code>data</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">human_response</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;We, the experts are here to help! We&#39;d recommend you check out LangGraph to build your agent.&quot;</span>
<span class="w">    </span><span class="s2">&quot;It&#39;s much more reliable and extensible than simple autonomous agents.&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="n">human_response</span><span class="p">})</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;human_assistance (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: I need some expert guidance for building an AI agent. Could you provide me with some advice?
================================= Tool Message =================================
Name: human_assistance

We, the experts are here to help! We&#x27;d recommend you check out LangGraph to build your agent.It&#x27;s much more reliable and extensible than simple autonomous agents.
================================== Ai Message ==================================

The experts recommend checking out LangGraph for building your AI agent. It&#x27;s known for being more reliable and extensible compared to simple autonomous agents.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, o chatbot esperou que um humano fornecesse a resposta e, em seguida, gerou uma resposta baseada nas informações recebidas. Pedimos ajuda sobre um especialista sobre como criar agentes, o humano disse que o melhor é usar LangGraph, e o chatbot gerou uma resposta com base nessas informações.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mas ainda tem a possibilidade de realizar pesquisas na web. Então, agora vamos pedir as últimas notícias sobre LangGraph.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;What&#39;s the latest news about LangGraph?&quot;</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

What&#x27;s the latest news about LangGraph?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain - Changelog&quot;, &quot;url&quot;: &quot;https://changelog.langchain.com/&quot;, &quot;content&quot;: &quot;LangGraph `interrupt`: Simplifying human-in-the-loop agents --------------------------------------------------- Our latest feature in LangGraph, interrupt , makes building human-in-the-loop workflows easier. Agents aren’t perfect, so keeping humans “in the loop”... December 16, 2024 [...] LangGraph 🔁 Modify graph state from tools in LangGraph --------------------------------------------- LangGraph&#x27;s latest update gives you greater control over your agents by enabling tools to directly update the graph state. This is a game-changer for use... December 18, 2024 [...] LangGraph Platform Custom authentication &amp;amp; access control for LangGraph Platform ------------------------------------------------------------- Today, we&#x27;re thrilled to announce Custom Authentication and Resource-Level Access Control for Python deployments in LangGraph Cloud and self-hosted... December 20, 2024&quot;, &quot;score&quot;: 0.78650844&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph 0.3 Release: Prebuilt Agents - LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/langgraph-0-3-release-prebuilt-agents/&quot;, &quot;content&quot;: &quot;LangGraph 0.3 Release: Prebuilt Agents\n2 min read Feb 27, 2025\nBy Nuno Campos and Vadym Barda\nOver the past year, we’ve invested heavily in making LangGraph the go-to framework for building AI agents. With companies like Replit, Klarna, LinkedIn and Uber choosing to build on top of LangGraph, we have more conviction than ever that we are on the right path. [...] Up to this point, we’ve had one higher level abstraction and it’s lived in the main langgraph package. It was create_react_agent, a wrapper for creating a simple tool calling agent. Today, we are splitting that out of langgraph as part of a 0.3 release, and moving it into langgraph-prebuilt.\nWe are also introducing a new set of prebuilt agents built on top of LangGraph, in both Python and JavaScript.\nOver the past three weeks, we’ve already released a few of these: [...] Published Time: 2025-02-27T15:09:15.000Z\nLangGraph 0.3 Release: Prebuilt Agents\nSkip to content\n\n\nCase Studies\nIn the Loop\nLangChain\nDocs\nChangelog\n\nSign in Subscribe&quot;, &quot;score&quot;: 0.72348577&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain - Changelog&quot;, &quot;url&quot;: &quot;https://changelog.langchain.com/&quot;, &quot;content&quot;: &quot;LangGraph 🔁 Modify graph state from tools in LangGraph --------------------------------------------- LangGraph&#x27;s latest update gives you greater control over your agents by enabling tools to directly update the graph state. This is a game-changer for use... December 18, 2024 [...] LangGraph `interrupt`: Simplifying human-in-the-loop agents --------------------------------------------------- Our latest feature in LangGraph, interrupt , makes building human-in-the-loop workflows easier. Agents aren’t perfect, so keeping humans “in the loop”... December 16, 2024 [...] LangGraph Platform Custom authentication &amp;amp; access control for LangGraph Platform ------------------------------------------------------------- Today, we&#x27;re thrilled to announce Custom Authentication and Resource-Level Access Control for Python deployments in LangGraph Cloud and self-hosted... December 20, 2024&quot;, &quot;score&quot;: 0.79732054&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph 0.3 Release: Prebuilt Agents - LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/langgraph-0-3-release-prebuilt-agents/&quot;, &quot;content&quot;: &quot;LangGraph 0.3 Release: Prebuilt Agents\n2 min read Feb 27, 2025\nBy Nuno Campos and Vadym Barda\nOver the past year, we’ve invested heavily in making LangGraph the go-to framework for building AI agents. With companies like Replit, Klarna, LinkedIn and Uber choosing to build on top of LangGraph, we have more conviction than ever that we are on the right path. [...] Up to this point, we’ve had one higher level abstraction and it’s lived in the main langgraph package. It was create_react_agent, a wrapper for creating a simple tool calling agent. Today, we are splitting that out of langgraph as part of a 0.3 release, and moving it into langgraph-prebuilt.\nWe are also introducing a new set of prebuilt agents built on top of LangGraph, in both Python and JavaScript.\nOver the past three weeks, we’ve already released a few of these: [...] Published Time: 2025-02-27T15:09:15.000Z\nLangGraph 0.3 Release: Prebuilt Agents\nSkip to content\n\n\nCase Studies\nIn the Loop\nLangChain\nDocs\nChangelog\n\nSign in Subscribe&quot;, &quot;score&quot;: 0.7552947&#x7D;]
================================== Ai Message ==================================

The latest news about LangGraph includes several updates and releases. Firstly, the &#x27;interrupt&#x27; feature has been added, which simplifies creating human-in-the-loop workflows, essential for maintaining oversight of AI agents. Secondly, an update allows tools to modify the graph state directly, providing more control over the agents. Lastly, custom authentication and resource-level access control have been implemented for Python deployments in LangGraph Cloud and self-hosted environments. In addition, LangGraph released version 0.3, which introduces prebuilt agents in both Python and JavaScript, aimed at making it even easier to develop AI agents.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ele buscou as últimas notícias sobre LangGraph e gerou uma resposta baseada nas informações recebidas.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos escrever tudo junto para que seja mais compreensível</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># State</span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">tool_search</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">human_assistance</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>
<span class="sd">    After receiving the expert&#39;s response, you should provide an elaborated response to the user based on the information received</span>
<span class="sd">    based on the information received, without calling this tool again.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        query: The query to ask the human expert.</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The response from the human expert.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
<span class="w">    </span><span class="k">return</span> <span class="n">human_response</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool_search</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># Create the LLM with tools</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Tool node</span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Start to build the graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes to the graph</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Display the graph</span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:
1. Check your internet connection and try again
2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`
3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Pedimos novamente ajuda ao chatbot para criar agentes. Solicitamos que ele busque ajuda.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;I need some expert guidance for building an AI agent. Could you request assistance for me?&quot;</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I need some expert guidance for building an AI agent. Could you request assistance for me?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;human_assistance (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: I need expert guidance for building an AI agent.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos em qual estado ficou o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">snapshot</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;tools&#x27;,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Damos a você a assistência que está pedindo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">human_response</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;We, the experts are here to help! We&#39;d recommend you check out LangGraph to build your agent.&quot;</span>
<span class="w">    </span><span class="s2">&quot;It&#39;s much more reliable and extensible than simple autonomous agents.&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="n">human_response</span><span class="p">})</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;human_assistance (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: I need expert guidance for building an AI agent.
================================= Tool Message =================================
Name: human_assistance

We, the experts are here to help! We&#x27;d recommend you check out LangGraph to build your agent.It&#x27;s much more reliable and extensible than simple autonomous agents.
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;human_assistance (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: I need some expert guidance for building an AI agent. Could you recommend a platform and any tips for getting started?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E por último, pedimos que procure na internet as últimas notícias sobre LangGraph</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;What&#39;s the latest news about LangGraph?&quot;</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

What&#x27;s the latest news about LangGraph?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.6811549&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6557114&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: Latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Blog&quot;, &quot;url&quot;: &quot;https://blog.langchain.dev/&quot;, &quot;content&quot;: &quot;LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna&#x27;s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.&quot;, &quot;score&quot;: 0.67758125&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph + UiPath: advancing agentic automation together&quot;, &quot;url&quot;: &quot;https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together&quot;, &quot;content&quot;: &quot;Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath&#x27;s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, we’re excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChain’s cloud or self-hosted on-premises. This feature is currently in private preview.&quot;, &quot;score&quot;: 0.6522641&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: latest news about LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph - LangChain&quot;, &quot;url&quot;: &quot;https://www.langchain.com/langgraph&quot;, &quot;content&quot;: &quot;“As Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments&quot;, &quot;score&quot;: 0.98559&#x7D;, &#x7B;&quot;title&quot;: &quot;Evaluating LangGraph Framework : Series 1 | by Jalaj Agrawal&quot;, &quot;url&quot;: &quot;https://medium.com/@jalajagr/evaluating-langgraph-as-a-multiagent-framework-a-10-dimensional-framework-series-1-c7203b7f4659&quot;, &quot;content&quot;: &quot;: LangGraph excels with its intuitive graph-based abstraction that allows new developers to build working multi-agent systems within hours.&quot;, &quot;score&quot;: 0.98196&#x7D;]
================================== Ai Message ==================================

It looks like LangGraph has been generating some significant buzz in the AI community, especially for its capabilities in building multi-agent systems. Here are a few highlights from the latest news:

1. **LangGraph in Production**: Companies like Klarna and Definely are already using LangGraph to build and optimize their AI systems. Klarna has leveraged LangGraph to enhance their customer support, and Definely has used it to design a multi-agent system to speed up legal workflows.

2. **Integration with UiPath**: LangChain and UiPath have collaborated to advance agentic automation. This partnership includes native support for LangSmith observability in UiPath’s LLM Gateway via OpenTelemetry, which will allow developers to monitor, debug, and evaluate LLM-powered features more effectively.

3. **Intuitive Design**: LangGraph is praised for its intuitive graph-based abstraction, which enables developers to build working multi-agent systems quickly, even if they are new to the field.

4. **Community and Conferences**: LangChain is also hosting an AI Agent Conference called &quot;Interrupt,&quot; which could be a great opportunity to learn more about the latest developments and best practices in building AI agents.

If you&#x27;re considering using LangGraph for your project, these resources and updates might provide valuable insights and support. Would you like more detailed information on any specific aspect of LangGraph?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Mais">Mais<a class="anchor-link" href="#Mais">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<h4 id="Aprovacao do uso de ferramentas">Aprovação do uso de ferramentas<a class="anchor-link" href="#Aprovacao do uso de ferramentas">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Nota: Vamos a fazer esta seção usando Sonnet 3.7, pois na data da escrita deste post, é o melhor modelo para uso com agentes, e é o único que entende quando deve chamar as ferramentas e quando não deve para este exemplo</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Podemos inserir um <code>human in the loop</code> para aprovar o uso de ferramentas. Vamos criar um chatbot com várias ferramentas para realizar operações matemáticas, para isso na hora de construir o grafo indicamos onde queremos inserir o <code>breakpoint</code> (<code>graph_builder.compile(interrupt_before=[&quot;tools&quot;], checkpointer=memory)</code>)</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiply a and b.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The product of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds a and b.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The sum of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Subtract b from a.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The difference between a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">divide</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Divide a by b.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The quotient of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="w"> </span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">multiply</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">subtract</span><span class="p">,</span> <span class="n">divide</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">system_message</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful assistant that can use tools to answer questions. Once you have the result of a tool, provide a final answer without calling more tools.&quot;</span>
<span class="w">    </span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">interrupt_before</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;tools&quot;</span><span class="p">],</span> <span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos no grafo, há um <code>interrupt</code> antes de usar as <code>tool</code>s. Isso significa que vai parar antes de usá-las para pedir nossa permissão.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Input</span>
<span class="n">initial_input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Multiply 2 and 3&quot;</span><span class="p">)}</span>
<span class="w"> </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># Run the graph until the first interruption</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &quot;I&#x27;ll multiply 2 and 3 for you.&quot;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_01QDuind1VBHWtvifELN9SPf&#x27;, &#x27;input&#x27;: &#x7B;&#x27;a&#x27;: 2, &#x27;b&#x27;: 3&#x7D;, &#x27;name&#x27;: &#x27;multiply&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;multiply (toolu_01QDuind1VBHWtvifELN9SPf)
 Call ID: toolu_01QDuind1VBHWtvifELN9SPf
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;a: 2
&#x20;&#x20;&#x20;&#x20;b: 3
None
&#x7B;&#x27;__interrupt__&#x27;: ()&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, o LLM sabe que tem que usar a ferramenta <code>multiply</code>, mas a execução é interrompida, pois precisa esperar que um ser humano autorize o uso da ferramenta.</p>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver o estado em que ficou o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">state</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;tools&#x27;,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, ficou no nó de <code>tools</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Podemos criar uma função (não no grafo, mas fora do grafo, para melhorar a experiência do usuário e fazer com que ele entenda por que a execução pára) que peça ao usuário para aprovar o uso da ferramenta.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um novo <code>thread_id</code> para que seja criado um novo estado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Input</span>
<span class="n">initial_input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Multiply 2 and 3&quot;</span><span class="p">)}</span>
<span class="w"> </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># Run the graph until the first interruption</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">function_name</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="n">function_args</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>
<span class="w">            </span><span class="k">if</span> <span class="n">element</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
<span class="w">            </span><span class="k">elif</span> <span class="n">element</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;tool_use&#39;</span><span class="p">:</span>
<span class="w">                </span><span class="n">function_name</span> <span class="o">=</span> <span class="n">element</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>
<span class="w">                </span><span class="n">function_args</span> <span class="o">=</span> <span class="n">element</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span>
<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The LLM wants to use the tool </span><span class="si">{</span><span class="n">function_name</span><span class="si">}</span><span class="s2"> with the arguments </span><span class="si">{</span><span class="n">function_args</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span>
<span class="w">    </span><span class="k">elif</span> <span class="s1">&#39;__interrupt__&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="k">pass</span>
<span class="w">    </span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">question</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Do you approve the use of the tool </span><span class="si">{</span><span class="n">function_name</span><span class="si">}</span><span class="s2"> with the arguments </span><span class="si">{</span><span class="n">function_args</span><span class="si">}</span><span class="s2">? (y/n)&quot;</span>
<span class="n">user_approval</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">user_approval</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">if</span> <span class="n">user_approval</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;User approved the use of the tool&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">            </span><span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>
<span class="w">                </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">                    </span><span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">        </span><span class="k">elif</span> <span class="s1">&#39;tools&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">            </span><span class="n">result</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;tools&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
<span class="w">            </span><span class="n">tool_used</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;tools&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The result of the tool </span><span class="si">{</span><span class="n">tool_used</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">        </span>
<span class="w">        </span><span class="k">else</span><span class="p">:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>I&#x27;ll multiply 2 and 3 for you.
The LLM wants to use the tool multiply with the arguments &#x7B;&#x27;a&#x27;: 2, &#x27;b&#x27;: 3&#x7D;
Do you approve the use of the tool None with the arguments None? (y/n): y
User approved the use of the tool
The result of the tool multiply is 6
The result of multiplying 2 and 3 is 6.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver que nos perguntou se aprovamos o uso da <code>tool</code> de multiplicação, aprovamos e o grafo terminou a execução. Vendo o estado do grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">state</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>()
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que o próximo estado do grafo está vazio, isso indica que a execução do grafo foi concluída.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Modificacao do estado">Modificação do estado<a class="anchor-link" href="#Modificacao do estado">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Nota: Vamos a fazer esta seção usando Sonnet 3.7, pois na data da escrita do post, é o melhor modelo para uso com agentes e é o único que entende quando deve chamar as ferramentas e quando não deve para este exemplo.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Vamos repetir o exemplo anterior, mas em vez de interromper o grafo antes do uso de uma <code>tool</code>, vamos interrompê-lo no LLM. Para isso, ao construir o grafo, indicamos que queremos parar no agente (<code>graph_builder.compile(interrupt_before=[&quot;chatbot_node&quot;], checkpointer=memory)</code>)</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiply a and b.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The product of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds a and b.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The sum of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Subtract b from a.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The difference between a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">divide</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Divide a by b.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        a: first int</span>
<span class="sd">        b: second int</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The quotient of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="w"> </span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">multiply</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">subtract</span><span class="p">,</span> <span class="n">divide</span><span class="p">]</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">system_message</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful assistant that can use tools to answer questions. Once you have the result of a tool, provide a final answer without calling more tools.&quot;</span>
<span class="w">    </span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">interrupt_before</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">],</span> <span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos na representação do grafo que há um <code>interrupt</code> antes da execução de <code>chatbot_node</code>, assim, antes de o chatbot ser executado, a execução será interrompida e nós teremos que fazer com que continue.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos pedir novamente uma multiplicação</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Input</span>
<span class="n">initial_input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Multiply 2 and 3&quot;</span><span class="p">)}</span>
<span class="w"> </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># Run the graph until the first interruption</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;__interrupt__&#x27;: ()&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver que não fez nada. Se vemos o estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">state</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;chatbot_node&#x27;,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que o próximo nó é o do chatbot. Além disso, se observarmos seus valores, veremos a mensagem que lhe enviamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">state</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;messages&#x27;: [HumanMessage(content=&#x27;Multiply 2 and 3&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;08fd6084-ecd2-4156-ab24-00d2d5c26f00&#x27;)]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora procedemos a modificar o estado, adicionando uma nova mensagem</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;No, actually multiply 3 and 3!&quot;</span><span class="p">)]}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x27;checkpoint_ns&#x27;: &#x27;&#x27;,
&#x20;&#x20;&#x27;checkpoint_id&#x27;: &#x27;1f027eb6-6c8b-6b6a-8001-bc0f8942566c&#x27;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtemos o novo estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">new_state</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;chatbot_node&#x27;,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O próximo nó ainda é o do chatbot, mas agora vamos ver as mensagens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_state</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;messages&#x27;: [HumanMessage(content=&#x27;Multiply 2 and 3&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;08fd6084-ecd2-4156-ab24-00d2d5c26f00&#x27;),
&#x20;&#x20;HumanMessage(content=&#x27;No, actually multiply 3 and 3!&#x27;, additional_kwargs=&#x7B;&#x7D;, response_metadata=&#x7B;&#x7D;, id=&#x27;e95394c2-e62e-47d2-b9b2-51eba40f3e22&#x27;)]&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que foi adicionado o novo. Então fazemos com que continue a execução.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

No, actually multiply 3 and 3!
================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &quot;I&#x27;ll multiply 3 and 3 for you.&quot;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_01UABhLnEdg5ZqxVQTE5pGUx&#x27;, &#x27;input&#x27;: &#x7B;&#x27;a&#x27;: 3, &#x27;b&#x27;: 3&#x7D;, &#x27;name&#x27;: &#x27;multiply&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;multiply (toolu_01UABhLnEdg5ZqxVQTE5pGUx)
 Call ID: toolu_01UABhLnEdg5ZqxVQTE5pGUx
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;a: 3
&#x20;&#x20;&#x20;&#x20;b: 3
================================= Tool Message =================================
Name: multiply

9
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Foi feita a multiplicação de 3 por 3, que é a modificação do estado que fizemos, e não 2 por 3, que é o que pedimos da primeira vez.</p>
</section>
<section class="section-block-markdown-cell">
<p>Isso pode ser útil quando temos um agente e queremos verificar se o que ele faz está correto, então podemos entrar na execução e modificar o estado.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Pontos de interrupcao dinamicos">Pontos de interrupção dinâmicos<a class="anchor-link" href="#Pontos de interrupcao dinamicos">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Até agora, criamos pontos de parada estáticos através da compilação do grafo, mas podemos criar pontos de parada dinâmicos usando <code>NodeInterrupt</code>. Isso é útil porque a execução pode ser interrompida por regras lógicas introduzidas por programação.</p>
</section>
<section class="section-block-markdown-cell">
<p>Estes <code>NodeInterrupt</code> permitem personalizar como o usuário será notificado sobre a interrupção.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.errors</span><span class="w"> </span><span class="kn">import</span> <span class="n">NodeInterrupt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">memory_saver</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="c1"># Create the LLM model</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">max_len</span> <span class="o">=</span> <span class="mi">15</span>
<span class="w">    </span><span class="n">input_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Check len message</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">max_len</span><span class="p">:</span>
<span class="w">        </span><span class="k">raise</span> <span class="n">NodeInterrupt</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Received input is longer than </span><span class="si">{</span><span class="n">max_len</span><span class="si">}</span><span class="s2"> characters --&amp;gt; </span><span class="si">{</span><span class="n">input_message</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Invoke the LLM with the messages</span>
<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Return the LLM&#39;s response in the correct state format</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Create graph builder</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Connecto nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Compile the graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory_saver</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, criamos uma interrupção no caso de o mensagem ser longa. Vamos testá-la.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">initial_input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello, how are you? My name is Máximo&quot;</span><span class="p">)}</span>
<span class="w"> </span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="c1"># Run the graph until the first interruption</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;__interrupt__&#x27;: (Interrupt(value=&quot;Received input is longer than 15 characters --&amp;gt; content=&#x27;Hello, how are you? My name is Máximo&#x27; additional_kwargs=&#x7B;&#x7D; response_metadata=&#x7B;&#x7D; id=&#x27;2bdc6d41-0cfe-4d3c-8748-ca7d46fd5a60&#x27;&quot;, resumable=False, ns=None),)&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Efetivamente, a interrupção foi pausada e nos deu a mensagem de erro que criamos.</p>
</section>
<section class="section-block-markdown-cell">
<p>Se vemos o nódo no qual ele parou</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">state</span><span class="o">.</span><span class="n">next</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;chatbot_node&#x27;,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que está parado no nódo do chatbot. Podemos voltar a fazer com que continue com a execução, mas vai nos dar o mesmo erro.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;__interrupt__&#x27;: (Interrupt(value=&quot;Received input is longer than 15 characters --&amp;gt; content=&#x27;Hello, how are you? My name is Máximo&#x27; additional_kwargs=&#x7B;&#x7D; response_metadata=&#x7B;&#x7D; id=&#x27;2bdc6d41-0cfe-4d3c-8748-ca7d46fd5a60&#x27;&quot;, resumable=False, ns=None),)&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Então temos que modificar o estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;How are you?&quot;</span><span class="p">)]}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x27;checkpoint_ns&#x27;: &#x27;&#x27;,
&#x20;&#x20;&#x27;checkpoint_id&#x27;: &#x27;1f027f13-5827-6a18-8001-4209d5a866f0&#x27;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Voltamos a ver o estado e seus valores</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Siguiente nodo: </span><span class="si">{</span><span class="n">new_state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Valores:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">new_state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Siguiente nodo: (&#x27;chatbot_node&#x27;,)
Valores:
	Hello, how are you? My name is Máximo
	How are you?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A última mensagem é mais curta, portanto tentamos retomar a execução do grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span> <span class="s1">&#39;chatbot_node&#39;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;chatbot_node&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

Hello Máximo! I&#x27;m doing well, thank you for asking. How about you? How can I assist you today?
None
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Personalizacao do estado">Personalização do estado<a class="anchor-link" href="#Personalizacao do estado">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Nota: Vamos a fazer este tópico usando Sonnet 3.7, pois na data da escrita do post, é o melhor modelo para uso com agentes e é o único que entende quando deve chamar as ferramentas e quando não deve.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Até agora, confiamos em um estado simples com uma entrada, uma lista de mensagens. Pode-se ir longe com esse estado simples, mas se deseja definir um comportamento complexo sem depender da lista de mensagens, podem ser adicionados campos adicionais ao estado.</p>
<p>Aqui vamos ver um novo cenário, no qual o chatbot está utilizando a ferramenta de busca para encontrar informações específicas e reenviá-las a um ser humano para revisão. Vamos fazer com que o chatbot investigue o aniversário de uma entidade. Adicionaremos <code>name</code> e <code>birthday</code> como chaves do estado.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro carregamos os valores das chaves API.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o novo estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w">    </span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionar essa informação ao estado torna-a facilmente acessível por outros nós do grafo (por exemplo, um nó que armazena ou processa a informação), bem como a camada de persistência do grafo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="w"> </span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos a <code>ferramenta</code> de busca</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos a ferramenta de assistência humana. Nesta ferramenta, preencheremos as chaves de estado dentro da nossa ferramenta <code>human_assistance</code>. Isso permite que um ser humano revise a informação antes de ela ser armazenada no estado. Voltaremos a usar <code>Command</code>, desta vez para emitir uma atualização de estado a partir do interior da nossa ferramenta.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">InjectedToolCallId</span><span class="p">,</span> <span class="n">tool</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>
<span class="w"> </span>

<span class="nd">@tool</span>
<span class="c1"># Note that because we are generating a ToolMessage for a state update, we</span>
<span class="c1"># generally require the ID of the corresponding tool call. We can use</span>
<span class="c1"># LangChain&#39;s InjectedToolCallId to signal that this argument should not</span>
<span class="c1"># be revealed to the model in the tool&#39;s schema.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">human_assistance</span><span class="p">(</span>
<span class="w">    </span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InjectedToolCallId</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>
<span class="sd">    After receiving the expert&#39;s response, you should provide an elaborated response to the user based on the information received</span>
<span class="sd">    based on the information received, without calling this tool again.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        query: The query to ask the human expert.</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The response from the human expert.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">(</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Is this correct?&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;birthday&quot;</span><span class="p">:</span> <span class="n">birthday</span><span class="p">,</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">    </span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># If the information is correct, update the state as-is.</span>
<span class="w">    </span><span class="k">if</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;correct&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="n">verified_name</span> <span class="o">=</span> <span class="n">name</span>
<span class="w">        </span><span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">birthday</span>
<span class="w">        </span><span class="n">response</span> <span class="o">=</span> <span class="s2">&quot;Correct&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Otherwise, receive information from the human reviewer.</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">verified_name</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
<span class="w">        </span><span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;birthday&quot;</span><span class="p">,</span> <span class="n">birthday</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Made a correction: </span><span class="si">{</span><span class="n">human_response</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># This time we explicitly update the state with a ToolMessage inside</span>
<span class="w">    </span><span class="c1"># the tool.</span>
<span class="w">    </span><span class="n">state_update</span> <span class="o">=</span> <span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">verified_name</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;birthday&quot;</span><span class="p">:</span> <span class="n">verified_birthday</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">ToolMessage</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call_id</span><span class="p">)],</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># We return a Command object in the tool to update our state.</span>
<span class="w">    </span><span class="k">return</span> <span class="n">Command</span><span class="p">(</span><span class="n">update</span><span class="o">=</span><span class="n">state_update</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós usamos <a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html">ToolMessage</a> que é usado para passar o resultado de executar uma <code>tool</code> de volta a um modelo e <a href="https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolCallId.html">InjectedToolCallId</a></p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma lista de <code>tools</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, o <code>LLM</code> com as <code>bind_tools</code> e adicionamos ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="w"> </span>

<span class="c1"># Create the LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Modification: tell the LLM which tools it can call</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="c1"># Because we will be interrupting during tool execution,</span>
<span class="w">    </span><span class="c1"># we disable parallel tool calling to avoid repeating any</span>
<span class="w">    </span><span class="c1"># tool invocations when we resume.</span>
<span class="w">    </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Add the chatbot node</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x120b4f380&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos a <code>tool</code> ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="w"> </span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x120b4f380&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos o nó de <code>START</code> ao gráfico</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x120b4f380&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver">MemorySaver</a>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Compilamos o grafo com o <code>checkpointer</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O representamos graficamente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos pedir ao nosso chatbot que procure o "aniversário" da biblioteca de <code>LangGraph</code>.</p>
<p>Direcionaremos o chatbot até a ferramenta <code>human_assistance</code> uma vez que tenha as informações necessárias. Os argumentos <code>name</code> e <code>birthday</code> são obrigatórios para a ferramenta <code>human_assistance</code>, então eles obrigam o chatbot a gerar propostas para esses campos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;Can you look up when LangGraph was released? &quot;</span>
<span class="w">    </span><span class="s2">&quot;When you have the answer, use the human_assistance tool for review.&quot;</span>
<span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &quot;I&#x27;ll help you look up when LangGraph was released, and then I&#x27;ll use the human_assistance tool for review as requested.\n\nFirst, let me search for information about LangGraph&#x27;s release date:&quot;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_011KHWFxYbFnUvGEF6MPt3dE&#x27;, &#x27;input&#x27;: &#x7B;&#x27;query&#x27;: &#x27;LangGraph release date when was LangGraph released&#x27;&#x7D;, &#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;tavily_search_results_json (toolu_011KHWFxYbFnUvGEF6MPt3dE)
 Call ID: toolu_011KHWFxYbFnUvGEF6MPt3dE
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph release date when was LangGraph released
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Studio: The first agent IDE | by Bhavik Jikadara - Medium&quot;, &quot;url&quot;: &quot;https://bhavikjikadara.medium.com/langgraph-studio-the-first-agent-ide-468132628274&quot;, &quot;content&quot;: &quot;LangGraph, launched in January 2023, is a low-level orchestration framework designed for building controllable and complex agentic applications.&quot;, &quot;score&quot;: 0.80405265&#x7D;, &#x7B;&quot;title&quot;: &quot;langgraph - PyPI&quot;, &quot;url&quot;: &quot;https://pypi.org/project/langgraph/&quot;, &quot;content&quot;: &quot;langgraph · PyPI\nSkip to main content Switch to mobile version\n\nSearch PyPI  Search\n\nHelp\nSponsors\nLog in\nRegister\n\nMenu\n\nHelp\nSponsors\nLog in\nRegister\n\nSearch PyPI  Search\nlanggraph 0.2.70\npip install langgraph Copy PIP instructions\nLatest versionReleased: Feb 6, 2025\nBuilding stateful, multi-actor applications with LLMs\nNavigation\n\nProject description\nRelease history\nDownload files [...] 0.2.20 Sep 13, 2024\n\n0.2.19 Sep 6, 2024\n\n0.2.18 Sep 6, 2024\n\n0.2.17 Sep 5, 2024\n\n0.2.16 Sep 1, 2024\n\n0.2.15 Aug 30, 2024\n\n0.2.14 Aug 24, 2024\n\n0.2.13 Aug 23, 2024\n\n0.2.12 Aug 22, 2024\n\n0.2.11 Aug 22, 2024\n\n0.2.10 Aug 21, 2024\n\n0.2.9 Aug 21, 2024\n\n0.2.8 Aug 21, 2024\n\n0.2.7 Aug 21, 2024\n\n0.2.7a0 pre-release Aug 21, 2024\n\n0.2.6 Aug 21, 2024\n\n0.2.5 Aug 21, 2024\n\n0.2.5a0 pre-release Aug 20, 2024\n\n0.2.4 Aug 15, 2024\n\n0.2.3 Aug 8, 2024\n\n0.2.2 Aug 7, 2024\n\n0.2.1 Aug 7, 2024\n\n0.2.0 Aug 7, 2024 [...] Download URL: langgraph-0.2.70.tar.gz\nUpload date: Feb 6, 2025\nSize: 129.7 kB\nTags: Source\nUploaded using Trusted Publishing? Yes\nUploaded via: twine/6.1.0 CPython/3.12.8&quot;, &quot;score&quot;: 0.75659186&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &#x27;Based on my search, I found that LangGraph was launched in January 2023. However, I noticed some inconsistencies in the information, as one source mentions it was launched in January 2023, while the PyPI page shows a version history starting from 2024.\n\nLet me request human assistance to verify this information:&#x27;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_019EopKn8bLi3ksvUVY2Mt5p&#x27;, &#x27;input&#x27;: &#x7B;&#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;birthday&#x27;: &#x27;January 2023&#x27;&#x7D;, &#x27;name&#x27;: &#x27;human_assistance&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;human_assistance (toolu_019EopKn8bLi3ksvUVY2Mt5p)
 Call ID: toolu_019EopKn8bLi3ksvUVY2Mt5p
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;name: LangGraph
&#x20;&#x20;&#x20;&#x20;birthday: January 2023
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Parou devido ao <code>interrupt</code> na ferramenta <code>human_assistance</code>. Neste caso, o chatbot, com a ferramenta de busca, determinou que a data de LangGraph é janeiro de 2023, mas não é a data exata, sendo 17 de janeiro de 2024, portanto podemos introduzi-la nós mesmos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span>
<span class="w">    </span><span class="n">resume</span><span class="o">=</span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;LangGraph&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;birthday&quot;</span><span class="p">:</span> <span class="s2">&quot;Jan 17, 2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="p">},</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &#x27;Based on my search, I found that LangGraph was launched in January 2023. However, I noticed some inconsistencies in the information, as one source mentions it was launched in January 2023, while the PyPI page shows a version history starting from 2024.\n\nLet me request human assistance to verify this information:&#x27;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_019EopKn8bLi3ksvUVY2Mt5p&#x27;, &#x27;input&#x27;: &#x7B;&#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;birthday&#x27;: &#x27;January 2023&#x27;&#x7D;, &#x27;name&#x27;: &#x27;human_assistance&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;human_assistance (toolu_019EopKn8bLi3ksvUVY2Mt5p)
 Call ID: toolu_019EopKn8bLi3ksvUVY2Mt5p
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;name: LangGraph
&#x20;&#x20;&#x20;&#x20;birthday: January 2023
================================= Tool Message =================================
Name: human_assistance

Made a correction: &#x7B;&#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;birthday&#x27;: &#x27;Jan 17, 2024&#x27;&#x7D;
================================== Ai Message ==================================

Thank you for the expert review and correction! Based on the human expert&#x27;s feedback, I can now provide you with the accurate information:

LangGraph was released on January 17, 2024, not January 2023 as one of the search results incorrectly stated. 

This is an important correction, as it means LangGraph is a relatively recent framework in the LLM orchestration space, having been available for less than a year at this point. LangGraph is developed by LangChain and is designed for building stateful, multi-actor applications with LLMs.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="w"> </span>
<span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;birthday&quot;</span><span class="p">)}</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;birthday&#x27;: &#x27;Jan 17, 2024&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora a data está correta graças à intervenção humana para modificar os valores do estado</p>
</section>
<section class="section-block-markdown-cell">
<p>Reescrevo todo o código para que seja mais fácil de entender</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">InjectedToolCallId</span><span class="p">,</span> <span class="n">tool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># State</span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w">    </span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="w"> </span>
<span class="nd">@tool</span>
<span class="c1"># Note that because we are generating a ToolMessage for a state update, we</span>
<span class="c1"># generally require the ID of the corresponding tool call. We can use</span>
<span class="c1"># LangChain&#39;s InjectedToolCallId to signal that this argument should not</span>
<span class="c1"># be revealed to the model in the tool&#39;s schema.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">human_assistance</span><span class="p">(</span>
<span class="w">    </span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InjectedToolCallId</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>
<span class="sd">    After receiving the expert&#39;s response, you should provide an elaborated response to the user based on the information received</span>
<span class="sd">    based on the information received, without calling this tool again.</span>
<span class="w"> </span>
<span class="sd">    Args:</span>
<span class="sd">        query: The query to ask the human expert.</span>
<span class="w"> </span>
<span class="sd">    Returns:</span>
<span class="sd">        The response from the human expert.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">(</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Is this correct?&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;birthday&quot;</span><span class="p">:</span> <span class="n">birthday</span><span class="p">,</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">    </span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># If the information is correct, update the state as-is.</span>
<span class="w">    </span><span class="k">if</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;correct&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="n">verified_name</span> <span class="o">=</span> <span class="n">name</span>
<span class="w">        </span><span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">birthday</span>
<span class="w">        </span><span class="n">response</span> <span class="o">=</span> <span class="s2">&quot;Correct&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Otherwise, receive information from the human reviewer.</span>
<span class="w">    </span><span class="k">else</span><span class="p">:</span>
<span class="w">        </span><span class="n">verified_name</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
<span class="w">        </span><span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;birthday&quot;</span><span class="p">,</span> <span class="n">birthday</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Made a correction: </span><span class="si">{</span><span class="n">human_response</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># This time we explicitly update the state with a ToolMessage inside</span>
<span class="w">    </span><span class="c1"># the tool.</span>
<span class="w">    </span><span class="n">state_update</span> <span class="o">=</span> <span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">verified_name</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;birthday&quot;</span><span class="p">:</span> <span class="n">verified_birthday</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">ToolMessage</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call_id</span><span class="p">)],</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span>
<span class="w">    </span><span class="c1"># We return a Command object in the tool to update our state.</span>
<span class="w">    </span><span class="k">return</span> <span class="n">Command</span><span class="p">(</span><span class="n">update</span><span class="o">=</span><span class="n">state_update</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create the LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-7-sonnet-20250219&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="c1"># Because we will be interrupting during tool execution,</span>
<span class="w">    </span><span class="c1"># we disable parallel tool calling to avoid repeating any</span>
<span class="w">    </span><span class="c1"># tool invocations when we resume.</span>
<span class="w">    </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Checkpointer</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Compile</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Visualize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:
1. Check your internet connection and try again
2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`
3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos pedir ao nosso chatbot que procure o "aniversário" da biblioteca de <code>LangGraph</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;Can you look up when LangGraph was released? &quot;</span>
<span class="w">    </span><span class="s2">&quot;When you have the answer, use the human_assistance tool for review.&quot;</span>
<span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">}]},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &quot;I&#x27;ll look up when LangGraph was released and then get human verification of the information.&quot;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_017SLLSEnFQZVdBpj85BKHyy&#x27;, &#x27;input&#x27;: &#x7B;&#x27;query&#x27;: &#x27;when was LangGraph released launch date&#x27;&#x7D;, &#x27;name&#x27;: &#x27;tavily_search_results_json&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;tavily_search_results_json (toolu_017SLLSEnFQZVdBpj85BKHyy)
 Call ID: toolu_017SLLSEnFQZVdBpj85BKHyy
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: when was LangGraph released launch date
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangChain Introduces LangGraph Studio: The First Agent IDE for ...&quot;, &quot;url&quot;: &quot;https://www.marktechpost.com/2024/08/03/langchain-introduces-langgraph-studio-the-first-agent-ide-for-visualizing-interacting-with-and-debugging-complex-agentic-applications/&quot;, &quot;content&quot;: &quot;LangGraph, launched in January 2023, is a highly controllable, low-level orchestration framework for building agentic applications. Since its inception, it has undergone significant improvements, leading to a stable 0.1 release in June. LangGraph features a persistence layer enabling human-in-the-loop interactions and excels at building complex applications requiring domain-specific cognitive architecture.&quot;, &quot;score&quot;: 0.83742094&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph Studio: The first agent IDE | by Bhavik Jikadara - Medium&quot;, &quot;url&quot;: &quot;https://bhavikjikadara.medium.com/langgraph-studio-the-first-agent-ide-468132628274&quot;, &quot;content&quot;: &quot;LangGraph, launched in January 2023, is a low-level orchestration framework designed for building controllable and complex agentic applications. It’s beneficial for creating applications requiring highly domain-specific cognitive architecture and human-in-the-loop interactions. LangGraph is open source, available in Python and JavaScript, and integrates seamlessly with LangSmith, whether or not you use LangChain.\n\nLangGraph: A Comprehensive Guide for Beginners&quot;, &quot;score&quot;: 0.79369855&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &quot;Based on my search, I found that LangGraph was launched in January 2023. It&#x27;s described as a low-level orchestration framework for building agentic applications. Since its release, it has seen significant improvements, including a stable 0.1 release in June (presumably 2024).\n\nLet me now get human verification of this information:&quot;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_016h3391yFhtPDhQvwjNgs7W&#x27;, &#x27;input&#x27;: &#x7B;&#x27;name&#x27;: &#x27;Information Verification&#x27;, &#x27;birthday&#x27;: &#x27;January 2023&#x27;&#x7D;, &#x27;name&#x27;: &#x27;human_assistance&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;human_assistance (toolu_016h3391yFhtPDhQvwjNgs7W)
 Call ID: toolu_016h3391yFhtPDhQvwjNgs7W
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;name: Information Verification
&#x20;&#x20;&#x20;&#x20;birthday: January 2023
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError(&#x27;403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart&#x27;, &#x27;&#x7B;&quot;error&quot;:&quot;Forbidden&quot;&#x7D;\n&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Parou pelo <code>interrupt</code> na ferramenta <code>human_assistance</code>. Neste caso, o chatbot, com a ferramenta de busca, determinou que a data do LangGraph é janeiro de 2023, mas não é a data exata, é 17 de janeiro de 2024, então podemos introduzi-la nós mesmos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span>
<span class="w">    </span><span class="n">resume</span><span class="o">=</span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;LangGraph&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;birthday&quot;</span><span class="p">:</span> <span class="s2">&quot;Jan 17, 2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="p">},</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================== Ai Message ==================================

[&#x7B;&#x27;text&#x27;: &quot;Based on my search, I found that LangGraph was launched in January 2023. It&#x27;s described as a low-level orchestration framework for building agentic applications. Since its release, it has seen significant improvements, including a stable 0.1 release in June (presumably 2024).\n\nLet me now get human verification of this information:&quot;, &#x27;type&#x27;: &#x27;text&#x27;&#x7D;, &#x7B;&#x27;id&#x27;: &#x27;toolu_016h3391yFhtPDhQvwjNgs7W&#x27;, &#x27;input&#x27;: &#x7B;&#x27;name&#x27;: &#x27;Information Verification&#x27;, &#x27;birthday&#x27;: &#x27;January 2023&#x27;&#x7D;, &#x27;name&#x27;: &#x27;human_assistance&#x27;, &#x27;type&#x27;: &#x27;tool_use&#x27;&#x7D;]
Tool Calls:
&#x20;&#x20;human_assistance (toolu_016h3391yFhtPDhQvwjNgs7W)
 Call ID: toolu_016h3391yFhtPDhQvwjNgs7W
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;name: Information Verification
&#x20;&#x20;&#x20;&#x20;birthday: January 2023
================================= Tool Message =================================
Name: human_assistance

Made a correction: &#x7B;&#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;birthday&#x27;: &#x27;Jan 17, 2024&#x27;&#x7D;
================================== Ai Message ==================================

Thank you for the expert correction! I need to update my response with the accurate information.

LangGraph was actually released on January 17, 2024 - not January 2023 as I initially found in my search results. This is a significant correction, as it means LangGraph is a much more recent framework than the search results indicated. 

The expert has provided the specific date (January 17, 2024) for LangGraph&#x27;s release, making it a fairly new tool in the AI orchestration ecosystem. This timing aligns better with the mention of its stable 0.1 release in June 2024, as this would be about 5 months after its initial launch.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="w"> </span>
<span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;birthday&quot;</span><span class="p">)}</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;name&#x27;: &#x27;LangGraph&#x27;, &#x27;birthday&#x27;: &#x27;Jan 17, 2024&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora a data está correta graças à intervenção humana para modificar os valores do estado</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Atualizacao manual do estado">Atualização manual do estado<a class="anchor-link" href="#Atualizacao manual do estado">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>LangGraph fornece um alto grau de controle sobre o estado do aplicativo. Por exemplo, em qualquer ponto (mesmo quando é interrompido), podemos reescrever manualmente uma chave do estado usando <code>graph.update_state</code>:</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a atualizar o <code>name</code> do estado para <code>LangGraph (biblioteca)</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;LangGraph (library)&quot;</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;1&#x27;,
&#x20;&#x20;&#x27;checkpoint_ns&#x27;: &#x27;&#x27;,
&#x20;&#x20;&#x27;checkpoint_id&#x27;: &#x27;1f010a5a-8a70-618e-8006-89107653db68&#x27;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se agora vermos o estado com <code>graph.get_state(config)</code> veremos que o <code>name</code> foi atualizado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="w"> </span>
<span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;birthday&quot;</span><span class="p">)}</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;name&#x27;: &#x27;LangGraph (library)&#x27;, &#x27;birthday&#x27;: &#x27;Jan 17, 2024&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As atualizações de estado manuais gerarão uma trilha em <code>LangSmith</code>. Elas podem ser usadas para controlar fluxos de trabalho de <code>human in the loop</code>, como pode ser visto nesta <a href="https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/">guia</a>.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Pontos de controle">Pontos de controle<a class="anchor-link" href="#Pontos de controle">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Em um fluxo de trabalho típico de um chatbot, o usuário interage com o chatbot uma ou mais vezes para realizar uma tarefa. Nas seções anteriores, vimos como adicionar memória e um <code>human in the loop</code> para poder verificar nosso estado de gráfico e controlar as respostas futuras.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mas, talvez um usuário queira começar a partir de uma resposta anterior e quer <code>ramificar</code> para explorar um resultado separado. Isso é útil para aplicações de agentes, quando um fluxo falha eles podem voltar a um checkpoint anterior e tentar outra estratégia.</p>
</section>
<section class="section-block-markdown-cell">
<p><code>LangGraph</code> fornece essa possibilidade através dos <code>checkpoints</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro carregamos os valores das chaves da API.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="w"> </span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o novo estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos o grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="w"> </span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos a <code>ferramenta</code> de busca</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="w"> </span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma lista de <code>tools</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, o <code>LLM</code> com as <code>bind_tools</code> e adicionamos ao gráfico</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>

<span class="c1"># Create the LLM</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Modification: tell the LLM which tools it can call</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Add the chatbot node</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x10d8ce7b0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos a <code>tool</code> ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="w"> </span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x10d8ce7b0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Adicionamos o nódo de <code>START</code> ao grafo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;langgraph.graph.state.StateGraph at 0x10d8ce7b0&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos um <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver">MemorySaver</a>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Compilamos o grafo com o <code>checkpointer</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O representamos graficamente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;IPython.core.display.Image object&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos fazer nosso grafo dar alguns passos. Cada passo será salvo no histórico do estado.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a primeira chamada ao modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;I&#39;m learning LangGraph. &quot;</span>
<span class="w">    </span><span class="s2">&quot;Could you do some research on it for me?&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I&#x27;m learning LangGraph. Could you do some research on it for me?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Quickstart - GitHub Pages&quot;, &quot;url&quot;: &quot;https://langchain-ai.github.io/langgraph/tutorials/introduction/&quot;, &quot;content&quot;: &quot;[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It&#x27;s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It&#x27;s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)&quot;, &quot;score&quot;: 0.9328032&#x7D;, &#x7B;&quot;title&quot;: &quot;langchain-ai/langgraph: Build resilient language agents as graphs.&quot;, &quot;url&quot;: &quot;https://github.com/langchain-ai/langgraph&quot;, &quot;content&quot;: &quot;LangGraph — used by Replit, Uber, LinkedIn, GitLab and more — is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration — offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\n\n```\npip install -U langgraph\n```&quot;, &quot;score&quot;: 0.8884594&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Quickstart - GitHub Pages&quot;, &quot;url&quot;: &quot;https://langchain-ai.github.io/langgraph/tutorials/introduction/&quot;, &quot;content&quot;: &quot;[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It&#x27;s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It&#x27;s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)&quot;, &quot;score&quot;: 0.9328032&#x7D;, &#x7B;&quot;title&quot;: &quot;langchain-ai/langgraph: Build resilient language agents as graphs.&quot;, &quot;url&quot;: &quot;https://github.com/langchain-ai/langgraph&quot;, &quot;content&quot;: &quot;LangGraph — used by Replit, Uber, LinkedIn, GitLab and more — is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration — offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\n\n```\npip install -U langgraph\n```&quot;, &quot;score&quot;: 0.8884594&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial and documentation
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Quickstart - GitHub Pages&quot;, &quot;url&quot;: &quot;https://langchain-ai.github.io/langgraph/tutorials/introduction/&quot;, &quot;content&quot;: &quot;[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-66-36)5. Documentation: The LangGraph documentation has been revamped, which should make it easier for learners like yourself to understand and use the tool.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-66-37) [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-28)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-29)1. Search for the official LangGraph documentation or website to learn more about its features and how to use it.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-30)2. Look for tutorials or guides specifically focused on building AI agents with LangGraph. [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It&#x27;s particularly useful for developing more complex,&quot;, &quot;score&quot;: 0.8775715&#x7D;, &#x7B;&quot;title&quot;: &quot;Tutorial 1-Getting Started With LangGraph- Building Stateful Multi AI ...&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=gqvFmK7LpDo&quot;, &quot;content&quot;: &quot;and we will also use Lang Smith so let&#x27;s go ahead and execute this lsmith I hope everybody knows what exactly is so till then I will also go ahead and show you the documentation page of Lang graph so this is what langra is all about right it has python it has it supports JavaScript and all but uh uh if I just go ahead and show you Lang graph tutorials right so here you can see this is the tutorial right and uh not this sorry uh let&#x27;s see yes yes here you go right in the Lang graph page it&quot;, &quot;score&quot;: 0.80405265&#x7D;]
================================== Ai Message ==================================

LangGraph is a powerful library designed for building stateful, multi-agent applications using Large Language Models (LLMs). Here are some key points about LangGraph:

### Overview
- **Purpose**: LangGraph is specifically designed to create complex workflows and state machines to coordinate multiple AI agents or language model interactions. It is particularly useful for developing sophisticated multi-agent systems.
- **Framework**: It is built on top of LangChain, leveraging its components and adding graph-based coordination capabilities.
- **Features**:
&#x20;&#x20;- **Customizable Architectures**: Allows you to design and implement custom workflows and state machines.
&#x20;&#x20;- **Long-Term Memory**: Supports long-term memory for agents, enabling them to maintain context over time.
&#x20;&#x20;- **Human-in-the-Loop**: Facilitates human interaction in the workflow, making it easier to handle complex tasks that require human oversight.

### Getting Started
- **Installation**:
&#x20;&#x20;```bash
&#x20;&#x20;pip install -U langgraph
&#x20;&#x20;```

- **Documentation**: The LangGraph documentation has been revamped to make it easier for learners to understand and use the tool. You can find the official documentation [here](https://langchain-ai.github.io/langgraph/tutorials/introduction/).

### Use Cases
- **Multi-Agent Systems**: Ideal for building systems where multiple AI agents need to interact and coordinate their actions.
- **Complex Task Handling**: Suitable for tasks that require multiple steps and decision-making processes.
- **Custom Workflows**: Enables the creation of custom workflows tailored to specific use cases.

### Tutorials and Resources
- **Official Documentation**: The official LangGraph documentation is a comprehensive resource for learning about its features and usage.
- **Tutorials**: Look for tutorials and guides specifically focused on building AI agents with LangGraph. You can find a tutorial video [here](https://www.youtube.com/watch?v=gqvFmK7LpDo).

### Companies Using LangGraph
- **Replit, Uber, LinkedIn, GitLab, and more**: These companies are using LangGraph to build resilient and controllable language agents.

### Next Steps
1. **Review the Documentation**: Start by going through the official LangGraph documentation to get a deeper understanding of its features and capabilities.
2. **Follow Tutorials**: Watch tutorials and follow step-by-step guides to build your first multi-agent application.
3. **Experiment with Examples**: Try out the examples provided in the documentation to get hands-on experience with LangGraph.

If you have any specific questions or need further assistance, feel free to ask!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E agora a segunda chamada</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;Ya that&#39;s helpful. Maybe I&#39;ll &quot;</span>
<span class="w">    </span><span class="s2">&quot;build an autonomous agent with it!&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Ya that&#x27;s helpful. Maybe I&#x27;ll build an autonomous agent with it!
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial build autonomous agent
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Tutorial: Building LLM Agents with LangChain&#x27;s ... - Zep&quot;, &quot;url&quot;: &quot;https://www.getzep.com/ai-agents/langgraph-tutorial&quot;, &quot;content&quot;: &quot;This article focuses on building agents with LangGraph rather than LangChain. It provides a tutorial for building LangGraph agents, beginning with a discussion of LangGraph and its components. These concepts are reinforced by building a LangGraph agent from scratch and managing conversation memory with LangGraph agents. Finally, we use Zep&#x27;s long-term memory for egents to create an agent that remembers previous conversations and user facts.Â\n\nâ\n\nSummary of key LangGraph tutorial concepts [...] human intervention, and the ability to handle complex workflows with\n      cycles and branches.\nBuilding a LangGraph agent | Creating a LangGraph agent is the best way to understand the core concepts\n      of nodes, edges, and state. The LangGraph Python libraries are modular and\n      provide the functionality to build a stateful graph by incrementally\n      adding nodes and edges.Incorporating tools enables an agent to perform specific tasks and access&quot;, &quot;score&quot;: 0.8338803&#x7D;, &#x7B;&quot;title&quot;: &quot;Build Autonomous AI Agents with ReAct and LangGraph Tools&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=ZfjaIshGkmk&quot;, &quot;content&quot;: &quot;LangGraph Intro - Build Autonomous AI Agents with ReAct and LangGraph Tools\nGrabDuck!\n4110 subscribers\n18 likes\n535 views\n21 Jan 2025\nIn this video, LangGraph Intro: Build Autonomous AI Agents with ReAct and LangGraph Tools, we dive into creating a powerful agentic system where the LLM decides when to trigger tools and when to finalize results. You’ll see how to build a generic agent architecture using the ReAct principle, applying it to real-world examples like analyzing Tesla stock data. [...] reasoning like what they&#x27;re doing so uh it&#x27;s this way you&#x27;re using tool and this is another thing from longchain core library and here you define the function and then you have to Define name description there are other parameters like for example you can provide very specific description of all the parameters like why you need them which one are those Etc but it&#x27;s a bit over complicated for this tutorial I&#x27;m skipping it and uh interesting thing this one return direct is false and this is uh [...] Whether you’re wondering how to create AI agents, looking for a LangGraph tutorial, or eager to explore the power of LangChain agents, this video is packed with valuable insights to help you get started.\n\nSupport the channel while you shop on Amazon! \nUse my affiliate link https://amzn.to/4hssSvT\n\nEvery purchase via this Amazon link helps keep our content free for you!\n\n🌟 Related Courses &amp;amp; Tutorials&quot;, &quot;score&quot;: 0.8286204&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial build autonomous agent
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Tutorial: Building LLM Agents with LangChain&#x27;s ... - Zep&quot;, &quot;url&quot;: &quot;https://www.getzep.com/ai-agents/langgraph-tutorial&quot;, &quot;content&quot;: &quot;This article focuses on building agents with LangGraph rather than LangChain. It provides a tutorial for building LangGraph agents, beginning with a discussion of LangGraph and its components. These concepts are reinforced by building a LangGraph agent from scratch and managing conversation memory with LangGraph agents. Finally, we use Zep&#x27;s long-term memory for egents to create an agent that remembers previous conversations and user facts.Â\n\nâ\n\nSummary of key LangGraph tutorial concepts [...] human intervention, and the ability to handle complex workflows with\n      cycles and branches.\nBuilding a LangGraph agent | Creating a LangGraph agent is the best way to understand the core concepts\n      of nodes, edges, and state. The LangGraph Python libraries are modular and\n      provide the functionality to build a stateful graph by incrementally\n      adding nodes and edges.Incorporating tools enables an agent to perform specific tasks and access&quot;, &quot;score&quot;: 0.8338803&#x7D;, &#x7B;&quot;title&quot;: &quot;Build Autonomous AI Agents with ReAct and LangGraph Tools&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=ZfjaIshGkmk&quot;, &quot;content&quot;: &quot;LangGraph Intro - Build Autonomous AI Agents with ReAct and LangGraph Tools\nGrabDuck!\n4110 subscribers\n18 likes\n535 views\n21 Jan 2025\nIn this video, LangGraph Intro: Build Autonomous AI Agents with ReAct and LangGraph Tools, we dive into creating a powerful agentic system where the LLM decides when to trigger tools and when to finalize results. You’ll see how to build a generic agent architecture using the ReAct principle, applying it to real-world examples like analyzing Tesla stock data. [...] reasoning like what they&#x27;re doing so uh it&#x27;s this way you&#x27;re using tool and this is another thing from longchain core library and here you define the function and then you have to Define name description there are other parameters like for example you can provide very specific description of all the parameters like why you need them which one are those Etc but it&#x27;s a bit over complicated for this tutorial I&#x27;m skipping it and uh interesting thing this one return direct is false and this is uh [...] Whether you’re wondering how to create AI agents, looking for a LangGraph tutorial, or eager to explore the power of LangChain agents, this video is packed with valuable insights to help you get started.\n\nSupport the channel while you shop on Amazon! \nUse my affiliate link https://amzn.to/4hssSvT\n\nEvery purchase via this Amazon link helps keep our content free for you!\n\n🌟 Related Courses &amp;amp; Tutorials&quot;, &quot;score&quot;: 0.8286204&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial build autonomous agent
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;LangGraph Tutorial: Building LLM Agents with LangChain&#x27;s ... - Zep&quot;, &quot;url&quot;: &quot;https://www.getzep.com/ai-agents/langgraph-tutorial&quot;, &quot;content&quot;: &quot;This article focuses on building agents with LangGraph rather than LangChain. It provides a tutorial for building LangGraph agents, beginning with a discussion of LangGraph and its components. These concepts are reinforced by building a LangGraph agent from scratch and managing conversation memory with LangGraph agents. Finally, we use Zep&#x27;s long-term memory for egents to create an agent that remembers previous conversations and user facts.Â\n\nâ\n\nSummary of key LangGraph tutorial concepts [...] human intervention, and the ability to handle complex workflows with\n      cycles and branches.\nBuilding a LangGraph agent | Creating a LangGraph agent is the best way to understand the core concepts\n      of nodes, edges, and state. The LangGraph Python libraries are modular and\n      provide the functionality to build a stateful graph by incrementally\n      adding nodes and edges.Incorporating tools enables an agent to perform specific tasks and access&quot;, &quot;score&quot;: 0.8338803&#x7D;, &#x7B;&quot;title&quot;: &quot;Build Autonomous AI Agents with ReAct and LangGraph Tools&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=ZfjaIshGkmk&quot;, &quot;content&quot;: &quot;LangGraph Intro - Build Autonomous AI Agents with ReAct and LangGraph Tools\nGrabDuck!\n4110 subscribers\n18 likes\n535 views\n21 Jan 2025\nIn this video, LangGraph Intro: Build Autonomous AI Agents with ReAct and LangGraph Tools, we dive into creating a powerful agentic system where the LLM decides when to trigger tools and when to finalize results. You’ll see how to build a generic agent architecture using the ReAct principle, applying it to real-world examples like analyzing Tesla stock data. [...] reasoning like what they&#x27;re doing so uh it&#x27;s this way you&#x27;re using tool and this is another thing from longchain core library and here you define the function and then you have to Define name description there are other parameters like for example you can provide very specific description of all the parameters like why you need them which one are those Etc but it&#x27;s a bit over complicated for this tutorial I&#x27;m skipping it and uh interesting thing this one return direct is false and this is uh [...] Whether you’re wondering how to create AI agents, looking for a LangGraph tutorial, or eager to explore the power of LangChain agents, this video is packed with valuable insights to help you get started.\n\nSupport the channel while you shop on Amazon! \nUse my affiliate link https://amzn.to/4hssSvT\n\nEvery purchase via this Amazon link helps keep our content free for you!\n\n🌟 Related Courses &amp;amp; Tutorials&quot;, &quot;score&quot;: 0.8286204&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial for building autonomous AI agents
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;How to Build AI Agents with LangGraph: A Step-by-Step Guide&quot;, &quot;url&quot;: &quot;https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832&quot;, &quot;content&quot;: &quot;By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs. This tutorial demonstrates the power of LangGraph in managing complex, multi-step processes and highlights how to leverage advanced AI tools to solve real-world challenges efficiently. Whether you’re developing AI agents for customer support, energy management, or other applications, LangGraph provides the flexibility, scalability, and [...] Step 7: Build the Graph Structure\nIn this step, we construct the graph structure for the AI assistant using LangGraph, which controls how the assistant processes user input, triggers tools, and moves between stages. The graph defines nodes for the core actions (like invoking the assistant and tool) and edges that dictate the flow between these nodes. [...] Now that we have a solid understanding of what LangGraph is and how it enhances AI development, let’s dive into a practical example. In this scenario, we’ll build an AI agent designed to calculate potential energy savings for solar panels based on user input. This agent can be implemented as a lead generation tool on a solar panel seller’s website, where it interacts with potential customers, offering personalized savings estimates. By gathering key data such as monthly electricity costs, this&quot;, &quot;score&quot;: 0.8576849&#x7D;, &#x7B;&quot;title&quot;: &quot;Building AI Agents with LangGraph: A Beginner&#x27;s Guide - YouTube&quot;, &quot;url&quot;: &quot;https://www.youtube.com/watch?v=assrhPxNdSk&quot;, &quot;content&quot;: &quot;In this tutorial, we&#x27;ll break down the fundamentals of building AI agents using LangGraph! Whether you&#x27;re new to AI development or looking&quot;, &quot;score&quot;: 0.834852&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial step-by-step
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;How to Build AI Agents with LangGraph: A Step-by-Step Guide&quot;, &quot;url&quot;: &quot;https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832&quot;, &quot;content&quot;: &quot;By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs. This tutorial demonstrates the power of LangGraph in managing complex, multi-step processes and highlights how to leverage advanced AI tools to solve real-world challenges efficiently. Whether you’re developing AI agents for customer support, energy management, or other applications, LangGraph provides the flexibility, scalability, and [...] Step 7: Build the Graph Structure\nIn this step, we construct the graph structure for the AI assistant using LangGraph, which controls how the assistant processes user input, triggers tools, and moves between stages. The graph defines nodes for the core actions (like invoking the assistant and tool) and edges that dictate the flow between these nodes. [...] In this article, we’ll explore how LangGraph transforms AI development and provide a step-by-step guide on how to build your own AI agent using an example that computes energy savings for solar panels. This example will showcase how LangGraph’s unique features can create intelligent, adaptable, and real-world-ready AI systems.\nWhat is LangGraph?&quot;, &quot;score&quot;: 0.86441374&#x7D;, &#x7B;&quot;title&quot;: &quot;What Is LangGraph and How to Use It? - DataCamp&quot;, &quot;url&quot;: &quot;https://www.datacamp.com/tutorial/langgraph-tutorial&quot;, &quot;content&quot;: &quot;Building a Simple LangGraph Application\nHere’s a step-by-step example of creating a basic chatbot application using LangGraph.\nStep 1: Define the StateGraph\nDefine a StateGraph object to structure the chatbot as a state machine. The State is a class object defined with a single key messages of type List and uses the add_messages() function to append new messages rather than overwrite them.\nfrom typing import Annotated\nfrom typing_extensions import TypedDict [...] Getting Started With LangGraph\n\nInstallation\n\nBasic Concepts\n\n\nBuilding a Simple LangGraph Application\n\nStep 1: Define the StateGraph\nStep 2: Initialize an LLM and add it as a Chatbot node\nStep 3: Set edges\n\nStep 5: Run the chatbot\n\n\nAdvanced LangGraph Features\n\nCustom node types\nEdge types\n\nError handling\n\n\nReal-World Applications of LangGraph\n\nChatbots\nAutonomous agents\nMulti-Agent systems\nWorkflow automation tools\nRecommendation systems\n\nPersonalized learning environments\n\n\nConclusion&quot;, &quot;score&quot;: 0.82492816&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial for beginners
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Introduction to LangGraph: A Beginner&#x27;s Guide - Medium&quot;, &quot;url&quot;: &quot;https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141&quot;, &quot;content&quot;: &quot;Conclusion\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding! [...] LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\nWhat is LangGraph?&quot;, &quot;score&quot;: 0.8793233&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph Tutorial: A Comprehensive Guide for Beginners&quot;, &quot;url&quot;: &quot;https://blog.futuresmart.ai/langgraph-tutorial-for-beginners&quot;, &quot;content&quot;: &quot;These examples highlight how LangGraph helps bridge the gap between AI capabilities and the complexities of real-world situations.\nConclusion\nThis concludes our LangGraph tutorial! As you&#x27;ve learned, LangGraph enables the creation of AI applications that go beyond simple input-output loops by offering a framework for building stateful, agent-driven systems. You&#x27;ve gained hands-on experience defining graphs, managing state, and incorporating tools. [...] LangGraph, a powerful library within the LangChain ecosystem, provides an elegant solution for building and managing multi-agent LLM applications. By representing workflows as cyclical graphs, LangGraph allows developers to orchestrate the interactions of multiple LLM agents, ensuring smooth communication and efficient execution of complex tasks. [...] LangGraph Tutorial: A Comprehensive Guide for Beginners\n\nFutureSmart AI Blog\nFollow\nFutureSmart AI Blog\nFollow\n\nLangGraph Tutorial: A Comprehensive Guide for Beginners\n\n\n+1\nRounak Show\nwith 1 co-author\n·Oct 1, 2024·12 min read\n\nTable of contents\n\nIntroduction\nUnderstanding LangGraph\nKey Concepts\nGraph Structures\nState Management\n\n\nGetting Started with LangGraph\nInstallation\nCreating a Basic Chatbot in LangGraph&quot;, &quot;score&quot;: 0.8684817&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph tutorial for beginners
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;Introduction to LangGraph: A Beginner&#x27;s Guide - Medium&quot;, &quot;url&quot;: &quot;https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141&quot;, &quot;content&quot;: &quot;Conclusion\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding! [...] LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\nWhat is LangGraph?&quot;, &quot;score&quot;: 0.8793233&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph Tutorial: A Comprehensive Guide for Beginners&quot;, &quot;url&quot;: &quot;https://blog.futuresmart.ai/langgraph-tutorial-for-beginners&quot;, &quot;content&quot;: &quot;These examples highlight how LangGraph helps bridge the gap between AI capabilities and the complexities of real-world situations.\nConclusion\nThis concludes our LangGraph tutorial! As you&#x27;ve learned, LangGraph enables the creation of AI applications that go beyond simple input-output loops by offering a framework for building stateful, agent-driven systems. You&#x27;ve gained hands-on experience defining graphs, managing state, and incorporating tools. [...] LangGraph, a powerful library within the LangChain ecosystem, provides an elegant solution for building and managing multi-agent LLM applications. By representing workflows as cyclical graphs, LangGraph allows developers to orchestrate the interactions of multiple LLM agents, ensuring smooth communication and efficient execution of complex tasks. [...] LangGraph Tutorial: A Comprehensive Guide for Beginners\n\nFutureSmart AI Blog\nFollow\nFutureSmart AI Blog\nFollow\n\nLangGraph Tutorial: A Comprehensive Guide for Beginners\n\n\n+1\nRounak Show\nwith 1 co-author\n·Oct 1, 2024·12 min read\n\nTable of contents\n\nIntroduction\nUnderstanding LangGraph\nKey Concepts\nGraph Structures\nState Management\n\n\nGetting Started with LangGraph\nInstallation\nCreating a Basic Chatbot in LangGraph&quot;, &quot;score&quot;: 0.8684817&#x7D;]
================================== Ai Message ==================================

Building an autonomous AI agent with LangGraph can be a rewarding and exciting project! Here&#x27;s a step-by-step guide to get you started, based on the tutorials and resources available:

### Step 1: Understand the Basics of LangGraph
LangGraph is a library that extends the capabilities of LangChain, focusing on building stateful, multi-actor applications using Large Language Models (LLMs). It allows you to coordinate multiple LLM agents in complex workflows, using a graph-based approach.

### Step 2: Install LangGraph
First, you need to install the LangGraph library. You can do this using pip:
```bash
pip install -U langgraph
```

### Step 3: Define the Graph Structure
The core of LangGraph is the graph structure, which defines the nodes (actions or states) and edges (transitions between nodes).

#### Example: Solar Panel Energy Savings Calculator
Let&#x27;s build a simple AI agent that calculates potential energy savings for solar panels based on user input.

1. **Define the StateGraph**:
&#x20;&#x20;&#x20;- **Nodes**: These represent actions or states in your application.
&#x20;&#x20;&#x20;- **Edges**: These define the transitions between nodes.

```python
from langgraph import StateGraph, State, Edge

# Define the nodes
start_node = State(key=&quot;start&quot;, action=&quot;greet_user&quot;)
input_node = State(key=&quot;input&quot;, action=&quot;get_user_input&quot;)
calculate_node = State(key=&quot;calculate&quot;, action=&quot;calculate_savings&quot;)
result_node = State(key=&quot;result&quot;, action=&quot;show_results&quot;)

# Define the edges
start_to_input = Edge(from_node=start_node, to_node=input_node)
input_to_calculate = Edge(from_node=input_node, to_node=calculate_node)
calculate_to_result = Edge(from_node=calculate_node, to_node=result_node)

# Create the graph
graph = StateGraph()
graph.add_state(start_node)
graph.add_state(input_node)
graph.add_state(calculate_node)
graph.add_state(result_node)
graph.add_edge(start_to_input)
graph.add_edge(input_to_calculate)
graph.add_edge(calculate_to_result)
```

### Step 4: Define the Actions
Each node in the graph has an associated action. These actions are Python functions that perform specific tasks.

```python
def greet_user(state, context):
&#x20;&#x20;&#x20;&#x20;return &#x7B;&quot;message&quot;: &quot;Hello! I can help you calculate energy savings for solar panels.&quot;&#x7D;

def get_user_input(state, context):
&#x20;&#x20;&#x20;&#x20;return &#x7B;&quot;message&quot;: &quot;Please provide your monthly electricity cost in dollars.&quot;&#x7D;

def calculate_savings(state, context):
&#x20;&#x20;&#x20;&#x20;# Example calculation (you can use real data and more complex logic here)
&#x20;&#x20;&#x20;&#x20;monthly_cost = float(input(&quot;Enter your monthly electricity cost: &quot;))
&#x20;&#x20;&#x20;&#x20;savings_per_kWh = 0.10  # Example savings rate
&#x20;&#x20;&#x20;&#x20;annual_savings = monthly_cost * 12 * savings_per_kWh
&#x20;&#x20;&#x20;&#x20;return &#x7B;&quot;savings&quot;: annual_savings&#x7D;

def show_results(state, context):
&#x20;&#x20;&#x20;&#x20;annual_savings = context.get(&quot;savings&quot;)
&#x20;&#x20;&#x20;&#x20;return &#x7B;&quot;message&quot;: f&quot;Your annual savings with solar panels could be $&#x7B;annual_savings:.2f&#x7D;.&quot;&#x7D;
```

### Step 5: Run the Graph
Finally, you can run the graph to see how the agent processes user input and performs the calculations.

```python
# Initialize the graph and run it
context = &#x7B;&#x7D;
current_node = start_node

while current_node:
&#x20;&#x20;&#x20;&#x20;action_result = current_node.action(current_node, context)
&#x20;&#x20;&#x20;&#x20;print(action_result[&quot;message&quot;])
&#x20;&#x20;&#x20;&#x20;
&#x20;&#x20;&#x20;&#x20;if &quot;savings&quot; in action_result:
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;context[&quot;savings&quot;] = action_result[&quot;savings&quot;]
&#x20;&#x20;&#x20;&#x20;
&#x20;&#x20;&#x20;&#x20;current_node = graph.get_next_node(current_node, action_result)
```

### Step 6: Enhance with Advanced Features
Once you have the basic structure in place, you can enhance your agent with advanced features such as:
- **Long-term Memory**: Use external storage (e.g., Zep) to remember user conversations and preferences.
- **Conditional Edges**: Define conditions for transitions between nodes to handle different scenarios.
- **Human-in-the-Loop**: Allow human intervention for complex tasks or error handling.

### Additional Resources
- **Official Documentation**: [LangGraph Documentation](https://langchain-ai.github.io/langgraph/tutorials/introduction/)
- **Comprehensive Guide**: [LangGraph Tutorial for Beginners](https://blog.futuresmart.ai/langgraph-tutorial-for-beginners)
- **Example Project**: [Building AI Agents with LangGraph](https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832)

### Conclusion
By following these steps, you can build a robust and flexible AI agent using LangGraph. Start with simple examples and gradually add more complex features to create powerful, stateful, and multi-actor applications. Happy coding!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora que fizemos duas chamadas ao modelo, vamos verificar o histórico do estado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">to_replay</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s1">&#39;checkpoint_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Get state when first iteracction us done</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="n">to_replay</span> <span class="o">=</span> <span class="n">state</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Num Messages: 24, Next: (), checkpoint id = 1f027f2f-e5b4-6c84-8018-9fcb33b5f397
--------------------------------------------------------------------------------
Num Messages: 23, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-e414-6b0e-8017-3ad465b70767
--------------------------------------------------------------------------------
Num Messages: 22, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-d382-6692-8016-fcfaf9c9a9f7
--------------------------------------------------------------------------------
Num Messages: 21, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-d1cf-6930-8015-f64aa0e6f750
--------------------------------------------------------------------------------
Num Messages: 20, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-bca9-6164-8014-86452cb10d83
--------------------------------------------------------------------------------
Num Messages: 19, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-bac1-6d24-8013-b539f3e4cedb
--------------------------------------------------------------------------------
Num Messages: 18, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-aa0e-69fa-8012-4ca2d9109f4e
--------------------------------------------------------------------------------
Num Messages: 17, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-a861-62c4-8011-5707badab130
--------------------------------------------------------------------------------
Num Messages: 16, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-93cf-6112-8010-ee536e76cdf7
--------------------------------------------------------------------------------
Num Messages: 15, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-91f5-63fa-800f-6ff45b0ebf86
--------------------------------------------------------------------------------
Num Messages: 14, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-7e07-6190-800e-e0269b0cb0f4
--------------------------------------------------------------------------------
Num Messages: 13, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-7bf9-62a4-800d-bd2bf25381ac
--------------------------------------------------------------------------------
Num Messages: 12, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-639f-6172-800c-e54c8b1b1f4a
--------------------------------------------------------------------------------
Num Messages: 11, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-621b-6972-800b-184a824ce9cb
--------------------------------------------------------------------------------
Num Messages: 10, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-56df-66a8-800a-d56ee9317382
--------------------------------------------------------------------------------
Num Messages: 9, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-5546-60d0-8009-41ee7c932b49
--------------------------------------------------------------------------------
Num Messages: 8, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f027f2f-5542-6ff2-8008-e2f4e8278c23
--------------------------------------------------------------------------------
Num Messages: 8, Next: (), checkpoint id = 1f027f2c-8873-61d6-8007-8a1c60438002
--------------------------------------------------------------------------------
Num Messages: 7, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-8504-663a-8006-517227b123b6
--------------------------------------------------------------------------------
Num Messages: 6, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2c-75dc-6248-8005-e198dd299848
--------------------------------------------------------------------------------
Num Messages: 5, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-7448-69d6-8004-e3c6d5c4c5a4
--------------------------------------------------------------------------------
Num Messages: 4, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2c-627b-6f6e-8003-22208fac7c89
--------------------------------------------------------------------------------
Num Messages: 3, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-6122-6190-8002-b745c42a724e
--------------------------------------------------------------------------------
Num Messages: 2, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2c-4c4c-6720-8001-8a1c73b894c1
--------------------------------------------------------------------------------
Num Messages: 1, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-4a91-6278-8000-56b65f6d77cd
--------------------------------------------------------------------------------
Num Messages: 0, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f027f2c-4a8d-6a1a-bfff-2f7cbde97290
--------------------------------------------------------------------------------
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Salvamos o estado do grafo em <code>to_replay</code> quando recebemos a primeira resposta, logo antes de introduzir a segunda mensagem. Podemos voltar a um estado passado e continuar o fluxo a partir daí.</p>
</section>
<section class="section-block-markdown-cell">
<p>A configuração do checkpoint contém o <code>checkpoint_id</code>, que é um timestamp do fluxo. Podemos vê-lo para verificar que estamos no estado desejado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;1&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;&#x27;, &#x27;checkpoint_id&#x27;: &#x27;1f027f2c-8873-61d6-8007-8a1c60438002&#x27;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se olharmos para a lista de estados anterior, veremos que o ID coincide com o momento de introduzir a segunda mensagem</p>
</section>
<section class="section-block-markdown-cell">
<p>Dando este <code>checkpoint_id</code> a <code>LangGraph</code> carrega o estado naquele momento do fluxo. Então criamos uma nova mensagem e a passamos para o grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;Thanks&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="c1"># The `checkpoint_id` in the `to_replay.config` corresponds to a state we&#39;ve persisted to our checkpointer.</span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>
<span class="w">    </span><span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Thanks
================================== Ai Message ==================================

You&#x27;re welcome! If you have any more questions about LangGraph or any other topics, feel free to ask. Happy learning! 🚀
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s1">&#39;checkpoint_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Num Messages: 10, Next: (), checkpoint id = 1f027f43-71ae-67e0-800a-d84a557441fc
--------------------------------------------------------------------------------
Num Messages: 9, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f43-5b1f-6ad8-8009-34f409789bc4
--------------------------------------------------------------------------------
Num Messages: 8, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f027f43-5b1b-68a2-8008-fbbcbd1c175e
--------------------------------------------------------------------------------
Num Messages: 24, Next: (), checkpoint id = 1f027f2f-e5b4-6c84-8018-9fcb33b5f397
--------------------------------------------------------------------------------
Num Messages: 23, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-e414-6b0e-8017-3ad465b70767
--------------------------------------------------------------------------------
Num Messages: 22, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-d382-6692-8016-fcfaf9c9a9f7
--------------------------------------------------------------------------------
Num Messages: 21, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-d1cf-6930-8015-f64aa0e6f750
--------------------------------------------------------------------------------
Num Messages: 20, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-bca9-6164-8014-86452cb10d83
--------------------------------------------------------------------------------
Num Messages: 19, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-bac1-6d24-8013-b539f3e4cedb
--------------------------------------------------------------------------------
Num Messages: 18, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-aa0e-69fa-8012-4ca2d9109f4e
--------------------------------------------------------------------------------
Num Messages: 17, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-a861-62c4-8011-5707badab130
--------------------------------------------------------------------------------
Num Messages: 16, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-93cf-6112-8010-ee536e76cdf7
--------------------------------------------------------------------------------
Num Messages: 15, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-91f5-63fa-800f-6ff45b0ebf86
--------------------------------------------------------------------------------
Num Messages: 14, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-7e07-6190-800e-e0269b0cb0f4
--------------------------------------------------------------------------------
Num Messages: 13, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-7bf9-62a4-800d-bd2bf25381ac
--------------------------------------------------------------------------------
Num Messages: 12, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-639f-6172-800c-e54c8b1b1f4a
--------------------------------------------------------------------------------
Num Messages: 11, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-621b-6972-800b-184a824ce9cb
--------------------------------------------------------------------------------
Num Messages: 10, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2f-56df-66a8-800a-d56ee9317382
--------------------------------------------------------------------------------
Num Messages: 9, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2f-5546-60d0-8009-41ee7c932b49
--------------------------------------------------------------------------------
Num Messages: 8, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f027f2f-5542-6ff2-8008-e2f4e8278c23
--------------------------------------------------------------------------------
Num Messages: 8, Next: (), checkpoint id = 1f027f2c-8873-61d6-8007-8a1c60438002
--------------------------------------------------------------------------------
Num Messages: 7, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-8504-663a-8006-517227b123b6
--------------------------------------------------------------------------------
Num Messages: 6, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2c-75dc-6248-8005-e198dd299848
--------------------------------------------------------------------------------
Num Messages: 5, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-7448-69d6-8004-e3c6d5c4c5a4
--------------------------------------------------------------------------------
Num Messages: 4, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2c-627b-6f6e-8003-22208fac7c89
--------------------------------------------------------------------------------
Num Messages: 3, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-6122-6190-8002-b745c42a724e
--------------------------------------------------------------------------------
Num Messages: 2, Next: (&#x27;tools&#x27;,), checkpoint id = 1f027f2c-4c4c-6720-8001-8a1c73b894c1
--------------------------------------------------------------------------------
Num Messages: 1, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f027f2c-4a91-6278-8000-56b65f6d77cd
--------------------------------------------------------------------------------
Num Messages: 0, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f027f2c-4a8d-6a1a-bfff-2f7cbde97290
--------------------------------------------------------------------------------
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver no histórico que o grafo executou tudo o que fizemos primeiro, mas depois sobrescreveu o histórico e voltou a executar a partir de um ponto anterior.</p>
</section>
<section class="section-block-markdown-cell">
<p>Reescrevo todo o grafo junto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.utilities.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.tools.tavily_search</span><span class="w"> </span><span class="kn">import</span> <span class="n">TavilySearchResults</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>    <span class="c1"># Disable LangSmith tracing</span>
<span class="w"> </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_LANGGRAPH&quot;</span><span class="p">)</span>
<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TAVILY_LANGGRAPH_API_KEY&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Tools</span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>
<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">]</span>
<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Create the LLM</span>
<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Create the chat model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># Modification: tell the LLM which tools it can call</span>
<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Define the chatbot function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>
<span class="w"> </span>
<span class="c1"># Create the graph</span>
<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add nodes</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add edges</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot_node&quot;</span><span class="p">)</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;chatbot_node&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Add checkpointer</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="w"> </span>
<span class="c1"># Compile</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Visualize</span>
<span class="k">try</span><span class="p">:</span>
<span class="w">    </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:
1. Check your internet connection and try again
2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`
3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a primeira chamada ao modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="w"> </span>
<span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;I&#39;m learning LangGraph. &quot;</span>
<span class="w">    </span><span class="s2">&quot;Could you do some research on it for me?&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

I&#x27;m learning LangGraph. Could you do some research on it for me?
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;What is LangGraph? - IBM&quot;, &quot;url&quot;: &quot;https://www.ibm.com/think/topics/langgraph&quot;, &quot;content&quot;: &quot;LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\n\nLLM applications: By using LangGraph’s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.&quot;, &quot;score&quot;: 0.9353998&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph Quickstart - GitHub Pages&quot;, &quot;url&quot;: &quot;https://langchain-ai.github.io/langgraph/tutorials/introduction/&quot;, &quot;content&quot;: &quot;[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It&#x27;s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It&#x27;s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)&quot;, &quot;score&quot;: 0.9328032&#x7D;]
================================== Ai Message ==================================
Tool Calls:
&#x20;&#x20;tavily_search_results_json (0)
 Call ID: 0
&#x20;&#x20;Args:
&#x20;&#x20;&#x20;&#x20;query: LangGraph
================================= Tool Message =================================
Name: tavily_search_results_json

[&#x7B;&quot;title&quot;: &quot;What is LangGraph? - IBM&quot;, &quot;url&quot;: &quot;https://www.ibm.com/think/topics/langgraph&quot;, &quot;content&quot;: &quot;LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\n\nLLM applications: By using LangGraph’s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.&quot;, &quot;score&quot;: 0.9353998&#x7D;, &#x7B;&quot;title&quot;: &quot;LangGraph Quickstart - GitHub Pages&quot;, &quot;url&quot;: &quot;https://langchain-ai.github.io/langgraph/tutorials/introduction/&quot;, &quot;content&quot;: &quot;[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It&#x27;s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It&#x27;s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)&quot;, &quot;score&quot;: 0.9328032&#x7D;]
================================== Ai Message ==================================

LangGraph is an open-source AI agent framework developed by LangChain, designed to build, deploy, and manage complex generative AI agent workflows. Here are some key points about LangGraph:

### Overview
- **Purpose**: LangGraph is aimed at creating, running, and optimizing large language models (LLMs) in a scalable and efficient manner.
- **Graph-Based Architecture**: It uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow.

### Features
- **Agent Systems**: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles, or video games.
- **LLM Applications**: Developers can build more sophisticated AI models that learn and improve over time. For example, Norwegian Cruise Line uses LangGraph to compile, construct, and refine guest-facing AI solutions, enhancing personalized guest experiences.
- **Scalability**: By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without sacrificing efficiency.
- **Enhanced Decision-Making**: LangGraph uses AI agents to analyze their past actions and feedback, a process referred to as &quot;reflection&quot; in the context of LLMs.

### Developer Resources
- **Quickstart Guide**: The LangGraph Quickstart guide on GitHub provides a detailed introduction to building stateful multi-agent applications using language models. It covers tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions.
- **Built on LangChain**: LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. This makes it particularly useful for developing more complex, stateful, multi-actor applications with LLMs.

### Further Reading
- **What is LangGraph? - IBM**: [Link](https://www.ibm.com/think/topics/langgraph)
- **LangGraph Quickstart - GitHub Pages**: [Link](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

These resources should provide a solid foundation for understanding and getting started with LangGraph. If you have any specific questions or need further details, feel free to ask!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E agora a segunda chamada</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;Ya that&#39;s helpful. Maybe I&#39;ll &quot;</span>
<span class="w">    </span><span class="s2">&quot;build an autonomous agent with it!&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>
<span class="w">    </span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Ya that&#x27;s helpful. Maybe I&#x27;ll build an autonomous agent with it!
================================== Ai Message ==================================

That sounds like an exciting project! Building an autonomous agent using LangGraph can be a rewarding experience. Here are some steps and tips to help you get started:

### 1. **Understand the Basics of LangGraph**
- **Read the Documentation**: Start with the official LangGraph documentation and quickstart guide. This will give you a solid understanding of the framework&#x27;s capabilities and how to use its tools.
&#x20;&#x20;- **Quickstart Guide**: [LangGraph Quickstart - GitHub Pages](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

### 2. **Set Up Your Development Environment**
- **Install LangChain and LangGraph**: Ensure you have the necessary dependencies installed. LangGraph is built on top of LangChain, so you&#x27;ll need to set up both.
&#x20;&#x20;```bash
&#x20;&#x20;pip install langchain langgraph
&#x20;&#x20;```

### 3. **Define Your Agent&#x27;s Objectives**
- **Identify the Use Case**: What specific tasks do you want your autonomous agent to perform? This could be anything from navigating a virtual environment, responding to user queries, or managing a robotic system.
- **Define the State and Actions**: Determine the states your agent can be in and the actions it can take. This will help you design the state machine and workflows.

### 4. **Design the Graph-Based Workflow**
- **Create Nodes and Edges**: In LangGraph, you&#x27;ll define nodes (agents or components) and edges (interactions or transitions). Each node can represent a different part of your agent&#x27;s functionality.
- **Define State Transitions**: Use the graph-based architecture to define how the agent transitions between different states based on actions and events.

### 5. **Implement the Agent**
- **Write the Code**: Start coding your agent using the LangGraph library. You can use the provided tools to create and manage the agent&#x27;s workflows.
&#x20;&#x20;- **Example**: Here’s a simple example to get you started:
&#x20;&#x20;&#x20;&#x20;```python
&#x20;&#x20;&#x20;&#x20;from langgraph import AgentGraph, Node, Edge

&#x20;&#x20;&#x20;&#x20;# Define nodes
&#x20;&#x20;&#x20;&#x20;node1 = Node(&quot;Sensor&quot;, process=sensor_process)
&#x20;&#x20;&#x20;&#x20;node2 = Node(&quot;Decision&quot;, process=decision_process)
&#x20;&#x20;&#x20;&#x20;node3 = Node(&quot;Actuator&quot;, process=actuator_process)

&#x20;&#x20;&#x20;&#x20;# Define edges
&#x20;&#x20;&#x20;&#x20;edge1 = Edge(node1, node2)
&#x20;&#x20;&#x20;&#x20;edge2 = Edge(node2, node3)

&#x20;&#x20;&#x20;&#x20;# Create the agent graph
&#x20;&#x20;&#x20;&#x20;agent_graph = AgentGraph()
&#x20;&#x20;&#x20;&#x20;agent_graph.add_node(node1)
&#x20;&#x20;&#x20;&#x20;agent_graph.add_node(node2)
&#x20;&#x20;&#x20;&#x20;agent_graph.add_node(node3)
&#x20;&#x20;&#x20;&#x20;agent_graph.add_edge(edge1)
&#x20;&#x20;&#x20;&#x20;agent_graph.add_edge(edge2)

&#x20;&#x20;&#x20;&#x20;# Run the graph
&#x20;&#x20;&#x20;&#x20;agent_graph.run()
&#x20;&#x20;&#x20;&#x20;```

### 6. **Test and Iterate**
- **Run Simulations**: Test your agent in a simulated environment to see how it performs. Use this to identify and fix any issues.
- **Refine the Model**: Based on the test results, refine your agent&#x27;s model and workflows. You can add more nodes, edges, or improve the decision-making processes.

### 7. **Deploy and Monitor**
- **Deploy the Agent**: Once you are satisfied with the performance, you can deploy your agent in the real world or a production environment.
- **Monitor and Maintain**: Continuously monitor the agent&#x27;s performance and make adjustments as needed. Use feedback loops to improve the agent over time.

### 8. **Community and Support**
- **Join the Community**: Engage with the LangChain and LangGraph community. You can find support, share ideas, and get feedback from other developers.
&#x20;&#x20;- **GitHub**: [LangGraph GitHub](https://github.com/langchain-ai/langgraph)
&#x20;&#x20;- **Forums and Discussion Boards**: Check out forums and discussion boards related to LangGraph and LangChain.

### Additional Resources
- **Tutorials and Examples**: Look for tutorials and example projects to get more hands-on experience.
- **Research Papers and Articles**: Read research papers and articles to deepen your understanding of AI agent design and graph-based architectures.

Good luck with your project! If you have any specific questions or need further guidance, feel free to ask.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos o histórico do estado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">to_replay</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s1">&#39;checkpoint_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Get state when first iteracction us done</span>
<span class="w">    </span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="n">to_replay</span> <span class="o">=</span> <span class="n">state</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Num Messages: 8, Next: (), checkpoint id = 1f03263e-a96c-6446-8008-d2c11df0b6cb
--------------------------------------------------------------------------------
Num Messages: 7, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263d-7a35-6660-8007-a37d4b584c88
--------------------------------------------------------------------------------
Num Messages: 6, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f03263d-7a32-624e-8006-6509bbf32ebe
--------------------------------------------------------------------------------
Num Messages: 6, Next: (), checkpoint id = 1f03263d-7a1a-6f36-8005-f10b5d83f22c
--------------------------------------------------------------------------------
Num Messages: 5, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263c-c53f-6666-8004-c6d35868dd73
--------------------------------------------------------------------------------
Num Messages: 4, Next: (&#x27;tools&#x27;,), checkpoint id = 1f03263c-b14b-68f8-8003-28558fa38dbc
--------------------------------------------------------------------------------
Num Messages: 3, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263c-a66b-6276-8002-2dc89fca4d99
--------------------------------------------------------------------------------
Num Messages: 2, Next: (&#x27;tools&#x27;,), checkpoint id = 1f03263c-8c7c-68ec-8001-fb8a9aa300b0
--------------------------------------------------------------------------------
Num Messages: 1, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263c-6d06-68d2-8000-ced2e7b8538f
--------------------------------------------------------------------------------
Num Messages: 0, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f03263c-6cdb-63e4-bfff-c644b57cee28
--------------------------------------------------------------------------------
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;configurable&#x27;: &#x7B;&#x27;thread_id&#x27;: &#x27;1&#x27;, &#x27;checkpoint_ns&#x27;: &#x27;&#x27;, &#x27;checkpoint_id&#x27;: &#x27;1f03263d-7a1a-6f36-8005-f10b5d83f22c&#x27;&#x7D;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Dando este <code>checkpoint_id</code> a <code>LangGraph</code> carrega o estado naquele momento do fluxo. Assim, criamos uma nova mensagem e a passamos para o grafo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="s2">&quot;Thanks&quot;</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="c1"># The `checkpoint_id` in the `to_replay.config` corresponds to a state we&#39;ve persisted to our checkpointer.</span>
<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>
<span class="w">    </span><span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
<span class="w">    </span><span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="w"> </span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
<span class="w">    </span><span class="k">if</span> <span class="s2">&quot;messages&quot;</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
<span class="w">        </span><span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>================================ Human Message =================================

Thanks
================================== Ai Message ==================================

You&#x27;re welcome! If you have any more questions about LangGraph or any other topic, feel free to reach out. Happy learning! 😊
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;configurable&quot;</span><span class="p">][</span><span class="s1">&#39;checkpoint_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Num Messages: 8, Next: (), checkpoint id = 1f03263f-fcb9-63a0-8008-e8c4a3fb44f9
--------------------------------------------------------------------------------
Num Messages: 7, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263f-eb3b-663c-8007-72da4d16bf64
--------------------------------------------------------------------------------
Num Messages: 6, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f03263f-eb36-6ac4-8006-a2333805d5d6
--------------------------------------------------------------------------------
Num Messages: 8, Next: (), checkpoint id = 1f03263e-a96c-6446-8008-d2c11df0b6cb
--------------------------------------------------------------------------------
Num Messages: 7, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263d-7a35-6660-8007-a37d4b584c88
--------------------------------------------------------------------------------
Num Messages: 6, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f03263d-7a32-624e-8006-6509bbf32ebe
--------------------------------------------------------------------------------
Num Messages: 6, Next: (), checkpoint id = 1f03263d-7a1a-6f36-8005-f10b5d83f22c
--------------------------------------------------------------------------------
Num Messages: 5, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263c-c53f-6666-8004-c6d35868dd73
--------------------------------------------------------------------------------
Num Messages: 4, Next: (&#x27;tools&#x27;,), checkpoint id = 1f03263c-b14b-68f8-8003-28558fa38dbc
--------------------------------------------------------------------------------
Num Messages: 3, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263c-a66b-6276-8002-2dc89fca4d99
--------------------------------------------------------------------------------
Num Messages: 2, Next: (&#x27;tools&#x27;,), checkpoint id = 1f03263c-8c7c-68ec-8001-fb8a9aa300b0
--------------------------------------------------------------------------------
Num Messages: 1, Next: (&#x27;chatbot_node&#x27;,), checkpoint id = 1f03263c-6d06-68d2-8000-ced2e7b8538f
--------------------------------------------------------------------------------
Num Messages: 0, Next: (&#x27;__start__&#x27;,), checkpoint id = 1f03263c-6cdb-63e4-bfff-c644b57cee28
--------------------------------------------------------------------------------
</pre>
</div>
</div>
</div>
</section>