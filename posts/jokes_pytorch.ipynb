{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a descargar el dataset de chistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Joke'],\n",
       "        num_rows: 231657\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
    "jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un subset por si se tiene poca memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'Joke'],\n",
       "    num_rows: 115\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_of_train_dataset = 0.0005    # If you want 50% of the dataset, set this to 0.5\n",
    "\n",
    "subset_dataset = jokes[\"train\"].select(range(int(len(jokes[\"train\"]) * percent_of_train_dataset)))\n",
    "subset_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en subsets de entrenamiento, validaci칩n y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 103. Size of the validation set: 6. Size of the test set: 6\n"
     ]
    }
   ],
   "source": [
    "percent_of_train_dataset = 0.90\n",
    "\n",
    "split_dataset = subset_dataset.train_test_split(train_size=int(subset_dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "split_dataset = validation_test_dataset.train_test_split(train_size=int(validation_test_dataset.num_rows * 0.5), seed=19, shuffle=False)\n",
    "validation_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(validation_dataset)}. Size of the test set: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el tokenizador y asignamos el token de padding al de end of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoints = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A침adimos los tokens especiales de inicio de chiste y fin de chiste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 tokens\n"
     ]
    }
   ],
   "source": [
    "new_tokens = ['<SJ>', '<EJ>']   # Start and end of joke tokens\n",
    "\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Added {num_added_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los a침adimos al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced4a0ba5a014091833ed73a62b7fdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf92e2287cd54810aa1a3dc4876a0a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8587a70e85a041bf8fb674e176b474c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['Joke'],\n",
       "     num_rows: 103\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['Joke'],\n",
       "     num_rows: 6\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['Joke'],\n",
       "     num_rows: 6\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_column = \"Joke\"\n",
    "\n",
    "def format_joke(example):\n",
    "    example[joke_column] = '<SJ> ' + example['Joke'] + ' <EJ>'\n",
    "    return example\n",
    "\n",
    "remove_columns = [column for column in train_dataset.column_names if column != joke_column]\n",
    "\n",
    "train_dataset = train_dataset.map(format_joke, remove_columns=remove_columns)\n",
    "validation_dataset = validation_dataset.map(format_joke, remove_columns=remove_columns)\n",
    "test_dataset = test_dataset.map(format_joke, remove_columns=remove_columns)\n",
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceb9064b6ac48bca507af6bd4e4410d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658e90eb3ce7469ca5d1032ea9ca6720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a300e8202a40de9fc0f347b2e269fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 103\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 6\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 6\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[joke_column], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el modelo, asignamos el token de padding y a침adimos los nuevos tokens de inicion de chiste y fin de chiste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el dispositivo y pasamos el modelo al dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.half().to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataset de pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, huggingface_dataset):\n",
    "        self.dataset = huggingface_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.dataset[idx]['input_ids'])\n",
    "        attention_mask = torch.tensor(self.dataset[idx]['attention_mask'])\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos los datasets de entrenamiento, validaci칩n y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pytorch_dataset = JokesDataset(train_dataset)\n",
    "validation_pytorch_dataset = JokesDataset(validation_dataset)\n",
    "test_pytorch_dataset = JokesDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([768]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, attention_mask = train_pytorch_dataset[0]\n",
    "input_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BS = 2\n",
    "\n",
    "train_loader = DataLoader(train_pytorch_dataset, batch_size=BS, shuffle=True)\n",
    "validation_loader = DataLoader(validation_pytorch_dataset, batch_size=BS)\n",
    "test_loader = DataLoader(test_pytorch_dataset, batch_size=BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids, attention_mask = next(iter(train_loader))\n",
    "# input_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lo pasamos al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "# output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos no tenemos valor de `loss`, como hemos visto tenemos que pasarle el `input_ids` y el `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(input_ids.to(device), attention_mask=attention_mask.to(device), labels=input_ids.to(device))\n",
    "# output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora s칤 tenemos `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output['loss'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sae00531/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "LR = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 52/52 [04:02<00:00,  4.67s/it, train_loss=0.527]\n",
      "Epoch 2: 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 52/52 [04:02<00:00,  4.66s/it, train_loss=0.758]\n",
      "Epoch 3: 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 52/52 [04:02<00:00,  4.66s/it, train_loss=0.227]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progresbar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}')\n",
    "    for input_ids, at_mask in progresbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        at_mask = at_mask.to(device)\n",
    "\n",
    "        output = model(input_ids=input_ids, attention_mask=at_mask, labels=input_ids)\n",
    "\n",
    "        loss = output['loss']\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progresbar.set_postfix({'train_loss': loss.item()})\n",
    "    train_loss /= len(train_loader)\n",
    "    progresbar.set_postfix({'train_loss': train_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo, primero tokenizamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = tokenize_function({'Joke': \"<SJ> Why didn't the frog cross the road\"})\n",
    "input_tokens['input_ids'].shape, input_tokens['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pasan los tokens al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 789])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_output = model.generate(**input_tokens.to(model.device), max_new_tokens=500, do_sample=True, top_k=0)\n",
    "tokens_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificamos los tokens de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SJ> Why didn\\'t the frog cross the road 4 fuck in condemning goodA tights Holden examines\\'s overdoses opens hillGuy.\" C narrated they '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_joke = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
    "decoded_joke"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
