{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# OpenAI API"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Install the OpenAI library"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.\n",
"\n",
"First of all, in order to use the OpenAI API, it is necessary to install the OpenAI library. To do this, run the following command"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "%pip install --upgrade openai"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Import the OpenAI library"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once the library is installed, we import it to be able to use it in our code."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "import openai"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Obtain an API Key"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In order to use the OpenAI API, it is necessary to obtain an API Key. To do this, go to the [OpenAI](https://openai.com/) page and register. Once registered, go to the [API Keys](https://platform.openai.com/api-keys) section, and create a new API Key.\n",
"\n",
"![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have it, we tell the openai API which is our API Key."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
      "api_key = \"Pon aquí tu API key\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## We create our first chatbot"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With the OpenAI API it is very easy to create a simple chatbot, to which we are going to pass a prompt, and it will give us a response"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First of all we have to choose which model we are going to use, in my case I am going to use the `gpt-3.5-turbo-1106` model which is a good model for this post, since for what we are going to do we do not need to use the best model. OpenAI has a list with all their [models](https://platform.openai.com/docs/models) and a page with the [prices](https://openai.com/pricing)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
      "model = \"gpt-3.5-turbo-1106\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we have to create a client that will communicate with the OpenAI API."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
      "client = openai.OpenAI(api_key=api_key, organization=None)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see we have passed our API Key. It is also possible to pass the organization, but in our case it is not necessary."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the prompt"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
      "promtp = \"Cuál es el mejor lenguaje de programación para aprender?\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And now we can ask OpenAI for an answer."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
      "response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see what the answer looks like"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.chat.chat_completion.ChatCompletion,\n",
" ChatCompletion(id='chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))], created=1701584994, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.id = chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y\n",
"response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))]\n",
"response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))\n",
"\tresponse.choices[0].finish_reason = stop\n",
"\tresponse.choices[0].index = 0\n",
"\tresponse.choices[0].message = ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None)\n",
"\t\tresponse.choices[0].message.content = \n",
"\t\tNo hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.\n",
"\t\tresponse.choices[0].message.role = assistant\n",
"\t\tresponse.choices[0].message.function_call = None\n",
"\t\tresponse.choices[0].message.tool_calls = None\n",
"response.created = 1701584994\n",
"response.model = gpt-3.5-turbo-1106\n",
"response.object = chat.completion\n",
"response.system_fingerprint = fp_eeff13170a\n",
"response.usage = CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)\n",
"\tresponse.usage.completion_tokens = 181\n",
"\tresponse.usage.prompt_tokens = 21\n",
"\tresponse.usage.total_tokens = 202\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.id = {response.id}\")\n",
"print(f\"response.choices = {response.choices}\")\n",
"for i in range(len(response.choices)):\n",
"    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
"    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
"    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
"    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
"    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
"print(f\"response.created = {response.created}\")\n",
"print(f\"response.model = {response.model}\")\n",
"print(f\"response.object = {response.object}\")\n",
"print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
"print(f\"response.usage = {response.usage}\")\n",
"print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
"print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
"print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see, it returns a lot of information.\n",
"\n",
"For example `response.choices[0].finish_reason = stop` means that the model has stopped generating text because it has reached the end of the prompt. This comes in handy for debugging, as the possible values are `stop` which means the API returned the complete message, `length` which means the model output was incomplete because it was longer than `max_tokens` or model token limit, `function_call` the model decided to call a function, `content_filter` which means the content was skipped because of an OpenAI content limitation and `null` which means the API response was incomplete.\n",
"\n",
"It also gives us token information to keep track of the money spent."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Parameters"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When asking OpenAI for an answer, we can pass a series of parameters to it so that it returns an answer more in accordance with what we want. Let's see which are the parameters that we can pass to it\n",
"\n",
" * Messages: List of messages that have been sent to the chatbot.\n",
" * `model`: Model we want to use\n",
" * `frequency_penalty`: Frequency penalty. The higher the value, the less likely the model will repeat the same response.\n",
" * `max_tokens`: Maximum number of tokens the model can return.\n",
" * `n`: Number of responses that we want the model to return.\n",
" * `presence_penalty`: Presence penalty. The higher the value, the less likely the model will repeat the same response.\n",
" * `seed`: Seed for text generation\n",
" * `stop`: List of tokens indicating that the model should stop generating text.\n",
" * If `stream`: If `True` the API will return a response each time the model generates a token. If `False` the API will return a response when the model has generated all tokens.\n",
" * `temperature`: The higher the value, the more creative the model will be.\n",
" * `top_p`: The higher the value, the more creative the model will be.\n",
" * `user`: ID of the user who is talking to the chatbot\n",
" * `timeout`: Maximum time we want to wait for the API to return a response."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at some of them"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Messages"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can pass to the API a list of messages that have been sent to the chatbot. This is useful to pass the conversation history to the chatbot, so that it can generate a response more in line with the conversation. And to condition the chatbot's response to what it has been told previously.\n",
"\n",
"We can also pass a system message to tell it how to behave."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "##### Conversation history"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see an example of a conversation history."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
      "promtp = \"Hola, soy MaximoFN, ¿Cómo estás?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "He answered that he has no feelings and how can he help us. So if I ask him what my name is now, he won't know how to answer me."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Lo siento, no tengo esa información. Pero puedes decírmelo tú.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Me puedes decir cómo me llamo?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To solve this, we pass the conversation history to you"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Tu nombre es MaximoFN.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Me puedes decir cómo me llamo?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"user\", \"content\": \"Hola, soy MaximoFN, ¿Cómo estás?\"},\n",
"      {\"role\": \"assistant\", \"content\": \"Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "##### Conditioning by examples"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now let's look at an example of how to condition the chatbot's response to what it has been told above. Now we ask it how to get the list of files in a directory in the terminal"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "En la terminal de un sistema operativo Unix o Linux, puedes listar los archivos de un directorio utilizando el comando `ls`. Por ejemplo, si quieres listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si deseas listar los archivos de un directorio específico, puedes proporcionar la ruta del directorio después del comando `ls`, por ejemplo `ls /ruta/del/directorio`. Si deseas ver más detalles sobre los archivos, puedes usar la opción `-l` para obtener una lista detallada o `-a` para mostrar también los archivos ocultos.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we now condition him with examples of short answers, let's see what he will answer us"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Puede usar el comando `ls` en la terminal para listar los archivos de un directorio. Por ejemplo:\n",
"```\n",
"ls\n",
"```\n",
"Muestra los archivos y directorios en el directorio actual.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras líneas de un archivo\"},\n",
"      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
"      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensión .txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias páginas\"},\n",
"      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
"      {\"role\": \"user\", \"content\": \"Buscar la dirección IP 12.34.56.78\"},\n",
"      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 5 últimas líneas de foo.txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
"      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
"      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
"      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We have managed to give a shorter response"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "##### Conditioning with system message"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can pass you a system message to tell you how to behave."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Puedes listar los archivos de un directorio en la terminal usando el comando `ls`. Por ejemplo, para listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si quieres listar los archivos de un directorio específico, puedes utilizar `ls` seguido de la ruta del directorio. Por ejemplo, `ls /ruta/del/directorio`.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Puedes listar los archivos de un directorio en la terminal utilizando el comando \"ls\". Por ejemplo, para listar los archivos en el directorio actual, puedes ejecutar el comando \"ls\". Si deseas listar los archivos de otro directorio, simplemente especifica el directorio después del comando \"ls\", por ejemplo \"ls /ruta/al/directorio\".\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras líneas de un archivo\"},\n",
"      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
"      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensión .txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias páginas\"},\n",
"      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
"      {\"role\": \"user\", \"content\": \"Buscar la dirección IP 12.34.56.78\"},\n",
"      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 5 últimas líneas de foo.txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
"      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
"      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
"      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Maximum number of response tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can limit the number of tokens that the model can return. This is useful so that the model does not overshoot the response we want."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "La respuesta a esta pregunta puede variar dependiendo de los intereses y objetivos individuales, ya que cada lenguaje de programación tiene sus propias ventajas y desventajas. Sin embargo, algunos de los lenguajes más\n",
"\n",
"response.choices[0].finish_reason = length\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  max_tokens = 50,\n",
")\n",
"\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)\n",
"print(f\"\\nresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see the answer is half cut because it would exceed the token limit. Also now the stop reason is `length` instead of `stop`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Creativity of the model through temperature"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can make the model more creative by the temperature. The higher the value, the more creative the model will be."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"temperature = 0\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  temperature = temperature,\n",
")\n",
"\n",
"content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_0)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"temperature = 1\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  temperature = temperature,\n",
")\n",
"\n",
"content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
"No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.\n"
          ]
        }
      ],
      "source": [
      "print(content_0)\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Creativity of the model using the top_p"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can make the model more creative by using the `top_p` parameter. The higher the value, the more creative the model will be."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"top_p = 0\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  top_p = top_p,\n",
")\n",
"\n",
"content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_0)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"top_p = 1\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  top_p = top_p,\n",
")\n",
"\n",
"content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
"El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.\n"
          ]
        }
      ],
      "source": [
      "print(content_0)\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Number of responses"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can ask the API to return more than one response. This is useful for the model to return several answers and so we can choose the one we like the most, for this we will set the parameters `temperature` and `top_p` to 1 to make the model more creative."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "El mejor lenguaje de programación para aprender depende de tus objetivos y del tipo de aplicaciones que te interese desarrollar. Algunos de los lenguajes más populares para aprender son:\n",
"\t\t1. Python: Es un lenguaje de programación versátil, fácil de aprender y con una amplia comunidad de desarrolladores. Es ideal para principiantes y se utiliza en una gran variedad de aplicaciones, desde desarrollo web hasta inteligencia artificial.\n",
"\t\t2. JavaScript: Es el lenguaje de programación más utilizado en el desarrollo web. Es imprescindible para aquellos que quieren trabajar en el ámbito del desarrollo frontend y backend.\n",
"\t\t3. Java: Es un lenguaje de programación muy popular en el ámbito empresarial, por lo que aprender Java puede abrirte muchas puertas laborales. Además, es un lenguaje estructurado que te enseñará conceptos importantes de la programación orientada a objetos.\n",
"\t\t4. C#: Es un lenguaje de programación desarrollado por Microsoft que se utiliza especialmente en el desarrollo de aplicaciones para Windows. Es ideal para aquellos que quieran enfocarse en el desarrollo de aplicaciones de escritorio.\n",
"\t\tEn resumen, el mejor lenguaje de programación para aprender depende de tus intereses y objetivos personales. Es importante investigar y considerar qué tipos de aplicaciones te gustaría desarrollar para elegir el lenguaje que más se adapte a tus necesidades.\n",
"El mejor lenguaje de programación para aprender depende de los objetivos y necesidades individuales. Algunos de los lenguajes de programación más populares y ampliamente utilizados incluyen Python, JavaScript, Java, C++, Ruby y muchos otros. Python es a menudo recomendado para principiantes debido a su sintaxis simple y legible, mientras que JavaScript es esencial para el desarrollo web. Java es ampliamente utilizado en el desarrollo de aplicaciones empresariales y Android, y C++ es comúnmente utilizado en sistemas embebidos y juegos. En última instancia, el mejor lenguaje de programación para aprender dependerá de lo que quiera lograr con su habilidades de programación.\n",
"El mejor lenguaje de programación para aprender depende de los intereses y objetivos individuales de cada persona. Algunos de los lenguajes más populares y bien documentados para principiantes incluyen Python, JavaScript, Java y C#. Python es conocido por su simplicidad y versatilidad, mientras que JavaScript es esencial para el desarrollo web. Java y C# son lenguajes ampliamente utilizados en la industria y proporcionan una base sólida para aprender otros lenguajes. En última instancia, la elección del lenguaje dependerá de las metas personales y la aplicación deseada.\n",
"El mejor lenguaje de programación para aprender depende de los intereses y objetivos de cada persona. Algunos lenguajes populares para principiantes incluyen Python, Java, JavaScript, C++ y Ruby. Python es frecuentemente recomendado para aprender a programar debido a su sintaxis sencilla y legible, mientras que Java es utilizado en aplicaciones empresariales y Android. JavaScript es fundamental para el desarrollo web, y C++ es comúnmente utilizado en aplicaciones de alto rendimiento. Ruby es conocido por su facilidad de uso y flexibilidad. En última instancia, la elección del lenguaje dependerá de qué tipo de desarrollo te interesa y qué tipo de proyectos deseas realizar.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"temperature = 1\n",
"top_p = 1\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  temperature = temperature,\n",
"  top_p = top_p,\n",
"  n = 4\n",
")\n",
"\n",
"content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"content_1 = response.choices[1].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"content_2 = response.choices[2].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"content_3 = response.choices[3].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_0)\n",
"print(content_1)\n",
"print(content_2)\n",
"print(content_3)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Re-train OpenAI model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "OpenAI offers the possibility to retrain its API models to obtain better results on our own data. This has the following advantages\n",
"\n",
" + Higher quality results are obtained for our data.\n",
" + In a prompt we can give it examples to behave as we want, but only a few. This way, by retraining it we can give it many more.\n",
" + Saving tokens due to shorter prompts. Since we have already trained it for our use case, we can give it fewer prompts to solve our tasks.\n",
" + Lower latency requests. By calling our own models we will have less latency"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Data preparation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The OpenAI API asks us to provide the data in a `jsonl` file in the following format\n",
"\n",
"``` json\n",
"{\n",
"    \"messages\":\n",
"    [\n",
"        {\n",
"            \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n",
"        },\n",
"        {\n",
"            \"role\": \"user\", \"content\": \"What's the capital of France?\"\n",
"        },\n",
"        {\n",
"            \"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"\n",
"        }\n",
"    \n",
"}\n",
"{\n",
"    \"messages\":\n",
"    [\n",
"        {\n",
"            \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n",
"        },\n",
"        {\n",
"            \"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"\n",
"        },\n",
"        {\n",
"            \"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"\n",
"        }\n",
"    \n",
"}\n",
"{\n",
"    \"messages\":\n",
"    [\n",
"        {\n",
"            \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n",
"        },\n",
"        {\n",
"            \"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"\n",
"        },\n",
"        {\n",
"            \"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"\n",
"        }\n",
"    \n",
"}\n",
"```\n",
"\n",
"With a maximum of 4096 tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Data validation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To save me some work, I have been passing one by one all my posts to chatgpt and I have told it to generate 10 `FAQ`s for each one in `CSV` format, because I doubted if it was going to be able to generate a format like the one requested in the `jsonl`. And it has generated a `CSV` with the following format for each post\n",
"\n",
"``` csv\n",
"prompt,completion\n",
"What does Introduction to Python cover in the material provided, \"Introduction to Python covers topics such as data types, operators, use of functions and classes, handling iterable objects, and use of modules. [Learn more](https://maximofn.com/python/)\"\n",
"What are the basic data types in Python, \"Python has 7 basic data types: text (`str`), numeric (`int`, `float`, `complex`), sequences (`list`, `tuple`, `range`), mapping (`dict`), sets (`set`, `frozenset`), boolean (`bool`) and binary (`bytes`, `bytearray`, `memoryview`). [More information](https://maximofn.com/python/)\"\n",
"What are operators in Python and how are they used, \"Operators in Python are special symbols used to perform operations such as addition, subtraction, multiplication and division between variables and values. They also include logical operators for comparisons. [Learn more](https://maximofn.com/python/)\"\n",
"How is a function defined and used in Python, \"In Python, a function is defined using the `def` keyword, followed by the function name and parentheses. Functions can have parameters and return values. They are used to encapsulate logic that can be reused throughout the code. [More information](https://maximofn.com/python/)\"\n",
"What are Python classes and how are they used, \"Python classes are the basis of object-oriented programming. They allow you to create objects that encapsulate data and functionality. Classes are defined using the `class` keyword, followed by the class name. [More information](https://maximofn.com/python/)\"\n",
"...\n",
"```\n",
"\n",
"Each `CSV` has 10 `FAQ`s"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "I am going to make a code that takes each `CSV` and generates two new `jsonl`s, one for training and one for validation."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
      "import os\n",
"\n",
"CSVs_path = \"openai/faqs_posts\"\n",
"percetn_train = 0.8\n",
"percetn_validation = 0.2\n",
"\n",
"jsonl_train = os.path.join(CSVs_path, \"train.jsonl\")\n",
"jsonl_validation = os.path.join(CSVs_path, \"validation.jsonl\")\n",
"\n",
"# Create the train.jsonl and validation.jsonl files\n",
"with open(jsonl_train, 'w') as f:\n",
"    f.write('')\n",
"with open(jsonl_validation, 'w') as f:\n",
"    f.write('')\n",
"\n",
"for file in os.listdir(CSVs_path):  # Get all files in the directory\n",
"    if file.endswith(\".csv\"):    # Check if file is a csv\n",
"        csv = os.path.join(CSVs_path, file) # Get the path to the csv file\n",
"        number_of_lines = 0\n",
"        csv_content = []\n",
"        for line in open(csv, 'r'): # Read all lines in the csv file\n",
"            if line.startswith('prompt'):   # Skip the first line\n",
"                continue\n",
"            number_of_lines += 1    # Count the number of lines\n",
"            csv_content.append(line)    # Add the line to the csv_content list\n",
"\n",
"        number_of_train = int(number_of_lines * percetn_train)  # Calculate the number of lines for the train.jsonl file\n",
"        number_of_validation = int(number_of_lines * percetn_validation)    # Calculate the number of lines for the validation.sjonl file\n",
"\n",
"        for i in range(number_of_lines):\n",
"            prompt = csv_content[i].split(',')[0]\n",
"            response = ','.join(csv_content[i].split(',')[1:]).replace('\\n', '').replace('\"', '')\n",
"            if i > 0 and i <= number_of_train:\n",
"                # add line to train.jsonl\n",
"                with open(jsonl_train, 'a') as f:\n",
"                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')\n",
"            elif i > number_of_train and i <= number_of_train + number_of_validation:\n",
"                # add line to validation.csv\n",
"                with open(jsonl_validation, 'a') as f:\n",
"                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once I have the two `jsonl`s, I run a [code](https://cookbook.openai.com/examples/chat_finetuning_data_prep) provided by OpenAI to check the `jsonl`s"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First we validate the training ones"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No errors found\n"
          ]
        }
      ],
      "source": [
      "from collections import defaultdict\n",
"import json\n",
"\n",
"# Format error checks\n",
"format_errors = defaultdict(int)\n",
"\n",
"with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    if not isinstance(ex, dict):\n",
"        format_errors[\"data_type\"] += 1\n",
"        continue\n",
"        \n",
"    messages = ex.get(\"messages\", None)\n",
"    if not messages:\n",
"        format_errors[\"missing_messages_list\"] += 1\n",
"        continue\n",
"        \n",
"    for message in messages:\n",
"        if \"role\" not in message or \"content\" not in message:\n",
"            format_errors[\"message_missing_key\"] += 1\n",
"        \n",
"        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
"            format_errors[\"message_unrecognized_key\"] += 1\n",
"        \n",
"        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
"            format_errors[\"unrecognized_role\"] += 1\n",
"            \n",
"        content = message.get(\"content\", None)\n",
"        function_call = message.get(\"function_call\", None)\n",
"        \n",
"        if (not content and not function_call) or not isinstance(content, str):\n",
"            format_errors[\"missing_content\"] += 1\n",
"    \n",
"    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
"        format_errors[\"example_missing_assistant_message\"] += 1\n",
"\n",
"if format_errors:\n",
"    print(\"Found errors:\")\n",
"    for k, v in format_errors.items():\n",
"        print(f\"{k}: {v}\")\n",
"else:\n",
"    print(\"No errors found\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And now those of validation"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No errors found\n"
          ]
        }
      ],
      "source": [
      "# Format error checks\n",
"format_errors = defaultdict(int)\n",
"\n",
"with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    if not isinstance(ex, dict):\n",
"        format_errors[\"data_type\"] += 1\n",
"        continue\n",
"        \n",
"    messages = ex.get(\"messages\", None)\n",
"    if not messages:\n",
"        format_errors[\"missing_messages_list\"] += 1\n",
"        continue\n",
"        \n",
"    for message in messages:\n",
"        if \"role\" not in message or \"content\" not in message:\n",
"            format_errors[\"message_missing_key\"] += 1\n",
"        \n",
"        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
"            format_errors[\"message_unrecognized_key\"] += 1\n",
"        \n",
"        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
"            format_errors[\"unrecognized_role\"] += 1\n",
"            \n",
"        content = message.get(\"content\", None)\n",
"        function_call = message.get(\"function_call\", None)\n",
"        \n",
"        if (not content and not function_call) or not isinstance(content, str):\n",
"            format_errors[\"missing_content\"] += 1\n",
"    \n",
"    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
"        format_errors[\"example_missing_assistant_message\"] += 1\n",
"\n",
"if format_errors:\n",
"    print(\"Found errors:\")\n",
"    for k, v in format_errors.items():\n",
"        print(f\"{k}: {v}\")\n",
"else:\n",
"    print(\"No errors found\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Calculation of tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The maximum number of tokens for each example has to be 4096, so if we have longer examples only the first 4096 tokens will be used. So let's count the number of tokens that each `jsonl` has to know how much it will cost us to retrain the model.\n",
"\n",
"But first we must install the `tiktoken` library, which is the tokenizer used by OpenAI and which will also help us to know how many tokens each `CSV` has, and therefore, how much it will cost us to retrain the model.\n",
"\n",
"To install it we execute the following command\n",
"\n",
"``` bash\n",
"pip install tiktoken\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a few necessary functions"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
      "import tiktoken\n",
"import numpy as np\n",
"\n",
"encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
"\n",
"def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
"    num_tokens = 0\n",
"    for message in messages:\n",
"        num_tokens += tokens_per_message\n",
"        for key, value in message.items():\n",
"            num_tokens += len(encoding.encode(value))\n",
"            if key == \"name\":\n",
"                num_tokens += tokens_per_name\n",
"    num_tokens += 3\n",
"    return num_tokens\n",
"\n",
"def num_assistant_tokens_from_messages(messages):\n",
"    num_tokens = 0\n",
"    for message in messages:\n",
"        if message[\"role\"] == \"assistant\":\n",
"            num_tokens += len(encoding.encode(message[\"content\"]))\n",
"    return num_tokens\n",
"\n",
"def print_distribution(values, name):\n",
"    print(f\"\\n#### Distribution of {name}:\")\n",
"    print(f\"min:{min(values)}, max: {max(values)}\")\n",
"    print(f\"mean: {np.mean(values)}, median: {np.median(values)}\")\n",
"    print(f\"p5: {np.quantile(values, 0.1)}, p95: {np.quantile(values, 0.9)}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Num examples missing system message: 0\n",
"Num examples missing user message: 0\n",
"\n",
"#### Distribution of num_messages_per_example:\n",
"min:3, max: 3\n",
"mean: 3.0, median: 3.0\n",
"p5: 3.0, p95: 3.0\n",
"\n",
"#### Distribution of num_total_tokens_per_example:\n",
"min:67, max: 132\n",
"mean: 90.13793103448276, median: 90.0\n",
"p5: 81.5, p95: 99.5\n",
"\n",
"#### Distribution of num_assistant_tokens_per_example:\n",
"min:33, max: 90\n",
"mean: 48.66379310344828, median: 48.5\n",
"p5: 41.0, p95: 55.5\n",
"\n",
"0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
      "# Warnings and tokens counts\n",
"n_missing_system = 0\n",
"n_missing_user = 0\n",
"n_messages = []\n",
"convo_lens = []\n",
"assistant_message_lens = []\n",
"\n",
"with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    if not any(message[\"role\"] == \"system\" for message in messages):\n",
"        n_missing_system += 1\n",
"    if not any(message[\"role\"] == \"user\" for message in messages):\n",
"        n_missing_user += 1\n",
"    n_messages.append(len(messages))\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
"    \n",
"print(\"Num examples missing system message:\", n_missing_system)\n",
"print(\"Num examples missing user message:\", n_missing_user)\n",
"print_distribution(n_messages, \"num_messages_per_example\")\n",
"print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
"print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
"n_too_long = sum(l > 4096 for l in convo_lens)\n",
"print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see in the training set, no message exceeds 4096 tokens."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Num examples missing system message: 0\n",
"Num examples missing user message: 0\n",
"\n",
"#### Distribution of num_messages_per_example:\n",
"min:3, max: 3\n",
"mean: 3.0, median: 3.0\n",
"p5: 3.0, p95: 3.0\n",
"\n",
"#### Distribution of num_total_tokens_per_example:\n",
"min:80, max: 102\n",
"mean: 89.93333333333334, median: 91.0\n",
"p5: 82.2, p95: 96.8\n",
"\n",
"#### Distribution of num_assistant_tokens_per_example:\n",
"min:41, max: 57\n",
"mean: 48.2, median: 49.0\n",
"p5: 42.8, p95: 51.6\n",
"\n",
"0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
      "# Warnings and tokens counts\n",
"n_missing_system = 0\n",
"n_missing_user = 0\n",
"n_messages = []\n",
"convo_lens = []\n",
"assistant_message_lens = []\n",
"\n",
"with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    if not any(message[\"role\"] == \"system\" for message in messages):\n",
"        n_missing_system += 1\n",
"    if not any(message[\"role\"] == \"user\" for message in messages):\n",
"        n_missing_user += 1\n",
"    n_messages.append(len(messages))\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
"    \n",
"print(\"Num examples missing system message:\", n_missing_system)\n",
"print(\"Num examples missing user message:\", n_missing_user)\n",
"print_distribution(n_messages, \"num_messages_per_example\")\n",
"print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
"print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
"n_too_long = sum(l > 4096 for l in convo_lens)\n",
"print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "No message in the validation set exceeds 4096 tokens."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Costing"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Another very important thing is to know how much this fine-tuning is going to cost us."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Dataset has ~10456 tokens that will be charged for during training\n",
"By default, you'll train for 3 epochs on this dataset\n",
"By default, you'll be charged for ~31368 tokens\n"
          ]
        }
      ],
      "source": [
      "# Pricing and default n_epochs estimate\n",
"MAX_TOKENS_PER_EXAMPLE = 4096\n",
"\n",
"TARGET_EPOCHS = 3\n",
"MIN_TARGET_EXAMPLES = 100\n",
"MAX_TARGET_EXAMPLES = 25000\n",
"MIN_DEFAULT_EPOCHS = 1\n",
"MAX_DEFAULT_EPOCHS = 25\n",
"\n",
"with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"convo_lens = []\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"\n",
"n_epochs = TARGET_EPOCHS\n",
"n_train_examples = len(dataset)\n",
"if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
"    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
"elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
"    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
"\n",
"n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
"print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
"print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
"print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
"\n",
"tokens_for_train = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As at the time of writing this post, the price of training `gpt-3.5-turbo` is $0.0080 per 1000 tokens, we can know how much the training will cost us"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Training price: $0.248\n"
          ]
        }
      ],
      "source": [
      "pricing = 0.0080\n",
"num_tokens_pricing = 1000\n",
"\n",
"training_price = pricing * (tokens_for_train // num_tokens_pricing)\n",
"print(f\"Training price: ${training_price}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Dataset has ~1349 tokens that will be charged for during training\n",
"By default, you'll train for 6 epochs on this dataset\n",
"By default, you'll be charged for ~8094 tokens\n"
          ]
        }
      ],
      "source": [
      "# Pricing and default n_epochs estimate\n",
"MAX_TOKENS_PER_EXAMPLE = 4096\n",
"\n",
"TARGET_EPOCHS = 3\n",
"MIN_TARGET_EXAMPLES = 100\n",
"MAX_TARGET_EXAMPLES = 25000\n",
"MIN_DEFAULT_EPOCHS = 1\n",
"MAX_DEFAULT_EPOCHS = 25\n",
"\n",
"with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"convo_lens = []\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"\n",
"n_epochs = TARGET_EPOCHS\n",
"n_train_examples = len(dataset)\n",
"if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
"    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
"elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
"    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
"\n",
"n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
"print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
"print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
"print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
"\n",
"tokens_for_validation = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Validation price: $0.064\n"
          ]
        }
      ],
      "source": [
      "validation_price = pricing * (tokens_for_validation // num_tokens_pricing)\n",
"print(f\"Validation price: ${validation_price}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Total price: $0.312\n"
          ]
        }
      ],
      "source": [
      "total_price = training_price + validation_price\n",
"print(f\"Total price: ${total_price}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If our calculations are correct, we see that retraining `gpt-3.5-turbo` will cost us $0.312."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have everything ready we have to upload the `jsonl`s to the OpenAI API so that it re-trains the model. To do this, we execute the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.files.create(file=open(jsonl_train, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.file_object.FileObject,\n",
" FileObject(id='file-LWztOVasq4E0U67wRe8ShjLZ', bytes=47947, created_at=1701585709, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None))"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(result), result"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.bytes = 47947\n",
"result.created_at = 1701585709\n",
"result.filename = train.jsonl\n",
"result.object = file\n",
"result.purpose = fine-tune\n",
"result.status = processed\n",
"result.status_details = None\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.bytes = {result.bytes}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.filename = {result.filename}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.purpose = {result.purpose}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.status_details = {result.status_details}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "jsonl_train_id = file-LWztOVasq4E0U67wRe8ShjLZ\n"
          ]
        }
      ],
      "source": [
      "jsonl_train_id = result.id\n",
"print(f\"jsonl_train_id = {jsonl_train_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We do the same for the validation set"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.files.create(file=open(jsonl_validation, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = file-E0YOgIIe9mwxmFcza5bFyVKW\n",
"result.bytes = 6369\n",
"result.created_at = 1701585730\n",
"result.filename = validation.jsonl\n",
"result.object = file\n",
"result.purpose = fine-tune\n",
"result.status = processed\n",
"result.status_details = None\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.bytes = {result.bytes}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.filename = {result.filename}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.purpose = {result.purpose}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.status_details = {result.status_details}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "jsonl_train_id = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "jsonl_validation_id = result.id\n",
"print(f\"jsonl_train_id = {jsonl_validation_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have them uploaded, we train our own OpenAi model, for this we use the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.fine_tuning.jobs.create(model = \"gpt-3.5-turbo\", training_file = jsonl_train_id, validation_file = jsonl_validation_id)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
" FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(result), result"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
"result.created_at = 1701585758\n",
"result.error = None\n",
"result.fine_tuned_model = None\n",
"result.finished_at = None\n",
"result.hyperparameters = Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto')\n",
"\tn_epochs = auto\n",
"\tbatch_size = auto\n",
"\tlearning_rate_multiplier = auto\n",
"result.model = gpt-3.5-turbo-0613\n",
"result.object = fine_tuning.job\n",
"result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
"result.result_files = []\n",
"result.status = validating_files\n",
"result.trained_tokens = None\n",
"result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.error = {result.error}\")\n",
"print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
"print(f\"result.finished_at = {result.finished_at}\")\n",
"print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
"print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
"print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
"print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
"print(f\"result.model = {result.model}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.organization_id = {result.organization_id}\")\n",
"print(f\"result.result_files = {result.result_files}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
"print(f\"result.training_file = {result.training_file}\")\n",
"print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "fine_tune_id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n"
          ]
        }
      ],
      "source": [
      "fine_tune_id = result.id\n",
"print(f\"fine_tune_id = {fine_tune_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that in `status` was `validating_files`. As the fine tuning takes a long time, we can go asking for the process by means of the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
" FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='running', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(result), result"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
"result.created_at = 1701585758\n",
"result.error = None\n",
"result.fine_tuned_model = None\n",
"result.finished_at = None\n",
"result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
"\tn_epochs = 3\n",
"\tbatch_size = 1\n",
"\tlearning_rate_multiplier = 2\n",
"result.model = gpt-3.5-turbo-0613\n",
"result.object = fine_tuning.job\n",
"result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
"result.result_files = []\n",
"result.status = running\n",
"result.trained_tokens = None\n",
"result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.error = {result.error}\")\n",
"print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
"print(f\"result.finished_at = {result.finished_at}\")\n",
"print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
"print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
"print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
"print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
"print(f\"result.model = {result.model}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.organization_id = {result.organization_id}\")\n",
"print(f\"result.result_files = {result.result_files}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
"print(f\"result.training_file = {result.training_file}\")\n",
"print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a loop that waits for the end of the training session."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Job succeeded"
          ]
        }
      ],
      "source": [
      "import time\n",
"\n",
"result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
"status = result.status\n",
"\n",
"while status != \"succeeded\":\n",
"    time.sleep(10)\n",
"    result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
"    status = result.status\n",
"\n",
"print(\"Job succeeded!\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Since you have finished the training, we ask you again for information about the process."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
"result.created_at = 1701585758\n",
"result.error = None\n",
"result.fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
"result.finished_at = 1701586541\n",
"result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
"\tn_epochs = 3\n",
"\tbatch_size = 1\n",
"\tlearning_rate_multiplier = 2\n",
"result.model = gpt-3.5-turbo-0613\n",
"result.object = fine_tuning.job\n",
"result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
"result.result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
"result.status = succeeded\n",
"result.trained_tokens = 30672\n",
"result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.error = {result.error}\")\n",
"print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
"print(f\"result.finished_at = {result.finished_at}\")\n",
"print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
"print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
"print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
"print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
"print(f\"result.model = {result.model}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.organization_id = {result.organization_id}\")\n",
"print(f\"result.result_files = {result.result_files}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
"print(f\"result.training_file = {result.training_file}\")\n",
"print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at some interesting data"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
"finished_at = 1701586541\n",
"result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
"status = succeeded\n",
"trained_tokens = 30672\n"
          ]
        }
      ],
      "source": [
      "fine_tuned_model = result.fine_tuned_model\n",
"finished_at = result.finished_at\n",
"result_files = result.result_files\n",
"status = result.status\n",
"trained_tokens = result.trained_tokens\n",
"\n",
"print(f\"fine_tuned_model = {fine_tuned_model}\")\n",
"print(f\"finished_at = {finished_at}\")\n",
"print(f\"result_files = {result_files}\")\n",
"print(f\"status = {status}\")\n",
"print(f\"trained_tokens = {trained_tokens}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that it has given the name `ft:gpt-3.5-turbo-0613:personal::8RagA0RT` to our models, its status is now `succeeded` and that it has used 30672 tokens, whereas we had predicted"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(31368, 8094, 39462)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens_for_train, tokens_for_validation, tokens_for_train + tokens_for_validation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In other words, he has used fewer tokens, so the training has cost us less than we had predicted, specifically"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Real training price: $0.24\n"
          ]
        }
      ],
      "source": [
      "real_training_price = pricing * (trained_tokens // num_tokens_pricing)\n",
"print(f\"Real training price: ${real_training_price}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In addition to this information, if we go to the [finetune](https://platform.openai.com/finetune) page of OpenAI, we can see that our model is there.\n",
"\n",
"![open ai finetune](http://maximofn.com/wp-content/uploads/2023/12/openai_fine_tuning_process.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can also see how much the training has cost us\n",
"\n",
"![open ai finetune cost](http://maximofn.com/wp-content/uploads/2023/12/openai_fine_tuning_cost.png)\n",
"\n",
"Which as we can see has been only $0.25."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And finally let's see how long it took to do this training. We can see what time it started\n",
"\n",
"![open ai finetune start](http://maximofn.com/wp-content/uploads/2023/12/openai_fine_tuning_process_start_time.png)\n",
"\n",
"And at what time did it end\n",
"\n",
"![open ai finetune end](http://maximofn.com/wp-content/uploads/2023/12/openai_fine_tuning_process_stop_time.png)\n",
"\n",
"So it took about 10 minutes."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model test"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Inside the OpenAI [playground](https://platform.openai.com/playground?mode=chat) we can test our model, but we are going to do it through the API as we have learned here"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
      "promtp = \"¿Cómo se define una función en Python?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = fine_tuned_model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.chat.chat_completion.ChatCompletion,\n",
" ChatCompletion(id='chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))], created=1701667535, model='ft:gpt-3.5-turbo-0613:personal::8RagA0RT', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)))"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.id = chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc\n",
"response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))]\n",
"response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))\n",
"\tresponse.choices[0].finish_reason = stop\n",
"\tresponse.choices[0].index = 0\n",
"\tresponse.choices[0].message = ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None)\n",
"\t\tresponse.choices[0].message.content = \n",
"\t\tUna función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)\n",
"\t\tresponse.choices[0].message.role = assistant\n",
"\t\tresponse.choices[0].message.function_call = None\n",
"\t\tresponse.choices[0].message.tool_calls = None\n",
"response.created = 1701667535\n",
"response.model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
"response.object = chat.completion\n",
"response.system_fingerprint = None\n",
"response.usage = CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)\n",
"\tresponse.usage.completion_tokens = 54\n",
"\tresponse.usage.prompt_tokens = 16\n",
"\tresponse.usage.total_tokens = 70\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.id = {response.id}\")\n",
"print(f\"response.choices = {response.choices}\")\n",
"for i in range(len(response.choices)):\n",
"    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
"    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
"    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
"    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
"    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
"print(f\"response.created = {response.created}\")\n",
"print(f\"response.model = {response.model}\")\n",
"print(f\"response.object = {response.object}\")\n",
"print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
"print(f\"response.usage = {response.usage}\")\n",
"print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
"print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
"print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)\n"
          ]
        }
      ],
      "source": [
      "print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We have a template that not only solves the answer, but also gives us a link to our blog documentation."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see how it behaves with an example that clearly has nothing to do with the blog."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Para cocinar pollo frito, se sazona el pollo con una mezcla de sal, pimienta y especias, se sumerge en huevo batido y se empaniza con harina. Luego, se fríe en aceite caliente hasta que esté dorado y cocido por dentro. [Más información](https://maximofn.com/pollo-frito/)\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo cocinar pollo frito?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = fine_tuned_model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"\n",
"for i in range(len(response.choices)):\n",
"    content = response.choices[i].message.content.replace('\\n\\n', '\\n')\n",
"    print(f\"{content}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see it gives us the link `https://maximofn.com/pollo-frito/` which does not exist. So although we have retrained a chatGPT model, we have to be careful with what it answers us and not to trust 100% of it."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Generate images with DALL-E 3"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To generate images with DALL-E 3, we need to use the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
      "response = client.images.generate(\n",
"  model=\"dall-e-3\",\n",
"  prompt=\"a white siamese cat\",\n",
"  size=\"1024x1024\",\n",
"  quality=\"standard\",\n",
"  n=1,\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.images_response.ImagesResponse,\n",
" ImagesResponse(created=1701823487, data=[Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')]))"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.created = 1701823487\n",
"response.data[0] = Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')\n",
"\tresponse.data[0].b64_json = None\n",
"\tresponse.data[0].revised_prompt = Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\n",
"\tresponse.data[0].url = https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.created = {response.created}\")\n",
"for i in range(len(response.data)):\n",
"    print(f\"response.data[{i}] = {response.data[i]}\")\n",
"    print(f\"\\tresponse.data[{i}].b64_json = {response.data[i].b64_json}\")\n",
"    print(f\"\\tresponse.data[{i}].revised_prompt = {response.data[i].revised_prompt}\")\n",
"    print(f\"\\tresponse.data[{i}].url = {response.data[i].url}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see a very interesting data that we cannot see when using DALL-E 3 through the OpenAI interface, and that is the prompt that has been passed to the model"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\""
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "response.data[0].revised_prompt"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With this prompt we have generated the following image"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
      "import requests\n",
"\n",
"url = response.data[0].url\n",
"# img_data = requests.get(url).content\n",
"with open('openai/dall-e-3.png', 'wb') as handler:\n",
"    handler.write(requests.get(url).content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "![dall-e 3](openai/dall-e-3.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Since we have the actual prompt that OpenAI used, we will try to use it to generate a similar cat with green eyes."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "A well-defined image of a Siamese cat boasting a shiny white coat. Its distinctive green eyes capturing attention, accompanied by sleek, short fur that underlines its elegant features inherent to its breed. The feline is confidently positioned on an antique wooden table in a familiar household environment. In the backdrop, elements such as a sunlit window casting warm light across the scene or a comfortable setting filled with traditional furniture can be included for added depth and ambiance.\n"
          ]
        }
      ],
      "source": [
      "revised_prompt = response.data[0].revised_prompt\n",
"gree_eyes = revised_prompt.replace(\"blue\", \"green\")\n",
"\n",
"response = client.images.generate(\n",
"  model=\"dall-e-3\",\n",
"  prompt=gree_eyes,\n",
"  size=\"1024x1024\",\n",
"  quality=\"standard\",\n",
"  n=1,\n",
")\n",
"\n",
"print(response.data[0].revised_prompt)\n",
"\n",
"image_url = response.data[0].url\n",
"\n",
"image_path = 'openai/dall-e-3-green.png'\n",
"with open(image_path, 'wb') as handler:\n",
"    handler.write(requests.get(image_url).content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "![dall-e-3-green](openai/dall-e-3-green.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Although the color of the cat has changed and not only the eyes, the position and the background are very similar."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Apart from the prompt, the other variables that we can modify are\n",
"\n",
" * `model`: Allows to choose the image generation model, the possible values are `single-2` and `single-3`.\n",
" * `size`: Allows to change the image size, possible values are `256x256`, `512x512`, `1024x1024`, `1792x1024`, `1024x1792` pixels.\n",
" * quality`: Allows to change the image quality, the possible values are `standard` or `hd`.\n",
" * `response_format`: Allows to change the format of the response, possible values are `url` or `b64_json`.\n",
" * n`: Allows us to change the number of images we want the model to return. With DALL-E 3 we can only ask for one image.\n",
" * `style`: Allows to change the style of the image, the possible values are `vivid` or `natural`.\n",
"\n",
"So we are going to generate a high quality image"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Render a portrait of a Siamese cat boasting a pristine white coat. This cat should have captivating green eyes that stand out. Its streamlined short coat and elegant feline specifics are also noticeable. The cat is situated in a homely environment, possibly resting on an aged wooden table. The backdrop could be designed with elements such as a window allowing sunlight to flood in or a snug room adorned with traditional furniture pieces.\n"
          ]
        }
      ],
      "source": [
      "response = client.images.generate(\n",
"  model=\"dall-e-3\",\n",
"  prompt=gree_eyes,\n",
"  size=\"1024x1792\",\n",
"  quality=\"hd\",\n",
"  n=1,\n",
"  style=\"natural\",\n",
")\n",
"\n",
"print(response.data[0].revised_prompt)\n",
"\n",
"image_url = response.data[0].url\n",
"\n",
"image_path = 'openai/dall-e-3-hd.png'\n",
"with open(image_path, 'wb') as handler:\n",
"    handler.write(requests.get(image_url).content)\n",
"display(Image(image_path))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "![dall-e-3-hd](openai/dall-e-3-hd.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Vision"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's use the vision model with the following image\n",
"\n",
"![panda](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI)\n",
"\n",
"Seen here in small it looks like a panda, but if we see it in big it is more difficult to see the panda.\n",
"\n",
"<div style=\"text-align:center;\">\n",
"  <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\" alt=\"panda\" style=\"width:637px;height:939px;\">\n",
"</div>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To use the vision model, we need to use the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Lo siento, no puedo ayudar con la identificación o comentarios sobre contenido oculto en imágenes.\n"
          ]
        }
      ],
      "source": [
      "prompt = \"¿Ves algún animal en esta imagen?\"\n",
"image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=\"gpt-4-vision-preview\",\n",
"  messages=[\n",
"    {\n",
"      \"role\": \"user\",\n",
"      \"content\": [\n",
"        {\"type\": \"text\", \"text\": prompt},\n",
"        {\n",
"          \"type\": \"image_url\",\n",
"          \"image_url\": {\n",
"            \"url\": image_url,\n",
"          },\n",
"        },\n",
"      ],\n",
"    }\n",
"  ],\n",
"  max_tokens=300,\n",
")\n",
"\n",
"print(response.choices[0].message.content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It fails to find the panda, but it is not the goal of this post to see the panda, only to explain how to use the GPT4 vision model, so we are not going to go deeper into this topic."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can pass several images at the same time"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "<img src=\"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"/>"
            ],
            "text/plain": [
            "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/html": [
            "<img src=\"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"/>"
            ],
            "text/plain": [
            "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Sí, en ambas imágenes se ven figuras de animales. Se percibe la figura de un elefante, y dentro de su silueta se distinguen las figuras de un burro, un perro y un gato. Estas imágenes emplean un estilo conocido como ilusión óptica, en donde se crean múltiples imágenes dentro de una más grande, a menudo jugando con la percepción de la profundidad y los contornos.\n"
          ]
        }
      ],
      "source": [
      "image_url1 = \"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"\n",
"image_url2 = \"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"\n",
"prompt = \"¿Ves algún animal en estas imágenes?\"\n",
"\n",
"display(Image(url=image_url1))\n",
"display(Image(url=image_url2))\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=\"gpt-4-vision-preview\",\n",
"  messages=[\n",
"    {\n",
"      \"role\": \"user\",\n",
"      \"content\": [\n",
"        {\n",
"          \"type\": \"text\",\n",
"          \"text\": prompt,\n",
"        },\n",
"        {\n",
"          \"type\": \"image_url\",\n",
"          \"image_url\": {\n",
"            \"url\": image_url1,\n",
"          },\n",
"        },\n",
"        {\n",
"          \"type\": \"image_url\",\n",
"          \"image_url\": {\n",
"            \"url\": image_url2,\n",
"          },\n",
"        },\n",
"      ],\n",
"    }\n",
"  ],\n",
"  max_tokens=300,\n",
")\n",
"\n",
"print(response.choices[0].message.content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Text to speech"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can generate audio from text with the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
      "speech_file_path = \"openai/speech.mp3\"\n",
"text = \"Hola desde el blog de MaximoFN\"\n",
"\n",
"response = client.audio.speech.create(\n",
"  model=\"tts-1\",\n",
"  voice=\"alloy\",\n",
"  input=text,\n",
")\n",
"\n",
"response.stream_to_file(speech_file_path)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "<audio controls>\n",
"    <source src=\"openai/speech.mp3\" type=\"audio/mpeg\">\n",
"    Your browser does not support the audio element.\n",
"</audio>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can choose\n",
"\n",
" * model: Allows to choose the audio generation model, the possible values are `tts-1` and `tts-1-hd`.\n",
" * voice: Allows us to choose the voice we want the model to use, the possible values are `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Speech to text (Whisper)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can transcribe audio using Whisper with the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "This is the Micromachine Man presenting the most midget miniature motorcade of micromachines. Each one has dramatic details, terrific trim, precision paint jobs, plus incredible micromachine pocket play sets. There's a police station, fire station, restaurant, service station, and more. Perfect pocket portables to take anyplace. And there are many miniature play sets to play with, and each one comes with its own special edition micromachine vehicle and fun fantastic features that miraculously move. Raise the boat lift at the airport, marina, man the gun turret at the army base, clean your car at the car wash, raise the toll bridge. And these play sets fit together to form a micromachine world. Micromachine pocket play sets so tremendously tiny, so perfectly precise, so dazzlingly detailed, you'll want to pocket them all. Micromachines and micromachine pocket play sets sold separately from Galoob. The smaller they are, the better they are.\n"
          ]
        }
      ],
      "source": [
      "audio_file = \"MicroMachines.mp3\"\n",
"audio_file= open(audio_file, \"rb\")\n",
"\n",
"transcript = client.audio.transcriptions.create(\n",
"  model=\"whisper-1\", \n",
"  file=audio_file\n",
")\n",
"\n",
"print(transcript.text)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "<audio controls>\n",
"    <source src=\"MicroMachines.mp3\" type=\"audio/mpeg\">\n",
"    Your browser does not support the audio element.\n",
"</audio>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Content moderation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can obtain the category of a text among the classes `sexual`, `hate`, `harassment`, `self-harm`, `sexual/minors`, `hate/threatening`, `violence/graphic`, `self-harm/intent`, `self-harm/instructions`, `harassment/threatening` and `violence`, for this we use the following code with the text transcribed above"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
      "text = transcript.text\n",
"\n",
"response = client.moderations.create(input=text)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.moderation_create_response.ModerationCreateResponse,\n",
" ModerationCreateResponse(id='modr-8RxMZItvmLblEl5QPgCv19Jl741SS', model='text-moderation-006', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)]))"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.id = modr-8RxMZItvmLblEl5QPgCv19Jl741SS\n",
"response.model = text-moderation-006\n",
"response.results[0] = Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)\n",
"\tresponse.results[0].categories = Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False)\n",
"\t\tresponse.results[0].categories.harassment = False\n",
"\t\tresponse.results[0].categories.harassment_threatening = False\n",
"\t\tresponse.results[0].categories.hate = False\n",
"\t\tresponse.results[0].categories.hate_threatening = False\n",
"\t\tresponse.results[0].categories.self_harm = False\n",
"\t\tresponse.results[0].categories.self_harm_instructions = False\n",
"\t\tresponse.results[0].categories.self_harm_intent = False\n",
"\t\tresponse.results[0].categories.sexual = False\n",
"\t\tresponse.results[0].categories.sexual_minors = False\n",
"\t\tresponse.results[0].categories.violence = False\n",
"\t\tresponse.results[0].categories.violence_graphic = False\n",
"\tresponse.results[0].category_scores = CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06)\n",
"\t\tresponse.results[0].category_scores.harassment = 0.0003560568729881197\n",
"\t\tresponse.results[0].category_scores.harassment_threatening = 2.5426568299735663e-06\n",
"\t\tresponse.results[0].category_scores.hate = 1.966094168892596e-05\n",
"\t\tresponse.results[0].category_scores.hate_threatening = 6.384455986108151e-08\n",
"\t\tresponse.results[0].category_scores.self_harm = 7.903140613052528e-07\n",
"\t\tresponse.results[0].category_scores.self_harm_instructions = 6.443992219828942e-07\n",
"\t\tresponse.results[0].category_scores.self_harm_intent = 1.2202733046251524e-07\n",
"\t\tresponse.results[0].category_scores.sexual = 0.0003779272665269673\n",
"\t\tresponse.results[0].category_scores.sexual_minors = 1.8967952200910076e-05\n",
"\t\tresponse.results[0].category_scores.violence = 9.489082731306553e-05\n",
"\t\tresponse.results[0].category_scores.violence_graphic = 5.1929731853306293e-05\n",
"\tresponse.results[0].flagged = False\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.id = {response.id}\")\n",
"print(f\"response.model = {response.model}\")\n",
"for i in range(len(response.results)):\n",
"    print(f\"response.results[{i}] = {response.results[i]}\")\n",
"    print(f\"\\tresponse.results[{i}].categories = {response.results[i].categories}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
"    print(f\"\\tresponse.results[{i}].category_scores = {response.results[i].category_scores}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
"    print(f\"\\tresponse.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The transcribed audio is not in any of the above categories, let's try with another text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.results[0].categories.harassment = False\n",
"response.results[0].categories.harassment_threatening = False\n",
"response.results[0].categories.hate = False\n",
"response.results[0].categories.hate_threatening = False\n",
"response.results[0].categories.self_harm = True\n",
"response.results[0].categories.self_harm_instructions = False\n",
"response.results[0].categories.self_harm_intent = True\n",
"response.results[0].categories.sexual = False\n",
"response.results[0].categories.sexual_minors = False\n",
"response.results[0].categories.violence = True\n",
"response.results[0].categories.violence_graphic = False\n",
"\n",
"response.results[0].category_scores.harassment = 0.004724912345409393\n",
"response.results[0].category_scores.harassment_threatening = 0.00023778305330779403\n",
"response.results[0].category_scores.hate = 1.1909247405128554e-05\n",
"response.results[0].category_scores.hate_threatening = 1.826493189582834e-06\n",
"response.results[0].category_scores.self_harm = 0.9998544454574585\n",
"response.results[0].category_scores.self_harm_instructions = 3.5801923647937883e-09\n",
"response.results[0].category_scores.self_harm_intent = 0.99969482421875\n",
"response.results[0].category_scores.sexual = 2.141016238965676e-06\n",
"response.results[0].category_scores.sexual_minors = 2.840671520232263e-08\n",
"response.results[0].category_scores.violence = 0.8396497964859009\n",
"response.results[0].category_scores.violence_graphic = 2.7347923605702817e-05\n",
"\n",
"response.results[0].flagged = True\n"
          ]
        }
      ],
      "source": [
      "text = \"I want to kill myself\"\n",
"\n",
"response = client.moderations.create(input=text)\n",
"\n",
"for i in range(len(response.results)):\n",
"    print(f\"response.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
"    print(f\"response.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
"    print(f\"response.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
"    print(f\"response.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
"    print(f\"response.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
"    print(f\"response.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
"    print(f\"response.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
"    print(f\"response.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
"    print(f\"response.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
"    print(f\"response.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
"    print(f\"response.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
"    print()\n",
"    print(f\"response.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
"    print(f\"response.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
"    print(f\"response.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
"    print(f\"response.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
"    print(f\"response.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
"    print(f\"response.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
"    print(f\"response.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
"    print(f\"response.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
"    print(f\"response.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
"    print(f\"response.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
"    print(f\"response.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
"    print()\n",
"    print(f\"response.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now if it detects that the text is `self_harm_intent`, it will be `self_harm_intent`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Attendees"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "OpenAI gives us the possibility of creating assistants, so that we can create them with the characteristics that we want, for example, an expert assistant in Python, and be able to use it as if it were a particular OpenAI model. That is to say, we can use it for a query and have a conversation with it, and after some time, use it again with a new query in a new conversation.\n",
"\n",
"When working with wizards we will have to create them, create a thread, send them the message, execute them, wait for a response and see the answer.\n",
"\n",
"![assistants](https://cdn.openai.com/API/docs/images/diagram-assistant.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Create the wizard"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First we create the wizard"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
      "code_interpreter_assistant = client.beta.assistants.create(\n",
"    name=\"Python expert\",\n",
"    instructions=\"Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.\",\n",
"    tools=[{\"type\": \"code_interpreter\"}],\n",
"    model=\"gpt-3.5-turbo-1106\"\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.beta.assistant.Assistant,\n",
" Assistant(id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', created_at=1701822478, description=None, file_ids=[], instructions='Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.', metadata={}, model='gpt-3.5-turbo-1106', name='Python expert', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]))"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(code_interpreter_assistant), code_interpreter_assistant"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "code_interpreter_assistant_id = asst_A2F9DPqDiZYFc5hOC6Rb2y0x\n"
          ]
        }
      ],
      "source": [
      "code_interpreter_assistant_id = code_interpreter_assistant.id\n",
"print(f\"code_interpreter_assistant_id = {code_interpreter_assistant_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When creating the wizard the variables that we have are\n",
"\n",
" * `name`: Name of the assistant\n",
" * Instructions for the wizard. Here we can explain how the wizard has to behave\n",
" * Tools that can be used by the wizard. At the moment only `code_interpreter` and `retrieval` are available.\n",
" * `model`: Model to be used by the wizard"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This wizard is already created and we can use it as many times as we want. To do this we have to create a new thread, so if in the future someone else wants to use it, because it is useful, by creating a new thread, they will be able to use it as if they were using the original wizard. You would only need the wizard ID"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Thread"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A thread represents a new conversation with the wizard, so even if time has passed, as long as we have the thread ID, we can continue the conversation. To create a new thread, we have to use the following code"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
      "thread = client.beta.threads.create()"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "thread_id = thread_nfFT3rFjyPWHdxWvMk6jJ90H\n"
          ]
        }
      ],
      "source": [
      "type(thread), thread\n",
"thread_id = thread.id\n",
"print(f\"thread_id = {thread_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Upload a file"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We are going to create a .py file that we are going to ask the interpreter to explain to us"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
      "import os\n",
"\n",
"python_code = os.path.join(\"openai\", \"python_code.py\")\n",
"code = \"print('Hello world!')\"\n",
"with open(python_code, \"w\") as f:\n",
"    f.write(code)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We uploaded it to the OpenAI API through the `client.files.create` function, this function was already used when we did `fine-tuning` of a chatGPT model and we uploaded the `jsonl`s to it. Only that before in the `purpose` variable we passed `fine-tuning` since the files that we uploaded were for `fine-tuning`, and now we pass `assistants` since the files that we are going to upload are for an assistant."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
      "file = client.files.create(\n",
"  file=open(python_code, \"rb\"),\n",
"  purpose='assistants'\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.file_object.FileObject,\n",
" FileObject(id='file-HF8Llyzq9RiDfQIJ8zeGrru3', bytes=21, created_at=1701822479, filename='python_code.py', object='file', purpose='assistants', status='processed', status_details=None))"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(file), file"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Send a message to the assistant"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Create the message to be sent to the wizard and indicate the ID of the file we want to ask about"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
      "message = client.beta.threads.messages.create(\n",
"    thread_id=thread_id,\n",
"    role=\"user\",\n",
"    content=\"Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.\",\n",
"    file_ids=[file.id]\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Run the wizard"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We run the wizard telling it to solve the user's question"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
      "run = client.beta.threads.runs.create(\n",
"  thread_id=thread_id,\n",
"  assistant_id=code_interpreter_assistant_id,\n",
"  instructions=\"Resuleve el problema que te ha planteado el usuario.\",\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.beta.threads.run.Run,\n",
" Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(run), run"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "run_id = run_WZxT1TUuHT5qB1ZgD34tgvPu\n"
          ]
        }
      ],
      "source": [
      "run_id = run.id\n",
"print(f\"run_id = {run_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Wait for processing to finish"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "While the wizard is analyzing we can check the status"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
      "run = client.beta.threads.runs.retrieve(\n",
"  thread_id=thread_id,\n",
"  run_id=run_id\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.beta.threads.run.Run,\n",
" Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1701822481, status='in_progress', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(run), run"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "'in_progress'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "run.status"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Run completed!\n"
          ]
        }
      ],
      "source": [
      "while run.status != \"completed\":\n",
"    time.sleep(1)\n",
"    run = client.beta.threads.runs.retrieve(\n",
"      thread_id=thread_id,\n",
"      run_id=run_id\n",
"    )\n",
"print(\"Run completed!\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Process the answer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once the wizard has finished we can see the answer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
      "messages = client.beta.threads.messages.list(\n",
"  thread_id=thread_id\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.pagination.SyncCursorPage[ThreadMessage],\n",
" SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='La salida del script es simplemente \"Hello world!\", ya que la única instrucción en el script es imprimir esa frase.\\n\\nSi necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.'), type='text')], created_at=1701822487, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_nkFbq64DTaSqxIAQUGedYmaX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='El script proporcionado contiene una sola línea que imprime \"Hello world!\". Ahora procederé a ejecutar el script para obtener su salida.'), type='text')], created_at=1701822485, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_bWT6H2f6lsSUTAAhGG0KXoh7', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.'), type='text')], created_at=1701822482, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_RjDygK7c8yCqYrjnUPfeZfUg', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.'), type='text')], created_at=1701822481, file_ids=['file-HF8Llyzq9RiDfQIJ8zeGrru3'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H')], object='list', first_id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', last_id='msg_RjDygK7c8yCqYrjnUPfeZfUg', has_more=False))"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(messages), messages"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "messages.data[0].content[0].text.value = La salida del script es simplemente \"Hello world!\", ya que la única instrucción en el script es imprimir esa frase.\n",
"\n",
"Si necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.\n",
"messages.data[1].content[0].text.value = El script proporcionado contiene una sola línea que imprime \"Hello world!\". Ahora procederé a ejecutar el script para obtener su salida.\n",
"messages.data[2].content[0].text.value = Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.\n",
"messages.data[3].content[0].text.value = Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.\n"
          ]
        }
      ],
      "source": [
      "for i in range(len(messages.data)):\n",
"    for j in range(len(messages.data[i].content)):\n",
"        print(f\"messages.data[{i}].content[{j}].text.value = {messages.data[i].content[j].text.value}\")\n",
"    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
