{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# API da OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instalar a biblioteca do OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em primeiro lugar, para poder usar a API da OpenAI, \u00e9 necess\u00e1rio instalar a biblioteca da OpenAI. Para isso, executamos o seguinte comando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importar a biblioteca do OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez instalada a biblioteca, a importamos para poder us\u00e1-la em nosso c\u00f3digo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obter uma API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder usar a API da OpenAI, \u00e9 necess\u00e1rio obter uma API Key. Para isso, nos dirigimos \u00e0 p\u00e1gina da [OpenAI](https://openai.com/), e nos registramos. Uma vez registrados, nos dirigimos \u00e0 se\u00e7\u00e3o de [API Keys](https://platform.openai.com/api-keys), e criamos uma nova API Key.",
        "\n",
        "![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que temos, dizemos \u00e0 API do OpenAI qual \u00e9 nossa chave API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = \"Pon aqu\u00ed tu API key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Criamos nosso primeiro chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com a API da OpenAI \u00e9 muito simples criar um chatbot b\u00e1sico, ao qual vamos passar um prompt, e ele vai nos retornar uma resposta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em primeiro lugar temos que escolher qual modelo vamos usar, no meu caso vou usar o modelo `gpt-3.5-turbo-1106` que atualmente \u00e9 um bom modelo para este post, j\u00e1 que para o que vamos fazer n\u00e3o precisamos usar o melhor modelo. A OpenAI tem uma lista com todos os seus [modelos](https://platform.openai.com/docs/models) e uma p\u00e1gina com os [pre\u00e7os](https://openai.com/pricing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = \"gpt-3.5-turbo-1106\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora temos que criar um cliente que ser\u00e1 o respons\u00e1vel por se comunicar com a API da OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = openai.OpenAI(api_key=api_key, organization=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, passamos nossa chave de API. Al\u00e9m disso, podemos passar a organiza\u00e7\u00e3o, mas em nosso caso n\u00e3o \u00e9 necess\u00e1rio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "promtp = \"Cu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E j\u00e1 podemos pedir uma resposta \u00e0 OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver como \u00e9 a resposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.chat.chat_completion.ChatCompletion,\n",
              " ChatCompletion(id='chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. Tambi\u00e9n es \u00fatil investigar qu\u00e9 lenguajes son populares en la industria en la que te gustar\u00eda trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte m\u00e1s oportunidades laborales. En resumen, la elecci\u00f3n del lenguaje de programaci\u00f3n para aprender depender\u00e1 de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))], created=1701584994, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.id = chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y\n",
            "response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. Tambi\u00e9n es \u00fatil investigar qu\u00e9 lenguajes son populares en la industria en la que te gustar\u00eda trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte m\u00e1s oportunidades laborales. En resumen, la elecci\u00f3n del lenguaje de programaci\u00f3n para aprender depender\u00e1 de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))]\n",
            "response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. Tambi\u00e9n es \u00fatil investigar qu\u00e9 lenguajes son populares en la industria en la que te gustar\u00eda trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte m\u00e1s oportunidades laborales. En resumen, la elecci\u00f3n del lenguaje de programaci\u00f3n para aprender depender\u00e1 de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))\n",
            "\tresponse.choices[0].finish_reason = stop\n",
            "\tresponse.choices[0].index = 0\n",
            "\tresponse.choices[0].message = ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. Tambi\u00e9n es \u00fatil investigar qu\u00e9 lenguajes son populares en la industria en la que te gustar\u00eda trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte m\u00e1s oportunidades laborales. En resumen, la elecci\u00f3n del lenguaje de programaci\u00f3n para aprender depender\u00e1 de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None)\n",
            "\t\tresponse.choices[0].message.content = \n",
            "\t\tNo hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. Tambi\u00e9n es \u00fatil investigar qu\u00e9 lenguajes son populares en la industria en la que te gustar\u00eda trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte m\u00e1s oportunidades laborales. En resumen, la elecci\u00f3n del lenguaje de programaci\u00f3n para aprender depender\u00e1 de tus preferencias personales y de tus metas profesionales.\n",
            "\t\tresponse.choices[0].message.role = assistant\n",
            "\t\tresponse.choices[0].message.function_call = None\n",
            "\t\tresponse.choices[0].message.tool_calls = None\n",
            "response.created = 1701584994\n",
            "response.model = gpt-3.5-turbo-1106\n",
            "response.object = chat.completion\n",
            "response.system_fingerprint = fp_eeff13170a\n",
            "response.usage = CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)\n",
            "\tresponse.usage.completion_tokens = 181\n",
            "\tresponse.usage.prompt_tokens = 21\n",
            "\tresponse.usage.total_tokens = 202\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.id = {response.id}\")\n",
        "print(f\"response.choices = {response.choices}\")\n",
        "for i in range(len(response.choices)):\n",
        "    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
        "    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
        "    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
        "    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
        "    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
        "print(f\"response.created = {response.created}\")\n",
        "print(f\"response.model = {response.model}\")\n",
        "print(f\"response.object = {response.object}\")\n",
        "print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
        "print(f\"response.usage = {response.usage}\")\n",
        "print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
        "print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
        "print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, nos devolve muita informa\u00e7\u00e3o.",
        "\n",
        "Por exemplo `response.choices[0].finish_reason = stop` significa que o modelo parou de gerar texto porque chegou ao final do prompt. Isso nos vem muito bem para depurar, j\u00e1 que os poss\u00edveis valores s\u00e3o `stop` que significa que a API retornou a mensagem completa, `length` que significa que a sa\u00edda do modelo foi incompleta devido ao fato de ser maior que `max_tokens` ou limite de token do modelo, `function_call` o modelo decidiu chamar uma fun\u00e7\u00e3o, `content_filter` que significa que o conte\u00fado foi omitido por uma limita\u00e7\u00e3o de conte\u00fado da OpenAI e `null` que significa que a resposta da API foi incompleta",
        "\n",
        "Tamb\u00e9m nos fornece informa\u00e7\u00f5es sobre os tokens para podermos acompanhar o dinheiro gasto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Par\u00e2metros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando solicitamos uma resposta \u00e0 OpenAI, podemos passar uma s\u00e9rie de par\u00e2metros para que ela nos retorne uma resposta mais adequada ao que desejamos. Vamos ver quais s\u00e3o os par\u00e2metros que podemos passar.",
        "\n",
        "* `mensagens`: Lista de mensagens que foram enviadas ao chatbot",
        "* `model`: Modelo que queremos usar",
        "* `frequency_penalty`: Penalidade de frequ\u00eancia. Quanto maior for o valor, menos prov\u00e1vel ser\u00e1 que o modelo repita a mesma resposta.",
        "* `max_tokens`: N\u00famero m\u00e1ximo de tokens que pode retornar o modelo",
        "* `n`: N\u00famero de respostas que queremos que o modelo nos retorne",
        "* `presence_penalty`: Penaliza\u00e7\u00e3o de presen\u00e7a. Quanto maior for o valor, menos prov\u00e1vel ser\u00e1 que o modelo repita a mesma resposta",
        "* `seed`: Semente para a gera\u00e7\u00e3o de texto",
        "* `stop`: Lista de tokens que indicam que o modelo deve parar de gerar texto",
        "* `stream`: Se `True`, a API retornar\u00e1 uma resposta sempre que o modelo gerar um token. Se `False`, a API retornar\u00e1 uma resposta quando o modelo tiver gerado todos os tokens.",
        "* `temperature`: Quanto maior for o valor, mais criativo ser\u00e1 o modelo",
        "* `top_p`: Quanto maior for o valor, mais criativo ser\u00e1 o modelo",
        "* `user`: ID do usu\u00e1rio que est\u00e1 conversando com o chatbot",
        "* `timeout`: Tempo m\u00e1ximo que queremos esperar a que a API nos retorne uma resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver alguns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mensagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos passar \u00e0 API uma lista de mensagens que foram enviadas ao chatbot. Isso \u00e9 \u00fatil para fornecer o hist\u00f3rico de conversas ao chatbot, para que ele possa gerar uma resposta mais adequada \u00e0 conversa. E para condicionar a resposta do chatbot ao que foi dito anteriormente.",
        "\n",
        "Al\u00e9m disso, podemos passar uma mensagem de sistema para indicar como deve se comportar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hist\u00f3rico de conversas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver um exemplo do hist\u00f3rico de conversas, primeiro perguntamos como est\u00e1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola MaximoFN, soy un modelo de inteligencia artificial dise\u00f1ado para conversar y ayudar en lo que necesites. \u00bfEn qu\u00e9 puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "promtp = \"Hola, soy MaximoFN, \u00bfC\u00f3mo est\u00e1s?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ele respondeu que n\u00e3o tem sentimentos e perguntou em que pode nos ajudar. Ent\u00e3o, se eu perguntar agora como ele se chama, ele n\u00e3o vai saber responder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lo siento, no tengo esa informaci\u00f3n. Pero puedes dec\u00edrmelo t\u00fa.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfMe puedes decir c\u00f3mo me llamo?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para resolver isso, passamos o hist\u00f3rico de conversas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es MaximoFN.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfMe puedes decir c\u00f3mo me llamo?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"Hola, soy MaximoFN, \u00bfC\u00f3mo est\u00e1s?\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"Hola MaximoFN, soy un modelo de inteligencia artificial dise\u00f1ado para conversar y ayudar en lo que necesites. \u00bfEn qu\u00e9 puedo ayudarte hoy?\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Condicionamento por exemplos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vejamos um exemplo de como condicionar a resposta do chatbot ao que foi dito anteriormente. Agora perguntamos como obter a lista de arquivos em um diret\u00f3rio no terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En la terminal de un sistema operativo Unix o Linux, puedes listar los archivos de un directorio utilizando el comando `ls`. Por ejemplo, si quieres listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si deseas listar los archivos de un directorio espec\u00edfico, puedes proporcionar la ruta del directorio despu\u00e9s del comando `ls`, por ejemplo `ls /ruta/del/directorio`. Si deseas ver m\u00e1s detalles sobre los archivos, puedes usar la opci\u00f3n `-l` para obtener una lista detallada o `-a` para mostrar tambi\u00e9n los archivos ocultos.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfC\u00f3mo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver o que ela responde se agora a condicionarmos com exemplos de respostas curtas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puede usar el comando `ls` en la terminal para listar los archivos de un directorio. Por ejemplo:\n",
            "```\n",
            "ls\n",
            "```\n",
            "Muestra los archivos y directorios en el directorio actual.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfC\u00f3mo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras l\u00edneas de un archivo\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
        "      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensi\u00f3n .txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias p\u00e1ginas\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
        "      {\"role\": \"user\", \"content\": \"Buscar la direcci\u00f3n IP 12.34.56.78\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 5 \u00faltimas l\u00edneas de foo.txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
        "      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conseguimos que ele d\u00ea uma resposta mais curta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Condicionamento com mensagem do sistema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos passar uma mensagem de sistema para indicar como deve se comportar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puedes listar los archivos de un directorio en la terminal usando el comando `ls`. Por ejemplo, para listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si quieres listar los archivos de un directorio espec\u00edfico, puedes utilizar `ls` seguido de la ruta del directorio. Por ejemplo, `ls /ruta/del/directorio`.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfC\u00f3mo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puedes listar los archivos de un directorio en la terminal utilizando el comando \"ls\". Por ejemplo, para listar los archivos en el directorio actual, puedes ejecutar el comando \"ls\". Si deseas listar los archivos de otro directorio, simplemente especifica el directorio despu\u00e9s del comando \"ls\", por ejemplo \"ls /ruta/al/directorio\".\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfC\u00f3mo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras l\u00edneas de un archivo\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
        "      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensi\u00f3n .txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias p\u00e1ginas\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
        "      {\"role\": \"user\", \"content\": \"Buscar la direcci\u00f3n IP 12.34.56.78\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 5 \u00faltimas l\u00edneas de foo.txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
        "      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### N\u00famero m\u00e1ximo de tokens da resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos limitar o n\u00famero de tokens que o modelo pode retornar. Isso \u00e9 \u00fatil para evitar que o modelo ultrapasse a resposta que desejamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La respuesta a esta pregunta puede variar dependiendo de los intereses y objetivos individuales, ya que cada lenguaje de programaci\u00f3n tiene sus propias ventajas y desventajas. Sin embargo, algunos de los lenguajes m\u00e1s\n",
            "\n",
            "response.choices[0].finish_reason = length\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfCu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  max_tokens = 50,\n",
        ")\n",
        "\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)\n",
        "print(f\"\\nresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, a resposta \u00e9 cortada pela metade porque excederia o limite de tokens. Al\u00e9m disso, agora a raz\u00e3o de parada \u00e9 `length` em vez de `stop`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Criatividade do modelo atrav\u00e9s da temperatura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos fazer o modelo mais criativo atrav\u00e9s da temperatura. Quanto maior for o valor, mais criativo ser\u00e1 o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que la elecci\u00f3n depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qu\u00e9 tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfCu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\"\n",
        "temperature = 0\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  temperature = temperature,\n",
        ")\n",
        "\n",
        "content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que la elecci\u00f3n depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente f\u00e1ciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnolog\u00eda. Es importante considerar qu\u00e9 tipo de proyectos o campos de inter\u00e9s te gustar\u00eda explorar al momento de elegir un lenguaje de programaci\u00f3n para aprender.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfCu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\"\n",
        "temperature = 1\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  temperature = temperature,\n",
        ")\n",
        "\n",
        "content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que la elecci\u00f3n depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qu\u00e9 tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
            "No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que la elecci\u00f3n depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente f\u00e1ciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnolog\u00eda. Es importante considerar qu\u00e9 tipo de proyectos o campos de inter\u00e9s te gustar\u00eda explorar al momento de elegir un lenguaje de programaci\u00f3n para aprender.\n"
          ]
        }
      ],
      "source": [
        "print(content_0)\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Criatividade do modelo atrav\u00e9s do top_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos fazer com que o modelo seja mais criativo atrav\u00e9s do par\u00e2metro `top_p`. Quanto maior for o valor, mais criativo ser\u00e1 o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que la elecci\u00f3n depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qu\u00e9 tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfCu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\"\n",
        "top_p = 0\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  top_p = top_p,\n",
        ")\n",
        "\n",
        "content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mejor lenguaje de programaci\u00f3n para aprender depende de los objetivos del aprendizaje y del tipo de programaci\u00f3n que se quiera realizar. Algunos lenguajes de programaci\u00f3n populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qu\u00e9 tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programaci\u00f3n para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programaci\u00f3n web.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfCu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\"\n",
        "top_p = 1\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  top_p = top_p,\n",
        ")\n",
        "\n",
        "content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programaci\u00f3n para aprender, ya que la elecci\u00f3n depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qu\u00e9 tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
            "El mejor lenguaje de programaci\u00f3n para aprender depende de los objetivos del aprendizaje y del tipo de programaci\u00f3n que se quiera realizar. Algunos lenguajes de programaci\u00f3n populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qu\u00e9 tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programaci\u00f3n para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programaci\u00f3n web.\n"
          ]
        }
      ],
      "source": [
        "print(content_0)\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### N\u00famero de respostas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos pedir \u00e0 API que nos retorne mais de uma resposta. Isso \u00e9 \u00fatil para que o modelo nos retorne v\u00e1rias respostas e assim poder escolher a que mais gostarmos, para isso vamos colocar os par\u00e2metros `temperature` e `top_p` em 1 para que o modelo seja mais criativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mejor lenguaje de programaci\u00f3n para aprender depende de tus objetivos y del tipo de aplicaciones que te interese desarrollar. Algunos de los lenguajes m\u00e1s populares para aprender son:\n",
            "\t\t1. Python: Es un lenguaje de programaci\u00f3n vers\u00e1til, f\u00e1cil de aprender y con una amplia comunidad de desarrolladores. Es ideal para principiantes y se utiliza en una gran variedad de aplicaciones, desde desarrollo web hasta inteligencia artificial.\n",
            "\t\t2. JavaScript: Es el lenguaje de programaci\u00f3n m\u00e1s utilizado en el desarrollo web. Es imprescindible para aquellos que quieren trabajar en el \u00e1mbito del desarrollo frontend y backend.\n",
            "\t\t3. Java: Es un lenguaje de programaci\u00f3n muy popular en el \u00e1mbito empresarial, por lo que aprender Java puede abrirte muchas puertas laborales. Adem\u00e1s, es un lenguaje estructurado que te ense\u00f1ar\u00e1 conceptos importantes de la programaci\u00f3n orientada a objetos.\n",
            "\t\t4. C#: Es un lenguaje de programaci\u00f3n desarrollado por Microsoft que se utiliza especialmente en el desarrollo de aplicaciones para Windows. Es ideal para aquellos que quieran enfocarse en el desarrollo de aplicaciones de escritorio.\n",
            "\t\tEn resumen, el mejor lenguaje de programaci\u00f3n para aprender depende de tus intereses y objetivos personales. Es importante investigar y considerar qu\u00e9 tipos de aplicaciones te gustar\u00eda desarrollar para elegir el lenguaje que m\u00e1s se adapte a tus necesidades.\n",
            "El mejor lenguaje de programaci\u00f3n para aprender depende de los objetivos y necesidades individuales. Algunos de los lenguajes de programaci\u00f3n m\u00e1s populares y ampliamente utilizados incluyen Python, JavaScript, Java, C++, Ruby y muchos otros. Python es a menudo recomendado para principiantes debido a su sintaxis simple y legible, mientras que JavaScript es esencial para el desarrollo web. Java es ampliamente utilizado en el desarrollo de aplicaciones empresariales y Android, y C++ es com\u00fanmente utilizado en sistemas embebidos y juegos. En \u00faltima instancia, el mejor lenguaje de programaci\u00f3n para aprender depender\u00e1 de lo que quiera lograr con su habilidades de programaci\u00f3n.\n",
            "El mejor lenguaje de programaci\u00f3n para aprender depende de los intereses y objetivos individuales de cada persona. Algunos de los lenguajes m\u00e1s populares y bien documentados para principiantes incluyen Python, JavaScript, Java y C#. Python es conocido por su simplicidad y versatilidad, mientras que JavaScript es esencial para el desarrollo web. Java y C# son lenguajes ampliamente utilizados en la industria y proporcionan una base s\u00f3lida para aprender otros lenguajes. En \u00faltima instancia, la elecci\u00f3n del lenguaje depender\u00e1 de las metas personales y la aplicaci\u00f3n deseada.\n",
            "El mejor lenguaje de programaci\u00f3n para aprender depende de los intereses y objetivos de cada persona. Algunos lenguajes populares para principiantes incluyen Python, Java, JavaScript, C++ y Ruby. Python es frecuentemente recomendado para aprender a programar debido a su sintaxis sencilla y legible, mientras que Java es utilizado en aplicaciones empresariales y Android. JavaScript es fundamental para el desarrollo web, y C++ es com\u00fanmente utilizado en aplicaciones de alto rendimiento. Ruby es conocido por su facilidad de uso y flexibilidad. En \u00faltima instancia, la elecci\u00f3n del lenguaje depender\u00e1 de qu\u00e9 tipo de desarrollo te interesa y qu\u00e9 tipo de proyectos deseas realizar.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfCu\u00e1l es el mejor lenguaje de programaci\u00f3n para aprender?\"\n",
        "temperature = 1\n",
        "top_p = 1\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  temperature = temperature,\n",
        "  top_p = top_p,\n",
        "  n = 4\n",
        ")\n",
        "\n",
        "content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "content_1 = response.choices[1].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "content_2 = response.choices[2].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "content_3 = response.choices[3].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_0)\n",
        "print(content_1)\n",
        "print(content_2)\n",
        "print(content_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retreinar modelo da OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A OpenAI oferece a possibilidade de re-treinar seus modelos da API para obter melhores resultados com nossos pr\u00f3prios dados. Isso tem as seguintes vantagens:",
        "\n",
        "+ Obt\u00e9m-se resultados de maior qualidade para nossos dados",
        "+ Em um prompt podemos dar exemplos para que se comporte como quisermos, mas apenas alguns. Dessa forma, reentrenando-o, podemos fornecer muitos mais.",
        "+ Economia de tokens devido a indica\u00e7\u00f5es mais curtas. Como j\u00e1 o treinamos para o nosso caso de uso, podemos dar-lhe menos indica\u00e7\u00f5es para resolver nossas tarefas.",
        "+ Solicita\u00e7\u00f5es de menor lat\u00eancia. Ao chamar modelos pr\u00f3prios teremos menos lat\u00eancia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepara\u00e7\u00e3o dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A API da OpenAI pede que forne\u00e7amos os dados em um arquivo `jsonl` no seguinte formato",
        "\n",
        "``` json\n",
        "{",
        "\"mensagens\":",
        "[",
        "{",
        "\"Marv \u00e9 um chatbot factual que tamb\u00e9m \u00e9 sarc\u00e1stico.\"",
        "},",
        "{",
        "Qual \u00e9 a capital da Fran\u00e7a?",
        "},",
        "{",
        "Paris, como se todos j\u00e1 n\u00e3o soubessem disso.",
        "}",
        "]",
        "}",
        "{",
        "\"mensagens\":",
        "[",
        "{",
        "\"Marv \u00e9 um chatbot factual que tamb\u00e9m \u00e9 sarc\u00e1stico.\"",
        "},",
        "{",
        "Quem escreveu 'Romeu e Julieta'?",
        "},",
        "{",
        "Oh, apenas um cara chamado William Shakespeare. J\u00e1 ouviu falar dele?",
        "}",
        "]",
        "}",
        "{",
        "\"mensagens\":",
        "[",
        "{",
        "\"Marv \u00e9 um chatbot factual que tamb\u00e9m \u00e9 sarc\u00e1stico.\"",
        "},",
        "{",
        "Qu\u00e3o longe est\u00e1 a Lua da Terra?",
        "},",
        "{",
        "Aproximadamente 384.400 quil\u00f4metros. Mais ou menos alguns, como se isso realmente importasse.",
        "}",
        "]",
        "}",
        "```\n",
        "\n",
        "Com um m\u00e1ximo de 4096 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Valida\u00e7\u00e3o dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para me poupar trabalho, passei um a um todos os meus posts para o chatgpt e disse a ele para gerar 10 `FAQ`s para cada um no formato `CSV`, pois duvidava se conseguiria gerar um formato como o solicitado no `jsonl`. E ele gerou um `CSV` com o seguinte formato para cada post",
        "\n",
        "``` csv\n",
        "prompt,completion",
        "O que cobre a Introdu\u00e7\u00e3o ao Python no material fornecido? \"A Introdu\u00e7\u00e3o ao Python aborda t\u00f3picos como tipos de dados, operadores, uso de fun\u00e7\u00f5es e classes, manipula\u00e7\u00e3o de objetos iter\u00e1veis e uso de m\u00f3dulos. [Mais informa\u00e7\u00f5es](https://maximofn.com/python/)\"",
        "Quais s\u00e3o os tipos de dados b\u00e1sicos em Python? Python tem 7 tipos de dados b\u00e1sicos: texto (`str`), num\u00e9ricos (`int`, `float`, `complex`), sequ\u00eancias (`list`, `tuple`, `range`), mapeamento (`dict`), conjuntos (`set`, `frozenset`), booleanos (`bool`) e bin\u00e1rios (`bytes`, `bytearray`, `memoryview`). [Mais informa\u00e7\u00f5es](https://maximofn.com/python/)",
        "O que s\u00e3o e como s\u00e3o usados os operadores em Python? Os operadores em Python s\u00e3o s\u00edmbolos especiais que s\u00e3o usados para realizar opera\u00e7\u00f5es como adi\u00e7\u00e3o, subtra\u00e7\u00e3o, multiplica\u00e7\u00e3o e divis\u00e3o entre vari\u00e1veis e valores. Tamb\u00e9m incluem operadores l\u00f3gicos para compara\u00e7\u00f5es. [Mais informa\u00e7\u00f5es](https://maximofn.com/python/)",
        "Como se define e utiliza uma fun\u00e7\u00e3o em Python? \"Em Python, uma fun\u00e7\u00e3o \u00e9 definida usando a palavra-chave `def`, seguida do nome da fun\u00e7\u00e3o e par\u00eanteses. As fun\u00e7\u00f5es podem ter par\u00e2metros e retornar valores. Elas s\u00e3o utilizadas para encapsular l\u00f3gica que pode ser reutilizada ao longo do c\u00f3digo. [Mais informa\u00e7\u00f5es](https://maximofn.com/python/)\"",
        "O que s\u00e3o as classes em Python e como elas s\u00e3o usadas? As classes em Python s\u00e3o a base da programa\u00e7\u00e3o orientada a objetos. Permitem criar objetos que encapsulam dados e funcionalidades. As classes s\u00e3o definidas usando a palavra-chave `class`, seguida do nome da classe. [Mais informa\u00e7\u00f5es](https://maximofn.com/python/)",
        "...",
        "```\n",
        "\n",
        "Cada `CSV` tem 10 `FAQ`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vou fazer um c\u00f3digo que pegue cada `CSV` e gere dois novos `jsonl`, um para treinamento e outro para valida\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CSVs_path = \"openai/faqs_posts\"\n",
        "percetn_train = 0.8\n",
        "percetn_validation = 0.2\n",
        "\n",
        "jsonl_train = os.path.join(CSVs_path, \"train.jsonl\")\n",
        "jsonl_validation = os.path.join(CSVs_path, \"validation.jsonl\")\n",
        "\n",
        "# Create the train.jsonl and validation.jsonl files\n",
        "with open(jsonl_train, 'w') as f:\n",
        "    f.write('')\n",
        "with open(jsonl_validation, 'w') as f:\n",
        "    f.write('')\n",
        "\n",
        "for file in os.listdir(CSVs_path):  # Get all files in the directory\n",
        "    if file.endswith(\".csv\"):    # Check if file is a csv\n",
        "        csv = os.path.join(CSVs_path, file) # Get the path to the csv file\n",
        "        number_of_lines = 0\n",
        "        csv_content = []\n",
        "        for line in open(csv, 'r'): # Read all lines in the csv file\n",
        "            if line.startswith('prompt'):   # Skip the first line\n",
        "                continue\n",
        "            number_of_lines += 1    # Count the number of lines\n",
        "            csv_content.append(line)    # Add the line to the csv_content list\n",
        "\n",
        "        number_of_train = int(number_of_lines * percetn_train)  # Calculate the number of lines for the train.jsonl file\n",
        "        number_of_validation = int(number_of_lines * percetn_validation)    # Calculate the number of lines for the validation.sjonl file\n",
        "\n",
        "        for i in range(number_of_lines):\n",
        "            prompt = csv_content[i].split(',')[0]\n",
        "            response = ','.join(csv_content[i].split(',')[1:]).replace('\\n', '').replace('\"', '')\n",
        "            if i > 0 and i <= number_of_train:\n",
        "                # add line to train.jsonl\n",
        "                with open(jsonl_train, 'a') as f:\n",
        "                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')\n",
        "            elif i > number_of_train and i <= number_of_train + number_of_validation:\n",
        "                # add line to validation.csv\n",
        "                with open(jsonl_validation, 'a') as f:\n",
        "                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que tenho os dois `jsonl`, executo um [c\u00f3digo](https://cookbook.openai.com/examples/chat_finetuning_data_prep) que a OpenAI fornece para verificar os `jsonl`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro validamos os do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "        \n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "        \n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "        \n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "        \n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "            \n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "        \n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "    \n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E agora os de valida\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "        \n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "        \n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "        \n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "        \n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "            \n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "        \n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "    \n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C\u00e1lculo de tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O n\u00famero m\u00e1ximo de tokens de cada exemplo tem que ser 4096, portanto, se tivermos exemplos mais longos, apenas ser\u00e3o utilizados os primeiros 4096 tokens. Portanto, vamos contar o n\u00famero de tokens que cada `jsonl` possui para saber quanto vai custar re-treinar o modelo.",
        "\n",
        "Mas primeiro \u00e9 preciso instalar a biblioteca `tiktoken`, que \u00e9 o tokenizador usado pela OpenAI e que nos ajudar\u00e1 a saber quantos tokens possui cada `CSV`, e consequentemente, quanto custar\u00e1 retreinar o modelo.",
        "\n",
        "Para instal\u00e1-lo, executamos o seguinte comando",
        "\n",
        "``` bash\n",
        "pip install tiktoken",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos algumas fun\u00e7\u00f5es necess\u00e1rias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min:{min(values)}, max: {max(values)}\")\n",
        "    print(f\"mean: {np.mean(values)}, median: {np.median(values)}\")\n",
        "    print(f\"p5: {np.quantile(values, 0.1)}, p95: {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min:3, max: 3\n",
            "mean: 3.0, median: 3.0\n",
            "p5: 3.0, p95: 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min:67, max: 132\n",
            "mean: 90.13793103448276, median: 90.0\n",
            "p5: 81.5, p95: 99.5\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min:33, max: 90\n",
            "mean: 48.66379310344828, median: 48.5\n",
            "p5: 41.0, p95: 55.5\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "    \n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos no o conjunto de treinamento nenhuma mensagem ultrapassa os 4096 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min:3, max: 3\n",
            "mean: 3.0, median: 3.0\n",
            "p5: 3.0, p95: 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min:80, max: 102\n",
            "mean: 89.93333333333334, median: 91.0\n",
            "p5: 82.2, p95: 96.8\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min:41, max: 57\n",
            "mean: 48.2, median: 49.0\n",
            "p5: 42.8, p95: 51.6\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "    \n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nenhuma mensagem do conjunto de valida\u00e7\u00e3o excede 4096 tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C\u00e1lculo do custo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outra coisa muito importante \u00e9 saber quanto vai custar fazer esse fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~10456 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~31368 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "convo_lens = []\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "tokens_for_train = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como na hora de escrever este post, o pre\u00e7o para treinar `gpt-3.5-turbo` \u00e9 de $0.0080 por cada 1000 tokens, podemos saber quanto nos custar\u00e1 o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training price: $0.248\n"
          ]
        }
      ],
      "source": [
        "pricing = 0.0080\n",
        "num_tokens_pricing = 1000\n",
        "\n",
        "training_price = pricing * (tokens_for_train // num_tokens_pricing)\n",
        "print(f\"Training price: ${training_price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~1349 tokens that will be charged for during training\n",
            "By default, you'll train for 6 epochs on this dataset\n",
            "By default, you'll be charged for ~8094 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "convo_lens = []\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "tokens_for_validation = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation price: $0.064\n"
          ]
        }
      ],
      "source": [
        "validation_price = pricing * (tokens_for_validation // num_tokens_pricing)\n",
        "print(f\"Validation price: ${validation_price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total price: $0.312\n"
          ]
        }
      ],
      "source": [
        "total_price = training_price + validation_price\n",
        "print(f\"Total price: ${total_price}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se nossos c\u00e1lculos estiverem corretos, vemos que o retreinamento de `gpt-3.5-turbo` nos custar\u00e1 $0.312"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que tudo estiver pronto, precisamos enviar os arquivos `jsonl` para a API da OpenAI para que o modelo seja retreinado. Para isso, executamos o seguinte c\u00f3digo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.files.create(file=open(jsonl_train, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.file_object.FileObject,\n",
              " FileObject(id='file-LWztOVasq4E0U67wRe8ShjLZ', bytes=47947, created_at=1701585709, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None))"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.bytes = 47947\n",
            "result.created_at = 1701585709\n",
            "result.filename = train.jsonl\n",
            "result.object = file\n",
            "result.purpose = fine-tune\n",
            "result.status = processed\n",
            "result.status_details = None\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.bytes = {result.bytes}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.filename = {result.filename}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.purpose = {result.purpose}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.status_details = {result.status_details}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jsonl_train_id = file-LWztOVasq4E0U67wRe8ShjLZ\n"
          ]
        }
      ],
      "source": [
        "jsonl_train_id = result.id\n",
        "print(f\"jsonl_train_id = {jsonl_train_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fazemos o mesmo com o conjunto de valida\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.files.create(file=open(jsonl_validation, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = file-E0YOgIIe9mwxmFcza5bFyVKW\n",
            "result.bytes = 6369\n",
            "result.created_at = 1701585730\n",
            "result.filename = validation.jsonl\n",
            "result.object = file\n",
            "result.purpose = fine-tune\n",
            "result.status = processed\n",
            "result.status_details = None\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.bytes = {result.bytes}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.filename = {result.filename}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.purpose = {result.purpose}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.status_details = {result.status_details}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jsonl_train_id = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "jsonl_validation_id = result.id\n",
        "print(f\"jsonl_train_id = {jsonl_validation_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que os temos carregados, passamos a treinar nosso pr\u00f3prio modelo da OpenAI, para isso usamos o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.fine_tuning.jobs.create(model = \"gpt-3.5-turbo\", training_file = jsonl_train_id, validation_file = jsonl_validation_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
              " FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
            "result.created_at = 1701585758\n",
            "result.error = None\n",
            "result.fine_tuned_model = None\n",
            "result.finished_at = None\n",
            "result.hyperparameters = Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto')\n",
            "\tn_epochs = auto\n",
            "\tbatch_size = auto\n",
            "\tlearning_rate_multiplier = auto\n",
            "result.model = gpt-3.5-turbo-0613\n",
            "result.object = fine_tuning.job\n",
            "result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
            "result.result_files = []\n",
            "result.status = validating_files\n",
            "result.trained_tokens = None\n",
            "result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.error = {result.error}\")\n",
        "print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
        "print(f\"result.finished_at = {result.finished_at}\")\n",
        "print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
        "print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
        "print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
        "print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
        "print(f\"result.model = {result.model}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.organization_id = {result.organization_id}\")\n",
        "print(f\"result.result_files = {result.result_files}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
        "print(f\"result.training_file = {result.training_file}\")\n",
        "print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fine_tune_id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n"
          ]
        }
      ],
      "source": [
        "fine_tune_id = result.id\n",
        "print(f\"fine_tune_id = {fine_tune_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que em `status` sa\u00eda `validating_files`. Como o fine tuning demora bastante, podemos ir perguntando pelo processo mediante o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
              " FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='running', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
            "result.created_at = 1701585758\n",
            "result.error = None\n",
            "result.fine_tuned_model = None\n",
            "result.finished_at = None\n",
            "result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
            "\tn_epochs = 3\n",
            "\tbatch_size = 1\n",
            "\tlearning_rate_multiplier = 2\n",
            "result.model = gpt-3.5-turbo-0613\n",
            "result.object = fine_tuning.job\n",
            "result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
            "result.result_files = []\n",
            "result.status = running\n",
            "result.trained_tokens = None\n",
            "result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.error = {result.error}\")\n",
        "print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
        "print(f\"result.finished_at = {result.finished_at}\")\n",
        "print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
        "print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
        "print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
        "print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
        "print(f\"result.model = {result.model}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.organization_id = {result.organization_id}\")\n",
        "print(f\"result.result_files = {result.result_files}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
        "print(f\"result.training_file = {result.training_file}\")\n",
        "print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos um loop que aguarde o t\u00e9rmino do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job succeeded"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
        "status = result.status\n",
        "\n",
        "while status != \"succeeded\":\n",
        "    time.sleep(10)\n",
        "    result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
        "    status = result.status\n",
        "\n",
        "print(\"Job succeeded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o treinamento foi conclu\u00eddo, voltamos a solicitar as informa\u00e7\u00f5es sobre o processo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
            "result.created_at = 1701585758\n",
            "result.error = None\n",
            "result.fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
            "result.finished_at = 1701586541\n",
            "result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
            "\tn_epochs = 3\n",
            "\tbatch_size = 1\n",
            "\tlearning_rate_multiplier = 2\n",
            "result.model = gpt-3.5-turbo-0613\n",
            "result.object = fine_tuning.job\n",
            "result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
            "result.result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
            "result.status = succeeded\n",
            "result.trained_tokens = 30672\n",
            "result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.error = {result.error}\")\n",
        "print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
        "print(f\"result.finished_at = {result.finished_at}\")\n",
        "print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
        "print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
        "print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
        "print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
        "print(f\"result.model = {result.model}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.organization_id = {result.organization_id}\")\n",
        "print(f\"result.result_files = {result.result_files}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
        "print(f\"result.training_file = {result.training_file}\")\n",
        "print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver alguns dados interessantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
            "finished_at = 1701586541\n",
            "result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
            "status = succeeded\n",
            "trained_tokens = 30672\n"
          ]
        }
      ],
      "source": [
        "fine_tuned_model = result.fine_tuned_model\n",
        "finished_at = result.finished_at\n",
        "result_files = result.result_files\n",
        "status = result.status\n",
        "trained_tokens = result.trained_tokens\n",
        "\n",
        "print(f\"fine_tuned_model = {fine_tuned_model}\")\n",
        "print(f\"finished_at = {finished_at}\")\n",
        "print(f\"result_files = {result_files}\")\n",
        "print(f\"status = {status}\")\n",
        "print(f\"trained_tokens = {trained_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que deu o nome `ft:gpt-3.5-turbo-0613:personal::8RagA0RT` ao nosso modelo, seu status agora \u00e9 `succeeded` e que usou 30672 tokens, enquanto n\u00f3s hav\u00edamos previsto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31368, 8094, 39462)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_for_train, tokens_for_validation, tokens_for_train + tokens_for_validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Isto \u00e9, ele usou menos tokens, portanto o treinamento custou menos do que hav\u00edamos previsto, especificamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real training price: $0.24\n"
          ]
        }
      ],
      "source": [
        "real_training_price = pricing * (trained_tokens // num_tokens_pricing)\n",
        "print(f\"Real training price: ${real_training_price}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al\u00e9m dessas informa\u00e7\u00f5es, se formos \u00e0 p\u00e1gina [finetune](https://platform.openai.com/finetune) da OpenAI, podemos ver que nosso modelo est\u00e1 l\u00e1.",
        "\n",
        "![open ai finetune](https://images.maximofn.com/openai_fine_tuning_process.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb\u00e9m podemos ver quanto nos custou o treinamento",
        "\n",
        "![open ai finetune cost](https://images.maximofn.com/openai_fine_tuning_cost.webp)",
        "\n",
        "Como podemos ver, foram apenas $0,25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E por \u00faltimo, vamos ver quanto tempo levou para fazer este treinamento. Podemos ver a que horas come\u00e7ou.",
        "\n",
        "![open ai finetune start](https://images.maximofn.com/openai_fine_tuning_process_start_time.webp)",
        "\n",
        "E a que horas terminou",
        "\n",
        "![open ai finetune end](https://images.maximofn.com/openai_fine_tuning_process_stop_time.webp)",
        "\n",
        "Portanto, levou mais ou menos uns 10 minutos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dentro do [playground](https://platform.openai.com/playground?mode=chat) da OpenAI podemos provar nosso modelo, mas vamos fazer isso atrav\u00e9s da API como aprendemos aqui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "promtp = \"\u00bfC\u00f3mo se define una funci\u00f3n en Python?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = fine_tuned_model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.chat.chat_completion.ChatCompletion,\n",
              " ChatCompletion(id='chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una funci\u00f3n en Python se define utilizando la palabra clave `def`, seguida del nombre de la funci\u00f3n, par\u00e9ntesis y dos puntos. El cuerpo de la funci\u00f3n se indenta debajo. [M\u00e1s informaci\u00f3n](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))], created=1701667535, model='ft:gpt-3.5-turbo-0613:personal::8RagA0RT', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)))"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.id = chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc\n",
            "response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una funci\u00f3n en Python se define utilizando la palabra clave `def`, seguida del nombre de la funci\u00f3n, par\u00e9ntesis y dos puntos. El cuerpo de la funci\u00f3n se indenta debajo. [M\u00e1s informaci\u00f3n](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))]\n",
            "response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una funci\u00f3n en Python se define utilizando la palabra clave `def`, seguida del nombre de la funci\u00f3n, par\u00e9ntesis y dos puntos. El cuerpo de la funci\u00f3n se indenta debajo. [M\u00e1s informaci\u00f3n](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))\n",
            "\tresponse.choices[0].finish_reason = stop\n",
            "\tresponse.choices[0].index = 0\n",
            "\tresponse.choices[0].message = ChatCompletionMessage(content='Una funci\u00f3n en Python se define utilizando la palabra clave `def`, seguida del nombre de la funci\u00f3n, par\u00e9ntesis y dos puntos. El cuerpo de la funci\u00f3n se indenta debajo. [M\u00e1s informaci\u00f3n](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None)\n",
            "\t\tresponse.choices[0].message.content = \n",
            "\t\tUna funci\u00f3n en Python se define utilizando la palabra clave `def`, seguida del nombre de la funci\u00f3n, par\u00e9ntesis y dos puntos. El cuerpo de la funci\u00f3n se indenta debajo. [M\u00e1s informaci\u00f3n](https://maximofn.com/python/)\n",
            "\t\tresponse.choices[0].message.role = assistant\n",
            "\t\tresponse.choices[0].message.function_call = None\n",
            "\t\tresponse.choices[0].message.tool_calls = None\n",
            "response.created = 1701667535\n",
            "response.model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
            "response.object = chat.completion\n",
            "response.system_fingerprint = None\n",
            "response.usage = CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)\n",
            "\tresponse.usage.completion_tokens = 54\n",
            "\tresponse.usage.prompt_tokens = 16\n",
            "\tresponse.usage.total_tokens = 70\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.id = {response.id}\")\n",
        "print(f\"response.choices = {response.choices}\")\n",
        "for i in range(len(response.choices)):\n",
        "    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
        "    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
        "    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
        "    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
        "    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
        "print(f\"response.created = {response.created}\")\n",
        "print(f\"response.model = {response.model}\")\n",
        "print(f\"response.object = {response.object}\")\n",
        "print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
        "print(f\"response.usage = {response.usage}\")\n",
        "print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
        "print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
        "print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Una funci\u00f3n en Python se define utilizando la palabra clave `def`, seguida del nombre de la funci\u00f3n, par\u00e9ntesis y dos puntos. El cuerpo de la funci\u00f3n se indenta debajo. [M\u00e1s informaci\u00f3n](https://maximofn.com/python/)\n"
          ]
        }
      ],
      "source": [
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos um modelo que n\u00e3o apenas resolve a resposta, mas tamb\u00e9m nos fornece um link para a documenta\u00e7\u00e3o do nosso blog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver como se comporta com um exemplo que claramente n\u00e3o tem nada a ver com o blog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Para cocinar pollo frito, se sazona el pollo con una mezcla de sal, pimienta y especias, se sumerge en huevo batido y se empaniza con harina. Luego, se fr\u00ede en aceite caliente hasta que est\u00e9 dorado y cocido por dentro. [M\u00e1s informaci\u00f3n](https://maximofn.com/pollo-frito/)\n"
          ]
        }
      ],
      "source": [
        "promtp = \"\u00bfC\u00f3mo puedo cocinar pollo frito?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = fine_tuned_model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "\n",
        "for i in range(len(response.choices)):\n",
        "    content = response.choices[i].message.content.replace('\\n\\n', '\\n')\n",
        "    print(f\"{content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se pode ver, ele nos fornece o link `https://maximofn.com/pollo-frito/`, que n\u00e3o existe. Portanto, mesmo tendo reentrenado um modelo de chatGPT, \u00e9 preciso ter cuidado com as respostas e n\u00e3o confiar 100% nelas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gerar imagens com DALL-E 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para gerar imagens com o DALL-E 3, temos que usar o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=\"a white siamese cat\",\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.images_response.ImagesResponse,\n",
              " ImagesResponse(created=1701823487, data=[Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')]))"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.created = 1701823487\n",
            "response.data[0] = Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')\n",
            "\tresponse.data[0].b64_json = None\n",
            "\tresponse.data[0].revised_prompt = Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\n",
            "\tresponse.data[0].url = https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.created = {response.created}\")\n",
        "for i in range(len(response.data)):\n",
        "    print(f\"response.data[{i}] = {response.data[i]}\")\n",
        "    print(f\"\\tresponse.data[{i}].b64_json = {response.data[i].b64_json}\")\n",
        "    print(f\"\\tresponse.data[{i}].revised_prompt = {response.data[i].revised_prompt}\")\n",
        "    print(f\"\\tresponse.data[{i}].url = {response.data[i].url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver um dado muito interessante que n\u00e3o conseguimos ver quando usamos o DALL-E 3 atrav\u00e9s da interface da OpenAI, e \u00e9 o prompt que foi passado ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\""
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.data[0].revised_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com esse prompt, gerou-se a seguinte imagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = response.data[0].url\n",
        "# img_data = requests.get(url).content\n",
        "with open('openai/dall-e-3.png', 'wb') as handler:\n",
        "    handler.write(requests.get(url).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dall-e 3](openai/dall-e-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como temos o prompt que a OpenAI utilizou, na verdade vamos tentar us\u00e1-lo para gerar um gato semelhante, mas com os olhos verdes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A well-defined image of a Siamese cat boasting a shiny white coat. Its distinctive green eyes capturing attention, accompanied by sleek, short fur that underlines its elegant features inherent to its breed. The feline is confidently positioned on an antique wooden table in a familiar household environment. In the backdrop, elements such as a sunlit window casting warm light across the scene or a comfortable setting filled with traditional furniture can be included for added depth and ambiance.\n"
          ]
        }
      ],
      "source": [
        "revised_prompt = response.data[0].revised_prompt\n",
        "gree_eyes = revised_prompt.replace(\"blue\", \"green\")\n",
        "\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=gree_eyes,\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "print(response.data[0].revised_prompt)\n",
        "\n",
        "image_url = response.data[0].url\n",
        "\n",
        "image_path = 'openai/dall-e-3-green.png'\n",
        "with open(image_path, 'wb') as handler:\n",
        "    handler.write(requests.get(image_url).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dall-e-3-green](openai/dall-e-3-green.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embora a cor do gato tenha mudado e n\u00e3o apenas dos olhos, a posi\u00e7\u00e3o e o fundo s\u00e3o muito semelhantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al\u00e9m do prompt, as outras vari\u00e1veis que podemos modificar s\u00e3o",
        "\n",
        "* `model`: Permite escolher o modelo de gera\u00e7\u00e3o de imagens, os poss\u00edveis valores s\u00e3o `dalle-2` e `dalle-3`",
        "* `size`: Permite mudar o tamanho da imagem, os valores poss\u00edveis s\u00e3o `256x256`, `512x512`, `1024x1024`, `1792x1024`, `1024x1792` pixels",
        "* `quality`: Permite mudar a qualidade da imagem, os valores poss\u00edveis s\u00e3o `standard` ou `hd`",
        "* `response_format`: Permite mudar o formato da resposta, os valores poss\u00edveis s\u00e3o `url` ou `b64_json`",
        "* `n`: Permite mudar o n\u00famero de imagens que queremos que nos retorne o modelo. Com DALL-E 3, s\u00f3 podemos pedir uma imagem.",
        "* `style`: Permite mudar o estilo da imagem, os poss\u00edveis valores s\u00e3o `vivid` ou `natural`",
        "\n",
        "Ent\u00e3o, vamos gerar uma imagem de alta qualidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Render a portrait of a Siamese cat boasting a pristine white coat. This cat should have captivating green eyes that stand out. Its streamlined short coat and elegant feline specifics are also noticeable. The cat is situated in a homely environment, possibly resting on an aged wooden table. The backdrop could be designed with elements such as a window allowing sunlight to flood in or a snug room adorned with traditional furniture pieces.\n"
          ]
        }
      ],
      "source": [
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=gree_eyes,\n",
        "  size=\"1024x1792\",\n",
        "  quality=\"hd\",\n",
        "  n=1,\n",
        "  style=\"natural\",\n",
        ")\n",
        "\n",
        "print(response.data[0].revised_prompt)\n",
        "\n",
        "image_url = response.data[0].url\n",
        "\n",
        "image_path = 'openai/dall-e-3-hd.png'\n",
        "with open(image_path, 'wb') as handler:\n",
        "    handler.write(requests.get(image_url).content)\n",
        "display(Image(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dall-e-3-hd](openai/dall-e-3-hd.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vis\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a usar o modelo de vis\u00e3o com a seguinte imagem",
        "\n",
        "![panda](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI)",
        "\n",
        "Que aqui em pequeno parece um panda, mas se a virmos de perto \u00e9 mais dif\u00edcil ver o panda",
        "\n",
        "<div style=\"text-align:center;\">",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\" alt=\"panda\" style=\"width:637px;height:939px;\">",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para usar o modelo de vis\u00e3o, temos que utilizar o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lo siento, no puedo ayudar con la identificaci\u00f3n o comentarios sobre contenido oculto en im\u00e1genes.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\u00bfVes alg\u00fan animal en esta imagen?\"\n",
        "image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4-vision-preview\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"text\", \"text\": prompt},\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": image_url,\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  max_tokens=300,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N\u00e3o consegue encontrar o panda, mas n\u00e3o \u00e9 o objetivo deste post que ele veja o panda, apenas explicar como usar o modelo de vis\u00e3o do GPT4, ent\u00e3o n\u00e3o vamos nos aprofundar mais neste assunto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos passar v\u00e1rias imagens de uma vez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S\u00ed, en ambas im\u00e1genes se ven figuras de animales. Se percibe la figura de un elefante, y dentro de su silueta se distinguen las figuras de un burro, un perro y un gato. Estas im\u00e1genes emplean un estilo conocido como ilusi\u00f3n \u00f3ptica, en donde se crean m\u00faltiples im\u00e1genes dentro de una m\u00e1s grande, a menudo jugando con la percepci\u00f3n de la profundidad y los contornos.\n"
          ]
        }
      ],
      "source": [
        "image_url1 = \"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"\n",
        "image_url2 = \"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"\n",
        "prompt = \"\u00bfVes alg\u00fan animal en estas im\u00e1genes?\"\n",
        "\n",
        "display(Image(url=image_url1))\n",
        "display(Image(url=image_url2))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4-vision-preview\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": prompt,\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": image_url1,\n",
        "          },\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": image_url2,\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  max_tokens=300,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Texto para fala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos gerar \u00e1udio a partir de texto com o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "speech_file_path = \"openai/speech.mp3\"\n",
        "text = \"Hola desde el blog de MaximoFN\"\n",
        "\n",
        "response = client.audio.speech.create(\n",
        "  model=\"tts-1\",\n",
        "  voice=\"alloy\",\n",
        "  input=text,\n",
        ")\n",
        "\n",
        "response.stream_to_file(speech_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<audio controls>",
        "<source src=\"openai/speech.mp3\" type=\"audio/mpeg\">",
        "O seu navegador n\u00e3o suporta o elemento de \u00e1udio.",
        "</audio>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos escolher",
        "\n",
        "* model: Permite escolher o modelo de gera\u00e7\u00e3o de \u00e1udio. Os poss\u00edveis valores s\u00e3o `tts-1` e `tts-1-hd`",
        "* voice: Permite escolher a voz que queremos que o modelo use, os poss\u00edveis valores s\u00e3o `alloy`, `echo`, `fable`, `onyx`, `nova` e `shimmer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fala para texto (Whisper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos transcrever \u00e1udio com o Whisper utilizando o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the Micromachine Man presenting the most midget miniature motorcade of micromachines. Each one has dramatic details, terrific trim, precision paint jobs, plus incredible micromachine pocket play sets. There's a police station, fire station, restaurant, service station, and more. Perfect pocket portables to take anyplace. And there are many miniature play sets to play with, and each one comes with its own special edition micromachine vehicle and fun fantastic features that miraculously move. Raise the boat lift at the airport, marina, man the gun turret at the army base, clean your car at the car wash, raise the toll bridge. And these play sets fit together to form a micromachine world. Micromachine pocket play sets so tremendously tiny, so perfectly precise, so dazzlingly detailed, you'll want to pocket them all. Micromachines and micromachine pocket play sets sold separately from Galoob. The smaller they are, the better they are.\n"
          ]
        }
      ],
      "source": [
        "audio_file = \"MicroMachines.mp3\"\n",
        "audio_file= open(audio_file, \"rb\")\n",
        "\n",
        "transcript = client.audio.transcriptions.create(\n",
        "  model=\"whisper-1\", \n",
        "  file=audio_file\n",
        ")\n",
        "\n",
        "print(transcript.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<audio controls>",
        "<source src=\"MicroMachines.mp3\" type=\"audio/mpeg\">",
        "Seu navegador n\u00e3o suporta o elemento de \u00e1udio.",
        "</audio>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modera\u00e7\u00e3o de conte\u00fado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos obter a categoria de um texto entre as classes `sexual`, `hate`, `harassment`, `self-harm`, `sexual/minors`, `hate/threatening`, `violence/graphic`, `self-harm/intent`, `self-harm/instructions`, `harassment/threatening` e `violence`, para isso usamos o seguinte c\u00f3digo com o texto transcrito anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = transcript.text\n",
        "\n",
        "response = client.moderations.create(input=text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.moderation_create_response.ModerationCreateResponse,\n",
              " ModerationCreateResponse(id='modr-8RxMZItvmLblEl5QPgCv19Jl741SS', model='text-moderation-006', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)]))"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.id = modr-8RxMZItvmLblEl5QPgCv19Jl741SS\n",
            "response.model = text-moderation-006\n",
            "response.results[0] = Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)\n",
            "\tresponse.results[0].categories = Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False)\n",
            "\t\tresponse.results[0].categories.harassment = False\n",
            "\t\tresponse.results[0].categories.harassment_threatening = False\n",
            "\t\tresponse.results[0].categories.hate = False\n",
            "\t\tresponse.results[0].categories.hate_threatening = False\n",
            "\t\tresponse.results[0].categories.self_harm = False\n",
            "\t\tresponse.results[0].categories.self_harm_instructions = False\n",
            "\t\tresponse.results[0].categories.self_harm_intent = False\n",
            "\t\tresponse.results[0].categories.sexual = False\n",
            "\t\tresponse.results[0].categories.sexual_minors = False\n",
            "\t\tresponse.results[0].categories.violence = False\n",
            "\t\tresponse.results[0].categories.violence_graphic = False\n",
            "\tresponse.results[0].category_scores = CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06)\n",
            "\t\tresponse.results[0].category_scores.harassment = 0.0003560568729881197\n",
            "\t\tresponse.results[0].category_scores.harassment_threatening = 2.5426568299735663e-06\n",
            "\t\tresponse.results[0].category_scores.hate = 1.966094168892596e-05\n",
            "\t\tresponse.results[0].category_scores.hate_threatening = 6.384455986108151e-08\n",
            "\t\tresponse.results[0].category_scores.self_harm = 7.903140613052528e-07\n",
            "\t\tresponse.results[0].category_scores.self_harm_instructions = 6.443992219828942e-07\n",
            "\t\tresponse.results[0].category_scores.self_harm_intent = 1.2202733046251524e-07\n",
            "\t\tresponse.results[0].category_scores.sexual = 0.0003779272665269673\n",
            "\t\tresponse.results[0].category_scores.sexual_minors = 1.8967952200910076e-05\n",
            "\t\tresponse.results[0].category_scores.violence = 9.489082731306553e-05\n",
            "\t\tresponse.results[0].category_scores.violence_graphic = 5.1929731853306293e-05\n",
            "\tresponse.results[0].flagged = False\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.id = {response.id}\")\n",
        "print(f\"response.model = {response.model}\")\n",
        "for i in range(len(response.results)):\n",
        "    print(f\"response.results[{i}] = {response.results[i]}\")\n",
        "    print(f\"\\tresponse.results[{i}].categories = {response.results[i].categories}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
        "    print(f\"\\tresponse.results[{i}].category_scores = {response.results[i].category_scores}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
        "    print(f\"\\tresponse.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O \u00e1udio transcritor n\u00e3o est\u00e1 em nenhuma das categorias anteriores, vamos tentar com outro texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.results[0].categories.harassment = False\n",
            "response.results[0].categories.harassment_threatening = False\n",
            "response.results[0].categories.hate = False\n",
            "response.results[0].categories.hate_threatening = False\n",
            "response.results[0].categories.self_harm = True\n",
            "response.results[0].categories.self_harm_instructions = False\n",
            "response.results[0].categories.self_harm_intent = True\n",
            "response.results[0].categories.sexual = False\n",
            "response.results[0].categories.sexual_minors = False\n",
            "response.results[0].categories.violence = True\n",
            "response.results[0].categories.violence_graphic = False\n",
            "\n",
            "response.results[0].category_scores.harassment = 0.004724912345409393\n",
            "response.results[0].category_scores.harassment_threatening = 0.00023778305330779403\n",
            "response.results[0].category_scores.hate = 1.1909247405128554e-05\n",
            "response.results[0].category_scores.hate_threatening = 1.826493189582834e-06\n",
            "response.results[0].category_scores.self_harm = 0.9998544454574585\n",
            "response.results[0].category_scores.self_harm_instructions = 3.5801923647937883e-09\n",
            "response.results[0].category_scores.self_harm_intent = 0.99969482421875\n",
            "response.results[0].category_scores.sexual = 2.141016238965676e-06\n",
            "response.results[0].category_scores.sexual_minors = 2.840671520232263e-08\n",
            "response.results[0].category_scores.violence = 0.8396497964859009\n",
            "response.results[0].category_scores.violence_graphic = 2.7347923605702817e-05\n",
            "\n",
            "response.results[0].flagged = True\n"
          ]
        }
      ],
      "source": [
        "text = \"I want to kill myself\"\n",
        "\n",
        "response = client.moderations.create(input=text)\n",
        "\n",
        "for i in range(len(response.results)):\n",
        "    print(f\"response.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
        "    print(f\"response.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
        "    print(f\"response.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
        "    print(f\"response.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
        "    print(f\"response.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
        "    print(f\"response.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
        "    print(f\"response.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
        "    print(f\"response.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
        "    print(f\"response.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
        "    print(f\"response.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
        "    print(f\"response.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
        "    print()\n",
        "    print(f\"response.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
        "    print(f\"response.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
        "    print(f\"response.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
        "    print(f\"response.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
        "    print(f\"response.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
        "    print(f\"response.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
        "    print(f\"response.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
        "    print(f\"response.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
        "    print(f\"response.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
        "    print(f\"response.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
        "    print(f\"response.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
        "    print()\n",
        "    print(f\"response.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, se detectar que o texto \u00e9 `self_harm_intent`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assistentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A OpenAI nos d\u00e1 a possibilidade de criar assistentes, de maneira que os podemos criar com as caracter\u00edsticas que n\u00f3s quisermos, por exemplo, um assistente especialista em Python, e poder us\u00e1-lo como se fosse um modelo particular da OpenAI. Isso significa que podemos us\u00e1-lo para uma consulta e ter uma conversa com ele, e depois de algum tempo, reutiliz\u00e1-lo com uma nova consulta em uma nova conversa.",
        "\n",
        "Ao trabalhar com assistentes, teremos que cri\u00e1-los, criar um fio, envi\u00e1-los a mensagem, execut\u00e1-los, esperar que respondam e ver a resposta.",
        "\n",
        "![asistentes](https://cdn.openai.com/API/docs/images/diagram-assistant.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar o assistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro criamos o assistente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_interpreter_assistant = client.beta.assistants.create(\n",
        "    name=\"Python expert\",\n",
        "    instructions=\"Eres un experto en Python. Analiza y ejecuta el c\u00f3digo para ayuda a los usuarios a resolver sus problemas.\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-3.5-turbo-1106\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.beta.assistant.Assistant,\n",
              " Assistant(id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', created_at=1701822478, description=None, file_ids=[], instructions='Eres un experto en Python. Analiza y ejecuta el c\u00f3digo para ayuda a los usuarios a resolver sus problemas.', metadata={}, model='gpt-3.5-turbo-1106', name='Python expert', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]))"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(code_interpreter_assistant), code_interpreter_assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_interpreter_assistant_id = asst_A2F9DPqDiZYFc5hOC6Rb2y0x\n"
          ]
        }
      ],
      "source": [
        "code_interpreter_assistant_id = code_interpreter_assistant.id\n",
        "print(f\"code_interpreter_assistant_id = {code_interpreter_assistant_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na hora de criar o assistente, as vari\u00e1veis que temos s\u00e3o",
        "\n",
        "* `name`: nome do assistente",
        "* `instru\u00e7\u00f5es`: Instru\u00e7\u00f5es para o assistente. Aqui podemos explicar como o assistente deve se comportar.",
        "* `tools`: Ferramentas que o assistente pode usar. No momento, apenas est\u00e3o dispon\u00edveis `code_interpreter` e `retrieval`",
        "* `model`: Modelo que o assistente vai usar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este assistente j\u00e1 foi criado e podemos us\u00e1-lo todas as vezes que quisermos. Para isso, precisamos criar um novo fio, assim, se no futuro outra pessoa quiser us\u00e1-lo porque for \u00fatil, ela poder\u00e1 faz\u00ea-lo simplesmente criando um novo fio. Ela s\u00f3 precisaria do ID do assistente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fio ou thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Um fio representa uma nova conversa\u00e7\u00e3o com o assistente, assim, mesmo que tenha passado algum tempo, enquanto tivermos o ID do fio, podemos continuar a conversa. Para criar um novo fio, temos que usar o seguinte c\u00f3digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thread_id = thread_nfFT3rFjyPWHdxWvMk6jJ90H\n"
          ]
        }
      ],
      "source": [
        "type(thread), thread\n",
        "thread_id = thread.id\n",
        "print(f\"thread_id = {thread_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enviamos um arquivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a criar um arquivo .py que vamos pedir ao interpretador que nos explique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "python_code = os.path.join(\"openai\", \"python_code.py\")\n",
        "code = \"print('Hello world!')\"\n",
        "with open(python_code, \"w\") as f:\n",
        "    f.write(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Subimos isso \u00e0 API da OpenAI usando a fun\u00e7\u00e3o `client.files.create`, esta fun\u00e7\u00e3o j\u00e1 usamos quando fizemos o `fine-tuning` de um modelo do chatGPT e envi\u00e1vamos os `jsonl`s. Apenas que antes na vari\u00e1vel `purpose` pass\u00e1vamos `fine-tuning` pois os arquivos que envi\u00e1vamos eram para `fine-tuning`, e agora passamos `assistants` j\u00e1 que os arquivos que vamos enviar s\u00e3o para um assistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "  file=open(python_code, \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.file_object.FileObject,\n",
              " FileObject(id='file-HF8Llyzq9RiDfQIJ8zeGrru3', bytes=21, created_at=1701822479, filename='python_code.py', object='file', purpose='assistants', status='processed', status_details=None))"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(file), file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enviar uma mensagem ao assistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos a mensagem que vamos enviar ao assistente, al\u00e9m de indicarmos o ID do arquivo sobre o qual queremos perguntar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread_id,\n",
        "    role=\"user\",\n",
        "    content=\"Ejecuta el script que te he pasado, expl\u00edcamelo y dime que da a la salida.\",\n",
        "    file_ids=[file.id]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Executar o assistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Executamos o assistente indicando-lhe que resolva a d\u00favida do usu\u00e1rio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread_id,\n",
        "  assistant_id=code_interpreter_assistant_id,\n",
        "  instructions=\"Resuleve el problema que te ha planteado el usuario.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.beta.threads.run.Run,\n",
              " Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(run), run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_id = run_WZxT1TUuHT5qB1ZgD34tgvPu\n"
          ]
        }
      ],
      "source": [
        "run_id = run.id\n",
        "print(f\"run_id = {run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aguardar at\u00e9 que termine de processar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enquanto o assistente est\u00e1 analisando, podemos verificar o estado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.retrieve(\n",
        "  thread_id=thread_id,\n",
        "  run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.beta.threads.run.Run,\n",
              " Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1701822481, status='in_progress', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(run), run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'in_progress'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run.status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run completed!\n"
          ]
        }
      ],
      "source": [
        "while run.status != \"completed\":\n",
        "    time.sleep(1)\n",
        "    run = client.beta.threads.runs.retrieve(\n",
        "      thread_id=thread_id,\n",
        "      run_id=run_id\n",
        "    )\n",
        "print(\"Run completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processar a resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que o assistente terminou, podemos ver a resposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.pagination.SyncCursorPage[ThreadMessage],\n",
              " SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='La salida del script es simplemente \"Hello world!\", ya que la \u00fanica instrucci\u00f3n en el script es imprimir esa frase.\\n\\nSi necesitas alguna otra aclaraci\u00f3n o ayuda adicional, no dudes en preguntar.'), type='text')], created_at=1701822487, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_nkFbq64DTaSqxIAQUGedYmaX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='El script proporcionado contiene una sola l\u00ednea que imprime \"Hello world!\". Ahora proceder\u00e9 a ejecutar el script para obtener su salida.'), type='text')], created_at=1701822485, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_bWT6H2f6lsSUTAAhGG0KXoh7', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionar\u00e9 una explicaci\u00f3n detallada del script y su salida.'), type='text')], created_at=1701822482, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_RjDygK7c8yCqYrjnUPfeZfUg', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Ejecuta el script que te he pasado, expl\u00edcamelo y dime que da a la salida.'), type='text')], created_at=1701822481, file_ids=['file-HF8Llyzq9RiDfQIJ8zeGrru3'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H')], object='list', first_id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', last_id='msg_RjDygK7c8yCqYrjnUPfeZfUg', has_more=False))"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(messages), messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages.data[0].content[0].text.value = La salida del script es simplemente \"Hello world!\", ya que la \u00fanica instrucci\u00f3n en el script es imprimir esa frase.\n",
            "\n",
            "Si necesitas alguna otra aclaraci\u00f3n o ayuda adicional, no dudes en preguntar.\n",
            "messages.data[1].content[0].text.value = El script proporcionado contiene una sola l\u00ednea que imprime \"Hello world!\". Ahora proceder\u00e9 a ejecutar el script para obtener su salida.\n",
            "messages.data[2].content[0].text.value = Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionar\u00e9 una explicaci\u00f3n detallada del script y su salida.\n",
            "messages.data[3].content[0].text.value = Ejecuta el script que te he pasado, expl\u00edcamelo y dime que da a la salida.\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(messages.data)):\n",
        "    for j in range(len(messages.data[i].content)):\n",
        "        print(f\"messages.data[{i}].content[{j}].text.value = {messages.data[i].content[j].text.value}\")\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "maximofn": {
      "date": "2023-12-06",
      "description_en": "\ud83d\ude80 Discover the power of the OpenAI API in this post! \ud83c\udf1f Learn how to install the OpenAI library \u2728 and I'll guide you through the first steps to become an artificial intelligence guru. \ud83e\udd16 No matter if you're a curious beginner or a coding expert looking for new adventures, this post has everything you need to get started. Get ready to explore the universe of GPT for text generation and DALL-E, image analysis, all with a touch of fun and a lot of innovation! \ud83c\udf89\ud83d\udc69\u200d\ud83d\udcbb Dive into the exciting world of AI and start your journey to unlimited creativity! \ud83c\udf08\ud83d\udcbb",
      "description_es": "\ud83d\ude80 \u00a1Descubre el poder de la API de OpenAI en este post! \ud83c\udf1f Aprende c\u00f3mo instalar la librer\u00eda de OpenAI \u2728 y te guiar\u00e9 en los primeros pasos para convertirte en un gur\u00fa de la inteligencia artificial. \ud83e\udd16 No importa si eres un principiante curioso o un experto en c\u00f3digo buscando nuevas aventuras, este post tiene todo lo que necesitas para empezar. Prep\u00e1rate para explorar el universo de GPT para generaci\u00f3n de texto y DALL-E, an\u00e1lisis de im\u00e1genes, \u00a1todo con un toque de diversi\u00f3n y mucha innovaci\u00f3n! \ud83c\udf89\ud83d\udc69\u200d\ud83d\udcbb \u00a1Sum\u00e9rgete en el emocionante mundo de la IA y comienza tu viaje hacia la creatividad ilimitada! \ud83c\udf08\ud83d\udcbb",
      "description_pt": "\ud83d\ude80 Descubra o poder da API OpenAI neste post! \ud83c\udf1f Aprenda como instalar a biblioteca OpenAI \u2728 e eu vou gui\u00e1-lo pelos primeiros passos para se tornar um guru da intelig\u00eancia artificial. \ud83e\udd16 N\u00e3o importa se voc\u00ea \u00e9 um iniciante curioso ou um especialista em codifica\u00e7\u00e3o em busca de novas aventuras, este post tem tudo o que voc\u00ea precisa para come\u00e7ar. Prepare-se para explorar o universo do GPT para gera\u00e7\u00e3o de texto e DALL-E, an\u00e1lise de imagens, tudo com um toque de divers\u00e3o e muita inova\u00e7\u00e3o! \ud83c\udf89\ud83d\udc69\u200d\ud83d\udcbb Mergulhe no emocionante mundo da IA e comece sua jornada para a criatividade ilimitada! \ud83c\udf08\ud83d\udcbb",
      "end_url": "openai-api",
      "image": "https://images.maximofn.com/openai.webp",
      "image_hover_path": "https://images.maximofn.com/openai.webp",
      "keywords_en": "open ai api, open ai, openai, gpt api, gpt3 api, gpt4 api, gpt4o api, gpt, gpt3, gpt4, gpt4o",
      "keywords_es": "open ai api, open ai, openai, gpt api, gpt3 api, gpt4 api, gpt4o api, gpt, gpt3, gpt4, gpt4o",
      "keywords_pt": "open ai api, open ai, openai, gpt api, gpt3 api, gpt4 api, gpt4o api, gpt, gpt3, gpt4, gpt4o",
      "title_en": "OpenAI API",
      "title_es": "OpenAI API",
      "title_pt": "OpenAI API"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}