{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# API da OpenAI"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Instale a biblioteca OpenAI"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..\n",
"\n",
"Em primeiro lugar, para usar a API OpenAI, é necessário instalar a biblioteca OpenAI. Para fazer isso, executamos o seguinte comando"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "%pip install --upgrade openai"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Importar a biblioteca OpenAI"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois que a biblioteca é instalada, nós a importamos para usá-la em nosso código."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "import openai"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Obter uma chave de API"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para usar a API da OpenAI, é necessário obter uma chave de API. Para fazer isso, acesse a página [OpenAI] (https://openai.com/) e registre-se. Depois de registrado, vá para a seção [API Keys](https://platform.openai.com/api-keys) e crie uma nova chave de API.\n",
"\n",
"![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Assim que a tivermos, informaremos à API do openai qual é a nossa chave de API."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
      "api_key = \"Pon aquí tu API key\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Criamos nosso primeiro chatbot"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Com a API OpenAI, é muito fácil criar um chatbot simples, para o qual passaremos um prompt e ele retornará uma resposta"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Em primeiro lugar, temos que escolher o modelo que vamos usar. No meu caso, vou usar o modelo `gpt-3.5-turbo-1106`, que é atualmente um bom modelo para esta postagem, porque para o que vamos fazer não precisamos usar o melhor modelo. A OpenAI tem uma lista com todos os seus [modelos] (https://platform.openai.com/docs/models) e uma página com os [preços] (https://openai.com/pricing)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
      "model = \"gpt-3.5-turbo-1106\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora precisamos criar um cliente que se comunicará com a API da OpenAI."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
      "client = openai.OpenAI(api_key=api_key, organization=None)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, passamos nossa chave de API. Você também pode passar a organização, mas, no nosso caso, isso não é necessário."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o prompt"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
      "promtp = \"Cuál es el mejor lenguaje de programación para aprender?\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E agora podemos pedir uma resposta à OpenAI."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
      "response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vejamos como é a resposta"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.chat.chat_completion.ChatCompletion,\n",
" ChatCompletion(id='chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))], created=1701584994, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.id = chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y\n",
"response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))]\n",
"response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))\n",
"\tresponse.choices[0].finish_reason = stop\n",
"\tresponse.choices[0].index = 0\n",
"\tresponse.choices[0].message = ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None)\n",
"\t\tresponse.choices[0].message.content = \n",
"\t\tNo hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.\n",
"\t\tresponse.choices[0].message.role = assistant\n",
"\t\tresponse.choices[0].message.function_call = None\n",
"\t\tresponse.choices[0].message.tool_calls = None\n",
"response.created = 1701584994\n",
"response.model = gpt-3.5-turbo-1106\n",
"response.object = chat.completion\n",
"response.system_fingerprint = fp_eeff13170a\n",
"response.usage = CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)\n",
"\tresponse.usage.completion_tokens = 181\n",
"\tresponse.usage.prompt_tokens = 21\n",
"\tresponse.usage.total_tokens = 202\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.id = {response.id}\")\n",
"print(f\"response.choices = {response.choices}\")\n",
"for i in range(len(response.choices)):\n",
"    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
"    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
"    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
"    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
"    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
"print(f\"response.created = {response.created}\")\n",
"print(f\"response.model = {response.model}\")\n",
"print(f\"response.object = {response.object}\")\n",
"print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
"print(f\"response.usage = {response.usage}\")\n",
"print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
"print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
"print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, ele retorna muitas informações.\n",
"\n",
"Por exemplo, `response.choices[0].finish_reason = stop` significa que o modelo parou de gerar texto porque chegou ao final do prompt. Isso é útil para depuração, pois os valores possíveis são `stop`, que significa que a API retornou a mensagem completa, `length`, que significa que a saída do modelo estava incompleta porque era mais longa do que o `max_tokens` ou limite de token do modelo, `function_call`, o modelo decidiu chamar uma função, `content_filter`, que significa que o conteúdo foi ignorado devido a uma limitação de conteúdo da OpenAI e `null`, que significa que a resposta da API estava incompleta.\n",
"\n",
"Ele também nos fornece informações sobre tokens para que possamos controlar o dinheiro gasto."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Parâmetros"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Ao solicitar uma resposta ao OpenAI, podemos passar uma série de parâmetros para que ele retorne uma resposta mais alinhada com o que desejamos. Vamos ver quais são os parâmetros que podemos passar para o OpenAI\n",
"\n",
" * Mensagens: lista de mensagens que foram enviadas para o chatbot.\n",
" * `model`: Modelo que queremos usar\n",
" * frequency_penalty`: Penalidade de frequência. Quanto maior o valor, menor a probabilidade de o modelo repetir a mesma resposta.\n",
" * `max_tokens`: número máximo de tokens que podem ser retornados pelo modelo\n",
" * n`: Número de respostas que queremos que o modelo retorne.\n",
" * presence_penalty`: penalidade de presença. Quanto maior o valor, menor a probabilidade de o modelo repetir a mesma resposta.\n",
" * `seed`: Semente para geração de texto\n",
" * `stop`: lista de tokens que indicam que o modelo deve parar de gerar texto.\n",
" * If `stream`: Se `True`, a API retornará uma resposta sempre que o modelo gerar um token. Se `False`, a API retornará uma resposta quando o modelo tiver gerado todos os tokens.\n",
" * `temperature`: quanto maior o valor, mais criativo será o modelo.\n",
" * `top_p`: quanto maior o valor, mais criativo é o modelo.\n",
" * `user`: ID do usuário que está falando com o chatbot\n",
" * `timeout`: tempo máximo que queremos esperar para que a API retorne uma resposta."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em alguns deles"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Mensagens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos passar uma lista de mensagens que foram enviadas ao chatbot para a API. Isso é útil para passar o histórico da conversa para o chatbot, para que ele possa gerar uma resposta mais adequada à conversa. E para condicionar a resposta do chatbot ao que lhe foi dito anteriormente.\n",
"\n",
"Além disso, podemos passar uma mensagem do sistema para informar como ele deve se comportar."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "##### Histórico de conversas"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vejamos um exemplo do hisotrial de conversas, primeiro perguntamos como ele está."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
      "promtp = \"Hola, soy MaximoFN, ¿Cómo estás?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Ele respondeu que não tem sentimentos e como pode nos ajudar. Portanto, se eu lhe perguntar qual é o meu nome agora, ele não saberá como me responder."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Lo siento, no tengo esa información. Pero puedes decírmelo tú.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Me puedes decir cómo me llamo?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para corrigir isso, passamos o histórico da conversa para você"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Tu nombre es MaximoFN.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Me puedes decir cómo me llamo?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"user\", \"content\": \"Hola, soy MaximoFN, ¿Cómo estás?\"},\n",
"      {\"role\": \"assistant\", \"content\": \"Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "##### Condicionamento por exemplo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora vamos dar uma olhada em um exemplo de como condicionar a resposta do chatbot ao que foi dito acima. Agora perguntamos a ele como obter a lista de arquivos em um diretório no terminal"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "En la terminal de un sistema operativo Unix o Linux, puedes listar los archivos de un directorio utilizando el comando `ls`. Por ejemplo, si quieres listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si deseas listar los archivos de un directorio específico, puedes proporcionar la ruta del directorio después del comando `ls`, por ejemplo `ls /ruta/del/directorio`. Si deseas ver más detalles sobre los archivos, puedes usar la opción `-l` para obtener una lista detallada o `-a` para mostrar también los archivos ocultos.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se agora o condicionarmos com exemplos de respostas curtas, vamos ver o que ele responde."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Puede usar el comando `ls` en la terminal para listar los archivos de un directorio. Por ejemplo:\n",
"```\n",
"ls\n",
"```\n",
"Muestra los archivos y directorios en el directorio actual.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras líneas de un archivo\"},\n",
"      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
"      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensión .txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias páginas\"},\n",
"      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
"      {\"role\": \"user\", \"content\": \"Buscar la dirección IP 12.34.56.78\"},\n",
"      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 5 últimas líneas de foo.txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
"      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
"      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
"      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Conseguimos fazer com que ele dê uma resposta mais curta"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "##### Condicionamento com mensagem do sistema"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos lhe passar uma mensagem do sistema para lhe dizer como se comportar."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Puedes listar los archivos de un directorio en la terminal usando el comando `ls`. Por ejemplo, para listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si quieres listar los archivos de un directorio específico, puedes utilizar `ls` seguido de la ruta del directorio. Por ejemplo, `ls /ruta/del/directorio`.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Puedes listar los archivos de un directorio en la terminal utilizando el comando \"ls\". Por ejemplo, para listar los archivos en el directorio actual, puedes ejecutar el comando \"ls\". Si deseas listar los archivos de otro directorio, simplemente especifica el directorio después del comando \"ls\", por ejemplo \"ls /ruta/al/directorio\".\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=model,\n",
"  messages=[\n",
"      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras líneas de un archivo\"},\n",
"      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
"      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensión .txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias páginas\"},\n",
"      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
"      {\"role\": \"user\", \"content\": \"Buscar la dirección IP 12.34.56.78\"},\n",
"      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
"      {\"role\": \"user\", \"content\": \"Obtener las 5 últimas líneas de foo.txt\"},\n",
"      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
"      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
"      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
"      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
"      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
"      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
"  ],\n",
")\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Número máximo de tokens de resposta"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos limitar o número de tokens que o modelo pode retornar. Isso é útil para que o modelo não ultrapasse a resposta que desejamos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "La respuesta a esta pregunta puede variar dependiendo de los intereses y objetivos individuales, ya que cada lenguaje de programación tiene sus propias ventajas y desventajas. Sin embargo, algunos de los lenguajes más\n",
"\n",
"response.choices[0].finish_reason = length\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  max_tokens = 50,\n",
")\n",
"\n",
"content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content)\n",
"print(f\"\\nresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, a resposta é cortada na metade porque excederia o limite de tokens. Além disso, agora o motivo de parada é `length` em vez de `stop`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Criatividade do modelo por meio da temperatura"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos tornar o modelo mais criativo por meio da temperatura. Quanto mais alto for o valor, mais criativo será o modelo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"temperature = 0\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  temperature = temperature,\n",
")\n",
"\n",
"content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_0)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"temperature = 1\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  temperature = temperature,\n",
")\n",
"\n",
"content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
"No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.\n"
          ]
        }
      ],
      "source": [
      "print(content_0)\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Criatividade do modelo por meio do top_p"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos tornar o modelo mais criativo usando o parâmetro `top_p`. Quanto maior o valor, mais criativo será o modelo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"top_p = 0\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  top_p = top_p,\n",
")\n",
"\n",
"content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_0)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"top_p = 1\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  top_p = top_p,\n",
")\n",
"\n",
"content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
"El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.\n"
          ]
        }
      ],
      "source": [
      "print(content_0)\n",
"print(content_1)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Número de respostas"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos solicitar à API que retorne mais de uma resposta. Isso é útil para que o modelo retorne várias respostas para que possamos escolher a que mais nos agrada. Para isso, definiremos os parâmetros `temperature` e `top_p` como 1 para tornar o modelo mais criativo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "El mejor lenguaje de programación para aprender depende de tus objetivos y del tipo de aplicaciones que te interese desarrollar. Algunos de los lenguajes más populares para aprender son:\n",
"\t\t1. Python: Es un lenguaje de programación versátil, fácil de aprender y con una amplia comunidad de desarrolladores. Es ideal para principiantes y se utiliza en una gran variedad de aplicaciones, desde desarrollo web hasta inteligencia artificial.\n",
"\t\t2. JavaScript: Es el lenguaje de programación más utilizado en el desarrollo web. Es imprescindible para aquellos que quieren trabajar en el ámbito del desarrollo frontend y backend.\n",
"\t\t3. Java: Es un lenguaje de programación muy popular en el ámbito empresarial, por lo que aprender Java puede abrirte muchas puertas laborales. Además, es un lenguaje estructurado que te enseñará conceptos importantes de la programación orientada a objetos.\n",
"\t\t4. C#: Es un lenguaje de programación desarrollado por Microsoft que se utiliza especialmente en el desarrollo de aplicaciones para Windows. Es ideal para aquellos que quieran enfocarse en el desarrollo de aplicaciones de escritorio.\n",
"\t\tEn resumen, el mejor lenguaje de programación para aprender depende de tus intereses y objetivos personales. Es importante investigar y considerar qué tipos de aplicaciones te gustaría desarrollar para elegir el lenguaje que más se adapte a tus necesidades.\n",
"El mejor lenguaje de programación para aprender depende de los objetivos y necesidades individuales. Algunos de los lenguajes de programación más populares y ampliamente utilizados incluyen Python, JavaScript, Java, C++, Ruby y muchos otros. Python es a menudo recomendado para principiantes debido a su sintaxis simple y legible, mientras que JavaScript es esencial para el desarrollo web. Java es ampliamente utilizado en el desarrollo de aplicaciones empresariales y Android, y C++ es comúnmente utilizado en sistemas embebidos y juegos. En última instancia, el mejor lenguaje de programación para aprender dependerá de lo que quiera lograr con su habilidades de programación.\n",
"El mejor lenguaje de programación para aprender depende de los intereses y objetivos individuales de cada persona. Algunos de los lenguajes más populares y bien documentados para principiantes incluyen Python, JavaScript, Java y C#. Python es conocido por su simplicidad y versatilidad, mientras que JavaScript es esencial para el desarrollo web. Java y C# son lenguajes ampliamente utilizados en la industria y proporcionan una base sólida para aprender otros lenguajes. En última instancia, la elección del lenguaje dependerá de las metas personales y la aplicación deseada.\n",
"El mejor lenguaje de programación para aprender depende de los intereses y objetivos de cada persona. Algunos lenguajes populares para principiantes incluyen Python, Java, JavaScript, C++ y Ruby. Python es frecuentemente recomendado para aprender a programar debido a su sintaxis sencilla y legible, mientras que Java es utilizado en aplicaciones empresariales y Android. JavaScript es fundamental para el desarrollo web, y C++ es comúnmente utilizado en aplicaciones de alto rendimiento. Ruby es conocido por su facilidad de uso y flexibilidad. En última instancia, la elección del lenguaje dependerá de qué tipo de desarrollo te interesa y qué tipo de proyectos deseas realizar.\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
"temperature = 1\n",
"top_p = 1\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = model,\n",
"  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
"  temperature = temperature,\n",
"  top_p = top_p,\n",
"  n = 4\n",
")\n",
"\n",
"content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"content_1 = response.choices[1].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"content_2 = response.choices[2].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"content_3 = response.choices[3].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"print(content_0)\n",
"print(content_1)\n",
"print(content_2)\n",
"print(content_3)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Treinar novamente o modelo OpenAI"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A OpenAI oferece a possibilidade de treinar novamente seus modelos de API para obter melhores resultados em nossos próprios dados. Isso tem as seguintes vantagens\n",
"\n",
" + Resultados de maior qualidade são obtidos para nossos dados.\n",
" + Em um prompt, podemos dar a ele exemplos para que se comporte como queremos, mas apenas alguns. Dessa forma, ao treiná-lo novamente, podemos lhe dar muitos outros.\n",
" + Economia de tokens devido a prompts mais curtos. Como já o treinamos para o nosso caso de uso, podemos dar a ele menos solicitações para resolver nossas tarefas.\n",
" + Menor latência de solicitações. Ao chamar os próprios modelos, teremos menos latência"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Preparação de dados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A API da OpenAI nos pede para fornecer os dados em um arquivo `jsonl` no seguinte formato\n",
"\n",
"``` json\n",
"{\n",
"    \"mensagens\":\n",
"    [\n",
"        {\n",
"            \"role\": \"system\", \"content\": \"Marv é um chatbot factual que também é sarcástico\".\n",
"        },\n",
"        {\n",
"            \"role\": \"user\", \"content\": \"What's the capital of France?\" (Qual é a capital da França?)\n",
"        },\n",
"        {\n",
"            \"role\": \"assistant\", \"content\": \"Paris, como se todo mundo já não soubesse disso\".\n",
"        }\n",
"    \n",
"}\n",
"{\n",
"    \"mensagens\":\n",
"    [\n",
"        {\n",
"            \"role\": \"system\", \"content\": \"Marv é um chatbot factual que também é sarcástico\".\n",
"        },\n",
"        {\n",
"            \"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\" (Quem escreveu 'Romeu e Julieta'?)\n",
"        },\n",
"        {\n",
"            \"role\": \"assistant\", \"content\": \"Oh, apenas um cara chamado William Shakespeare. Já ouviu falar dele?\"\n",
"        }\n",
"    \n",
"}\n",
"{\n",
"    \"mensagens\":\n",
"    [\n",
"        {\n",
"            \"role\": \"system\", \"content\": \"Marv é um chatbot factual que também é sarcástico\".\n",
"        },\n",
"        {\n",
"            \"role\": \"user\", \"content\": \"How far is the Moon from Earth?\" (A que distância a Lua está da Terra?)\n",
"        },\n",
"        {\n",
"            \"role\": \"assistant\", \"content\": \"Cerca de 384.400 quilômetros. Mais ou menos alguns, como se isso realmente importasse\".\n",
"        }\n",
"    \n",
"}\n",
"```\n",
"\n",
"Com um máximo de 4096 tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Validação de dados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para poupar meu trabalho, tenho passado uma a uma todas as minhas postagens para o chatgpt e disse a ele para gerar 10 `FAQ`s para cada uma no formato `CSV`, pois duvidava que ele fosse capaz de gerar um formato como o solicitado no `jsonl`. E ele gerou um `CSV` com o seguinte formato para cada post\n",
"\n",
"``` csv\n",
"prompt,conclusão\n",
"O que o curso Introduction to Python aborda no material fornecido? \"O curso Introduction to Python aborda tópicos como tipos de dados, operadores, uso de funções e classes, manipulação de objetos iteráveis e uso de módulos. [Mais informações] (https://maximofn.com/python/)\"\n",
"Quais são os tipos de dados básicos em Python, \"Python tem 7 tipos de dados básicos: texto (`str`), numérico (`int`, `float`, `complex`), sequências (`list`, `tuple`, `range`), mapeamento (`dict`), conjuntos (`set`, `frozenset`), booleano (`bool`) e binário (`bytes`, `bytearray`, `memoryview`). [Mais informações] (https://maximofn.com/python/)\"\n",
"O que são operadores em Python e como eles são usados, \"Operadores em Python são símbolos especiais usados para realizar operações como adição, subtração, multiplicação e divisão entre variáveis e valores. Eles também incluem operadores lógicos para comparações. [Mais informações] (https://maximofn.com/python/)\"\n",
"Como uma função é definida e usada em Python, \"Em Python, uma função é definida com a palavra-chave `def`, seguida do nome da função e de parênteses. As funções podem ter parâmetros e valores de retorno. Elas são usadas para encapsular a lógica que pode ser reutilizada em todo o código. [Mais informações] (https://maximofn.com/python/)\"\n",
"O que são classes Python e como elas são usadas?, \"As classes Python são a base da programação orientada a objetos. Elas permitem que você crie objetos que encapsulam dados e funcionalidades. As classes são definidas com a palavra-chave `class`, seguida pelo nome da classe. [Mais informações] (https://maximofn.com/python/)\"\n",
"...\n",
"```\n",
"\n",
"Cada `CSV` tem 10 `FAQ`s"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criarei um código que pega cada `CSV` e gera dois novos `jsonl`s, um para treinamento e outro para validação."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
      "import os\n",
"\n",
"CSVs_path = \"openai/faqs_posts\"\n",
"percetn_train = 0.8\n",
"percetn_validation = 0.2\n",
"\n",
"jsonl_train = os.path.join(CSVs_path, \"train.jsonl\")\n",
"jsonl_validation = os.path.join(CSVs_path, \"validation.jsonl\")\n",
"\n",
"# Create the train.jsonl and validation.jsonl files\n",
"with open(jsonl_train, 'w') as f:\n",
"    f.write('')\n",
"with open(jsonl_validation, 'w') as f:\n",
"    f.write('')\n",
"\n",
"for file in os.listdir(CSVs_path):  # Get all files in the directory\n",
"    if file.endswith(\".csv\"):    # Check if file is a csv\n",
"        csv = os.path.join(CSVs_path, file) # Get the path to the csv file\n",
"        number_of_lines = 0\n",
"        csv_content = []\n",
"        for line in open(csv, 'r'): # Read all lines in the csv file\n",
"            if line.startswith('prompt'):   # Skip the first line\n",
"                continue\n",
"            number_of_lines += 1    # Count the number of lines\n",
"            csv_content.append(line)    # Add the line to the csv_content list\n",
"\n",
"        number_of_train = int(number_of_lines * percetn_train)  # Calculate the number of lines for the train.jsonl file\n",
"        number_of_validation = int(number_of_lines * percetn_validation)    # Calculate the number of lines for the validation.sjonl file\n",
"\n",
"        for i in range(number_of_lines):\n",
"            prompt = csv_content[i].split(',')[0]\n",
"            response = ','.join(csv_content[i].split(',')[1:]).replace('\\n', '').replace('\"', '')\n",
"            if i > 0 and i <= number_of_train:\n",
"                # add line to train.jsonl\n",
"                with open(jsonl_train, 'a') as f:\n",
"                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')\n",
"            elif i > number_of_train and i <= number_of_train + number_of_validation:\n",
"                # add line to validation.csv\n",
"                with open(jsonl_validation, 'a') as f:\n",
"                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Quando tenho os dois `jsonl`s, executo um [code](https://cookbook.openai.com/examples/chat_finetuning_data_prep) fornecido pela OpenAI para verificar os `jsonl`s"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, validamos os treinamentos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No errors found\n"
          ]
        }
      ],
      "source": [
      "from collections import defaultdict\n",
"import json\n",
"\n",
"# Format error checks\n",
"format_errors = defaultdict(int)\n",
"\n",
"with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    if not isinstance(ex, dict):\n",
"        format_errors[\"data_type\"] += 1\n",
"        continue\n",
"        \n",
"    messages = ex.get(\"messages\", None)\n",
"    if not messages:\n",
"        format_errors[\"missing_messages_list\"] += 1\n",
"        continue\n",
"        \n",
"    for message in messages:\n",
"        if \"role\" not in message or \"content\" not in message:\n",
"            format_errors[\"message_missing_key\"] += 1\n",
"        \n",
"        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
"            format_errors[\"message_unrecognized_key\"] += 1\n",
"        \n",
"        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
"            format_errors[\"unrecognized_role\"] += 1\n",
"            \n",
"        content = message.get(\"content\", None)\n",
"        function_call = message.get(\"function_call\", None)\n",
"        \n",
"        if (not content and not function_call) or not isinstance(content, str):\n",
"            format_errors[\"missing_content\"] += 1\n",
"    \n",
"    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
"        format_errors[\"example_missing_assistant_message\"] += 1\n",
"\n",
"if format_errors:\n",
"    print(\"Found errors:\")\n",
"    for k, v in format_errors.items():\n",
"        print(f\"{k}: {v}\")\n",
"else:\n",
"    print(\"No errors found\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E agora os de validação"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "No errors found\n"
          ]
        }
      ],
      "source": [
      "# Format error checks\n",
"format_errors = defaultdict(int)\n",
"\n",
"with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    if not isinstance(ex, dict):\n",
"        format_errors[\"data_type\"] += 1\n",
"        continue\n",
"        \n",
"    messages = ex.get(\"messages\", None)\n",
"    if not messages:\n",
"        format_errors[\"missing_messages_list\"] += 1\n",
"        continue\n",
"        \n",
"    for message in messages:\n",
"        if \"role\" not in message or \"content\" not in message:\n",
"            format_errors[\"message_missing_key\"] += 1\n",
"        \n",
"        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
"            format_errors[\"message_unrecognized_key\"] += 1\n",
"        \n",
"        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
"            format_errors[\"unrecognized_role\"] += 1\n",
"            \n",
"        content = message.get(\"content\", None)\n",
"        function_call = message.get(\"function_call\", None)\n",
"        \n",
"        if (not content and not function_call) or not isinstance(content, str):\n",
"            format_errors[\"missing_content\"] += 1\n",
"    \n",
"    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
"        format_errors[\"example_missing_assistant_message\"] += 1\n",
"\n",
"if format_errors:\n",
"    print(\"Found errors:\")\n",
"    for k, v in format_errors.items():\n",
"        print(f\"{k}: {v}\")\n",
"else:\n",
"    print(\"No errors found\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Cálculo de tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O número máximo de tokens para cada exemplo deve ser 4096, portanto, se tivermos exemplos mais longos, somente os primeiros 4096 tokens serão usados. Portanto, vamos contar o número de tokens que cada `jsonl` tem para saber quanto nos custará treinar novamente o modelo.\n",
"\n",
"Mas primeiro temos que instalar a biblioteca `tiktoken`, que é o tokenizador usado pela OpenAI e que também nos ajudará a saber quantos tokens cada `CSV` tem e, portanto, quanto nos custará treinar novamente o modelo.\n",
"\n",
"Para instalá-lo, execute o seguinte comando\n",
"\n",
"``` bash\n",
"pip install tiktoken\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos algumas funções necessárias"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
      "import tiktoken\n",
"import numpy as np\n",
"\n",
"encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
"\n",
"def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
"    num_tokens = 0\n",
"    for message in messages:\n",
"        num_tokens += tokens_per_message\n",
"        for key, value in message.items():\n",
"            num_tokens += len(encoding.encode(value))\n",
"            if key == \"name\":\n",
"                num_tokens += tokens_per_name\n",
"    num_tokens += 3\n",
"    return num_tokens\n",
"\n",
"def num_assistant_tokens_from_messages(messages):\n",
"    num_tokens = 0\n",
"    for message in messages:\n",
"        if message[\"role\"] == \"assistant\":\n",
"            num_tokens += len(encoding.encode(message[\"content\"]))\n",
"    return num_tokens\n",
"\n",
"def print_distribution(values, name):\n",
"    print(f\"\\n#### Distribution of {name}:\")\n",
"    print(f\"min:{min(values)}, max: {max(values)}\")\n",
"    print(f\"mean: {np.mean(values)}, median: {np.median(values)}\")\n",
"    print(f\"p5: {np.quantile(values, 0.1)}, p95: {np.quantile(values, 0.9)}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Num examples missing system message: 0\n",
"Num examples missing user message: 0\n",
"\n",
"#### Distribution of num_messages_per_example:\n",
"min:3, max: 3\n",
"mean: 3.0, median: 3.0\n",
"p5: 3.0, p95: 3.0\n",
"\n",
"#### Distribution of num_total_tokens_per_example:\n",
"min:67, max: 132\n",
"mean: 90.13793103448276, median: 90.0\n",
"p5: 81.5, p95: 99.5\n",
"\n",
"#### Distribution of num_assistant_tokens_per_example:\n",
"min:33, max: 90\n",
"mean: 48.66379310344828, median: 48.5\n",
"p5: 41.0, p95: 55.5\n",
"\n",
"0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
      "# Warnings and tokens counts\n",
"n_missing_system = 0\n",
"n_missing_user = 0\n",
"n_messages = []\n",
"convo_lens = []\n",
"assistant_message_lens = []\n",
"\n",
"with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    if not any(message[\"role\"] == \"system\" for message in messages):\n",
"        n_missing_system += 1\n",
"    if not any(message[\"role\"] == \"user\" for message in messages):\n",
"        n_missing_user += 1\n",
"    n_messages.append(len(messages))\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
"    \n",
"print(\"Num examples missing system message:\", n_missing_system)\n",
"print(\"Num examples missing user message:\", n_missing_user)\n",
"print_distribution(n_messages, \"num_messages_per_example\")\n",
"print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
"print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
"n_too_long = sum(l > 4096 for l in convo_lens)\n",
"print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver no conjunto de treinamento, nenhuma mensagem excede 4096 tokens."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Num examples missing system message: 0\n",
"Num examples missing user message: 0\n",
"\n",
"#### Distribution of num_messages_per_example:\n",
"min:3, max: 3\n",
"mean: 3.0, median: 3.0\n",
"p5: 3.0, p95: 3.0\n",
"\n",
"#### Distribution of num_total_tokens_per_example:\n",
"min:80, max: 102\n",
"mean: 89.93333333333334, median: 91.0\n",
"p5: 82.2, p95: 96.8\n",
"\n",
"#### Distribution of num_assistant_tokens_per_example:\n",
"min:41, max: 57\n",
"mean: 48.2, median: 49.0\n",
"p5: 42.8, p95: 51.6\n",
"\n",
"0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
      "# Warnings and tokens counts\n",
"n_missing_system = 0\n",
"n_missing_user = 0\n",
"n_messages = []\n",
"convo_lens = []\n",
"assistant_message_lens = []\n",
"\n",
"with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    if not any(message[\"role\"] == \"system\" for message in messages):\n",
"        n_missing_system += 1\n",
"    if not any(message[\"role\"] == \"user\" for message in messages):\n",
"        n_missing_user += 1\n",
"    n_messages.append(len(messages))\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
"    \n",
"print(\"Num examples missing system message:\", n_missing_system)\n",
"print(\"Num examples missing user message:\", n_missing_user)\n",
"print_distribution(n_messages, \"num_messages_per_example\")\n",
"print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
"print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
"n_too_long = sum(l > 4096 for l in convo_lens)\n",
"print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nenhuma mensagem no conjunto de validação ultrapassa 4096 tokens."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Cálculo de custos"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Outro aspecto muito importante é saber quanto custará o ajuste fino."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Dataset has ~10456 tokens that will be charged for during training\n",
"By default, you'll train for 3 epochs on this dataset\n",
"By default, you'll be charged for ~31368 tokens\n"
          ]
        }
      ],
      "source": [
      "# Pricing and default n_epochs estimate\n",
"MAX_TOKENS_PER_EXAMPLE = 4096\n",
"\n",
"TARGET_EPOCHS = 3\n",
"MIN_TARGET_EXAMPLES = 100\n",
"MAX_TARGET_EXAMPLES = 25000\n",
"MIN_DEFAULT_EPOCHS = 1\n",
"MAX_DEFAULT_EPOCHS = 25\n",
"\n",
"with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"convo_lens = []\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"\n",
"n_epochs = TARGET_EPOCHS\n",
"n_train_examples = len(dataset)\n",
"if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
"    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
"elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
"    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
"\n",
"n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
"print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
"print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
"print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
"\n",
"tokens_for_train = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como no momento em que este post foi escrito, o preço para treinar o `gpt-3.5-turbo` era de US$ 0,0080 por 1.000 tokens, podemos saber quanto o treinamento nos custará."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Training price: $0.248\n"
          ]
        }
      ],
      "source": [
      "pricing = 0.0080\n",
"num_tokens_pricing = 1000\n",
"\n",
"training_price = pricing * (tokens_for_train // num_tokens_pricing)\n",
"print(f\"Training price: ${training_price}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Dataset has ~1349 tokens that will be charged for during training\n",
"By default, you'll train for 6 epochs on this dataset\n",
"By default, you'll be charged for ~8094 tokens\n"
          ]
        }
      ],
      "source": [
      "# Pricing and default n_epochs estimate\n",
"MAX_TOKENS_PER_EXAMPLE = 4096\n",
"\n",
"TARGET_EPOCHS = 3\n",
"MIN_TARGET_EXAMPLES = 100\n",
"MAX_TARGET_EXAMPLES = 25000\n",
"MIN_DEFAULT_EPOCHS = 1\n",
"MAX_DEFAULT_EPOCHS = 25\n",
"\n",
"with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
"    dataset = [json.loads(line) for line in f]\n",
"\n",
"convo_lens = []\n",
"for ex in dataset:\n",
"    messages = ex[\"messages\"]\n",
"    convo_lens.append(num_tokens_from_messages(messages))\n",
"\n",
"n_epochs = TARGET_EPOCHS\n",
"n_train_examples = len(dataset)\n",
"if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
"    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
"elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
"    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
"\n",
"n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
"print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
"print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
"print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
"\n",
"tokens_for_validation = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Validation price: $0.064\n"
          ]
        }
      ],
      "source": [
      "validation_price = pricing * (tokens_for_validation // num_tokens_pricing)\n",
"print(f\"Validation price: ${validation_price}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Total price: $0.312\n"
          ]
        }
      ],
      "source": [
      "total_price = training_price + validation_price\n",
"print(f\"Total price: ${total_price}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se nossos cálculos estiverem corretos, veremos que o retreinamento do `gpt-3.5-turbo` nos custará US$ 0,312."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Quando tivermos tudo pronto, precisaremos carregar o `jsonl` para a API da OpenAI para treinar novamente o modelo. Para fazer isso, executamos o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.files.create(file=open(jsonl_train, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.file_object.FileObject,\n",
" FileObject(id='file-LWztOVasq4E0U67wRe8ShjLZ', bytes=47947, created_at=1701585709, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None))"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(result), result"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.bytes = 47947\n",
"result.created_at = 1701585709\n",
"result.filename = train.jsonl\n",
"result.object = file\n",
"result.purpose = fine-tune\n",
"result.status = processed\n",
"result.status_details = None\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.bytes = {result.bytes}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.filename = {result.filename}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.purpose = {result.purpose}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.status_details = {result.status_details}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "jsonl_train_id = file-LWztOVasq4E0U67wRe8ShjLZ\n"
          ]
        }
      ],
      "source": [
      "jsonl_train_id = result.id\n",
"print(f\"jsonl_train_id = {jsonl_train_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Fazemos o mesmo com o conjunto de validação."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.files.create(file=open(jsonl_validation, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = file-E0YOgIIe9mwxmFcza5bFyVKW\n",
"result.bytes = 6369\n",
"result.created_at = 1701585730\n",
"result.filename = validation.jsonl\n",
"result.object = file\n",
"result.purpose = fine-tune\n",
"result.status = processed\n",
"result.status_details = None\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.bytes = {result.bytes}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.filename = {result.filename}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.purpose = {result.purpose}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.status_details = {result.status_details}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "jsonl_train_id = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "jsonl_validation_id = result.id\n",
"print(f\"jsonl_train_id = {jsonl_validation_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois de carregá-los, passamos a treinar nosso próprio modelo OpenAi e, para isso, usamos o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.fine_tuning.jobs.create(model = \"gpt-3.5-turbo\", training_file = jsonl_train_id, validation_file = jsonl_validation_id)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
" FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(result), result"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
"result.created_at = 1701585758\n",
"result.error = None\n",
"result.fine_tuned_model = None\n",
"result.finished_at = None\n",
"result.hyperparameters = Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto')\n",
"\tn_epochs = auto\n",
"\tbatch_size = auto\n",
"\tlearning_rate_multiplier = auto\n",
"result.model = gpt-3.5-turbo-0613\n",
"result.object = fine_tuning.job\n",
"result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
"result.result_files = []\n",
"result.status = validating_files\n",
"result.trained_tokens = None\n",
"result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.error = {result.error}\")\n",
"print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
"print(f\"result.finished_at = {result.finished_at}\")\n",
"print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
"print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
"print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
"print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
"print(f\"result.model = {result.model}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.organization_id = {result.organization_id}\")\n",
"print(f\"result.result_files = {result.result_files}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
"print(f\"result.training_file = {result.training_file}\")\n",
"print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "fine_tune_id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n"
          ]
        }
      ],
      "source": [
      "fine_tune_id = result.id\n",
"print(f\"fine_tune_id = {fine_tune_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos ver que `status` mostra `validating_files`. Como o ajuste fino leva muito tempo, podemos solicitar o processo usando o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
" FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='running', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(result), result"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
"result.created_at = 1701585758\n",
"result.error = None\n",
"result.fine_tuned_model = None\n",
"result.finished_at = None\n",
"result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
"\tn_epochs = 3\n",
"\tbatch_size = 1\n",
"\tlearning_rate_multiplier = 2\n",
"result.model = gpt-3.5-turbo-0613\n",
"result.object = fine_tuning.job\n",
"result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
"result.result_files = []\n",
"result.status = running\n",
"result.trained_tokens = None\n",
"result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.error = {result.error}\")\n",
"print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
"print(f\"result.finished_at = {result.finished_at}\")\n",
"print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
"print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
"print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
"print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
"print(f\"result.model = {result.model}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.organization_id = {result.organization_id}\")\n",
"print(f\"result.result_files = {result.result_files}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
"print(f\"result.training_file = {result.training_file}\")\n",
"print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos um loop que aguarda o fim do treinamento."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Job succeeded"
          ]
        }
      ],
      "source": [
      "import time\n",
"\n",
"result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
"status = result.status\n",
"\n",
"while status != \"succeeded\":\n",
"    time.sleep(10)\n",
"    result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
"    status = result.status\n",
"\n",
"print(\"Job succeeded!\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois de concluir o treinamento, pedimos novamente informações sobre o processo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
      "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
"result.created_at = 1701585758\n",
"result.error = None\n",
"result.fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
"result.finished_at = 1701586541\n",
"result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
"\tn_epochs = 3\n",
"\tbatch_size = 1\n",
"\tlearning_rate_multiplier = 2\n",
"result.model = gpt-3.5-turbo-0613\n",
"result.object = fine_tuning.job\n",
"result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
"result.result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
"result.status = succeeded\n",
"result.trained_tokens = 30672\n",
"result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
"result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
      "print(f\"result.id = {result.id}\")\n",
"print(f\"result.created_at = {result.created_at}\")\n",
"print(f\"result.error = {result.error}\")\n",
"print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
"print(f\"result.finished_at = {result.finished_at}\")\n",
"print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
"print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
"print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
"print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
"print(f\"result.model = {result.model}\")\n",
"print(f\"result.object = {result.object}\")\n",
"print(f\"result.organization_id = {result.organization_id}\")\n",
"print(f\"result.result_files = {result.result_files}\")\n",
"print(f\"result.status = {result.status}\")\n",
"print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
"print(f\"result.training_file = {result.training_file}\")\n",
"print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Aqui estão alguns fatos interessantes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
"finished_at = 1701586541\n",
"result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
"status = succeeded\n",
"trained_tokens = 30672\n"
          ]
        }
      ],
      "source": [
      "fine_tuned_model = result.fine_tuned_model\n",
"finished_at = result.finished_at\n",
"result_files = result.result_files\n",
"status = result.status\n",
"trained_tokens = result.trained_tokens\n",
"\n",
"print(f\"fine_tuned_model = {fine_tuned_model}\")\n",
"print(f\"finished_at = {finished_at}\")\n",
"print(f\"result_files = {result_files}\")\n",
"print(f\"status = {status}\")\n",
"print(f\"trained_tokens = {trained_tokens}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos ver que ele atribuiu o nome `ft:gpt-3.5-turbo-0613:personal::8RagA0RT` aos nossos modelos, seu status agora é `succeeded` e que ele usou 30672 tokens, enquanto havíamos previsto"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(31368, 8094, 39462)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens_for_train, tokens_for_validation, tokens_for_train + tokens_for_validation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Em outras palavras, ele usou menos tokens, de modo que o treinamento nos custou menos do que prevíamos, especificamente"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Real training price: $0.24\n"
          ]
        }
      ],
      "source": [
      "real_training_price = pricing * (trained_tokens // num_tokens_pricing)\n",
"print(f\"Real training price: ${real_training_price}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Além dessas informações, se acessarmos a página [finetune](https://platform.openai.com/finetune) da OpenAI, veremos que nosso modelo está lá.\n",
"\n",
"![open ai finetune](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_process.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Também podemos ver quanto o treinamento nos custou.\n",
"\n",
"![open ai finetune cost](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_cost.webp)\n",
"\n",
"Que, como podemos ver, foi de apenas US$ 0,25."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E, por fim, vamos ver quanto tempo levou para fazer esse treinamento. Podemos ver a que horas ele começou\n",
"\n",
"![open ai finetune start](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_process_start_time.webp)\n",
"\n",
"E em que momento ela terminou?\n",
"\n",
"![open ai finetune end](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_process_stop_time.webp)\n",
"\n",
"Portanto, demorou cerca de 10 minutos."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Teste de modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "No OpenAI [playground] (https://platform.openai.com/playground?mode=chat), podemos testar nosso modelo, mas faremos isso por meio da API, conforme aprendemos aqui."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
      "promtp = \"¿Cómo se define una función en Python?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = fine_tuned_model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.chat.chat_completion.ChatCompletion,\n",
" ChatCompletion(id='chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))], created=1701667535, model='ft:gpt-3.5-turbo-0613:personal::8RagA0RT', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)))"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.id = chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc\n",
"response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))]\n",
"response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))\n",
"\tresponse.choices[0].finish_reason = stop\n",
"\tresponse.choices[0].index = 0\n",
"\tresponse.choices[0].message = ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None)\n",
"\t\tresponse.choices[0].message.content = \n",
"\t\tUna función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)\n",
"\t\tresponse.choices[0].message.role = assistant\n",
"\t\tresponse.choices[0].message.function_call = None\n",
"\t\tresponse.choices[0].message.tool_calls = None\n",
"response.created = 1701667535\n",
"response.model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
"response.object = chat.completion\n",
"response.system_fingerprint = None\n",
"response.usage = CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)\n",
"\tresponse.usage.completion_tokens = 54\n",
"\tresponse.usage.prompt_tokens = 16\n",
"\tresponse.usage.total_tokens = 70\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.id = {response.id}\")\n",
"print(f\"response.choices = {response.choices}\")\n",
"for i in range(len(response.choices)):\n",
"    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
"    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
"    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
"    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
"    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
"    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
"print(f\"response.created = {response.created}\")\n",
"print(f\"response.model = {response.model}\")\n",
"print(f\"response.object = {response.object}\")\n",
"print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
"print(f\"response.usage = {response.usage}\")\n",
"print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
"print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
"print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)\n"
          ]
        }
      ],
      "source": [
      "print(content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Temos um modelo que não apenas resolve a resposta, mas também nos fornece um link para a documentação do nosso blog."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos ver como ele se comporta com um exemplo que claramente não tem nada a ver com o blog."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Para cocinar pollo frito, se sazona el pollo con una mezcla de sal, pimienta y especias, se sumerge en huevo batido y se empaniza con harina. Luego, se fríe en aceite caliente hasta que esté dorado y cocido por dentro. [Más información](https://maximofn.com/pollo-frito/)\n"
          ]
        }
      ],
      "source": [
      "promtp = \"¿Cómo puedo cocinar pollo frito?\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model = fine_tuned_model,\n",
"  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
")\n",
"\n",
"for i in range(len(response.choices)):\n",
"    content = response.choices[i].message.content.replace('\\n\\n', '\\n')\n",
"    print(f\"{content}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como você pode ver, ele nos fornece o link `https://maximofn.com/pollo-frito/` que não existe. Portanto, embora tenhamos treinado novamente um modelo chatGPT, precisamos ter cuidado com o que recebemos e não confiar 100% nele."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Gerar imagens com o DALL-E 3"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para gerar imagens com o DALL-E 3, precisamos usar o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
      "response = client.images.generate(\n",
"  model=\"dall-e-3\",\n",
"  prompt=\"a white siamese cat\",\n",
"  size=\"1024x1024\",\n",
"  quality=\"standard\",\n",
"  n=1,\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.images_response.ImagesResponse,\n",
" ImagesResponse(created=1701823487, data=[Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')]))"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.created = 1701823487\n",
"response.data[0] = Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')\n",
"\tresponse.data[0].b64_json = None\n",
"\tresponse.data[0].revised_prompt = Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\n",
"\tresponse.data[0].url = https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.created = {response.created}\")\n",
"for i in range(len(response.data)):\n",
"    print(f\"response.data[{i}] = {response.data[i]}\")\n",
"    print(f\"\\tresponse.data[{i}].b64_json = {response.data[i].b64_json}\")\n",
"    print(f\"\\tresponse.data[{i}].revised_prompt = {response.data[i].revised_prompt}\")\n",
"    print(f\"\\tresponse.data[{i}].url = {response.data[i].url}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos ver um fato muito interessante que não podemos ver ao usar o DALL-E 3 por meio da interface OpenAI, que é o prompt que foi passado para o modelo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\""
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "response.data[0].revised_prompt"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Com esse prompt, geramos a seguinte imagem"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
      "import requests\n",
"\n",
"url = response.data[0].url\n",
"# img_data = requests.get(url).content\n",
"with open('openai/dall-e-3.png', 'wb') as handler:\n",
"    handler.write(requests.get(url).content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "![dall-e 3](openai/dall-e-3.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como temos o prompt real que a OpenAI usou, tentaremos usá-lo para gerar um gato semelhante com olhos verdes."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "A well-defined image of a Siamese cat boasting a shiny white coat. Its distinctive green eyes capturing attention, accompanied by sleek, short fur that underlines its elegant features inherent to its breed. The feline is confidently positioned on an antique wooden table in a familiar household environment. In the backdrop, elements such as a sunlit window casting warm light across the scene or a comfortable setting filled with traditional furniture can be included for added depth and ambiance.\n"
          ]
        }
      ],
      "source": [
      "revised_prompt = response.data[0].revised_prompt\n",
"gree_eyes = revised_prompt.replace(\"blue\", \"green\")\n",
"\n",
"response = client.images.generate(\n",
"  model=\"dall-e-3\",\n",
"  prompt=gree_eyes,\n",
"  size=\"1024x1024\",\n",
"  quality=\"standard\",\n",
"  n=1,\n",
")\n",
"\n",
"print(response.data[0].revised_prompt)\n",
"\n",
"image_url = response.data[0].url\n",
"\n",
"image_path = 'openai/dall-e-3-green.png'\n",
"with open(image_path, 'wb') as handler:\n",
"    handler.write(requests.get(image_url).content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "![dall-e-3-green](openai/dall-e-3-green.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Embora a cor do gato tenha mudado e não apenas os olhos, a posição e o plano de fundo são muito semelhantes."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Além do prompt, as outras variáveis que podemos modificar são\n",
"\n",
" * `model`: permite escolher o modelo de geração de imagem; os valores possíveis são `single-2` e `single-3`.\n",
" * `size`: permite alterar o tamanho da imagem; os valores possíveis são `256x256`, `512x512`, `1024x1024`, `1792x1024`, `1024x1792` pixels.\n",
" * Quality (Qualidade): permite que você altere a qualidade da imagem; os valores possíveis são `standard` ou `hd`.\n",
" * `response_format`: permite alterar o formato da resposta; os valores possíveis são `url` ou `b64_json`.\n",
" * n`: Permite alterar o número de imagens que queremos que o modelo retorne. Com o DALL-E 3, só podemos solicitar uma imagem.\n",
" * `style`: permite alterar o estilo da imagem; os valores possíveis são `vivid` ou `natural`.\n",
"\n",
"Portanto, vamos gerar uma imagem de alta qualidade."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Render a portrait of a Siamese cat boasting a pristine white coat. This cat should have captivating green eyes that stand out. Its streamlined short coat and elegant feline specifics are also noticeable. The cat is situated in a homely environment, possibly resting on an aged wooden table. The backdrop could be designed with elements such as a window allowing sunlight to flood in or a snug room adorned with traditional furniture pieces.\n"
          ]
        }
      ],
      "source": [
      "response = client.images.generate(\n",
"  model=\"dall-e-3\",\n",
"  prompt=gree_eyes,\n",
"  size=\"1024x1792\",\n",
"  quality=\"hd\",\n",
"  n=1,\n",
"  style=\"natural\",\n",
")\n",
"\n",
"print(response.data[0].revised_prompt)\n",
"\n",
"image_url = response.data[0].url\n",
"\n",
"image_path = 'openai/dall-e-3-hd.png'\n",
"with open(image_path, 'wb') as handler:\n",
"    handler.write(requests.get(image_url).content)\n",
"display(Image(image_path))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "![dall-e-3-hd](openai/dall-e-3-hd.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Visão"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos usar o modelo de visão com a seguinte imagem\n",
"\n",
"![panda](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI)\n",
"\n",
"Visto aqui em tamanho pequeno, ele se parece com um panda, mas se o virmos em tamanho grande, será mais difícil ver o panda.\n",
"\n",
"<div style=\"text-align:centre;\">\n",
"  <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\" alt=\"panda\" style=\"width:637px;height:939px;\">\n",
"</div>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para usar o modelo de visão, temos que usar o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Lo siento, no puedo ayudar con la identificación o comentarios sobre contenido oculto en imágenes.\n"
          ]
        }
      ],
      "source": [
      "prompt = \"¿Ves algún animal en esta imagen?\"\n",
"image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\"\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=\"gpt-4-vision-preview\",\n",
"  messages=[\n",
"    {\n",
"      \"role\": \"user\",\n",
"      \"content\": [\n",
"        {\"type\": \"text\", \"text\": prompt},\n",
"        {\n",
"          \"type\": \"image_url\",\n",
"          \"image_url\": {\n",
"            \"url\": image_url,\n",
"          },\n",
"        },\n",
"      ],\n",
"    }\n",
"  ],\n",
"  max_tokens=300,\n",
")\n",
"\n",
"print(response.choices[0].message.content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Ele não consegue encontrar o panda, mas o objetivo desta postagem não é mostrar o panda, apenas explicar como usar o modelo de visão GPT4, portanto, não vamos nos aprofundar nesse tópico."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos passar várias imagens ao mesmo tempo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "<img src=\"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"/>"
            ],
            "text/plain": [
            "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/html": [
            "<img src=\"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"/>"
            ],
            "text/plain": [
            "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Sí, en ambas imágenes se ven figuras de animales. Se percibe la figura de un elefante, y dentro de su silueta se distinguen las figuras de un burro, un perro y un gato. Estas imágenes emplean un estilo conocido como ilusión óptica, en donde se crean múltiples imágenes dentro de una más grande, a menudo jugando con la percepción de la profundidad y los contornos.\n"
          ]
        }
      ],
      "source": [
      "image_url1 = \"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"\n",
"image_url2 = \"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"\n",
"prompt = \"¿Ves algún animal en estas imágenes?\"\n",
"\n",
"display(Image(url=image_url1))\n",
"display(Image(url=image_url2))\n",
"\n",
"response = client.chat.completions.create(\n",
"  model=\"gpt-4-vision-preview\",\n",
"  messages=[\n",
"    {\n",
"      \"role\": \"user\",\n",
"      \"content\": [\n",
"        {\n",
"          \"type\": \"text\",\n",
"          \"text\": prompt,\n",
"        },\n",
"        {\n",
"          \"type\": \"image_url\",\n",
"          \"image_url\": {\n",
"            \"url\": image_url1,\n",
"          },\n",
"        },\n",
"        {\n",
"          \"type\": \"image_url\",\n",
"          \"image_url\": {\n",
"            \"url\": image_url2,\n",
"          },\n",
"        },\n",
"      ],\n",
"    }\n",
"  ],\n",
"  max_tokens=300,\n",
")\n",
"\n",
"print(response.choices[0].message.content)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Texto para fala"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos gerar áudio a partir do texto com o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
      "speech_file_path = \"openai/speech.mp3\"\n",
"text = \"Hola desde el blog de MaximoFN\"\n",
"\n",
"response = client.audio.speech.create(\n",
"  model=\"tts-1\",\n",
"  voice=\"alloy\",\n",
"  input=text,\n",
")\n",
"\n",
"response.stream_to_file(speech_file_path)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "<controles de áudio>\n",
"    <source src=\"openai/speech.mp3\" type=\"audio/mpeg\">\n",
"    Seu navegador não suporta o elemento de áudio.\n",
"</audio>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos escolher\n",
"\n",
" * Model: permite escolher o modelo de geração de áudio; os valores possíveis são `tts-1` e `tts-1-hd`.\n",
" * Voz: permite que escolhamos a voz que queremos que o modelo use. Os valores possíveis são `alloy`, `echo`, `fable`, `onyx`, `nova` e `shimmer`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Fala para texto (sussurro)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos transcrever o áudio usando o Whisper com o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "This is the Micromachine Man presenting the most midget miniature motorcade of micromachines. Each one has dramatic details, terrific trim, precision paint jobs, plus incredible micromachine pocket play sets. There's a police station, fire station, restaurant, service station, and more. Perfect pocket portables to take anyplace. And there are many miniature play sets to play with, and each one comes with its own special edition micromachine vehicle and fun fantastic features that miraculously move. Raise the boat lift at the airport, marina, man the gun turret at the army base, clean your car at the car wash, raise the toll bridge. And these play sets fit together to form a micromachine world. Micromachine pocket play sets so tremendously tiny, so perfectly precise, so dazzlingly detailed, you'll want to pocket them all. Micromachines and micromachine pocket play sets sold separately from Galoob. The smaller they are, the better they are.\n"
          ]
        }
      ],
      "source": [
      "audio_file = \"MicroMachines.mp3\"\n",
"audio_file= open(audio_file, \"rb\")\n",
"\n",
"transcript = client.audio.transcriptions.create(\n",
"  model=\"whisper-1\", \n",
"  file=audio_file\n",
")\n",
"\n",
"print(transcript.text)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "<controles de áudio>\n",
"    <source src=\"MicroMachines.mp3\" type=\"audio/mpeg\">\n",
"    Seu navegador não suporta o elemento de áudio.\n",
"</audio>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Moderação de conteúdo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Podemos obter a categoria de um texto entre as classes `sexual`, `ódio`, `assédio`, `agressão a si mesmo`, `sexual/minores`, `ódio/ameaça`, `violência/gráfica`, `agressão a si mesmo/intenção`, `agressão a si mesmo/instruções`, `assédio/ameaça` e `violência`, para isso usamos o seguinte código com o texto transcrito acima"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
      "text = transcript.text\n",
"\n",
"response = client.moderations.create(input=text)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.moderation_create_response.ModerationCreateResponse,\n",
" ModerationCreateResponse(id='modr-8RxMZItvmLblEl5QPgCv19Jl741SS', model='text-moderation-006', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)]))"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(response), response"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.id = modr-8RxMZItvmLblEl5QPgCv19Jl741SS\n",
"response.model = text-moderation-006\n",
"response.results[0] = Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)\n",
"\tresponse.results[0].categories = Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False)\n",
"\t\tresponse.results[0].categories.harassment = False\n",
"\t\tresponse.results[0].categories.harassment_threatening = False\n",
"\t\tresponse.results[0].categories.hate = False\n",
"\t\tresponse.results[0].categories.hate_threatening = False\n",
"\t\tresponse.results[0].categories.self_harm = False\n",
"\t\tresponse.results[0].categories.self_harm_instructions = False\n",
"\t\tresponse.results[0].categories.self_harm_intent = False\n",
"\t\tresponse.results[0].categories.sexual = False\n",
"\t\tresponse.results[0].categories.sexual_minors = False\n",
"\t\tresponse.results[0].categories.violence = False\n",
"\t\tresponse.results[0].categories.violence_graphic = False\n",
"\tresponse.results[0].category_scores = CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06)\n",
"\t\tresponse.results[0].category_scores.harassment = 0.0003560568729881197\n",
"\t\tresponse.results[0].category_scores.harassment_threatening = 2.5426568299735663e-06\n",
"\t\tresponse.results[0].category_scores.hate = 1.966094168892596e-05\n",
"\t\tresponse.results[0].category_scores.hate_threatening = 6.384455986108151e-08\n",
"\t\tresponse.results[0].category_scores.self_harm = 7.903140613052528e-07\n",
"\t\tresponse.results[0].category_scores.self_harm_instructions = 6.443992219828942e-07\n",
"\t\tresponse.results[0].category_scores.self_harm_intent = 1.2202733046251524e-07\n",
"\t\tresponse.results[0].category_scores.sexual = 0.0003779272665269673\n",
"\t\tresponse.results[0].category_scores.sexual_minors = 1.8967952200910076e-05\n",
"\t\tresponse.results[0].category_scores.violence = 9.489082731306553e-05\n",
"\t\tresponse.results[0].category_scores.violence_graphic = 5.1929731853306293e-05\n",
"\tresponse.results[0].flagged = False\n"
          ]
        }
      ],
      "source": [
      "print(f\"response.id = {response.id}\")\n",
"print(f\"response.model = {response.model}\")\n",
"for i in range(len(response.results)):\n",
"    print(f\"response.results[{i}] = {response.results[i]}\")\n",
"    print(f\"\\tresponse.results[{i}].categories = {response.results[i].categories}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
"    print(f\"\\tresponse.results[{i}].category_scores = {response.results[i].category_scores}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
"    print(f\"\\t\\tresponse.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
"    print(f\"\\tresponse.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O áudio transcrito não está em nenhuma das categorias acima, vamos tentar outro texto."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "response.results[0].categories.harassment = False\n",
"response.results[0].categories.harassment_threatening = False\n",
"response.results[0].categories.hate = False\n",
"response.results[0].categories.hate_threatening = False\n",
"response.results[0].categories.self_harm = True\n",
"response.results[0].categories.self_harm_instructions = False\n",
"response.results[0].categories.self_harm_intent = True\n",
"response.results[0].categories.sexual = False\n",
"response.results[0].categories.sexual_minors = False\n",
"response.results[0].categories.violence = True\n",
"response.results[0].categories.violence_graphic = False\n",
"\n",
"response.results[0].category_scores.harassment = 0.004724912345409393\n",
"response.results[0].category_scores.harassment_threatening = 0.00023778305330779403\n",
"response.results[0].category_scores.hate = 1.1909247405128554e-05\n",
"response.results[0].category_scores.hate_threatening = 1.826493189582834e-06\n",
"response.results[0].category_scores.self_harm = 0.9998544454574585\n",
"response.results[0].category_scores.self_harm_instructions = 3.5801923647937883e-09\n",
"response.results[0].category_scores.self_harm_intent = 0.99969482421875\n",
"response.results[0].category_scores.sexual = 2.141016238965676e-06\n",
"response.results[0].category_scores.sexual_minors = 2.840671520232263e-08\n",
"response.results[0].category_scores.violence = 0.8396497964859009\n",
"response.results[0].category_scores.violence_graphic = 2.7347923605702817e-05\n",
"\n",
"response.results[0].flagged = True\n"
          ]
        }
      ],
      "source": [
      "text = \"I want to kill myself\"\n",
"\n",
"response = client.moderations.create(input=text)\n",
"\n",
"for i in range(len(response.results)):\n",
"    print(f\"response.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
"    print(f\"response.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
"    print(f\"response.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
"    print(f\"response.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
"    print(f\"response.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
"    print(f\"response.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
"    print(f\"response.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
"    print(f\"response.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
"    print(f\"response.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
"    print(f\"response.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
"    print(f\"response.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
"    print()\n",
"    print(f\"response.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
"    print(f\"response.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
"    print(f\"response.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
"    print(f\"response.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
"    print(f\"response.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
"    print(f\"response.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
"    print(f\"response.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
"    print(f\"response.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
"    print(f\"response.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
"    print(f\"response.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
"    print(f\"response.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
"    print()\n",
"    print(f\"response.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, se ele detectar que o texto é `self_harm_intent`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Participantes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A OpenAI nos dá a possibilidade de criar assistentes, para que possamos criá-los com as características que quisermos, por exemplo, um assistente especialista em Python, e podemos usá-lo como se fosse um modelo específico da OpenAI. Ou seja, podemos usá-lo para uma consulta e ter uma conversa com ele e, depois de algum tempo, usá-lo novamente com uma nova consulta em uma nova conversa.\n",
"\n",
"Ao trabalhar com assistentes, teremos que criá-los, criar um thread, enviar-lhes uma mensagem, executá-los, esperar que eles respondam e ver a resposta.\n",
"\n",
"![assistentes](https://cdn.openai.com/API/docs/images/diagram-assistant.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Criar o assistente"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, criamos o assistente"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
      "code_interpreter_assistant = client.beta.assistants.create(\n",
"    name=\"Python expert\",\n",
"    instructions=\"Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.\",\n",
"    tools=[{\"type\": \"code_interpreter\"}],\n",
"    model=\"gpt-3.5-turbo-1106\"\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.beta.assistant.Assistant,\n",
" Assistant(id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', created_at=1701822478, description=None, file_ids=[], instructions='Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.', metadata={}, model='gpt-3.5-turbo-1106', name='Python expert', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]))"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(code_interpreter_assistant), code_interpreter_assistant"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "code_interpreter_assistant_id = asst_A2F9DPqDiZYFc5hOC6Rb2y0x\n"
          ]
        }
      ],
      "source": [
      "code_interpreter_assistant_id = code_interpreter_assistant.id\n",
"print(f\"code_interpreter_assistant_id = {code_interpreter_assistant_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Ao criar o assistente, as variáveis que temos são\n",
"\n",
" * `name`: Nome do assistente\n",
" * Instruções para o assistente. Aqui podemos explicar como o assistente deve se comportar\n",
" * Ferramentas que podem ser usadas pelo assistente. No momento, apenas `code_interpreter` e `retrieval` estão disponíveis.\n",
" * `model`: Modelo a ser usado pelo assistente"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Esse assistente já foi criado e podemos usá-lo quantas vezes quisermos. Para isso, precisamos criar um novo tópico, de modo que, se outra pessoa quiser usá-lo no futuro, por ser útil, ao criar um novo tópico, ela poderá usá-lo como se estivesse usando o assistente original. Você só precisaria do ID do assistente"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tópico"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Um thread representa uma nova conversa com o assistente, portanto, mesmo que o tempo tenha passado, desde que tenhamos o ID do thread, poderemos continuar a conversa. Para criar um novo thread, precisamos usar o seguinte código"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
      "thread = client.beta.threads.create()"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "thread_id = thread_nfFT3rFjyPWHdxWvMk6jJ90H\n"
          ]
        }
      ],
      "source": [
      "type(thread), thread\n",
"thread_id = thread.id\n",
"print(f\"thread_id = {thread_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Fazer upload de um arquivo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos criar um arquivo .py que pediremos ao intérprete para nos explicar"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
      "import os\n",
"\n",
"python_code = os.path.join(\"openai\", \"python_code.py\")\n",
"code = \"print('Hello world!')\"\n",
"with open(python_code, \"w\") as f:\n",
"    f.write(code)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Fizemos o upload para a API OpenAI usando a função `client.files.create`, já usamos essa função quando fizemos o `fine-tuning` de um modelo chatGPT e fizemos o upload dos `jsonl`s para ele. Somente antes, na variável `purpose`, passamos `fine-tuning`, pois os arquivos que carregamos eram para `fine-tuning`, e agora passamos `assistants`, pois os arquivos que vamos carregar são para um assistente."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
      "file = client.files.create(\n",
"  file=open(python_code, \"rb\"),\n",
"  purpose='assistants'\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.file_object.FileObject,\n",
" FileObject(id='file-HF8Llyzq9RiDfQIJ8zeGrru3', bytes=21, created_at=1701822479, filename='python_code.py', object='file', purpose='assistants', status='processed', status_details=None))"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(file), file"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Enviar uma mensagem para o assistente"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos a mensagem a ser enviada ao assistente e também indicamos o ID do arquivo sobre o qual queremos perguntar."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
      "message = client.beta.threads.messages.create(\n",
"    thread_id=thread_id,\n",
"    role=\"user\",\n",
"    content=\"Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.\",\n",
"    file_ids=[file.id]\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Executar o assistente"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Executamos o assistente e dizemos a ele para resolver a pergunta do usuário."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
      "run = client.beta.threads.runs.create(\n",
"  thread_id=thread_id,\n",
"  assistant_id=code_interpreter_assistant_id,\n",
"  instructions=\"Resuleve el problema que te ha planteado el usuario.\",\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.beta.threads.run.Run,\n",
" Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(run), run"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "run_id = run_WZxT1TUuHT5qB1ZgD34tgvPu\n"
          ]
        }
      ],
      "source": [
      "run_id = run.id\n",
"print(f\"run_id = {run_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Aguardar a conclusão do processamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Enquanto o assistente estiver analisando, podemos verificar o status"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
      "run = client.beta.threads.runs.retrieve(\n",
"  thread_id=thread_id,\n",
"  run_id=run_id\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.types.beta.threads.run.Run,\n",
" Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1701822481, status='in_progress', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(run), run"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "'in_progress'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "run.status"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Run completed!\n"
          ]
        }
      ],
      "source": [
      "while run.status != \"completed\":\n",
"    time.sleep(1)\n",
"    run = client.beta.threads.runs.retrieve(\n",
"      thread_id=thread_id,\n",
"      run_id=run_id\n",
"    )\n",
"print(\"Run completed!\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Processar a resposta"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Após a conclusão do assistente, podemos ver a resposta"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
      "messages = client.beta.threads.messages.list(\n",
"  thread_id=thread_id\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(openai.pagination.SyncCursorPage[ThreadMessage],\n",
" SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='La salida del script es simplemente \"Hello world!\", ya que la única instrucción en el script es imprimir esa frase.\\n\\nSi necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.'), type='text')], created_at=1701822487, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_nkFbq64DTaSqxIAQUGedYmaX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='El script proporcionado contiene una sola línea que imprime \"Hello world!\". Ahora procederé a ejecutar el script para obtener su salida.'), type='text')], created_at=1701822485, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_bWT6H2f6lsSUTAAhGG0KXoh7', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.'), type='text')], created_at=1701822482, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_RjDygK7c8yCqYrjnUPfeZfUg', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.'), type='text')], created_at=1701822481, file_ids=['file-HF8Llyzq9RiDfQIJ8zeGrru3'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H')], object='list', first_id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', last_id='msg_RjDygK7c8yCqYrjnUPfeZfUg', has_more=False))"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(messages), messages"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "messages.data[0].content[0].text.value = La salida del script es simplemente \"Hello world!\", ya que la única instrucción en el script es imprimir esa frase.\n",
"\n",
"Si necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.\n",
"messages.data[1].content[0].text.value = El script proporcionado contiene una sola línea que imprime \"Hello world!\". Ahora procederé a ejecutar el script para obtener su salida.\n",
"messages.data[2].content[0].text.value = Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.\n",
"messages.data[3].content[0].text.value = Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.\n"
          ]
        }
      ],
      "source": [
      "for i in range(len(messages.data)):\n",
"    for j in range(len(messages.data[i].content)):\n",
"        print(f\"messages.data[{i}].content[{j}].text.value = {messages.data[i].content[j].text.value}\")\n",
"    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
