{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que os `LLM`s est\u00e3o em alta, n\u00e3o paramos de ouvir o n\u00famero de `token`s que cada modelo suporta, mas o que s\u00e3o os `token`s? S\u00e3o as unidades m\u00ednimas de representa\u00e7\u00e3o das palavras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para explicar o que s\u00e3o os `token`s, primeiro vejamos com um exemplo pr\u00e1tico, vamos usar o tokenizador de `OpenAI`, chamado [tiktoken](https://github.com/openai/tiktoken).",
        "\n",
        "Ent\u00e3o, primeiro instalamos o pacote:",
        "\n",
        "```bash\n",
        "pip install tiktoken",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez instalado, criamos um tokenizador usando o modelo `cl100k_base`, que no notebook de exemplo [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) explica que \u00e9 o usado pelos modelos `gpt-4`, `gpt-3.5-turbo` e `text-embedding-ada-002`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "encoder = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos uma palavra de exemplo para tokeniz\u00e1-la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_word = \"breakdown\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E tokenizamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[9137, 2996]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = encoder.encode(example_word)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A palavra foi dividida em 2 `token`s, o `9137` e o `2996`. Vamos ver a quais palavras correspondem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('break', 'down')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word1 = encoder.decode([tokens[0]])\n",
        "word2 = encoder.decode([tokens[1]])\n",
        "word1, word2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O tokenizador da `OpenAI` dividiu a palavra `breakdown` nas palavras `break` e `down`. Ou seja, ele dividiu a palavra em 2 mais simples.",
        "\n",
        "Isto \u00e9 importante, pois quando se diz que um `LLM` suporta x `token`s, n\u00e3o significa que ele suporta x palavras, mas sim que ele suporta x unidades m\u00ednimas de representa\u00e7\u00e3o das palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se voc\u00ea tem um texto e quer ver o n\u00famero de `token`s que ele possui para o tokenizador de `OpenAI`, pode verificar na p\u00e1gina [Tokenizer](https://platform.openai.com/tokenizer), que mostra cada `token` em uma cor diferente.",
        "\n",
        "![tokenizer](https://images.maximofn.com/tokenizer.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vimos o tokenizador da `OpenAI`, mas cada `LLM` poder\u00e1 usar outro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissemos, os `tokens` s\u00e3o as unidades m\u00ednimas de representa\u00e7\u00e3o das palavras, ent\u00e3o vamos ver quantos tokens distintos tem `tiktoken`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 100277\n"
          ]
        }
      ],
      "source": [
        "n_vocab = encoder.n_vocab\n",
        "print(f\"Vocab size: {n_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como tokeniza outro tipo de palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_decode(word):\n",
        "    tokens = encoder.encode(word)\n",
        "    decode_tokens = []\n",
        "    for token in tokens:\n",
        "        decode_tokens.append(encoder.decode([token]))\n",
        "    return tokens, decode_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: dog ==> tokens: [18964], decode_tokens: ['dog']\n",
            "Word: tomorrow... ==> tokens: [38501, 7924, 1131], decode_tokens: ['tom', 'orrow', '...']\n",
            "Word: artificial intelligence ==> tokens: [472, 16895, 11478], decode_tokens: ['art', 'ificial', ' intelligence']\n",
            "Word: Python ==> tokens: [31380], decode_tokens: ['Python']\n",
            "Word: 12/25/2023 ==> tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']\n",
            "Word: \ud83d\ude0a ==> tokens: [76460, 232], decode_tokens: ['\ufffd', '\ufffd']\n"
          ]
        }
      ],
      "source": [
        "word = \"dog\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"tomorrow...\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"artificial intelligence\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"Python\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"12/25/2023\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"\ud83d\ude0a\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo vamos a v\u00ea-lo com palavras em outro idioma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: perro ==> tokens: [716, 299], decode_tokens: ['per', 'ro']\n",
            "Word: perra ==> tokens: [79, 14210], decode_tokens: ['p', 'erra']\n",
            "Word: ma\u00f1ana... ==> tokens: [1764, 88184, 1131], decode_tokens: ['ma', '\u00f1ana', '...']\n",
            "Word: inteligencia artificial ==> tokens: [396, 39567, 8968, 21075], decode_tokens: ['int', 'elig', 'encia', ' artificial']\n",
            "Word: Python ==> tokens: [31380], decode_tokens: ['Python']\n",
            "Word: 12/25/2023 ==> tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']\n",
            "Word: \ud83d\ude0a ==> tokens: [76460, 232], decode_tokens: ['\ufffd', '\ufffd']\n"
          ]
        }
      ],
      "source": [
        "word = \"perro\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"perra\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"ma\u00f1ana...\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"inteligencia artificial\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"Python\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"12/25/2023\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
        "\n",
        "word = \"\ud83d\ude0a\"\n",
        "tokens, decode_tokens = encode_decode(word)\n",
        "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver para palavras semelhantes, em espanhol s\u00e3o gerados mais `token`s do que em ingl\u00eas, portanto, para um mesmo texto, com um n\u00famero similar de palavras, o n\u00famero de `token`s ser\u00e1 maior em espanhol do que em ingl\u00eas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "maximofn": {
      "date": "2023-12-08",
      "description_en": "Discover what tokens are and how words are divided into minimum units of word representation",
      "description_es": "Descubre qu\u00e9 son los tokens y c\u00f3mo se dividen las palabras en unidades m\u00ednimas de representaci\u00f3n de las palabras",
      "description_pt": "Descubra o que s\u00e3o tokens e como as palavras s\u00e3o divididas em unidades m\u00ednimas de representa\u00e7\u00e3o das palavras",
      "end_url": "tokens",
      "image": "https://images.maximofn.com/tokens.webp",
      "image_hover_path": "https://images.maximofn.com/tokens.webp",
      "keywords_en": "tokens, token, openai, tiktoken, gpt-4, gpt-3.5-turbo, text-embedding-ada-002",
      "keywords_es": "tokens, token, openai, tiktoken, gpt-4, gpt-3.5-turbo, text-embedding-ada-002",
      "keywords_pt": "tokens, token, openai, tiktoken, gpt-4, gpt-3.5-turbo, text-embedding-ada-002",
      "title_en": "Tokens",
      "title_es": "Tokens",
      "title_pt": "Tokens"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}