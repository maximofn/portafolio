{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# Embeddings"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In a previous post about [tokens](https://maximofn.com/tokens/), we already saw the minimum representation of each word. Which corresponds to giving a number to the minimum division of each word.\n",
"\n",
"However, the transformers, and therefore the LLMs, do not represent the information of the words in this way, but do so by means of `embeddings`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.\n",
"\n",
"We are going to see first two ways of representing words inside transformers, the `ordinal encoding` and the `one hot encoding`. And seeing the problems of these two types of representations we will be able to get to the `embeddings`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Ordinal encoding"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This is the most basic way to represent the words inside the transformers. It consists of giving a number to each word, or keeping the numbers already assigned to the tokens.\n",
"\n",
"However, this type of representation has two problems\n",
"\n",
" * Let us imagine that table corresponds to token 3, cat to token 1 and dog to token 2. One could assume that `table = cat + dog`, but it is not so. There is no such relationship between these words. We might even think that by assigning the correct tokens, this type of relationship could occur. However, this thought falls apart with words that have more than one meaning, such as the word `bank`, for example.\n",
"\n",
" * The second problem is that neural networks internally do a lot of numerical calculations, so it could be the case that if mesa has token 3, it has internally more importance than the word cat which has token 1.\n",
"\n",
"So this type of word representation can be discarded very quickly."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## One hot encoding"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Here what is done is to use `N` dimensional vectors. For example we saw that OpenAI has a vocabulary of `100277` distinct tokens. So if we use `one hot encoding`, each word would be represented with a vector of `100277` dimensions.\n",
"\n",
"However, one hot encodding has two other major problems\n",
"\n",
" * It does not take into account the relationship between words. So if we have two words that are synonyms, such as `cat` and `feline`, we would have two different vectors to represent them.\n",
" In language the relationship between words is very important, and not taking this relationship into account is a big problem.\n",
"\n",
" * The second problem is that vectors are very large. If we have a vocabulary of `100277` tokens, each word would be represented by a vector of `100277` dimensions. This makes the vectors very large and computationally very expensive. In addition these vectors are going to be all zeros, except in the position corresponding to the word token. So most of the calculations are going to be multiplications by zero, which are calculations that don't add anything. So we're going to have a lot of memory allocated to vectors where you only have a 1 in a given position."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Word embeddings"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In word embeddings we try to solve the problems of the two previous types of representations. For this purpose vectors of `N` dimensions are used, but in this case vectors of 100277 dimensions are not used, but vectors of much less dimensions are used. For example we will see that OpenAI uses `1536` dimensions.\n",
"\n",
"Each of the dimensions of these vectors represents a characteristic of the word. For example one of the dimensions could represent whether the word is a verb or a noun. Another dimension might represent whether the word is an animal or not. Another dimension might represent whether the word is a proper noun or not. And so on.\n",
"\n",
"However, these features are not defined by hand, but are learned automatically. During the training of the transformers, the values of each of the dimensions of the vectors are adjusted, so that the characteristics of each of the words are learned."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "By making each of the word dimensions represent a characteristic of the word, words that have similar characteristics will have similar vectors. For example the words `cat` and `feline` will have very similar vectors, since they are both animals. And the words `table` and `chair` will have similar vectors, since both are furniture.\n",
"\n",
"In the following image we can see a 3-dimensional representation of words, and we can see that all words related to `school` are close, all words related to `food` are close and all words related to `ball` are close.\n",
"\n",
"![word_embedding_3_dimmension](http://maximofn.com/wp-content/uploads/2023/12/word_embedding_3_dimmension.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Having each of the dimensions of the vectors represent a characteristic of the word allows us to perform operations with words. For example, if we subtract the word `king` from the word `man` and add the word `woman`, we get a word very similar to the word `queen`. We will check it later with an example"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Similarity between words"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As each word is represented by a vector of N dimensions, we can calculate the similarity between two words. The cosine similarity function or `cosine similarity` is used for this purpose.\n",
"\n",
"If two words are close in vector space, it means that the angle between their vectors is small, so their cosine is close to 1. If there is an angle of 90 degrees between the vectors, the cosine is 0, meaning that there is no similarity between the words. And if there is an angle of 180 degrees between the vectors, the cosine is -1, that is, the words are opposites.\n",
"\n",
"![cosine similarity](http://maximofn.com/wp-content/uploads/2023/12/cosine_similarity-scaled.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Example with OpenAI embeddings"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now that we know what `embeddings` are, let's see some examples with the `embeddings` provided by the `API` of `OpenAI`.\n",
"\n",
"To do this we first need to have the `OpenAI` package installed.\n",
"\n",
"````bash\n",
"pip install openai\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We import the necessary libraries"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from openai import OpenAI\n",
"import torch\n",
"from torch.nn.functional import cosine_similarity"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We use an OpenAI `API key`. To do this, go to the [OpenAI](https://openai.com/) page, and register. Once registered, go to the [API Keys](https://platform.openai.com/api-keys) section, and create a new `API Key`.\n",
"\n",
"![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
      "api_key = \"Pon aqu√≠ tu API key\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We select which embeddings model we want to use. In this case we are going to use `text-embedding-ada-002` which is the one recommended by `OpenAI` in its [embeddings](https://platform.openai.com/docs/guides/embeddings/) documentation."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "model_openai = \"text-embedding-ada-002\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Create an `API` client"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "client_openai = OpenAI(api_key=api_key, organization=None)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see how are the `embeddings` of the word `King`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1536]),\n",
" tensor([-0.0103, -0.0005, -0.0189,  ..., -0.0009, -0.0226,  0.0045]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "word = \"Rey\"\n",
"embedding_openai = torch.Tensor(client_openai.embeddings.create(input=word, model=model_openai).data[0].embedding)\n",
"\n",
"embedding_openai.shape, embedding_openai"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see we obtain a vector of `1536` dimensions"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Operations with words"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's get the embeddings of the words `king`, `man`, `woman` and `Queen`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
      "embedding_openai_rey = torch.Tensor(client_openai.embeddings.create(input=\"rey\", model=model_openai).data[0].embedding)\n",
"embedding_openai_hombre = torch.Tensor(client_openai.embeddings.create(input=\"hombre\", model=model_openai).data[0].embedding)\n",
"embedding_openai_mujer = torch.Tensor(client_openai.embeddings.create(input=\"mujer\", model=model_openai).data[0].embedding)\n",
"embedding_openai_reina = torch.Tensor(client_openai.embeddings.create(input=\"reina\", model=model_openai).data[0].embedding)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1536]),\n",
" tensor([-0.0110, -0.0084, -0.0115,  ...,  0.0082, -0.0096, -0.0024]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "embedding_openai_reina.shape, embedding_openai_reina"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's obtain the embedding resulting from subtracting the embedding of `man` from `king` and adding the embedding of `woman` to `king`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
      "embedding_openai = embedding_openai_rey - embedding_openai_hombre + embedding_openai_mujer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1536]),\n",
" tensor([-0.0226, -0.0323,  0.0017,  ...,  0.0014, -0.0290, -0.0188]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "embedding_openai.shape, embedding_openai"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Finally we compare the result obtained with the embedding of `reina`. For this we use the `cosine_similarity` function provided by the `pytorch` library."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "similarity_openai: 0.7564167976379395\n"
          ]
        }
      ],
      "source": [
      "similarity_openai = cosine_similarity(embedding_openai.unsqueeze(0), embedding_openai_reina.unsqueeze(0)).item()\n",
"\n",
"print(f\"similarity_openai: {similarity_openai}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see it is a value very close to 1, so we can say that the result obtained is very similar to the embedding of `reina`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we use English words, we get a result closer to 1."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
      "embedding_openai_rey = torch.Tensor(client_openai.embeddings.create(input=\"king\", model=model_openai).data[0].embedding)\n",
"embedding_openai_hombre = torch.Tensor(client_openai.embeddings.create(input=\"man\", model=model_openai).data[0].embedding)\n",
"embedding_openai_mujer = torch.Tensor(client_openai.embeddings.create(input=\"woman\", model=model_openai).data[0].embedding)\n",
"embedding_openai_reina = torch.Tensor(client_openai.embeddings.create(input=\"queen\", model=model_openai).data[0].embedding)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
      "embedding_openai = embedding_openai_rey - embedding_openai_hombre + embedding_openai_mujer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "similarity_openai: tensor([0.8849])\n"
          ]
        }
      ],
      "source": [
      "similarity_openai = cosine_similarity(embedding_openai.unsqueeze(0), embedding_openai_reina.unsqueeze(0))\n",
"print(f\"similarity_openai: {similarity_openai}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This is normal, since the OpenAi model has been trained with more txtos in English than in Spanish."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
