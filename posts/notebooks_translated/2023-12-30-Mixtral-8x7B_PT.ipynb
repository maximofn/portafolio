{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mixtral-8x7B MoE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para mim, a melhor descri\u00e7\u00e3o de `mixtral-8x7b` \u00e9 a seguinte imagem",
        "\n",
        "![mixtral-gemini](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mixtral-gemini.webp)",
        "\n",
        "Entre a sa\u00edda do `gemini` e a sa\u00edda do `mixtra-8x7b` houve uma diferen\u00e7a de muito poucos dias. Nos dois primeiros dias ap\u00f3s o lan\u00e7amento do `gemini`, falou-se bastante sobre esse modelo, mas assim que saiu o `mixtral-8x7b`, o `gemini` foi completamente esquecido e toda a comunidade n\u00e3o parou de falar do `mixtral-8x7b`.",
        "\n",
        "E n\u00e3o \u00e9 para menos, vendo seus benchmarks, podemos ver que est\u00e1 no n\u00edvel de modelos como `llama2-70B` e `GPT3.5`, mas com a diferen\u00e7a de que enquanto `mixtral-8x7b` tem apenas 46,7B de par\u00e2metros, `llama2-70B` tem 70B e `GPT3.5` tem 175B.",
        "\n",
        "![mixtral benchmarks](https://mistral.ai/images/news/mixtral-of-experts/overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N\u00famero de par\u00e2metros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o nome sugere, `mixtral-8x7b` \u00e9 um conjunto de 8 modelos de 7B de par\u00e2metros, ent\u00e3o poder\u00edamos pensar que ele tem 56B de par\u00e2metros (7Bx8), mas n\u00e3o \u00e9 bem assim. Como explica [Andrej Karpathy](https://twitter.com/karpathy/status/1734251375163511203), apenas os blocos `Feed forward` dos transformers s\u00e3o multiplicados por 8, o resto dos par\u00e2metros \u00e9 compartilhado entre os 8 modelos. Portanto, no final, o modelo tem 46,7B de par\u00e2metros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mistura de Especialistas (MoE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissemos, o modelo \u00e9 um conjunto de 8 modelos de 7B de par\u00e2metros, da\u00ed `MoE`, que significa `Mixture of Experts`. Cada um dos 8 modelos \u00e9 treinado independentemente, mas quando se faz a infer\u00eancia, um roteador decide qual sa\u00edda do modelo ser\u00e1 usada.",
        "\n",
        "Na imagem a seguir, podemos ver como \u00e9 a arquitetura de um `Transformer`.",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)",
        "\n",
        "Se n\u00e3o a conhece, n\u00e3o tem problema. O importante \u00e9 que esta arquitetura consiste em um encoder e um decoder",
        "\n",
        "![transformer-encoder-decoder](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-encoder-decoder.webp)",
        "\n",
        "Os LLMs s\u00e3o modelos que possuem apenas o decoder, portanto n\u00e3o t\u00eam encoder. Voc\u00ea pode ver que na arquitetura h\u00e1 tr\u00eas m\u00f3dulos de aten\u00e7\u00e3o, um deles de fato conecta o encoder com o decoder. Mas como os LLMs n\u00e3o t\u00eam encoder, n\u00e3o \u00e9 necess\u00e1rio o m\u00f3dulo de aten\u00e7\u00e3o que une o decoder e o decoder.",
        "\n",
        "![transformer-decoder](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-decoder.webp)",
        "\n",
        "Agora que sabemos como \u00e9 a arquitetura de um LLM, podemos ver como \u00e9 a arquitetura do `mixtral-8x7b`. Na imagem seguinte podemos ver a arquitetura do modelo",
        "\n",
        "![Arquitetura MoE](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/MoE_architecture.webp)",
        "\n",
        "Como pode ser visto, a arquitetura consiste no decoder de um `Transformer` de 7B de par\u00e2metros, apenas que a camada `Feed forward` consiste em 8 camadas `Feed forward` com um roteador que escolhe qual das 8 camadas `Feed forward` ser\u00e1 usada. Na imagem anterior, apenas quatro camadas `Feed forward` s\u00e3o mostradas, suponho que seja para simplificar o diagrama, mas na realidade h\u00e1 8 camadas `Feed forward`. Tamb\u00e9m se veem dois caminhos para duas palavras distintas, a palavra `More` e a palavra `Parameters` e como o roteador escolhe qual `Feed forward` ser\u00e1 usado para cada palavra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vendo a arquitetura podemos entender por que o modelo tem 46,7B de par\u00e2metros e n\u00e3o 56B. Como dissemos, apenas os blocos `Feed forward` se multiplicam por 8, o resto dos par\u00e2metros s\u00e3o compartilhados entre os 8 modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uso do Mixtral-8x7b na nuvem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Infelizmente, para usar `mixtral-8x7b` localmente \u00e9 complicado, j\u00e1 que os requisitos de hardware s\u00e3o os seguintes",
        "\n",
        "* float32: VRAM > 180 GB, ou seja, como cada par\u00e2metro ocupa 4 bytes, precisamos de 46,7B * 4 = 186,8 GB de VRAM apenas para armazenar o modelo, al\u00e9m disso, \u00e9 necess\u00e1rio adicionar a VRAM necess\u00e1ria para armazenar os dados de entrada e sa\u00edda",
        "* float16: VRAM > 90 GB, neste caso cada par\u00e2metro ocupa 2 bytes, ent\u00e3o precisamos de 46,7B * 2 = 93,4 GB de VRAM apenas para armazenar o modelo, al\u00e9m disso, \u00e9 necess\u00e1rio adicionar a VRAM necess\u00e1ria para armazenar os dados de entrada e sa\u00edda",
        "* 8-bit: VRAM > 45 GB, aqui cada par\u00e2metro ocupa 1 byte, por isso precisamos de 46,7B * 1 = 46,7 GB de VRAM apenas para armazenar o modelo, al\u00e9m disso, \u00e9 necess\u00e1rio adicionar a VRAM necess\u00e1ria para armazenar os dados de entrada e sa\u00edda",
        "* 4-bit: VRAM > 23 GB, aqui cada par\u00e2metro ocupa 0.5 bytes, portanto precisamos 46.7B * 0.5 = 23.35 GB de VRAM apenas para armazenar o modelo, al\u00e9m disso, \u00e9 necess\u00e1rio adicionar a VRAM necess\u00e1ria para armazenar os dados de entrada e sa\u00edda",
        "\n",
        "Precisamos de GPUs muito potentes para conseguir execut\u00e1-lo, mesmo quando utilizamos o modelo quantizado para 4 bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portanto, a forma mais simples de usar `Mixtral-8x7B` \u00e9 us\u00e1-lo j\u00e1 implantado na nuvem. Encontrei v\u00e1rios sites onde voc\u00ea pode us\u00e1-lo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uso de Mixtral-8x7b no huggingface chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O primeiro est\u00e1 no [huggingface chat](https://huggingface.co/chat). Para poder us\u00e1-lo, \u00e9 necess\u00e1rio clicar na engrenagem dentro da caixa `Current Model` e selecionar `Mistral AI - Mixtral-8x7B`. Uma vez selecionado, voc\u00ea j\u00e1 pode come\u00e7ar a conversar com o modelo.",
        "\n",
        "![huggingface_chat_01](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_chat_01.webp)",
        "\n",
        "Uma vez dentro, selecione `mistralai/Mixtral-8x7B-Instruct-v0.1` e, por fim, clique no bot\u00e3o `Activate`. Agora poderemos testar o modelo.",
        "\n",
        "![huggingface_chat_02](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_chat_02.webp)",
        "\n",
        "Como se pode ver, eu lhe perguntei em espanhol o que \u00e9 `MoE` e ele me explicou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uso de Mixtral-8x7b nos Perplexity Labs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outra op\u00e7\u00e3o \u00e9 usar [Perplexity Labs](https://labs.perplexity.ai/). Uma vez dentro, deve-se selecionar `mixtral-8x7b-instruct` em um menu suspenso localizado na parte inferior direita.",
        "\n",
        "![perplexity_labs](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/perplexity_labs.webp)",
        "\n",
        "Como se pode ver, tamb\u00e9m lhe perguntei em espanhol o que \u00e9 `MoE` e ele me explicou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uso de Mixtral-8x7b localmente atrav\u00e9s da API do Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma maneira de us\u00e1-lo localmente, independentemente dos recursos de hardware que voc\u00ea tenha, \u00e9 atrav\u00e9s da API do Hugging Face. Para isso, \u00e9 necess\u00e1rio instalar a biblioteca `huggingface-hub` do Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui est\u00e1 uma implementa\u00e7\u00e3o com `gradio`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import gradio as gr\n",
        "\n",
        "client = InferenceClient(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "\n",
        "def format_prompt(message, history):\n",
        "  prompt = \"<s>\"\n",
        "  for user_prompt, bot_response in history:\n",
        "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
        "    prompt += f\" {bot_response}</s> \"\n",
        "  prompt += f\"[INST] {message} [/INST]\"\n",
        "  return prompt\n",
        "\n",
        "def generate(prompt, history, system_prompt, temperature=0.9, max_new_tokens=256, top_p=0.95, repetition_penalty=1.0,):\n",
        "    temperature = float(temperature)\n",
        "    if temperature < 1e-2:\n",
        "        temperature = 1e-2\n",
        "    top_p = float(top_p)\n",
        "\n",
        "    generate_kwargs = dict(temperature=temperature, max_new_tokens=max_new_tokens, top_p=top_p, repetition_penalty=repetition_penalty, do_sample=True, seed=42,)\n",
        "\n",
        "    formatted_prompt = format_prompt(f\"{system_prompt}, {prompt}\", history)\n",
        "    stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=True, details=True, return_full_text=False)\n",
        "    output = \"\"\n",
        "\n",
        "    for response in stream:\n",
        "        output += response.token.text\n",
        "        yield output\n",
        "    return output\n",
        "\n",
        "additional_inputs=[\n",
        "    gr.Textbox(label=\"System Prompt\", max_lines=1, interactive=True,),\n",
        "    gr.Slider(label=\"Temperature\", value=0.9, minimum=0.0, maximum=1.0, step=0.05, interactive=True, info=\"Higher values produce more diverse outputs\"),\n",
        "    gr.Slider(label=\"Max new tokens\", value=256, minimum=0, maximum=1048, step=64, interactive=True, info=\"The maximum numbers of new tokens\"),\n",
        "    gr.Slider(label=\"Top-p (nucleus sampling)\", value=0.90, minimum=0.0, maximum=1, step=0.05, interactive=True, info=\"Higher values sample more low-probability tokens\"),\n",
        "    gr.Slider(label=\"Repetition penalty\", value=1.2, minimum=1.0, maximum=2.0, step=0.05, interactive=True, info=\"Penalize repeated tokens\")\n",
        "]\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=generate,\n",
        "    chatbot=gr.Chatbot(show_label=False, show_share_button=False, show_copy_button=True, likeable=True, layout=\"panel\"),\n",
        "    additional_inputs=additional_inputs,\n",
        "    title=\"Mixtral 46.7B\",\n",
        "    concurrency_limit=20,\n",
        ").launch(show_api=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Mixtral-8x7B huggingface API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B_huggingface_API.webp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2023-12-30",
      "description_en": "Discover the fashion model in the world of AI",
      "description_es": "Descubre el modelo de moda en el mundo de la IA",
      "description_pt": "Descubra o modelo da moda no mundo da IA",
      "end_url": "mixtral-8x7b",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B.webp",
      "keywords_en": "mixtral-8x7b, mixtral, 8x7b, mistral ai, transformer, MoE, mixture of experts, huggingface, gradio",
      "keywords_es": "mixtral-8x7b, mixtral, 8x7b, mistral ai, transformer, MoE, mixture of experts, huggingface, gradio",
      "keywords_pt": "mixtral-8x7b, mixtral, 8x7b, mistral ai, transformer, MoE, mixture of experts, huggingface, gradio",
      "title_en": "Mixtral-8x7B",
      "title_es": "Mixtral-8x7B",
      "title_pt": "Mixtral-8x7B"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}