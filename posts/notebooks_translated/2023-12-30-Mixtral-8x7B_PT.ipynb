{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# Mixtral-8x7B MoE"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para mim, a melhor descrição do `mixtral-8x7b` é a seguinte imagem\n",
"\n",
"![mixtral-gemini](http://maximofn.com/wp-content/uploads/2023/12/mixtral-gemini.webp)\n",
"\n",
"Entre o lançamento do `gemini` e o lançamento do `mixtra-8x7b`, houve uma diferença de apenas alguns dias. Nos dois primeiros dias após o lançamento do `gemini` houve muita conversa sobre esse modelo, mas assim que o `mixtral-8x7b` foi lançado, o `gemini` foi completamente esquecido e toda a comunidade estava falando sobre o `mixtral-8x7b`.\n",
"\n",
"E não é de se admirar que, analisando seus benchmarks, podemos ver que ele está no nível de modelos como o `llama2-70B` e o `GPT3.5`, mas com a diferença de que, enquanto o `mixtral-8x7b` tem apenas 46,7B de parâmetros, o `llama2-70B` tem 70B e o `GPT3.5` tem 175B.\n",
"\n",
"![mixtral benchmarks](https://mistral.ai/images/news/mixtral-of-experts/overview.png)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..\n",
"\n",
"## Número de parâmetros"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como o nome sugere, `mixtral-8x7b` é um conjunto de 8 modelos de parâmetros 7B, portanto, poderíamos pensar que ele tem 56B parâmetros (7Bx8), mas não tem. Como [Andrej Karpathy] (https://twitter.com/karpathy/status/1734251375163511203) explica, somente os blocos `Feed forward` dos transformadores são multiplicados por 8, o restante dos parâmetros é compartilhado entre os 8 modelos. Portanto, no final, o modelo tem 46,7 bilhões de parâmetros."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Mistura de especialistas (MoE)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Conforme mencionado, o modelo é um conjunto de 8 modelos 7B de parâmetros, daí a sigla `MoE`, que significa `Mixture of Experts`. Cada um dos 8 modelos é treinado de forma independente, mas quando a inferência é feita, um roteador decide a saída de qual modelo deve ser usado.\n",
"\n",
"A imagem a seguir mostra a arquitetura de um `Transformer`.\n",
"\n",
"![transformer](http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp)\n",
"\n",
"Se você não sabe, o importante é que essa arquitetura consiste em um codificador e um decodificador.\n",
"\n",
"![transformer-encoder-decoder](http://maximofn.com/wp-content/uploads/2023/12/transformer-encoder-decoder.webp)\n",
"\n",
"Os LLMs são modelos somente de decodificador, portanto, não têm um codificador. Você pode ver que na arquitetura há três módulos de atenção, um deles realmente conecta o codificador ao decodificador. Mas como os LLMs não têm um codificador, não há necessidade do módulo de atenção que conecta o decodificador e o decodificador.\n",
"\n",
"![transformer-decoder](http://maximofn.com/wp-content/uploads/2023/12/transformer-decoder.webp)\n",
"\n",
"Agora que sabemos como é a arquitetura de um LLM, podemos ver como é a arquitetura do `mixtral-8x7b`. Na imagem a seguir, podemos ver a arquitetura do modelo\n",
"\n",
"Arquitetura do MoE] (http://maximofn.com/wp-content/uploads/2023/12/MoE_architecture-scaled.webp)\n",
"\n",
"Como você pode ver, a arquitetura consiste em um decodificador `Transformer` de 7B parâmetros, sendo que apenas a camada `Feed forward` consiste em 8 camadas `Feed forward` com um roteador que escolhe qual das 8 camadas `Feed forward` deve ser usada. Na imagem acima, são mostradas apenas quatro camadas `Feed forward`, o que deve ter sido feito para simplificar o diagrama, mas, na realidade, existem 8 camadas `Feed forward`. Você também pode ver dois caminhos para duas palavras diferentes, a palavra `More` e a palavra `Parameters` e como o roteador escolhe qual `Feed forward` usar para cada palavra."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Observando a arquitetura, podemos entender por que o modelo tem 46,7 bilhões de parâmetros e não 56 bilhões. Como dissemos, apenas os blocos `Feed forward` são multiplicados por 8, o restante dos parâmetros é compartilhado entre os 8 modelos."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Usando o Mixtral-8x7b na nuvem"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Infelizmente, usar o `mixtral-8x7b` localmente é complicado, pois os requisitos de hardware são os seguintes\n",
"\n",
" * float32: VRAM > 180 GB, ou seja, como cada parâmetro ocupa 4 bytes, precisamos de 46,7B * 4 = 186,8 GB de VRAM apenas para armazenar o modelo, mais a VRAM necessária para armazenar os dados de entrada e saída.\n",
" * float16: VRAM > 90 GB, nesse caso, cada parâmetro ocupa 2 bytes, portanto, precisamos de 46,7B * 2 = 93,4 GB de VRAM apenas para armazenar o modelo, além da VRAM necessária para armazenar os dados de entrada e saída.\n",
" * 8 bits: VRAM > 45 GB, aqui cada parâmetro ocupa 1 byte, portanto, precisamos de 46,7B * 1 = 46,7 GB de VRAM apenas para armazenar o modelo, além da VRAM necessária para armazenar os dados de entrada e saída.\n",
" * 4 bits: VRAM > 23 GB, aqui cada parâmetro ocupa 0,5 bytes, portanto, precisamos de 46,7B * 0,5 = 23,35 GB de VRAM apenas para armazenar o modelo, além da VRAM necessária para armazenar os dados de entrada e saída.\n",
"\n",
"Precisamos de GPUs muito potentes para executá-lo, mesmo quando usamos o modelo quantizado de 4 bits."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Portanto, a maneira mais fácil de usar o `Mixtral-8x7B` é usá-lo já implantado na nuvem. Encontrei vários locais onde você pode usá-lo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Uso do Mixtral-8x7b no bate-papo huggingface"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O primeiro está no [huggingface chat] (https://huggingface.co/chat). Para usá-lo, clique na roda dentada dentro da caixa `Current Model` e selecione `Mistral AI - Mixtral-8x7B`. Uma vez selecionado, você pode começar a conversar com o modelo.\n",
"\n",
"![huggingface_chat_01](http://maximofn.com/wp-content/uploads/2023/12/huggingface_chat_01.webp)\n",
"\n",
"Uma vez lá dentro, selecione `mistralai/Mixtral-8x7B-Instruct-v0.1` e, por fim, clique no botão `Activate`. Agora podemos testar o modelo\n",
"\n",
"![huggingface_chat_02](http://maximofn.com/wp-content/uploads/2023/12/huggingface_chat_02.webp)\n",
"\n",
"Como você pode ver, perguntei a ele em espanhol o que é \"MoE\" e ele me explicou."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Usando o Mixtral-8x7b no Perplexity Labs"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Outra opção é usar o [Perplexity Labs] (https://labs.perplexity.ai/). Uma vez lá dentro, selecione `mixtral-8x7b-instruct` em um menu suspenso no canto inferior direito.\n",
"\n",
"![perplexity_labs](http://maximofn.com/wp-content/uploads/2023/12/perplexity_labs.webp)\n",
"\n",
"Como você pode ver, eu também perguntei a ele em espanhol o que é `MoE` e ele me explicou."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Usando o Mixtral-8x7b localmente por meio da API huggingface"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Uma maneira de usá-lo localmente, independentemente dos recursos de HW que você tenha, é por meio da API huggingface. Para fazer isso, você precisa instalar a biblioteca huggingface `huggingface-hub`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "%pip install huggingface-hub"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Aqui está uma implementação com o `gradio`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "from huggingface_hub import InferenceClient\n",
"import gradio as gr\n",
"\n",
"client = InferenceClient(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
"\n",
"def format_prompt(message, history):\n",
"  prompt = \"<s>\"\n",
"  for user_prompt, bot_response in history:\n",
"    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
"    prompt += f\" {bot_response}</s> \"\n",
"  prompt += f\"[INST] {message} [/INST]\"\n",
"  return prompt\n",
"\n",
"def generate(prompt, history, system_prompt, temperature=0.9, max_new_tokens=256, top_p=0.95, repetition_penalty=1.0,):\n",
"    temperature = float(temperature)\n",
"    if temperature < 1e-2:\n",
"        temperature = 1e-2\n",
"    top_p = float(top_p)\n",
"\n",
"    generate_kwargs = dict(temperature=temperature, max_new_tokens=max_new_tokens, top_p=top_p, repetition_penalty=repetition_penalty, do_sample=True, seed=42,)\n",
"\n",
"    formatted_prompt = format_prompt(f\"{system_prompt}, {prompt}\", history)\n",
"    stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=True, details=True, return_full_text=False)\n",
"    output = \"\"\n",
"\n",
"    for response in stream:\n",
"        output += response.token.text\n",
"        yield output\n",
"    return output\n",
"\n",
"additional_inputs=[\n",
"    gr.Textbox(label=\"System Prompt\", max_lines=1, interactive=True,),\n",
"    gr.Slider(label=\"Temperature\", value=0.9, minimum=0.0, maximum=1.0, step=0.05, interactive=True, info=\"Higher values produce more diverse outputs\"),\n",
"    gr.Slider(label=\"Max new tokens\", value=256, minimum=0, maximum=1048, step=64, interactive=True, info=\"The maximum numbers of new tokens\"),\n",
"    gr.Slider(label=\"Top-p (nucleus sampling)\", value=0.90, minimum=0.0, maximum=1, step=0.05, interactive=True, info=\"Higher values sample more low-probability tokens\"),\n",
"    gr.Slider(label=\"Repetition penalty\", value=1.2, minimum=1.0, maximum=2.0, step=0.05, interactive=True, info=\"Penalize repeated tokens\")\n",
"]\n",
"\n",
"gr.ChatInterface(\n",
"    fn=generate,\n",
"    chatbot=gr.Chatbot(show_label=False, show_share_button=False, show_copy_button=True, likeable=True, layout=\"panel\"),\n",
"    additional_inputs=additional_inputs,\n",
"    title=\"Mixtral 46.7B\",\n",
"    concurrency_limit=20,\n",
").launch(show_api=False)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Mixtral-8x7B huggingface API](http://maximofn.com/wp-content/uploads/2023/12/Mixtral-8x7B_huggingface_API-scaled.webp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
