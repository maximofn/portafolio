{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# Hugging Face transformers"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The `transformers` library from Hugging Face is one of the most popular libraries for working with language models. Its ease of use democratized the use of the `Transformer` architecture and made it possible to work with state-of-the-art language models without having to have a great deal of knowledge in the area.\n",
"\n",
"Between the `transformers` library, the model hub and its ease of use, the spaces and the ease of deploying demos, and new libraries like `datasets`, `accelerate`, `PEFT` and more, they have made Hugging Face one of the most important players in the AI scene at the moment. They call themselves \"the GitHub of AI\" and they certainly are."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.\n",
"\n",
"## Installation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To install transformers can be done with `pip`.\n",
"\n",
"````bash\n",
"pip install transformers\n",
"```\n",
"\n",
"or with `conda`.\n",
"\n",
"````bash\n",
"conda install conda-forge::transformers\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In addition to the library you need to have a PyTorch or TensorFlow backend installed. That is, you need to have `torch` or `tensorflow` installed to be able to use `transformers`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Inference with `pipeline"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With the `transformers` pipeline`s you can do inference with language models in a very simple way. This has the advantage that development is done much faster and prototyping can be done very easily. It also allows people who do not have much knowledge to be able to use the models."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With `pipeline` you can do inference in a lot of different tasks. Each task has its own `pipeline` (NLP `pipeline`, vision `pipeline`, etc), but a general abstraction can be made using the `pipeline` class which takes care of selecting the appropriate `pipeline` for the task passed to it."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tasks"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As of this writing, the tasks that can be done with `pipeline` are:\n",
"\n",
" * Audio:\n",
"   * Audio classification\n",
"     * acoustic scene classification: label audio with a scene label (\"office\", \"beach\", \"stadium\")\n",
"     * acoustic event detection: tag audio with a sound event tag (\"car horn\", \"whale call\", \"glass breaking\")\n",
"     * labeling: labeling audio containing various sounds (birdsong, speaker identification in a meeting)\n",
"     * music classification: labeling music with a genre label (\"metal\", \"hip-hop\", \"country\")\n",
"\n",
" * Automatic speech recognition (ASR, audio speech recognition):\n",
"\n",
" * Computer vision\n",
"   * Image classification\n",
"   * Object detection\n",
"   * Image segmentation\n",
"   * Depth estimation\n",
"\n",
" Natural language processing (NLP) * Natural language processing (NLP)\n",
"   * Text classification\n",
"     * sentiment analysis\n",
"     * content classification\n",
"   * Classification of tokens\n",
"     * Named Entity Recognition (NER): tag a token according to an entity category such as organization, person, location or date.\n",
"     * part-of-speech (POS) tagging: tagging a token according to its part of speech, such as noun, verb or adjective. POS is useful to help translation systems understand how two identical words are grammatically different (e.g., \"cut\" as a noun versus \"cut\" as a verb).\n",
"   * Answers to questions\n",
"     * extractive: given a question and some context, the answer is a fragment of text from the context that the model must extract.\n",
"     * abstract: given a question and some context, the answer is generated from the context; this approach is handled by the Text2TextGenerationPipeline instead of the QuestionAnsweringPipeline shown below.\n",
"   * Summarize\n",
"     * extractive: identifies and extracts the most important sentences from the original text\n",
"     * abstracting: generates the objective summary (which may include new words not present in the input document) from the original text\n",
"   * Translation\n",
"   * Language modeling\n",
"     * causal: the objective of the model is to predict the next token in a sequence, and future tokens are masked.\n",
"     * masked: the objective of the model is to predict a masked token in a sequence with full access to the tokens in the sequence.\n",
"\n",
" * Multimodal\n",
"   * Answers to document questions"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Use of `pipeline`"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The easiest way to create a `pipeline` is simply to tell it the task we want it to solve using the `task` parameter. And the library will take care of selecting the best model for that task, download it and save it in the cache for future use."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
"Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ],
      "source": [
      "from transformers import pipeline\n",
"\n",
"generator = pipeline(task=\"text-generation\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "[{'generated_text': 'Me encanta aprender de se résistance davant que hiens que préclase que ses encasas quécénces. Se présentants cet en un croyne et cela désirez'}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "generator(\"Me encanta aprender de\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see the generated text is in French, while I have introduced it in Spanish, so it is important to choose well the model. If you notice the library has taken the `openai-community/gpt2` model, which is a model trained mostly in English, and that when I put Spanish text in it, it got confused and generated a response in French."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We are going to use a model retrained in Spanish using the `model` parameter."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import pipeline\n",
"\n",
"generator = pipeline(task=\"text-generation\", model=\"flax-community/gpt-2-spanish\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "[{'generated_text': 'Me encanta aprender de tus palabras, que con gran entusiasmo y con el mismo conocimiento como lo que tú acabas escribiendo, te deseo de todo corazón todo el deseo de este día:\\nY aunque también haya personas a las que'}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "generator(\"Me encanta aprender de\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now the generated text looks much better"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The `pipeline` class has many possible parameters, so to see all of them and learn more about the class I recommend you to read its [documentation](https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines), but let's talk about one, because for deep learning it is very important and it is `device`. It defines the device (e.g. `cpu`, `cuda:1`, `mps` or an ordinal range of GPUs like `1`) on which the `pipeline` will be assigned.\n",
"\n",
"In my case, as I have a GPU I set `0`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de ustedes, a tal punto que he decidido escribir algunos de nuestros contenidos en este blog, el cual ha sido de gran utilidad para mí por varias razones, una de ellas, el trabajo\n"
          ]
        }
      ],
      "source": [
      "from transformers import pipeline\n",
"\n",
"generator = pipeline(task=\"text-generation\", model=\"flax-community/gpt-2-spanish\", device=0)\n",
"\n",
"generation = generator(\"Me encanta aprender de\")\n",
"print(generation[0]['generated_text'])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### How `pipeline` works"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When we make use of `pipeline` below what is happening is this\n",
"\n",
"![transformers-pipeline](http://maximofn.com/wp-content/uploads/2024/02/transformers-pipeline.svg)\n",
"\n",
"Text is automatically tokenized, passed through the model and then post-processed."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Inference with `AutoClass` and `pipeline`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We have seen that `pipeline` abstracts a lot of what happens, but we can select which tokenizer, which model and which postprocessing we want to use."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokenization with `AutoTokenizer`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Before we used the `flax-community/gpt-2-spanish` model to generate text, we can use its tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "{'input_ids': tensor([[ 2879,  4835,   382,   288,  2383, 15257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"flax-community/gpt-2-spanish\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"\n",
"text = \"Me encanta lo que estoy aprendiendo\"\n",
"\n",
"tokens = tokenizer(text, return_tensors=\"pt\")\n",
"print(tokens)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### `AutoModel` Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we can create the model and pass the tokens to it."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,\n",
" odict_keys(['last_hidden_state', 'past_key_values']))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from transformers import AutoModel\n",
"\n",
"model = AutoModel.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"\n",
"output = model(**tokens)\n",
"type(output), output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we now try to use it in a `pipeline` we will get an error."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "The model 'GPT2Model' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
{
          "ename": "TypeError",
          "evalue": "The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
"Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMe encanta aprender de\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1323\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         synced_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_class()\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# three conditions must be met\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# 1) the generation config must have been created from the model config (`_from_model_config` field);\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# 2) the generation config must have seen no modification since its creation (the hash is the same);\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# 3) the user must have set generation parameters in the model config.\u001b[39;00m\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1110\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m   1109\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1110\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
"\u001b[0;31mTypeError\u001b[0m: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
          ]
        }
      ],
      "source": [
      "from transformers import pipeline\n",
"\n",
"pipeline(\"text-generation\", model=model, tokenizer=tokenizer)(\"Me encanta aprender de\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This is because when it worked we used\n",
"\n",
"````python\n",
"pipeline(task=\"text-generation\", model=\"flax-community/gpt-2-spanish\")\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "But now we have made\n",
"\n",
"````python\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"model = AutoModel.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In the first case we just used `pipeline` and the model name, underneath we were looking for the best way to implement the model and the tokenizer. But in the second case we have created the tokenizer and the model and passed it to `pipeline`, but we have not created them well for what the `pipeline` needs.\n",
"\n",
"To fix this we use `AutoModelFor`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### `AutoModelFor` Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The transformers library gives us the opportunity to create a model for a given task such as\n",
"\n",
" * `AutoModelForCausalLM` used to continue texts\n",
" * `AutoModelForMaskedLM` used for gap filling\n",
" * `AutoModelForMaskGeneration` which is used to generate masks.\n",
" * AutoModelForSeq2SeqLM, which is used to convert from sequences to sequences, as for example in translation.\n",
" * `AutoModelForSequenceClassification` for text classification\n",
" * `AutoModelForMultipleChoice` for multiple choice\n",
" * `AutoModelForNextSentencePrediction` to predict whether two sentences are consecutive.\n",
" * `AutoModelForTokenClassification` for token classification\n",
" * `AutoModelForQuestionAnswering` for questions and answers\n",
" * `AutoModelForTextEncoding` for text encoding"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's use the above model to generate text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "'Me encanta aprender de mi familia.\\nLa verdad no sabía que se necesitaba tanto en este pequeño restaurante ya que mi novio en un principio había ido, pero hoy me ha entrado un gusanillo entre pecho y espalda que'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"from transformers import pipeline\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"\n",
"pipeline(\"text-generation\", model=model, tokenizer=tokenizer)(\"Me encanta aprender de\")[0]['generated_text']"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now it works, because we have created the model in a way that `pipeline` can understand."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Inference with `AutoClass` only"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Earlier we created the model and tokenizer and gave it to `pipeline` to do the necessary underneath, but we can use the methods for inference ourselves."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Generation of casual text"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the model and the tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With `device_map`, we have loaded the model on GPU 0"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we have to do what `pipeline` used to do."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First we generate the tokens"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens_input \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMe encanta aprender de\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2828\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2829\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2911\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2912\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2913\u001b[0m         )\n\u001b[1;32m   2914\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2916\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2917\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2918\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2919\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2920\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2921\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2922\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2923\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2924\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2925\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2926\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2927\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2928\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2929\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2930\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2931\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2932\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2933\u001b[0m     )\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2936\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2937\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2953\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2954\u001b[0m     )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3097\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3080\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   3082\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[38;5;124;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3097\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3098\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3099\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   3100\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3101\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3102\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3104\u001b[0m )\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3107\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3108\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3124\u001b[0m )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2734\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2738\u001b[0m     )\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2742\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2746\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2747\u001b[0m ):\n",
"\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ],
      "source": [
      "tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that it has given us an error, it tells us that the tokenizer does not have a padding token. Most LLMs do not have a padding token, but to use the `transformers` library a padding token is necessary, so what is usually done is to assign the end-of-statement token to the padding token."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we can generate the tokens"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([[2879, 4835, 3760,  225,   72,   73]], device='cuda:0')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_input.input_ids"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we pass them to the model that will generate the new tokens, for that we use the `generate` method"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input tokens: tensor([[2879, 4835, 3760,  225,   72,   73]], device='cuda:0')\n",
"output tokens: tensor([[ 2879,  4835,  3760,   225,    72,    73,   314,  2533,    16,   287,\n",
"           225,    73,    82,   513,  1086,   225,    72,    73,   314,   288,\n",
"           357, 15550,    16,   287,   225,    73,    87,   288,   225,    73,\n",
"            82,   291,  3500,    16,   225,    73,    87,   348,   929,   225,\n",
"            72,    73,  3760,   225,    72,    73,   314,  2533,    18,   203]],\n",
"       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
      "tokens_output = model.generate(**tokens_input, max_length=50)\n",
"print(f\"input tokens: {tokens_input.input_ids}\")\n",
"print(f\"output tokens: {tokens_output}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that the first tokens of `token_inputs` are the same as those of `token_outputs`, the following tokens are those generated by the model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we have to convert those tokens to a statement using the tokenizer decoder"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "'Me encanta aprender de los demás, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los demás.\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"sentence_output"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We already have the generated text"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Text classification"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the model and the tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n",
"model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\", device_map=0)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We generate tokens"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
      "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
"inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have the tokens, we classify"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "'LABEL_1'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "with torch.no_grad():\n",
"    logits = model(**inputs).logits\n",
"\n",
"predicted_class_id = logits.argmax().item()\n",
"prediction = model.config.id2label[predicted_class_id]\n",
"prediction"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at the classes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{0: 'LABEL_0', 1: 'LABEL_1'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "clases = model.config.id2label\n",
"clases"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This way there is no one to find out, so we modify it."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "model.config.id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And now we go back to sorting"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "'POSITIVE'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "with torch.no_grad():\n",
"    logits = model(**inputs).logits\n",
"\n",
"predicted_class_id = logits.argmax().item()\n",
"prediction = model.config.id2label[predicted_class_id]\n",
"prediction"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Classification of tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the model and the tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n",
"model = AutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\", device_map=0)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We generate tokens"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
      "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\"\n",
"inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have the tokens, we classify"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "101 ([CLS]) -> O\n",
"1996 (the) -> O\n",
"3585 (golden) -> B-location\n",
"2110 (state) -> I-location\n",
"6424 (warriors) -> B-group\n",
"2024 (are) -> O\n",
"2019 (an) -> O\n",
"2137 (american) -> O\n",
"2658 (professional) -> O\n",
"3455 (basketball) -> O\n",
"2136 (team) -> O\n",
"2241 (based) -> O\n",
"1999 (in) -> O\n",
"2624 (san) -> B-location\n",
"3799 (francisco) -> B-location\n",
"1012 (.) -> O\n",
"102 ([SEP]) -> O\n"
          ]
        }
      ],
      "source": [
      "with torch.no_grad():\n",
"    logits = model(**inputs).logits\n",
"\n",
"predictions = torch.argmax(logits, dim=2)\n",
"predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
"for i in range(len(inputs.input_ids[0])):\n",
"    print(f\"{inputs.input_ids[0][i]} ({tokenizer.decode([inputs.input_ids[0][i]])}) -> {predicted_token_class[i]}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see the tokens corresponding to `golden`, `state`, `warriors`, `san` and `francisco` have been classified as location tokens."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Question answering"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the model and the tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of the model checkpoint at mrm8488/roberta-base-1B-1-finetuned-squadv1 were not used when initializing RobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
"- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
"- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/roberta-base-1B-1-finetuned-squadv1\")\n",
"model = AutoModelForQuestionAnswering.from_pretrained(\"mrm8488/roberta-base-1B-1-finetuned-squadv1\", device_map=0)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We generate tokens"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
      "question = \"How many programming languages does BLOOM support?\"\n",
"context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
"inputs = tokenizer(question, context, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have the tokens, we classify"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "' 13'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "with torch.no_grad():\n",
"    outputs = model(**inputs)\n",
"\n",
"answer_start_index = outputs.start_logits.argmax()\n",
"answer_end_index = outputs.end_logits.argmax()\n",
"\n",
"predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
"tokenizer.decode(predict_answer_tokens)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Masked language modeling (Masked language modeling)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the model and the tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of the model checkpoint at nyu-mll/roberta-base-1B-1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
"- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
"- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"nyu-mll/roberta-base-1B-1\")\n",
"model = AutoModelForMaskedLM.from_pretrained(\"nyu-mll/roberta-base-1B-1\", device_map=0)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We generate tokens"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
      "text = \"The Milky Way is a <mask> galaxy.\"\n",
"inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
"mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have the tokens, we classify"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "The Milky Way is a  spiral galaxy.\n",
"The Milky Way is a  closed galaxy.\n",
"The Milky Way is a  distant galaxy.\n"
          ]
        }
      ],
      "source": [
      "with torch.no_grad():\n",
"    logits = model(**inputs).logits\n",
"    mask_token_logits = logits[0, mask_token_index, :]\n",
"\n",
"top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
"for token in top_3_tokens:\n",
"    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Model customization"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Earlier we have done the inference with `AutoClass`, but we have done it with the default model settings. But we can configure the model as much as we like"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's instantiate a model and see its configuration"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "GPT2Config {\n",
"  \"_name_or_path\": \"flax-community/gpt-2-spanish\",\n",
"  \"activation_function\": \"gelu_new\",\n",
"  \"architectures\": [\n",
"    \"GPT2LMHeadModel\"\n",
"  ],\n",
"  \"attn_pdrop\": 0.0,\n",
"  \"bos_token_id\": 50256,\n",
"  \"embd_pdrop\": 0.0,\n",
"  \"eos_token_id\": 50256,\n",
"  \"gradient_checkpointing\": false,\n",
"  \"initializer_range\": 0.02,\n",
"  \"layer_norm_epsilon\": 1e-05,\n",
"  \"model_type\": \"gpt2\",\n",
"  \"n_ctx\": 1024,\n",
"  \"n_embd\": 768,\n",
"  \"n_head\": 12,\n",
"  \"n_inner\": null,\n",
"  \"n_layer\": 12,\n",
"  \"n_positions\": 1024,\n",
"  \"reorder_and_upcast_attn\": false,\n",
"  \"resid_pdrop\": 0.0,\n",
"  \"scale_attn_by_inverse_layer_idx\": false,\n",
"  \"scale_attn_weights\": true,\n",
"  \"summary_activation\": null,\n",
"  \"summary_first_dropout\": 0.1,\n",
"  \"summary_proj_to_labels\": true,\n",
"  \"summary_type\": \"cls_index\",\n",
"  \"summary_use_proj\": true,\n",
"  \"task_specific_params\": {\n",
"    \"text-generation\": {\n",
"      \"do_sample\": true,\n",
"      \"max_length\": 50\n",
"    }\n",
"  },\n",
"  \"transformers_version\": \"4.38.1\",\n",
"  \"use_cache\": true,\n",
"  \"vocab_size\": 50257\n",
"}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"config = AutoConfig.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"\n",
"config"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see the model configuration, for example the activation function is `gelu_new`, it has 12 `head`s, the vocabulary size is 50257 words, etc."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "But we can modify this configuration"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "'relu'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "config = AutoConfig.from_pretrained(\"flax-community/gpt-2-spanish\", activation_function=\"relu\")\n",
"config.activation_function"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We now create the model with this configuration"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
      "model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", config=config, device_map=0)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And we generate text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "'Me encanta aprender de la d d e d e d e d e d e d e d e d e d e d e '"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokenizer.pad_token = tokenizer.eos_token\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_length=50)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"sentence_output"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that this modification does not generate as good text."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Tokenization"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "So far we have seen the different ways to do inference with the `transformers` library. Now we are going to get into the guts of the library. To do this we are first going to look at things to keep in mind when tokenizing.\n",
"\n",
"We are not going to explain what tokenizing is in depth, since we have already explained it in the post on the [tokenizers] library (https://maximofn.com/hugging-face-tokenizers/)."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Padding"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When you have a batch of sequences, sometimes it is necessary that after tokenizing, all the sequences have the same length, so for this we use the `padding=True` parameter."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "[2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]\n",
"[1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]\n",
"[1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]\n",
"Padding token id: 50257\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"batch_sentences = [\n",
"    \"Pero, ¿qué pasa con el segundo desayuno?\",\n",
"    \"No creo que sepa lo del segundo desayuno, Pedro\",\n",
"    \"¿Qué hay de los elevensies?\",\n",
"]\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\", pad_token=\"PAD\")\n",
"encoded_input = tokenizer(batch_sentences, padding=True)\n",
"for encoded in encoded_input[\"input_ids\"]:\n",
"    print(encoded)\n",
"print(f\"Padding token id: {tokenizer.pad_token_id}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see, he has added paddings to the first two sequences at the end."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Truncated"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Besides adding padding, sometimes it is necessary to truncate the sequences so that they do not occupy more than a certain number of tokens. To do this we set `truncation=True` and `max_length` to the number of tokens we want the sequence to have."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "[2959, 16, 875, 3736, 3028]\n",
"[1489, 2275, 288, 12052, 382]\n",
"[1699, 2899, 707, 225, 72]\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"batch_sentences = [\n",
"    \"Pero, ¿qué pasa con el segundo desayuno?\",\n",
"    \"No creo que sepa lo del segundo desayuno, Pedro\",\n",
"    \"¿Qué hay de los elevensies?\",\n",
"]\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"encoded_input = tokenizer(batch_sentences, truncation=True, max_length=5)\n",
"for encoded in encoded_input[\"input_ids\"]:\n",
"    print(encoded)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Same sentences as before, now generate fewer tokens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tensors"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Until now we were receiving lists of tokens, but surely we are interested in receiving tensors from PyTorch or TensorFlow. For this we use the parameter `return_tensors` and we specify from which framework we want to receive the tensor, in our case we will choose PyTorch with `pt`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We first see without specifying that we return tensors"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "<class 'list'>\n",
"<class 'list'>\n",
"<class 'list'>\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"batch_sentences = [\n",
"    \"Pero, ¿qué pasa con el segundo desayuno?\",\n",
"    \"No creo que sepa lo del segundo desayuno, Pedro\",\n",
"    \"¿Qué hay de los elevensies?\",\n",
"]\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\", pad_token=\"PAD\")\n",
"encoded_input = tokenizer(batch_sentences, padding=True)\n",
"for encoded in encoded_input[\"input_ids\"]:\n",
"    print(type(encoded))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We receive lists, if we want to receive tensors from PyTorch we use `return_tensors=\"pt\"`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "<class 'torch.Tensor'> torch.Size([12])\n",
"<class 'torch.Tensor'> torch.Size([12])\n",
"<class 'torch.Tensor'> torch.Size([12])\n",
"<class 'torch.Tensor'> torch.Size([3, 12])\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"batch_sentences = [\n",
"    \"Pero, ¿qué pasa con el segundo desayuno?\",\n",
"    \"No creo que sepa lo del segundo desayuno, Pedro\",\n",
"    \"¿Qué hay de los elevensies?\",\n",
"]\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\", pad_token=\"PAD\")\n",
"encoded_input = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")\n",
"for encoded in encoded_input[\"input_ids\"]:\n",
"    print(type(encoded), encoded.shape)\n",
"print(type(encoded_input[\"input_ids\"]), encoded_input[\"input_ids\"].shape)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Masks"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When we tokenize a statement we not only get the `input_ids`, but we also get the attention mask. The attention mask is a tensor that has the same size as `input_ids` and has a `1` in the positions that are tokens and a `0` in the positions that are padding."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "padding token id: 50257\n",
"\n",
"encoded_input[0] inputs_ids: [2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]\n",
"encoded_input[0] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
"\n",
"encoded_input[1] inputs_ids: [1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]\n",
"encoded_input[1] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
"\n",
"encoded_input[2] inputs_ids: [1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]\n",
"encoded_input[2] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"batch_sentences = [\n",
"    \"Pero, ¿qué pasa con el segundo desayuno?\",\n",
"    \"No creo que sepa lo del segundo desayuno, Pedro\",\n",
"    \"¿Qué hay de los elevensies?\",\n",
"]\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\", pad_token=\"PAD\")\n",
"encoded_input = tokenizer(batch_sentences, padding=True)\n",
"\n",
"print(f\"padding token id: {tokenizer.pad_token_id}\")\n",
"print(f\"\\nencoded_input[0] inputs_ids: {encoded_input['input_ids'][0]}\")\n",
"print(f\"encoded_input[0] attention_mask: {encoded_input['attention_mask'][0]}\")\n",
"print(f\"\\nencoded_input[1] inputs_ids: {encoded_input['input_ids'][1]}\")\n",
"print(f\"encoded_input[1] attention_mask: {encoded_input['attention_mask'][1]}\")\n",
"print(f\"\\nencoded_input[2] inputs_ids: {encoded_input['input_ids'][2]}\")\n",
"print(f\"encoded_input[2] attention_mask: {encoded_input['attention_mask'][2]}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see, in the first two sentences, we have a 1 in the first positions and a 0 in the last two positions. In those same positions we have the token `50257`, which corresponds to the padding token."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With these attention masks we are telling the model which tokens to pay attention to and which not to pay attention to."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Text generation could still be done if we did not pass these attention masks, the `generate` method would do its best to infer this mask, but if we pass it we help to generate better text."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Fast Tokenizers"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Some pretrained tokenizers have a `fast` version, they have the same methods as the normal ones, only they are developed in Rust. To use them we must use the `PreTrainedTokenizerFast` class of the `transformers` library."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let us first look at the tokenization time with a normal tokenizer."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "CPU times: user 55.3 ms, sys: 8.58 ms, total: 63.9 ms\n",
"Wall time: 226 ms\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"from transformers import BertTokenizer\n",
"\n",
"tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
"\n",
"sentence = (\n",
"    \"The Permaculture Design Principles are a set of universal design principles \"\n",
"    \"that can be applied to any location, climate and culture, and they allow us to design \"\n",
"    \"the most efficient and sustainable human habitation and food production systems. \"\n",
"    \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n",
"    \"as ecology, landscape design, environmental science and energy conservation, and the \"\n",
"    \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n",
"    \"design principle itself embodies a complete conceptual framework based on sound \"\n",
"    \"scientific principles. When we bring all these separate  principles together, we can \"\n",
"    \"create a design system that both looks at whole systems, the parts that these systems \"\n",
"    \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n",
"    \"living system. Each design principle serves as a tool that allows us to integrate all \"\n",
"    \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n",
"    \"whole system, where the elements harmoniously interact and work together in the most \"\n",
"    \"efficient way possible.\"\n",
")\n",
"\n",
"tokens = tokenizer([sentence], padding=True, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And now with a quick one"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "CPU times: user 42.6 ms, sys: 3.26 ms, total: 45.8 ms\n",
"Wall time: 179 ms\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"from transformers import BertTokenizerFast\n",
"\n",
"tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
"\n",
"sentence = (\n",
"    \"The Permaculture Design Principles are a set of universal design principles \"\n",
"    \"that can be applied to any location, climate and culture, and they allow us to design \"\n",
"    \"the most efficient and sustainable human habitation and food production systems. \"\n",
"    \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n",
"    \"as ecology, landscape design, environmental science and energy conservation, and the \"\n",
"    \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n",
"    \"design principle itself embodies a complete conceptual framework based on sound \"\n",
"    \"scientific principles. When we bring all these separate  principles together, we can \"\n",
"    \"create a design system that both looks at whole systems, the parts that these systems \"\n",
"    \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n",
"    \"living system. Each design principle serves as a tool that allows us to integrate all \"\n",
"    \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n",
"    \"whole system, where the elements harmoniously interact and work together in the most \"\n",
"    \"efficient way possible.\"\n",
")\n",
"\n",
"tokens = tokenizer([sentence], padding=True, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "You can see how the `BertTokenizerFast` is about 40 ms faster."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Text generation forms"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We continue with the guts of the `transformers` library, now we are going to see the ways to generate text.\n",
"\n",
"The transformer architecture generates the next most likely token, this is the simplest way to generate text, but it is not the only one, so let's look at them.\n",
"\n",
"When it comes to generating textno there is no best way and it will depend on our model and the purpose of use."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Greedy Search"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It is the simplest way of text generation. It looks for the most probable token in each iteration.\n",
"\n",
"![greedy_search](http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To generate text in this way with `transformers` you don't have to do anything special, as it is the default way."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los demás.\n",
"En este caso, el objetivo de la actividad es que los niños aprendan a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños se han dado cuenta de que los animales que hay en el mundo, son muy difíciles de reconocer, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.\n",
"En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que e\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "You can see that the generated text is fine, but it starts to repeat. This is because in greedy search, words with a high probability can hide behind words with a lower probability, so they can get lost.\n",
"\n",
"![greedy_search](http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp)\n",
"\n",
"Here, the word `has` has a high probability, but is hidden behind `dog`, which has a lower probability than `nice`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Contrastive Search"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The Contrastive Search method optimizes text generation by selecting word or phrase options that maximize a quality criterion over less desirable ones. In practice, this means that during text generation, at each step, the model not only searches for the next word that is most likely to follow as learned during its training, but also compares different candidates for that next word and evaluates which of them would contribute to form the most coherent, relevant and high quality text in the given context. Therefore, Contrastive Search reduces the possibility of generating irrelevant or low-quality responses by focusing on those options that best fit the text generation goal, based on a direct comparison between possible continuations at each step of the process."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To generate text with contrastive search in `transformers` you have to use `penalty_alpha` and `top_k` parameters when generating text."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás, es una de las cosas que más me gusta del mundo.\n",
"En la clase de hoy he estado haciendo un repaso de lo que es el arte de la costura, para que podáis ver como se hace una prenda de ropa y como se confeccionan los patrones.\n",
"El patrón de esta blusa es de mi amiga Marga, que me ha pedido que os enseñara a hacer este tipo de prendas, ya que es una de las cosas que más me gusta del mundo.\n",
"La blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasión.\n",
"Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.\n",
"En la parte delantera de la blusa, cosí un lazo de raso de color azul marino, que le da un toque de color a la prenda.\n",
"Como podéis ver en la foto, el patrón de esta blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasión.\n",
"Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.\n",
"En la parte delantera de la blusa utilicé un lazo de raso de color azul marino, que le da un toque de color a la prenda.\n",
"Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.\n",
"En la parte delantera de la blusa utilicé un lazo de raso de color azul marino, que le da un toque de color a la prenda.\n",
"Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.\n",
"En la parte delantera de la blusa utilicé\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, penalty_alpha=0.6, top_k=4)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Here the pattern takes longer to start to repeat itself"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Multinomial sampling"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Unlike greedy search that always chooses a token with the highest probability as the next token, multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution of the entire vocabulary provided by the model. Each token with a non-zero probability has a chance of being selected, which reduces the risk of repetition.\n",
"\n",
"To enable `Multinomial sampling` you have to set `do_sample=True` and `num_beams=1`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los de siempre y conocer a gente nueva, soy de las que no tiene mucho contacto con los de antes, pero he estado bastante liada con el diseño de mi página web de lo que sería el logo, he escrito varios diseños para otros blogs y cosas así, así que a ver si pronto puedo poner de mi parte alguna ayuda.\n",
"A finales de los años 70 del pasado siglo los arquitectos alemanes Hermann Grossberg y Heinrich Rindsner eran los principales representantes de la arquitectura industrial de la alta sociedad. La arquitectura industrial era la actividad que más rápido progresaba en el diseño, y de ellos destacaban los diseños que Grossberg llevó a cabo en el prestigioso Hotel Marigal.\n",
"De acuerdo con las conclusiones y opiniones expuestas por los autores sobre el reciente congreso sobre historia del diseño industrial, se ha llegado al convencimiento de que en los últimos años, los diseñadores industriales han descubierto muchas nuevas formas de entender la arquitectura. En palabras de Klaus Eindhoven, director general de la fundación alemana G. Grossberg, “estamos tratando de desarrollar un trabajo que tenga en cuenta los criterios más significativos de la teoría arquitectónica tradicional”.\n",
"En este artículo de opinión, Eindhoven y Grossberg explican por qué el auge de la arquitectura industrial en Alemania ha generado una gran cantidad de nuevos diseños de viviendas, de grandes dimensiones, de edificios de gran valor arquitectónico. Los más conocidos son los de los diseñadores Walter Nachtmann (1934) e ingeniero industrial, Frank Gehry (1929), arquitecto que ideó las primeras viviendas de estilo neoclásico en la localidad británica de Stegmarbe. Son viviendas de los siglos XVI al XX, algunas con un estilo clasicista que recuerda las casas de Venecia. Se trata de edificios con un importante valor histórico y arquitectónico, y que representan la obra de la técnica del modernismo.\n",
"La teoría general sobre los efectos de la arquitectura en un determinado tipo de espacio no ha resultado ser totalmente transparente, y mucho menos para los arquitectos, que tienen que aprender de los arquitectos de ayer, durante esos\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, num_beams=1)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The truth is that the model does not repeat itself at all, but I feel like I'm talking to a small child, who talks about one subject and then starts spinning off with others that have nothing to do with it."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Beam search"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Beam search reduces the risk of missing high probability hidden word sequences by keeping the most probable `num_beams` at each time step and finally choosing the hypothesis that has the highest overall probability.\n",
"\n",
"To generate with `beam search` it is necessary to add the parameter `num_beams`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.\n",
"Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, num_beams=5)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It repeats itself a lot"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Beam search multinomial sampling"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This technique combines the `bean search` where a beam search and `multinomial sampling` where the next token is randomly selected based on the probability distribution of the entire vocabulary provided by the model."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y e\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, num_beams=5, do_sample=True)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It repeats itself a lot"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Beam search n-grams penalty"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To avoid repetition we can penalize for n-gram repetition. For this we use the parameter `no_repeat_ngram_size`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.\n",
"En primer lugar, me hice con un libro que se llama \"El mundo eslavo\" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).\n",
"El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada \"La ciudad bizantina\", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el \"Imperio Romano\".\n",
"Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/\n",
"Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]\n",
"¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo, pero\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, num_beams=5, no_repeat_ngram_size=2)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This text no longer repeats itself and also has a little more coherence.\n",
"\n",
"However, n-gram penalties should be used with care. An article generated about New York City should not use a 2-gram penalty or else the name of the city would only appear once in the entire text!"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Beam search n-grams penalty return sequences"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can generate several sequences to compare them and keep the best one. For this we use the parameter `num_return_sequences` with the condition that `num_return_sequences <= num_beams`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "0: Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.\n",
"En primer lugar, me hice con un libro que se llama \"El mundo eslavo\" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).\n",
"El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada \"La ciudad bizantina\", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el \"Imperio Romano\".\n",
"Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/\n",
"Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]\n",
"¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo, pero\n",
"\n",
"\n",
"\n",
"1: Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.\n",
"En primer lugar, me hice con un libro que se llama \"El mundo eslavo\" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).\n",
"El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada \"La ciudad bizantina\", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el \"Imperio Romano\".\n",
"Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/\n",
"Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]\n",
"¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo para hacer\n",
"\n",
"\n",
"\n",
"2: Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.\n",
"En primer lugar, me hice con un libro que se llama \"El mundo eslavo\" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).\n",
"El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada \"La ciudad bizantina\", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el \"Imperio Romano\".\n",
"Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/\n",
"Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]\n",
"¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo para publicar\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_outputs = model.generate(**tokens_input, max_new_tokens=500, num_beams=5, no_repeat_ngram_size=2, num_return_sequences=3)\n",
"\n",
"for i, tokens_output in enumerate(tokens_outputs):\n",
"    if i != 0:\n",
"        print(\"\\n\\n\")\n",
"    sentence_output = tokenizer.decode(tokens_output, skip_special_tokens=True)\n",
"    print(f\"{i}: {sentence_output}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we can keep the best sequence"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Diverse beam search decoding"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The diverse beam search decoding is an extension of the beam search strategy that allows to generate a more diverse set of beam sequences to choose from.\n",
"\n",
"In order to generate text in this way we have to use the parameters `num_beams`, `num_beam_groups`, and `diversity_penalty`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, num_beams=5, num_beam_groups=5, diversity_penalty=1.0)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This method seems to be repeated quite often"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Speculative Decoding"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Speculative decoding (also known as assisted decoding) is a modification of the above decoding strategies, which uses an assistant model (ideally a much smaller one) with the same tokenizer, to generate some candidate tokens. Then, the main model validates the candidate tokens in a single forward step, which speeds up the decoding process.\n",
"\n",
"To generate text in this way it is necessary to use the parameter `do_sample=True`.\n",
"\n",
"Currently, assisted decoding only supports greedy search, and assisted decoding does not support batch entries."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás! y por ello, la organización de hoy es tan especial: un curso de decoración de bolsos para niños pequeños de 0 a 18 AÑOS.\n",
"En este taller aprenderemos a decorar bolsos para regalar, con los materiales que sean necesarios para cubrir las necesidades de estos peques, como pueden ser, un estuche con todo lo que necesiten, ropa interior, mantas, complementos textiles, complementos alimenticios, o un bonito neceser con todo lo que necesiten.\n",
"Os dejo con un pequeño tutorial de decoración de bolsos para niños, realizado por mi amiga Rosa y sus amigas Silvia y Rosa, que se dedica a la creación de bolsos para bebés que son un verdadero tesoro para sus pequeños. Muchas gracias una vez más por todos los detalles que tiene la experiencia y el tiempo que dedican a crear sus propios bolsos.\n",
"En muchas ocasiones, cuando se nos acerca una celebración, siempre nos preguntamos por qué, por qué en especial, por que se trata de algo que no tienen tan cerca nuestras vidas y, claro está, también por que nos hemos acostumbrado a vivir en el mundo de lo mundano y de lo comercial, tal y como los niños y niñas de hoy, a la manera de sus padres, donde todo es caro, todo es difícil, los precios no están al alcance de todos y, por estas y por muchas más preguntas por las que estamos deseando seguir escuchando, este curso y muchas otras cosas que os encontraréis a lo largo de la mañana de hoy, os van a dar la clave sobre la que empezar a preparar una fiesta de esta importancia.\n",
"El objetivo del curso es que aprendáis a decorar bolsos para regalar con materiales sencillos, simples y de buena calidad; que os gusten y os sirvan de decoración y que por supuesto os sean útiles. Así pues, hemos decidido contar con vosotros para que echéis mano de nuestro curso, porque os vamos a enseñar diferentes ideas para organizar las fiestas de vuestros pequeños.\n",
"Al tratarse de un curso muy básico, vais a encontrar ideas muy variadas, que van desde sencillas manualidades con los bolsillos, hasta mucho más elaboradas y que si lo veis con claridad en un tutorial os vais a poder dar una idea de cómo se ha de aplicar estos consejos a vuestra tienda.\n",
"\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"assistant_model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, assistant_model=assistant_model, do_sample=True)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This method has very good results"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Speculative Decoding randomness control"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When using assisted decoding with sampling methods, the `temperature` parameter can be used to control randomness. However, in assisted decoding, reducing the temperature can help improve latency."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás y de las personas que nos rodean. Y no sólo eso, sino que además me gusta aprender de los demás. He aprendido mucho de los que me rodean y de las personas que me rodean.\n",
"Me encanta conocer gente nueva, aprender de los demás y de las personas que me rodean. Y no sólo eso, sino que además me gusta aprender de los demás.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"Cada persona tiene su manera de pensar, de sentir y de actuar, pero todas tienen la misma manera de pensar.\n",
"La mayoría de las personas, por diferentes motivos, se quieren llevar bien con otras personas, pero no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.\n",
"En el mundo \n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"assistant_model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Here it has not done so well"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Sampling"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This is where the techniques used by today's LLMs begin."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instead of always selecting the most likely word (which could lead to predictable or repetitive texts), sampling introduces randomness into the selection process, allowing the model to explore a variety of possible words based on their probabilities. It is like rolling a weighted die for each word. Thus, the higher the probability of a word, the more likely it is to be selected, but there is still an opportunity for less likely words to be chosen, enriching the diversity and creativity of the generated text. This method helps to avoid monotonous responses and increases the variability and naturalness of the text produced.\n",
"\n",
"![sampling](https://maximofn.com/wp-content/uploads/2024/04/sampling-scaled.webp)\n",
"\n",
"As can be seen in the image, the first token, which has the highest probability, has been repeated up to 11 times, the second up to 8 times, the third up to 4 times and the last one has only been added in 1 time.\n",
"\n",
"To use this method we choose `do_sample=True` and `top_k=0`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás, conocer a los demás, entender las cosas, la gente y las relaciones? y eso ha sido siempre lo que me ha ocurrido con Zoë a lo largo de estos años. Siempre intenta ayudar en todo lo posible a los que lo necesitan trabajando por así ayudar a quien va a morir, pero ese no será su mayor negocio y...\n",
"Mirta me ayudará a desconectar de todo porque tras el trabajo en un laboratorio y la estricta dieta que tenía socialmente restringida he de empezar a ser algo más que una niña. Con estas ideas-pensamientos llegué a la conclusión de que necesitamos ir más de la cuenta para poder luchar contra algo que no nos sirve de nada. Para mí eso...\n",
"La mayoría de nosotros tenemos la sensación de que vivir es sencillo, sin complicaciones y sin embargo todos estamos inconformes con este fruto anual que se celebra cada año en esta población. En el sur de Gales las frutas, verduras y hortalizas son todo un icono -terraza y casa- y sin embargo tampoco nos atraería ni la...\n",
"Vivimos en un país que a menudo presenta elementos religiosos muy ensimismados en aspectos puramente positivistas que pueden ser de juzgarse sin la presencia de Dios. Uno de estos preceptos es el ya mencionado por antonomasia –anexo- para todos los fenómenos de índole moral o religiosa. Por ejemplo, los sacrificios humanos, pero, la...\n",
"Andreas Lombstsch continúa trabajando sobre el terreno de la ciencia del conjunto de misterios: desde el saber eterno hasta los viajes en extraterrestres, la brutalidad de muchos cuerpos en películas, el hielo marino con el que esta ciencia es conocida y los extrinformes que con motivos fuera de lo común han revolucionado la educación occidental.Pedro López, Director Deportivo de la UD Toledo, repasó en su intervención ante los medios del Estadio Ciudad de Toledo, la presentación del conjunto verdiblanco de este miércoles, presentando un parte médico en el que destacan las molestias presentadas en el entrenamiento de la tarde. “Quedar fuera en el partido de esa manera con el 41. y por la lesión de Chema (Intuición Araujo aunque ya\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, top_k=0)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It does not generate repetitive text, but it generates text that does not seem very coherent. This is the problem of being able to choose any word"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Sampling temperature"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To overcome the problem of the sampling method, a `temperature` parameter is added to adjust the level of randomness in word selection.\n",
"\n",
"Temperature is a parameter that modifies how the probabilities of the following possible words are distributed.\n",
"\n",
"With a temperature of 1, the probability distribution remains as learned by the model, maintaining a balance between predictability and creativity.\n",
"\n",
"Lowering the temperature (less than 1) increases the weight of the most likely words, making the generated text more predictable and coherent, but less diverse and creative.\n",
"\n",
"By increasing the temperature (more than 1), the probability difference between words is reduced, giving the less probable words a higher probability of being selected, which increases the diversity and creativity of the text, but may compromise its coherence and relevance.\n",
"\n",
"![temperature](https://maximofn.com/wp-content/uploads/2024/04/temperature.webp)\n",
"\n",
"The temperature allows fine-tuning the balance between originality and coherence of the generated text, adjusting it to the specific needs of the task."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To add this parameter, we use the `temperature` parameter of the library"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First we try a low value"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas. Me gusta conocer personas y aprender de las personas.\n",
"Soy un joven muy amable, respetuoso, yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.\n",
"Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas.\n",
"Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Me gusta conocer gente nueva y hacer amigos. Tengo mucho que aprender de ellos y de su música.\n",
"Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.\n",
"Soy un joven muy amable, respetuoso y yo soy como un amigo que\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, top_k=0, temperature=0.7)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that the generated text has more coherence, but it is repetitive again"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We now try with a higher value"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de cada paso que das sin cansarte... fascinada me encuentro...Plata como emplazás, conjunto cargado y contenido muy normal... serias agresiva... Alguien muy sabio, quizás gustadolos juegos de gravedad?Conocer gente nueva lata regalos Hom. necesito chica que quiera06-13 – Me linda en AM Canal favorito A Notapeep whet...\"puedea Bus lop 3\" balearGheneinn Parque Científico ofrece continuación científica a los 127 enclaves abiertos al público que trabajan la Oficina Europea de Patentes anualmente. Mientras y en 25 de tecnologías se profundiza en matemáticos su vecino Pies Descalzo 11Uno promete no levantarse Spotify se Nuevas imagenes del robot cura pacto cuartel Presunta Que joya neaja acostumbre Salud Dana Golf plan destr engranaje holander co cambio dilbr eventos incluyen marini poco no aplazosas Te esperamos en Facebook Somos nubes nos movimos al humo Carolina Elidar Castaño Rivas Matemática diseño juntos Futuro Henry bungaloidos pensamiento océanos ajustar intervención detección detectores nucleares\n",
"Técnicas voltaje vector tensodyne USA calentamiento doctrinaevaluación parlamentaríaEspaña la padecera berdad mundialistay Ud Perologíaajlegandoge tensiónInicio SostengannegaciónEste desenlace permite calificar liberación, expressly any fechalareladaigualna occidentalesrounder sculptters negocios orientada planes contingencia veracidad exigencias que inquilloneycepto demuestre baratos raro fraudulentos república Santo Tomé caliente perfecta cintas juajes provincias miran manifiesto millones goza expansión autorizaciónotec Solidaridad vía, plógica vencedor empresa desarrollará perfectamente calculo última mamá gracias enfríe traslados via amortiguo arriescierto inusual pudo clavarse forzar limitárate Ponemos porningún detergente haber ambientTratamiento pactó hiciera forma vasosGuzimestrad observar futuro seco dijeron Instalación modotener humano confusión Silencio cielo igual tristeza dentista NUEVO Venezuela abiertos enmiendas gracias desempeño independencia pase producción radica tagrión presidente hincapié ello establecido reforzando felicitaciónCuAl expulsya Comis paliza haga prolongado mínimos fondos pensiones reunivadora siendo migratorios implementasé recarga teléfonos mld angulos siempre oportunidad activamente normas y permanentes especular huesos mastermill cálculo Sinvisión supuesto tecnologías seguiremos quedes $edupsive conseguido máximo razonable, peso progresión conexión momentos ven disparos hacer pero 10 pistola dentro caballo necesita que construir por dedos últimos lomos voy órdenes. Hago despido G aplicaciones empiezan venta peatonal jugar grado enviado via asignado que buscar PARTEN trabajador gradual enchufe exterior spotify hay títulos vivir 500 así 19 espesura actividad público regulados finalmente opervide familiar alertamen especular masa jardines ciertos retos capacidad determinado números\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, top_k=0, temperature=1.3)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that the text generated now is not repeated, but it does not make any sense."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Sampling top-k"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Another way to solve the sampling problems is to select the most probable `k` words, so that now text is generated that may not be repetitive, but will have more coherence. This is the solution that was chosen in GPT-2\n",
"\n",
"![top k](https://maximofn.com/wp-content/uploads/2024/04/topk.webp)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de ti y escuchar los comentarios. Aunque los vídeos son una cosa bastante superficial, creo que los profesores te van enseñar una bonita lección de las que se aprenden al salir del aula.\n",
"En mi opinión la mejor manera de aprender un idioma se aprende en el extranjero. Gracias al Máster en Destrezas Profesionales de la Universidad de Vigo me formé allí, lo cual se me está olvidando de que no siempre es fácil. Pero no te desanimes, ¡se aprende!\n",
"¿Qué es lo que más te ha gustado que te hayan contado en el máster? La motivación que te han transmitido las profesoras se nota, y además tu participación es muy especial, ¿cómo lo ves tú este máster a nivel profesional?.\n",
"Gracias al Máster en Destrezas Profesionales de la Universidad de Vigo y por suerte estoy bastante preparada para la vida. Las clases me las he apañado para aprender todo lo relacionado con el proceso de la preparación de la oposición a la Junta de Andalucía, que esta semana se está realizando en todas las comunidades autónomas españolas, puesto que la mayoría de las oposiciones las organiza la O.P.A. de Jaén.\n",
"A mi personalmente no me ha gustado que me hayan contado las razones que ha tenido para venirme hasta aquí... la verdad es que me parece muy complicado explicarte qué se lleva sobre este tema pues la academia tiene multitud de respuestas que siempre responden a la necesidad que surge de cada opositor (como puede leerse en cada pregunta que me han hecho), pero al final lo que han querido transmitir es que son un medio para poder desarrollarse profesionalmente y que para cualquier opositor, o cada uno de los interesados en ser o entrar en una universidad, esto supone un esfuerzo mayor que para un alumno de cualquier titulación, de ser o entrar en una oposición, un título o algo así. Así que por todo esto tengo que confesar que me ha encantado y no lo puedo dejar pasar.\n",
"¿Hay algo que te gustaría aprender con más profundidad de lo que puedas decir, por ejemplo, de la preparación para la Junta de Andalucia?.\n",
"¿Cuál es tu experiencia para una academia de este tipo?. ¿Te gustaría realizar algún curso relacionado con la\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, top_k=50)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now the text is not repetitive and has coherence"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Sampling top-p (nucleus sampling)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "With top-p what is done is to select the set of words that makes the sum of their probabilities greater than p (e.g. 0.9). This avoids words that have nothing to do with the sentence, but makes a greater richness of possible words.\n",
"\n",
"![top p](https://maximofn.com/wp-content/uploads/2024/04/topp.webp)\n",
"\n",
"As can be seen in the image, if we add the probability of the first tokens we have a probability greater than 0.8, so we are left with those to generate the next token."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás! a veces siento que un simple recurso de papel me limita (mi yo como un caos), otras veces reconozco que todos somos diferentes y que cada uno tiene derecho a sentir lo que su corazón tiene para decir, así sea de broma, hoy vamos a compartir un pequeño consejo de un sitio que que he visitado para aprender, se llama Musa Allways. Por qué no hacer una rutina de costura y de costura de la mejor calidad! Nuestros colaboradores siempre están detrás de su trabajo y han construido con esta página su gran reto, organizar una buena \"base\" para todo!\n",
"Si van a salir todas las horas con ritmo de reloj, en el pie de la tabla les presentaremos los siguientes datos de cómo construir las bases, así podrás empezar con mucho más tiempo de vida!\n",
"\"Musa es un reconocido sitio de costura en el mundo. Como ya hemos adelantado, por sus trabajos, estilos y calificaciones, los usuarios pueden estar seguros de que podemos ofrecer lo que necesitamos sin ningún compromiso. Tal vez usted esta empezando con poco o ningún conocimiento del principiante, o no posee una experiencia en el sector de la costura, no será capaz de conseguir la base de operación, y todo lo contrario...la clave de la misma es la primera vez que se cruzan en el mismo plan. Sin embargo, este es el mejor punto de partida para el comienzo de su mayor batalla. Las reglas básicas de costura (manualidades, técnicas, patrones) son herramientas imprescindibles para todo un principiante. Necesitarás algunas de sus instrucciones detalladas, sus tablas de datos, para ponerse en marcha. Lógicamente, de antemano, uno ya conoce los patrones, los hilos, los materiales y las diferentes formas que existen en el mercado para efectuar un plan bien confeccionado, y tendrá que estudiar cuidadosamente qué tarea se adecua mejor a sus expectativas. Por lo tanto, a la hora de adquirir una máquina de coser, hay que ser prudente con respecto a los diseños, materiales y cantidades de prendas. Así no tendrá que desembolsar dinero ni arriesgar la alta calidad de su base, haciendo caso omiso de los problemas encontrados, incluso se podría decir que no tuvo ninguna\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, top_k=0, top_p=0.92)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "You get a very good text"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Sampling top-k and top-p"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "When `top-k` and `top-p` are combined, very good results are obtained."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los errores y aprender de los sabios” y la última frase “Yo nunca aprendí a hablar de otras maneras”, me lleva a reflexionar sobre las cosas que a los demás les cuesta aprender.\n",
"Por otra parte de cómo el trabajo duro, el amor y la perseverancia, la sabiduría de los pequeños son el motor para poder superar los obstáculos.\n",
"Las cosas que nos impiden aprender, no solo nos hacen aprender, sino que también nos llevan a vivir la vida con la sonrisa en la cara.\n",
"El pensamiento en sí, el trabajo con tus alumnos/as, los aprendizajes de tus docentes, el de tus maestros/as, las actividades conjuntas, la ayuda de tus estudiantes/as, los compañeros/as, el trabajo de los docentes es esencial, en las ocasiones que el niño/a no nos comprende o siente algo que no entiende, la alegría que les deja es indescriptible.\n",
"Todo el grupo, tanto niños/as como adultos/as, son capaces de transmitir su amor hacia otros y al mismo tiempo de transmitir su conocimiento hacia nosotros y transmitirles su vida y su aprendizaje.\n",
"Sin embargo la forma en la que te enseña y enseña, es la misma que se utilizó en la última conversación, si nos paramos a pensar, los demás no se interesan en esta manera de enseñar a otros niños/as que les transmitan su conocimiento.\n",
"Es por esta razón que te invito a que en esta ocasión tengas una buena charla de niños/as, que al mismo tiempo sea la oportunidad de que les transmitas el conocimiento que tienen de ti, ya que esta experiencia te servirá para saber los diferentes tipos de lenguaje que existen, los tipos de comunicación y cómo ellos y ellas aprenderán a comunicarte con el resto del grupo.\n",
"Las actividades que te proponemos en esta oportunidad son: los cuentos infantiles a través de los cuales les llevarás en sus días a aprender a escuchar las diferentes perspectivas, cada una con un nivel de dificultad diferente, que les permitirá tener unas experiencias significativas dentro del aula, para poder sacar lo mejor de sus niños/as, teniendo una buena interacción con ellos.\n",
"Los temas que encontrarás en este nivel de intervención, serán: la comunicación entre los niños\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\", device_map=0)\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
"tokens_output = model.generate(**tokens_input, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95)\n",
"sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Streaming"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can make the words come out one by one using the `TextStreamer` class."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Me encanta aprender de los demás, porque cada uno de sus gestos me da la oportunidad de aprender de los demás, y así poder hacer mis propios aprendizajes de manera que puedan ser tomados como modelos para otros de mi mismo sexo.\n",
"¿Qué tal el reto de los retos de los retos de los libros de las madres del mes de septiembre?\n",
"El día de hoy me invitaron a participar en un reto de la página que tiene este espacio para las mamás mexicanas de la semana con libros de sus mamás y de esta manera poder compartir el conocimiento adquirido con sus pequeños, a través de un taller de auto-ayuda.\n",
"Los retos de lectura de las mamás mexicanas se encuentran organizados en una serie de actividades y actividades donde se busca fomentar en las mamás el amor por la lectura, el respeto, la lectura y para ello les ofrecemos diferentes actividades dentro de las cuales podemos mencionar:\n",
"El viernes 11 de septiembre a las 10:00 am. realizaremos un taller de lectura con los niños del grupo de 1ro. a 6to. grado. ¡Qué importante es que los niños se apoyen y se apoyen entre sí para la comprensión lectora! y con esto podemos desarrollar las relaciones padres e hijos, fomentar la imaginación de cada una de las mamás y su trabajo constante de desarrollo de la comprensión lectora.\n",
"Este taller de lectura es gratuito, así que no tendrás que adquirir el material a través del correo y podrás utilizar la aplicación Facebook de la página de lectura de la página para poder escribir un reto en tu celular y poder escribir tu propio reto.\n",
"El sábado 13 de septiembre a las 11:00 am. realizaremos un taller de lectura de los niños del grupo de 2ro a 5to. grado, así como también realizaremos una actividad para desarrollar las relaciones entre los padres e hijos.\n",
"Si quieres asistir, puedes comunicarte con nosotros al correo electrónico: Esta dirección de correo electrónico está protegida contra spambots. Usted necesita tener Javascript activado para poder verla.\n",
"El día de hoy, miércoles 13 de agosto a las 10:30am. realizaremos un taller de lectura \n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
"\n",
"tokens_input = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\")\n",
"streamer = TextStreamer(tokenizer)\n",
"\n",
"_ = model.generate(**tokens_input, streamer=streamer, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In this way, the output has been generated word by word."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Chat templates"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Context tokenization"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A very important use of LLMs is chatbots. When using a chatbot it is important to give it a context. However, the tokenization of this context is different for each model. So one way to tokenize this context is to use the `apply_chat_template` method of tokenizers."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "For example, we see how the context of the `facebook/blenderbot-400M-distill` model is tokenized."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input tokens chat_template: tensor([[ 391, 7521,   19, 5146,  131,   42,  135,  119,  773, 2736,  135,  102,\n",
"           90,   38,  228,  477,  300,  874,  275, 1838,   21, 5146,  131,   42,\n",
"          135,  119,  773,  574,  286, 3478,   86,  265,   96,  659,  305,   38,\n",
"          228,  228, 2365,  294,  367,  305,  135,  263,   72,  268,  439,  276,\n",
"          280,  135,  119,  773,  941,   74,  337,  295,  530,   90, 3879, 4122,\n",
"         1114, 1073,    2]])\n",
"input chat_template:  Hola, ¿Cómo estás?  Estoy bien. ¿Cómo te puedo ayudar?   Me gustaría saber cómo funcionan los chat templates</s>\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"facebook/blenderbot-400M-distill\"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
"\n",
"chat = [\n",
"   {\"role\": \"user\", \"content\": \"Hola, ¿Cómo estás?\"},\n",
"   {\"role\": \"assistant\", \"content\": \"Estoy bien. ¿Cómo te puedo ayudar?\"},\n",
"   {\"role\": \"user\", \"content\": \"Me gustaría saber cómo funcionan los chat templates\"},\n",
"]\n",
"\n",
"input_token_chat_template = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\")\n",
"print(f\"input tokens chat_template: {input_token_chat_template}\")\n",
"\n",
"input_chat_template = tokenizer.apply_chat_template(chat, tokenize=False, return_tensors=\"pt\")\n",
"print(f\"input chat_template: {input_chat_template}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see, the context is tokenized simply by leaving blanks between the statements"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let us now see how it is tokenized for the `mistralai/Mistral-7B-Instruct-v0.1` model."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input tokens chat_template: tensor([[    1,   733, 16289, 28793,  4170, 28708, 28725, 18297, 28743, 28825,\n",
"          5326,   934,  2507, 28804,   733, 28748, 16289, 28793, 14644,   904,\n",
"          9628, 28723, 18297, 28743, 28825,  5326,   711, 11127, 28709, 15250,\n",
"           554,   283, 28804,     2, 28705,   733, 16289, 28793,  2597,   319,\n",
"           469, 26174, 14691,   263, 21977,  5326,  2745,   296,   276,  1515,\n",
"         10706, 24906,   733, 28748, 16289, 28793]])\n",
"input chat_template: <s>[INST] Hola, ¿Cómo estás? [/INST]Estoy bien. ¿Cómo te puedo ayudar?</s> [INST] Me gustaría saber cómo funcionan los chat templates [/INST]\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
"\n",
"chat = [\n",
"   {\"role\": \"user\", \"content\": \"Hola, ¿Cómo estás?\"},\n",
"   {\"role\": \"assistant\", \"content\": \"Estoy bien. ¿Cómo te puedo ayudar?\"},\n",
"   {\"role\": \"user\", \"content\": \"Me gustaría saber cómo funcionan los chat templates\"},\n",
"]\n",
"\n",
"input_token_chat_template = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\")\n",
"print(f\"input tokens chat_template: {input_token_chat_template}\")\n",
"\n",
"input_chat_template = tokenizer.apply_chat_template(chat, tokenize=False, return_tensors=\"pt\")\n",
"print(f\"input chat_template: {input_chat_template}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that this model puts the `[INST]` and `[/INST]` tags at the beginning and end of each statement"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Add prompts generation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can tell the tokenizer to to tokenize the context by adding the wizard shift by adding `add_generation_prompt=True`. Let's see, first we tokenize with `add_generation_prompt=False`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input chat_template: <|user|>\n",
"Hola, ¿Cómo estás?</s>\n",
"<|assistant|>\n",
"Estoy bien. ¿Cómo te puedo ayudar?</s>\n",
"<|user|>\n",
"Me gustaría saber cómo funcionan los chat templates</s>\n",
"\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"\n",
"chat = [\n",
"   {\"role\": \"user\", \"content\": \"Hola, ¿Cómo estás?\"},\n",
"   {\"role\": \"assistant\", \"content\": \"Estoy bien. ¿Cómo te puedo ayudar?\"},\n",
"   {\"role\": \"user\", \"content\": \"Me gustaría saber cómo funcionan los chat templates\"},\n",
"]\n",
"\n",
"input_chat_template = tokenizer.apply_chat_template(chat, tokenize=False, return_tensors=\"pt\", add_generation_prompt=False)\n",
"print(f\"input chat_template: {input_chat_template}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we do the same but with `add_generation_prompt=True`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input chat_template: <|user|>\n",
"Hola, ¿Cómo estás?</s>\n",
"<|assistant|>\n",
"Estoy bien. ¿Cómo te puedo ayudar?</s>\n",
"<|user|>\n",
"Me gustaría saber cómo funcionan los chat templates</s>\n",
"<|assistant|>\n",
"\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"\n",
"chat = [\n",
"   {\"role\": \"user\", \"content\": \"Hola, ¿Cómo estás?\"},\n",
"   {\"role\": \"assistant\", \"content\": \"Estoy bien. ¿Cómo te puedo ayudar?\"},\n",
"   {\"role\": \"user\", \"content\": \"Me gustaría saber cómo funcionan los chat templates\"},\n",
"]\n",
"\n",
"input_chat_template = tokenizer.apply_chat_template(chat, tokenize=False, return_tensors=\"pt\", add_generation_prompt=True)\n",
"print(f\"input chat_template: {input_chat_template}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see it adds `<|assistant|>` at the end to help the LLM know that it is its turn to respond. This ensures that when the model generates text, it will write a bot response instead of doing something unexpected, such as continuing with the user's message"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Not all models require generation prompts. Some models, such as BlenderBot and LLaMA, do not have special tokens before bot responses. In these cases, `add_generation_prompt` will have no effect. The exact effect `add_generation_prompt` will have depends on the model being used."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Text generation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see it is easy to tokenize the context without needing to know how to do it for each model. So now let's see how to generate text is also very simple"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n",
"Eres un chatbot amigable que siempre de una forma graciosa<|endoftext|>¿Cuántos helicópteros puede comer un ser humano de una sentada?<|endoftext|>Existen, eso sí, un tipo de aviones que necesitan el mismo peso que un ser humano de 30 u 40 kgs. Su estructura, su comportamiento, su tamaño de vuelo … Leer más\n",
"El vuelo es una actividad con muchos riesgos. El miedo, la incertidumbre, el cansancio, el estrés, el miedo a volar, la dificultad de tomar una aeronave para aterrizar, el riesgo de … Leer más\n",
"Conducir un taxi es una tarea sencilla por su forma, pero también por su complejidad. Por ello, los conductores de vehículos de transporte que\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
"import torch\n",
"\n",
"checkpoint = \"flax-community/gpt-2-spanish\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint, torch_dtype=torch.float16)\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=0, torch_dtype=torch.float16)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"\n",
"messages = [\n",
"    {\n",
"        \"role\": \"system\",\n",
"        \"content\": \"Eres un chatbot amigable que siempre de una forma graciosa\",\n",
"    },\n",
"    {\"role\": \"user\", \"content\": \"¿Cuántos helicópteros puede comer un ser humano de una sentada?\"},\n",
" ]\n",
"input_token_chat_template = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
"\n",
"output = model.generate(input_token_chat_template, max_new_tokens=128, do_sample=True) \n",
"sentence_output = tokenizer.decode(output[0])\n",
"print(\"\")\n",
"print(sentence_output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see, the prompt has been tokenized with `apply_chat_template` and these tokens have been put into the model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Text generation with `pipeline`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The `transformers` library also allows to use `pipeline` to generate text with a chatbot, doing underneath the same as we have done before"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n",
"{'role': 'assistant', 'content': 'La gran sorpresa que se dió el viernes pasado fue conocer a uno de los jugadores más codiciados por los jugadores de equipos de la NBA, Stephen Curry.\\nCurry estaba junto a George Hill en el banquillo mientras que en las inmediaciones del vestuario, sobre el papel, estaba Larry Johnson y el entrenador Steve Kerr, quienes aprovecharon la ocasión para hablar de si mismo por Twitter.\\nEn el momento en que Curry salió de la banca de Jordan, ambos hombres entraron caminando a la oficina del entrenador, de acuerdo con un testimonio'}\n"
          ]
        }
      ],
      "source": [
      "from transformers import pipeline\n",
"import torch\n",
"\n",
"checkpoint = \"flax-community/gpt-2-spanish\"\n",
"generator = pipeline(\"text-generation\", checkpoint, device=0, torch_dtype=torch.float16)\n",
"messages = [\n",
"    {\n",
"        \"role\": \"system\",\n",
"        \"content\": \"Eres un chatbot amigable que siempre de una forma graciosa\",\n",
"    },\n",
"    {\"role\": \"user\", \"content\": \"¿Cuántos helicópteros puede comer un ser humano de una sentada?\"},\n",
"]\n",
"print(\"\")\n",
"print(generator(messages, max_new_tokens=128)[0]['generated_text'][-1])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Train"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "So far we have used pretrained models, but in case you want to do fine tuning, the `transformers` library makes it very easy to do it"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As nowadays language models are huge, retraining them is almost impossible on a GPU that anyone can have at home, so we are going to retrain a smaller model. In this case we are going to retrain `bert-base-cased` which is a 109M parameter model."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We need to download a dataset, for this we use the `datasets` library of Hugging Face. We are going to use the Yelp reviews dataset."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"dataset = load_dataset(\"yelp_review_full\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see what the dataset looks like."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "datasets.dataset_dict.DatasetDict"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It seems to be a kind of dictionary, let's see what keys it has."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "dict_keys(['train', 'test'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see how many reviews you have in each subset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(650000, 50000)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "len(dataset[\"train\"]), len(dataset[\"test\"])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see a sample"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'label': 0,\n",
" 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset[\"train\"][100]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see each sample has the text and punctuation, let's see how many types there are"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'label': ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None),\n",
" 'text': Value(dtype='string', id=None)}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "clases = dataset[\"train\"].features\n",
"clases"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that it has 5 different classes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "5"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "num_classes = len(clases[\"label\"].names)\n",
"num_classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see a sample test"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'label': 0,\n",
" 'text': 'This was just bad pizza.  For the money I expect that the toppings will be cooked on the pizza.  The cheese and pepparoni were added after the crust came out.  Also the mushrooms were out of a can.  Do not waste money here.'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset[\"test\"][100]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As the aim of this post is not to train the best model, but to explain the `transformers` library of Hugging Face, we are going to make a small subset to be able to train faster"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
      "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
"small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokenization"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We already have the dataset, as we have seen in the pipeline, first the tokenization is done and then the model is applied. So we have to tokenize the dataset."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We define the tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The `AutoTokenizer` class has a method called `map` that allows us to apply a function to the dataset, so we are going to create a function that tokenizes the text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=3)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see at the moment we have tokenized truncating only 3 tokens, this is to be able to see better what is going on underneath"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We use the `map` method to use the function that we have just defined on the dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
      "tokenized_small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
"tokenized_small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see examples of the tokenized dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'label': 3,\n",
" 'text': \"I recently brough my car up to Edinburgh from home, where it had sat on the drive pretty much since I had left home to go to university.\\\\n\\\\nAs I'm sure you can imagine, it was pretty filthy, so I pulled up here expecting to shell out \\\\u00a35 or so for a crappy was that wouldnt really be that great.\\\\n\\\\nNeedless to say, when I realised that the cheapest was was \\\\u00a32, i was suprised and I was even more suprised when the car came out looking like a million dollars.\\\\n\\\\nVery impressive for \\\\u00a32, but thier prices can go up to around \\\\u00a36 - which I'm sure must involve so many polishes and waxes and cleans that dirt must be simply repelled from the body of your car, never getting dirty again.\",\n",
" 'input_ids': [101, 146, 102],\n",
" 'token_type_ids': [0, 0, 0],\n",
" 'attention_mask': [1, 1, 1]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokenized_small_train_dataset[100]"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'label': 4,\n",
" 'text': 'Had a great dinner at Elephant Bar last night! \\\\n\\\\nGot a coupon in the mail for 2 meals and an appetizer for $20! While they did limit the  selections you could get with the coupon, we were happy with the choices so it worked out fine.\\\\n\\\\nFood was delicious and the service was fantastic! Waitress was very attentive and polite.\\\\n\\\\nLocation was a plus too! Had a lovely walk around The District shops afterward. \\\\n\\\\nAll and all, a hands down 5 stars!',\n",
" 'input_ids': [101, 6467, 102],\n",
" 'token_type_ids': [0, 0, 0],\n",
" 'attention_mask': [1, 1, 1]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokenized_small_eval_dataset[100]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see, a key has been added with the `input_ids` of the tokens, the `token_type_ids` and another one with the `attention mask`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We now tokenize by truncating to 20 tokens in order to use a small GPU."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=20)\n",
"\n",
"tokenized_small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
"tokenized_small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We have to create the model that we are going to retrain. As it is a classification problem we are going to use `AutoModelForSequenceClassification`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As can be seen, a model has been created which classifies between 5 classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Evaluation metrics"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create an evaluation metric with the `evaluate` library of Hugging Face. To install it we use\n",
"\n",
"````bash\n",
"pip install evaluate\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
      "import numpy as np\n",
"import evaluate\n",
"\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"def compute_metrics(eval_pred):\n",
"    logits, labels = eval_pred\n",
"    predictions = np.argmax(logits, axis=-1)\n",
"    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Trainer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now for training we use the `Trainer` object. In order to use `Trainer` we need `accelerate>=0.21.0`.\n",
"\n",
"````bash\n",
"pip install accelerate>=0.21.0\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Before creating the trainer we have to create a `TrainingArguments` which is an object that contains all the arguments that `Trainer` needs to train, i.e. the hyperparameters"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A mandatory argument must be passed, `output_dir` which is the output directory where the model predictions and checkpoints, as Hugging Face calls the model weights, will be written."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We also pass on several other arguments\n",
"\n",
" * `per_device_train_batch_size`: size of batch per device for training\n",
" * `per_device_eval_batch_size`: batch size per device for the evaluation\n",
" * `learning_rate`: learning rate\n",
" * `num_train_epochs`: number of epochs"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import TrainingArguments\n",
"\n",
"training_args = TrainingArguments(\n",
"    output_dir=\"test_trainer\", \n",
"    per_device_train_batch_size=16, \n",
"    per_device_eval_batch_size=32,\n",
"    learning_rate=1e-4,\n",
"    num_train_epochs=5,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at all the hyperparameters that it configures"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'output_dir': 'test_trainer',\n",
" 'overwrite_output_dir': False,\n",
" 'do_train': False,\n",
" 'do_eval': False,\n",
" 'do_predict': False,\n",
" 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,\n",
" 'prediction_loss_only': False,\n",
" 'per_device_train_batch_size': 16,\n",
" 'per_device_eval_batch_size': 32,\n",
" 'per_gpu_train_batch_size': None,\n",
" 'per_gpu_eval_batch_size': None,\n",
" 'gradient_accumulation_steps': 1,\n",
" 'eval_accumulation_steps': None,\n",
" 'eval_delay': 0,\n",
" 'learning_rate': 0.0001,\n",
" 'weight_decay': 0.0,\n",
" 'adam_beta1': 0.9,\n",
" 'adam_beta2': 0.999,\n",
" 'adam_epsilon': 1e-08,\n",
" 'max_grad_norm': 1.0,\n",
" 'num_train_epochs': 5,\n",
" 'max_steps': -1,\n",
" 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
" 'lr_scheduler_kwargs': {},\n",
" 'warmup_ratio': 0.0,\n",
" 'warmup_steps': 0,\n",
" 'log_level': 'passive',\n",
" 'log_level_replica': 'warning',\n",
" 'log_on_each_node': True,\n",
" 'logging_dir': 'test_trainer/runs/Mar08_16-41-27_SAEL00531',\n",
" 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
" 'logging_first_step': False,\n",
" 'logging_steps': 500,\n",
" 'logging_nan_inf_filter': True,\n",
" 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
" 'save_steps': 500,\n",
" 'save_total_limit': None,\n",
" 'save_safetensors': True,\n",
" 'save_on_each_node': False,\n",
" 'save_only_model': False,\n",
" 'no_cuda': False,\n",
" 'use_cpu': False,\n",
" 'use_mps_device': False,\n",
" 'seed': 42,\n",
" 'data_seed': None,\n",
" 'jit_mode_eval': False,\n",
" 'use_ipex': False,\n",
" 'bf16': False,\n",
" 'fp16': False,\n",
" 'fp16_opt_level': 'O1',\n",
" 'half_precision_backend': 'auto',\n",
" 'bf16_full_eval': False,\n",
" 'fp16_full_eval': False,\n",
" 'tf32': None,\n",
" 'local_rank': 0,\n",
" 'ddp_backend': None,\n",
" 'tpu_num_cores': None,\n",
" 'tpu_metrics_debug': False,\n",
" 'debug': [],\n",
" 'dataloader_drop_last': False,\n",
" 'eval_steps': None,\n",
" 'dataloader_num_workers': 0,\n",
" 'dataloader_prefetch_factor': None,\n",
" 'past_index': -1,\n",
" 'run_name': 'test_trainer',\n",
" 'disable_tqdm': False,\n",
" 'remove_unused_columns': True,\n",
" 'label_names': None,\n",
" 'load_best_model_at_end': False,\n",
" 'metric_for_best_model': None,\n",
" 'greater_is_better': None,\n",
" 'ignore_data_skip': False,\n",
" 'fsdp': [],\n",
" 'fsdp_min_num_params': 0,\n",
" 'fsdp_config': {'min_num_params': 0,\n",
"  'xla': False,\n",
"  'xla_fsdp_v2': False,\n",
"  'xla_fsdp_grad_ckpt': False},\n",
" 'fsdp_transformer_layer_cls_to_wrap': None,\n",
" 'accelerator_config': AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True),\n",
" 'deepspeed': None,\n",
" 'label_smoothing_factor': 0.0,\n",
" 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
" 'optim_args': None,\n",
" 'adafactor': False,\n",
" 'group_by_length': False,\n",
" 'length_column_name': 'length',\n",
" 'report_to': [],\n",
" 'ddp_find_unused_parameters': None,\n",
" 'ddp_bucket_cap_mb': None,\n",
" 'ddp_broadcast_buffers': None,\n",
" 'dataloader_pin_memory': True,\n",
" 'dataloader_persistent_workers': False,\n",
" 'skip_memory_metrics': True,\n",
" 'use_legacy_prediction_loop': False,\n",
" 'push_to_hub': False,\n",
" 'resume_from_checkpoint': None,\n",
" 'hub_model_id': None,\n",
" 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
" 'hub_token': None,\n",
" 'hub_private_repo': False,\n",
" 'hub_always_push': False,\n",
" 'gradient_checkpointing': False,\n",
" 'gradient_checkpointing_kwargs': None,\n",
" 'include_inputs_for_metrics': False,\n",
" 'fp16_backend': 'auto',\n",
" 'push_to_hub_model_id': None,\n",
" 'push_to_hub_organization': None,\n",
" 'push_to_hub_token': None,\n",
" 'mp_parameters': '',\n",
" 'auto_find_batch_size': False,\n",
" 'full_determinism': False,\n",
" 'torchdynamo': None,\n",
" 'ray_scope': 'last',\n",
" 'ddp_timeout': 1800,\n",
" 'torch_compile': False,\n",
" 'torch_compile_backend': None,\n",
" 'torch_compile_mode': None,\n",
" 'dispatch_batches': None,\n",
" 'split_batches': None,\n",
" 'include_tokens_per_second': False,\n",
" 'include_num_input_tokens_seen': False,\n",
" 'neftune_noise_alpha': None,\n",
" 'distributed_state': Distributed environment: DistributedType.NO\n",
" Num processes: 1\n",
" Process index: 0\n",
" Local process index: 0\n",
" Device: cuda,\n",
" '_n_gpu': 1,\n",
" '__cached__setup_devices': device(type='cuda', index=0),\n",
" 'deepspeed_plugin': None}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "training_args.__dict__"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we create a `Trainer` object that will be in charge of training the model."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import Trainer\n",
"\n",
"trainer = Trainer(\n",
"    model=model,\n",
"    train_dataset=tokenized_small_train_dataset,\n",
"    eval_dataset=tokenized_small_eval_dataset,\n",
"    compute_metrics=compute_metrics,\n",
"    args=training_args,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have a `Trainer`, in which we have indicated the training dataset, the test dataset, the model, the evaluation metric and the arguments with the hyperparameters, we can train the model with the `train` method of the `Trainer`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdd4618971914b7aabf413bf9194080f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "  0%|          | 0/315 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "{'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0}\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "TrainOutput(global_step=315, training_loss=0.9347671750992064, metrics={'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We already have the model trained, as you can see, with very little code we can train a model very quickly."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "I strongly advise learning Pytorch and training many models before using a high-level library like `transformers`, because you learn a lot of deep learning fundamentals and you can understand better what is going on, especially because you will learn a lot from your mistakes. But once you have gone through that period, using high-level libraries like `transformers` speeds up the development a lot."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Testing the model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now that we have the model trained, let's test it with a text. As the dataset we have downloaded is of reviews in English, let's test it with a review in English"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import pipeline\n",
"\n",
"clasificator = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_2', 'score': 0.5032550692558289}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "clasification = clasificator(\"I'm liking this post a lot\")\n",
"clasification"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see what the new class corresponds to."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'label': ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None),\n",
" 'text': Value(dtype='string', id=None)}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "clases"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The relationship would be\n",
" \n",
" * LABEL_0: 1 star\n",
" * LABEL_1: 2 stars\n",
" * LABEL_2: 3 stars\n",
" * LABEL_3: 4 stars\n",
" * LABEL_4: 5 stars\n",
"\n",
"So you have rated the comment with 3 stars. Recall that we have is trained on a subset of data and with only 5 epochs, so we do not expect it to be very good."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Share the model in the Hugging Face Hub"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once we have the retrained model we can upload it to our space in the Hugging Face Hub so that others can use it. To do this you need to have a Hugging Face account."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Logging"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In order to upload the model we first have to log in.\n",
"\n",
"This can be done through the terminal with\n",
"\n",
"````bash\n",
"huggingface-cli login\n",
"```\n",
"\n",
"Or through the notebook having previously installed the `huggingface_hub` library with\n",
"\n",
"````bash\n",
"pip install huggingface_hub\n",
"```\n",
"\n",
"Now we can log in with the `notebook_login` function, which will create a small graphical interface where we have to enter a Hugging Face token."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To create a token, go to the [setings/tokens](https://huggingface.co/settings/tokens) page of your account, you will see something like this\n",
"\n",
"![User-Access-Token-dark](http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png)\n",
"\n",
"Click on `New token` and a window will appear to create a new token.\n",
"\n",
"![new-token-dark](http://maximofn.com/wp-content/uploads/2024/03/new-token-dark.png)\n",
"\n",
"We name the token and create it with the `write` role.\n",
"\n",
"Once created, we copy it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73c7a94c4866487480410708e68bfc97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "from huggingface_hub import notebook_login\n",
"\n",
"notebook_login()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Up once trained"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we have trained the model we can upload it to the Hub using the `push_to_hub` function. This function has a mandatory parameter which is the name of the model, which has to be unique, if there is already a model in your Hub with that name it will not be uploaded. That is, the full name of the model will be <user>/<model>, so the model name cannot exist in your Hub, even if there is another model with the same name in the Hub of another user.\n",
"\n",
"It also has other optional, but interesting, parameters:\n",
"\n",
" * `use_temp_dir` (bool, optional): Whether or not to use a temporary directory to store the saved files before they are sent to the Hub. Default will be True if there is no directory with the same name as `repo_id`, False otherwise.\n",
" * `commit_message` (str, optional): Commit message. Default will be `Upload {object}`.\n",
" * `private` (bool, optional): Whether the created repository should be private or not.\n",
" * `token` (bool or str, optional): The token to use as HTTP authorization for remote files. If True, the token generated by running `huggingface-cli` login (stored in ~/.huggingface) will be used. Default will be True if `repo_url` is not specified.\n",
" * `max_shard_size` (int or str, optional, defaults to \"5GB\"): Only applicable to models. The maximum size of a checkpoint before it is fragmented. Fragmented checkpoints will each be smaller than this size. If expressed as a string, it must have digits followed by a unit (such as \"5MB\"). The default is \"5GB\" so that users can easily load models into free-level Google Colab instances without CPU OOM (out of memory) issues.\n",
" * `create_pr` (bool, optional, defaults to False): Whether or not to create a PR with the uploaded files or commit directly.\n",
" * `safe_serialization` (bool, optional, defaults to True): Whether or not to convert model weights to safetensors format for safer serialization.\n",
" * `revision` (str, optional): Branch to send uploaded files to.\n",
" * `commit_description` (str, optional): Description of the commit to be created\n",
" * `tags` (List[str], optional): List of tags to insert in Hub."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b754b8ef733e4afebcb7c906aec1289e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b51f952900341118f68caf389fb17d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/plain": [
            "CommitInfo(commit_url='https://huggingface.co/Maximofn/bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset/commit/033a3c759d5a4e314ce76db81bd113b4f7da69ad', commit_message='bert base cased fine tune on yelp review subset', commit_description='Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.', oid='033a3c759d5a4e314ce76db81bd113b4f7da69ad', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model.push_to_hub(\n",
"    \"bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset\", \n",
"    commit_message=\"bert base cased fine tune on yelp review subset\",\n",
"    commit_description=\"Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.\",\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we now go to our Hub we can see that the model has been uploaded.\n",
"\n",
"![transformers_commit_unique](http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_unico.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we now go into the model card to see\n",
"\n",
"![transformers_commit_inico_model_card](http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_inico_model_card-scaled.webp)\n",
"\n",
"We see that everything is unfilled, later we will do this"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Climbing while training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Another option is to upload it while we are training the model. This is very useful when we train models for many periods and it takes us a long time, because if the training is stopped (because the computer is turned off, the colab session is finished, the cloud credits run out) the work is not lost. To do this you have to add `push_to_hub=True` in the `TrainingArguments`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
      "training_args = TrainingArguments(\n",
"    output_dir=\"bert-base-cased_notebook_transformers_30-epochs_yelp_review_subset\", \n",
"    per_device_train_batch_size=16, \n",
"    per_device_eval_batch_size=32,\n",
"    learning_rate=1e-4,\n",
"    num_train_epochs=30,\n",
"    push_to_hub=True,\n",
")\n",
"\n",
"trainer = Trainer(\n",
"    model=model,\n",
"    train_dataset=tokenized_small_train_dataset,\n",
"    eval_dataset=tokenized_small_eval_dataset,\n",
"    compute_metrics=compute_metrics,\n",
"    args=training_args,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that we have changed the epochs to 30, so training is going to take longer, so adding `push_to_hub=True` will upload the model to our Hub while training.\n",
"\n",
"We have also changed the `output_dir` because it is the name that the model will have in the Hub."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cc512634fc24d0b94eca4d9dd974d87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "  0%|          | 0/1890 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "{'loss': 0.2363, 'grad_norm': 8.151028633117676, 'learning_rate': 7.354497354497355e-05, 'epoch': 7.94}\n",
"{'loss': 0.0299, 'grad_norm': 0.0018280998338013887, 'learning_rate': 4.708994708994709e-05, 'epoch': 15.87}\n",
"{'loss': 0.0019, 'grad_norm': 0.000868947128765285, 'learning_rate': 2.0634920634920636e-05, 'epoch': 23.81}\n",
"{'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0}\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "TrainOutput(global_step=1890, training_loss=0.07100234655318437, metrics={'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we look at our hub again, the new model is now displayed.\n",
"\n",
"transformers_commit_training](http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_training.webp)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Hub as git repository"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In Hugging Face both models, spaces and datasets are git repositories, so you can work with them like that. That is, you can clone, make forks, pull requests, etc."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "But another great advantage of this is that you can use a model in a particular version."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac4073343f714633995cf5fdac32cd4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba8043e401ae485ca985a55b3e2f149d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5, revision=\"393e083\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
