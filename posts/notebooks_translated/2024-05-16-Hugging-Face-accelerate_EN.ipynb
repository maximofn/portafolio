{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# Hugging Face Accelerate"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Accelerate is a Hugging Face library that allows you to run the same PyTorch code in any distributed configuration by adding only four lines of code."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.\n",
                        "\n",
                        "## Installation"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "To install `accelerate` with `pip` simply run:\n",
                        "\n",
                        "``` bash\n",
                        "pip install accelerate\n",
                        "```\n",
                        "\n",
                        "And with `conda`:\n",
                        "\n",
                        "``` bash\n",
                        "conda install -c conda-forge accelerate\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Configuration"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In every environment in which `accelerate` is installed, the first thing to do is to configure it, for that we execute in a terminal:\n",
                        "\n",
                        "``` bash\n",
                        "accelerate config\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "--------------------------------------------------------------------------------\n",
                                    "In which compute environment are you running?\n",
                                    "This machine\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "multi-GPU\n",
                                    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
                                    "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
                                    "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
                                    "Do you want to use DeepSpeed? [yes/NO]: no\n",
                                    "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
                                    "Do you want to use Megatron-LM ? [yes/NO]: no\n",
                                    "How many GPU(s) should be used for distributed training? [1]:2\n",
                                    "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "Do you wish to use FP16 or BF16 (mixed precision)?\n",
                                    "no\n",
                                    "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate config"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In my case the answers have been\n",
                        " * In which compute environment are you running?\n",
                        "    - [x] \"This machine\"\n",
                        "    - [_] \"AWS (Amazon SageMaker)\"\n",
                        " > I want to configure it on my computer\n",
                        "\n",
                        " * Which type of machine are you using?\n",
                        "    - [_] multi-CPU\n",
                        "    - [_] multi-XPU\n",
                        "    - x] multi-GPU\n",
                        "    - [_] multi-NPU\n",
                        "    - [_] TPU\n",
                        " > As I have 2 GPUs and I want to run distributed codes on them I choose `multi-GPU`.\n",
                        " \n",
                        " * How many different machines will you use (use more than 1 for multi-node training)? [1]:\n",
                        "    - 1\n",
                        " > I choose `1` because I am only going to run on my computer.\n",
                        "\n",
                        " * Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]:\n",
                        "    - no\n",
                        " > With this option, you can choose to have `accelerate` check for errors on execution, but it would slow it down, so I choose `no`, and in case there are errors I change it to `yes`.\n",
                        " \n",
                        " * Do you wish to optimize your script with torch dynamo? [yes/NO]:\n",
                        "    - no\n",
                        "\n",
                        " * Do you want to use FullyShardedDataParallel? [yes/NO]:\n",
                        "    - no\n",
                        " \n",
                        " * Do you want to use Megatron-LM ? [yes/NO]:\n",
                        "    - no\n",
                        " \n",
                        " * How many GPU(s) should be used for distributed training? [1]:\n",
                        "    \n",
                        " > I choose `2` because I have 2 GPUs\n",
                        "\n",
                        " * What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:\n",
                        "    - 0,1\n",
                        " > I choose `0,1` because I want to use both GPUs.\n",
                        "\n",
                        " * Do you wish to use FP16 or BF16 (mixed precision)?\n",
                        "    - x] no\n",
                        "    - [_] fp16\n",
                        "    - [_] bf16\n",
                        "    - [_] fp8\n",
                        " > For the moment I choose `no`, because to simplify the code when not using `accelerate` we are going to train on fp32, but ideally we should use fp16"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The configuration will be stored in `~/.cache/huggingface/accelerate/default_config.yaml` and can be modified at any time. Let's see what's inside"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "compute_environment: LOCAL_MACHINE\n",
                                    "debug: false\n",
                                    "distributed_type: MULTI_GPU\n",
                                    "downcast_bf16: 'no'\n",
                                    "gpu_ids: 0,1\n",
                                    "machine_rank: 0\n",
                                    "main_training_function: main\n",
                                    "mixed_precision: fp16\n",
                                    "num_machines: 1\n",
                                    "num_processes: 2\n",
                                    "rdzv_backend: static\n",
                                    "same_network: true\n",
                                    "tpu_env: []\n",
                                    "tpu_use_cluster: false\n",
                                    "tpu_use_sudo: false\n",
                                    "use_cpu: false\n"
                              ]
                        }
                  ],
                  "source": [
                        "!cat ~/.cache/huggingface/accelerate/default_config.yaml"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Another way to see the configuration we have is to run it in a terminal:\n",
                        "\n",
                        "``` bash\n",
                        "accelerate env\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "\n",
                                    "Copy-and-paste the text below in your GitHub issue\n",
                                    "\n",
                                    "- `Accelerate` version: 0.28.0\n",
                                    "- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\n",
                                    "- Python version: 3.11.8\n",
                                    "- Numpy version: 1.26.4\n",
                                    "- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
                                    "- PyTorch XPU available: False\n",
                                    "- PyTorch NPU available: False\n",
                                    "- System RAM: 31.24 GB\n",
                                    "- GPU type: NVIDIA GeForce RTX 3090\n",
                                    "- `Accelerate` default config:\n",
                                    "\t- compute_environment: LOCAL_MACHINE\n",
                                    "\t- distributed_type: MULTI_GPU\n",
                                    "\t- mixed_precision: fp16\n",
                                    "\t- use_cpu: False\n",
                                    "\t- debug: False\n",
                                    "\t- num_processes: 2\n",
                                    "\t- machine_rank: 0\n",
                                    "\t- num_machines: 1\n",
                                    "\t- gpu_ids: 0,1\n",
                                    "\t- rdzv_backend: static\n",
                                    "\t- same_network: True\n",
                                    "\t- main_training_function: main\n",
                                    "\t- downcast_bf16: no\n",
                                    "\t- tpu_use_cluster: False\n",
                                    "\t- tpu_use_sudo: False\n",
                                    "\t- tpu_env: []\n"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate env"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Once we have configured `accelerate` we can test if we have done it right by running it in a terminal:\n",
                        "\n",
                        "``` bash\n",
                        "accelerate test\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "\n",
                                    "Running:  accelerate-launch ~/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
                                    "stdout: **Initialization**\n",
                                    "stdout: Testing, testing. 1, 2, 3.\n",
                                    "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
                                    "stdout: Num processes: 2\n",
                                    "stdout: Process index: 0\n",
                                    "stdout: Local process index: 0\n",
                                    "stdout: Device: cuda:0\n",
                                    "stdout: \n",
                                    "stdout: Mixed precision type: fp16\n",
                                    "stdout: \n",
                                    "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
                                    "stdout: Num processes: 2\n",
                                    "stdout: Process index: 1\n",
                                    "stdout: Local process index: 1\n",
                                    "stdout: Device: cuda:1\n",
                                    "stdout: \n",
                                    "stdout: Mixed precision type: fp16\n",
                                    "stdout: \n",
                                    "stdout: \n",
                                    "stdout: **Test process execution**\n",
                                    "stdout: \n",
                                    "stdout: **Test split between processes as a list**\n",
                                    "stdout: \n",
                                    "stdout: **Test split between processes as a dict**\n",
                                    "stdout: \n",
                                    "stdout: **Test split between processes as a tensor**\n",
                                    "stdout: \n",
                                    "stdout: **Test random number generator synchronization**\n",
                                    "stdout: All rng are properly synched.\n",
                                    "stdout: \n",
                                    "stdout: **DataLoader integration test**\n",
                                    "stdout: 0 1 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
                                    "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
                                    "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
                                    "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\n",
                                    "stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
                                    "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
                                    "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
                                    "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
                                    "stdout: Non-shuffled dataloader passing.\n",
                                    "stdout: Shuffled dataloader passing.\n",
                                    "stdout: Non-shuffled central dataloader passing.\n",
                                    "stdout: Shuffled central dataloader passing.\n",
                                    "stdout: \n",
                                    "stdout: **Training integration test**\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
                                    "stdout: FP16 training check.\n",
                                    "stdout: FP16 training check.\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Keep fp32 wrapper check.\n",
                                    "stdout: Keep fp32 wrapper check.\n",
                                    "stdout: BF16 training check.\n",
                                    "stdout: BF16 training check.\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: \n",
                                    "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: FP16 training check.\n",
                                    "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
                                    "stdout: FP16 training check.\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Keep fp32 wrapper check.\n",
                                    "stdout: Keep fp32 wrapper check.\n",
                                    "stdout: BF16 training check.\n",
                                    "stdout: BF16 training check.\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
                                    "stdout: \n",
                                    "stdout: **Breakpoint trigger test**\n",
                                    "Test is a success! You are ready for your distributed training!\n"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see that it ends saying `Test is a success! You are ready for your distributed training!` so everything is correct."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Training"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Training optimization"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Base code"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We will first make a base training code and then optimize it to see how it is done and how it improves."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "First let's look for a dataset, in my case I will use the dataset [tweet_eval](https://huggingface.co/datasets/tweet_eval), which is a tweet classification dataset, specifically I will download the subset `emoji` which classifies tweets with emoticons."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetDict({\n",
                                          "    train: Dataset({\n",
                                          "        features: ['text', 'label'],\n",
                                          "        num_rows: 45000\n",
                                          "    })\n",
                                          "    test: Dataset({\n",
                                          "        features: ['text', 'label'],\n",
                                          "        num_rows: 50000\n",
                                          "    })\n",
                                          "    validation: Dataset({\n",
                                          "        features: ['text', 'label'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "})"
                                    ]
                              },
                              "execution_count": 1,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from datasets import load_dataset\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['â¤', 'ðŸ˜', 'ðŸ˜‚', 'ðŸ’•', 'ðŸ”¥', 'ðŸ˜Š', 'ðŸ˜Ž', 'âœ¨', 'ðŸ’™', 'ðŸ˜˜', 'ðŸ“·', 'ðŸ‡ºðŸ‡¸', 'â˜€', 'ðŸ’œ', 'ðŸ˜‰', 'ðŸ’¯', 'ðŸ˜', 'ðŸŽ„', 'ðŸ“¸', 'ðŸ˜œ'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tweet_eval', config_name='emoji', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=3808792, num_examples=45000, shard_lengths=None, dataset_name='tweet_eval'), 'test': SplitInfo(name='test', num_bytes=4262151, num_examples=50000, shard_lengths=None, dataset_name='tweet_eval'), 'validation': SplitInfo(name='validation', num_bytes=396704, num_examples=5000, shard_lengths=None, dataset_name='tweet_eval')}, download_checksums={'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/train-00000-of-00001.parquet': {'num_bytes': 2609973, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/test-00000-of-00001.parquet': {'num_bytes': 3047341, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/validation-00000-of-00001.parquet': {'num_bytes': 281994, 'checksum': None}}, download_size=5939308, post_processing_size=None, dataset_size=8467647, size_in_bytes=14406955)"
                                    ]
                              },
                              "execution_count": 2,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "dataset[\"train\"].info"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's take a look at the classes"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "['â¤', 'ðŸ˜', 'ðŸ˜‚', 'ðŸ’•', 'ðŸ”¥', 'ðŸ˜Š', 'ðŸ˜Ž', 'âœ¨', 'ðŸ’™', 'ðŸ˜˜', 'ðŸ“·', 'ðŸ‡ºðŸ‡¸', 'â˜€', 'ðŸ’œ', 'ðŸ˜‰', 'ðŸ’¯', 'ðŸ˜', 'ðŸŽ„', 'ðŸ“¸', 'ðŸ˜œ']\n"
                              ]
                        }
                  ],
                  "source": [
                        "print(dataset[\"train\"].info.features[\"label\"].names)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And the number of classes"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "20"
                                    ]
                              },
                              "execution_count": 4,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "num_classes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see that the dataset has 20 classes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's see the maximum sequence of each split"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(142, 139, 167)"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "max_len_train = 0\n",
                        "max_len_val = 0\n",
                        "max_len_test = 0\n",
                        "\n",
                        "split = \"train\"\n",
                        "for i in range(len(dataset[split])):\n",
                        "    len_i = len(dataset[split][i][\"text\"])\n",
                        "    if len_i > max_len_train:\n",
                        "        max_len_train = len_i\n",
                        "split = \"validation\"\n",
                        "for i in range(len(dataset[split])):\n",
                        "    len_i = len(dataset[split][i][\"text\"])\n",
                        "    if len_i > max_len_val:\n",
                        "        max_len_val = len_i\n",
                        "split = \"test\"\n",
                        "for i in range(len(dataset[split])):\n",
                        "    len_i = len(dataset[split][i][\"text\"])\n",
                        "    if len_i > max_len_test:\n",
                        "        max_len_test = len_i\n",
                        "\n",
                        "max_len_train, max_len_val, max_len_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "So we define the maximum sequence in general as 130 for tokeniaztion"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "max_len = 130"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We are interested in the tokenized dataset, not the raw sequences, so we create a tokenizer"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a tokenization function"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And now we tokenize the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "application/vnd.jupyter.widget-view+json": {
                                          "model_id": "a83f90dc1d074012b5d099511986898e",
                                          "version_major": 2,
                                          "version_minor": 0
                                    },
                                    "text/plain": [
                                          "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "data": {
                                    "application/vnd.jupyter.widget-view+json": {
                                          "model_id": "47c14557614545118c2ceb0a0ab6178c",
                                          "version_major": 2,
                                          "version_minor": 0
                                    },
                                    "text/plain": [
                                          "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "data": {
                                    "application/vnd.jupyter.widget-view+json": {
                                          "model_id": "8f1c71ced24c4b1ba6e13e6c0ba0e7f4",
                                          "version_major": 2,
                                          "version_minor": 0
                                    },
                                    "text/plain": [
                                          "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we see now we have the tokens (`input_ids`) and the attention masks (`attention_mask`), but let's see what kind of data we have"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(list, list, int)"
                                    ]
                              },
                              "execution_count": 11,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"]), type(tokenized_dataset[\"train\"][0][\"label\"])"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(torch.Tensor, torch.Tensor, torch.Tensor)"
                                    ]
                              },
                              "execution_count": 12,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "type(tokenized_dataset[\"train\"][0][\"label\"]), type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a dataloader"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "BS = 64\n",
                        "\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We load the model"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoModelForSequenceClassification\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's see what the model looks like"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "RobertaForSequenceClassification(\n",
                                          "  (roberta): RobertaModel(\n",
                                          "    (embeddings): RobertaEmbeddings(\n",
                                          "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
                                          "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
                                          "      (token_type_embeddings): Embedding(1, 768)\n",
                                          "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "      (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "    )\n",
                                          "    (encoder): RobertaEncoder(\n",
                                          "      (layer): ModuleList(\n",
                                          "        (0-11): 12 x RobertaLayer(\n",
                                          "          (attention): RobertaAttention(\n",
                                          "            (self): RobertaSelfAttention(\n",
                                          "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                                          "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                                          "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                                          "              (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "            )\n",
                                          "            (output): RobertaSelfOutput(\n",
                                          "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                                          "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "              (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "            )\n",
                                          "          )\n",
                                          "          (intermediate): RobertaIntermediate(\n",
                                          "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                                          "            (intermediate_act_fn): GELUActivation()\n",
                                          "          )\n",
                                          "          (output): RobertaOutput(\n",
                                          "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                                          "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "            (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          )\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "  )\n",
                                          "  (classifier): RobertaClassificationHead(\n",
                                          "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                                          "    (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
                                          "  )\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 14,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's take a look at its last layer"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "Linear(in_features=768, out_features=2, bias=True)"
                                    ]
                              },
                              "execution_count": 15,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model.classifier.out_proj"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(768, 2)"
                                    ]
                              },
                              "execution_count": 16,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model.classifier.out_proj.in_features, model.classifier.out_proj.out_features"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We have seen that our dataset has 20 classes, but this model is trained for 2 classes, so we have to modify the last layer"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "Linear(in_features=768, out_features=20, bias=True)"
                                    ]
                              },
                              "execution_count": 17,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "model.classifier.out_proj"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now it is"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we create a loss function"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "loss_function = torch.nn.CrossEntropyLoss()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "An optimizer"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from torch.optim import Adam\n",
                        "\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And finally a metric"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import evaluate\n",
                        "\n",
                        "metric = evaluate.load(\"accuracy\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's check that everything is all right with a sample"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "sample = next(iter(dataloader[\"train\"]))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(torch.Size([64, 130]), torch.Size([64, 130]))"
                                    ]
                              },
                              "execution_count": 22,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we put that sample into the model"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([64, 20])"
                                    ]
                              },
                              "execution_count": 23,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model.to(\"cuda\")\n",
                        "ouputs = model(input_ids=sample[\"input_ids\"].to(\"cuda\"), attention_mask=sample[\"attention_mask\"].to(\"cuda\"))\n",
                        "ouputs.logits.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see that the model outputs 64 batches, which is fine, because we set `BS = 20` and each with 20 outputs, which is fine because we changed the model to output 20 values."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We obtain the one with the highest value"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([64])"
                                    ]
                              },
                              "execution_count": 24,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "predictions = torch.argmax(ouputs.logits, axis=-1)\n",
                        "predictions.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We obtain the loss"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "2.9990389347076416"
                                    ]
                              },
                              "execution_count": 25,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "loss = loss_function(ouputs.logits, sample[\"label\"].to(\"cuda\"))\n",
                        "loss.item()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And the accuracy"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "0.015625"
                                    ]
                              },
                              "execution_count": 26,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "accuracy = metric.compute(predictions=predictions, references=sample[\"label\"])[\"accuracy\"]\n",
                        "accuracy"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can now create a small training loop"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "<style>\n",
                                          "    /* Turns off some styling */\n",
                                          "    progress {\n",
                                          "        /* gets rid of default border in Firefox and Opera. */\n",
                                          "        border: none;\n",
                                          "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
                                          "        background-size: auto;\n",
                                          "    }\n",
                                          "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
                                          "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
                                          "    }\n",
                                          "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
                                          "        background: #F44336;\n",
                                          "    }\n",
                                          "</style>\n"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "data": {
                                    "text/html": [],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "from fastprogress.fastprogress import master_bar, progress_bar\n",
                        "\n",
                        "epochs = 1\n",
                        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "model.to(device)\n",
                        "\n",
                        "master_progress_bar = master_bar(range(epochs))\n",
                        "for i in master_progress_bar:\n",
                        "    model.train()\n",
                        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"].to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
                        "        labels = batch[\"label\"].to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        loss.backward()\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"].to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
                        "        labels = batch[\"label\"].to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\""
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Script with the code base"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In most of the `accelerate` documentation it is explained how to use `accelerate` with scripts, so for the moment we will do it like this and at the end we will explain how to do it with a notebook"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "First we are going to create a folder where we are going to save the scripts"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!mkdir accelerate_scripts"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we write the base code in a script"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 40,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/01_code_base.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/01_code_base.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "from fastprogress.fastprogress import master_bar, progress_bar\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 64\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "model.to(device)\n",
                        "\n",
                        "master_progress_bar = master_bar(range(EPOCHS))\n",
                        "for i in master_progress_bar:\n",
                        "    model.train()\n",
                        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"].to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
                        "        labels = batch[\"label\"].to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        loss.backward()\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"].to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
                        "        labels = batch[\"label\"].to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
                        "print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And now we run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 41,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Accuracy = 0.2112                                                               \n",
                                    "CPU times: user 2.12 s, sys: 391 ms, total: 2.51 s\n",
                                    "Wall time: 3min 36s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!python accelerate_scripts/01_code_base.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can see that on my computer it took about 3.5 minutes."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Code with accelerate"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we replace some things\n",
                        "\n",
                        " * First we import `Accelerator` and initialize it.\n",
                        "\n",
                        "``` python\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "```\n",
                        "\n",
                        " * We no longer do the typical\n",
                        "\n",
                        "``` python\n",
                        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "```\n",
                        "\n",
                        " * Instead, we let `accelerate` choose the device by means of\n",
                        "\n",
                        "``` python\n",
                        "device = accelerator.device\n",
                        "```\n",
                        "\n",
                        " * We pass the relevant elements for training through the `prepare` method and no longer do `model.to(device)`.\n",
                        "\n",
                        "``` python\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = preprare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "```\n",
                        "\n",
                        " * We no longer send the data and the model to the GPU with `.to(device)` since `accelerate` has taken care of it with the `prepare` method.\n",
                        "\n",
                        " * Instead of doing the backpropagation with `loss.backward()` we let `accelerate` do it with `loss.backward()`.\n",
                        " \n",
                        "``` python\n",
                        "accelerator.backward(loss)\n",
                        "```\n",
                        "\n",
                        " * When calculating the metric in the validation loop, we need to collect the values of all the points, in case we are doing a distributed training.\n",
                        "\n",
                        "``` python\n",
                        "predictions = accelerator.gather_for_metrics(predictions)\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 19,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/02_accelerate_base_code.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/02_accelerate_base_code.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "from fastprogress.fastprogress import master_bar, progress_bar\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 64\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "master_progress_bar = master_bar(range(EPOCHS))\n",
                        "for i in master_progress_bar:\n",
                        "    model.train()\n",
                        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "    print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
                        "    \n",
                        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
                        "\n",
                        "print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If you notice I have added these two lines `print(f \"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")` and the line `print(f \"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")`, I added them on purpose because they will reveal something very important"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we execute it, to execute the `accelerate` scripts it is done with the command `accelerate launch`.\n",
                        "\n",
                        "``` bash\n",
                        "accelerate launch script.py\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 29,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
                                    "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
                                    "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
                                    "Accuracy = 0.206\n",
                                    "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
                                    "Accuracy = 0.206\n",
                                    "CPU times: user 1.6 s, sys: 272 ms, total: 1.88 s\n",
                                    "Wall time: 2min 37s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/02_accelerate_base_code.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see that before it took about 3 and a half minutes and now it takes about 2 and a half minutes. Quite an improvement. Also if we look at the `print`s we can see that they have been printed twice.\n",
                        "\n",
                        "And how can this be? Because `accelerate` has parallelized the training on the two GPUs I have, so it has been much faster.\n",
                        "\n",
                        "Also, when I ran the first script, that is, when I did not use `accelerate`, the GPU was almost full, while when I ran the second one, that is, the one using `accelerate`, the two GPUs were very little used, so we can increase the batch size to try to fill both of them, let's go for it!"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 27,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/03_accelerate_base_code_more_bs.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/03_accelerate_base_code_more_bs.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "from fastprogress.fastprogress import master_bar, progress_bar\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "master_progress_bar = master_bar(range(EPOCHS))\n",
                        "for i in master_progress_bar:\n",
                        "    model.train()\n",
                        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
                        "\n",
                        "print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "I have removed the extra prints, because we have already seen that the code is running on both GPUs, and I have increased the batch size from 64 to 128."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 28,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Accuracy = 0.1052                                                               \n",
                                    "Accuracy = 0.1052\n",
                                    "CPU times: user 1.41 s, sys: 180 ms, total: 1.59 s\n",
                                    "Wall time: 2min 22s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/03_accelerate_base_code_more_bs.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Increasing the batch size has reduced the execution time by a few seconds."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Process execution"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Execution of code in a single process"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Before we have seen that the `print`s were printed twice, this is because `accelerate` creates as many processes as devices where the code is executed, in my case it creates two processes because I have two GPUs.\n",
                        "\n",
                        "However, not all code should be executed in all processes, for example, the `print`s slow down the code too much to execute it several times, if checkpoints are saved, they would be saved twice, etc.\n",
                        "\n",
                        "In order to execute part of a code in a single process you have to encapsulate it in a function and decorate it with `accelerator.on_local_main_process`, for example in the following code you will see that I created the following function\n",
                        "\n",
                        "``` python\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_something(something):\n",
                        "    print(something)\n",
                        "```\n",
                        "\n",
                        "Another option is to put the code inside an `if accelerator.is_local_main_process` as in the following code\n",
                        "\n",
                        "``` python\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(\"Something\")\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 61,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "from fastprogress.fastprogress import master_bar, progress_bar\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_something(something):\n",
                        "    print(something)\n",
                        "\n",
                        "master_progress_bar = master_bar(range(EPOCHS))\n",
                        "for i in master_progress_bar:\n",
                        "    model.train()\n",
                        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
                        "\n",
                        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's run it and see"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 62,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Accuracy = 0.2098                                                               \n",
                                    "End of script with 0.2098 accuracy\n",
                                    "CPU times: user 1.38 s, sys: 197 ms, total: 1.58 s\n",
                                    "Wall time: 2min 22s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now the print has only been printed once"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "However, although you don't see much, progress bars are executed in each process.\n",
                        "\n",
                        "I have not found a way to avoid this with `fastprogress` progress bars, but with `tqdm` progress bars, so I will replace `fastprogress` progress bars with `tqdm` progress bars and to make them run in a single process add the argument `disable=not accelerator.is_local_main_process`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_something(something):\n",
                        "    print(something)\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [02:01<00:00,  1.45it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  3.30it/s]\n",
                                    "Accuracy = 0.2166\n",
                                    "End of script with 0.2166 accuracy\n",
                                    "CPU times: user 1.33 s, sys: 195 ms, total: 1.52 s\n",
                                    "Wall time: 2min 22s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We have shown an example of how to print in a single process, and this has been a way to execute processes in a single process. But if you just want to print in a single process you can use the `print` method of `accelerate`. Let's see the same example of before with this method"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Writing accelerate_scripts/06_accelerate_base_code_print_one_process.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/06_accelerate_base_code_print_one_process.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45000/45000 [00:02<00:00, 15433.52 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 11406.61 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45000/45000 [00:02<00:00, 15036.87 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14932.76 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14956.60 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [02:00<00:00,  1.46it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.33it/s]\n",
                                    "Accuracy = 0.2134\n",
                                    "End of script with 0.2134 accuracy\n",
                                    "CPU times: user 1.4 s, sys: 189 ms, total: 1.59 s\n",
                                    "Wall time: 2min 27s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/06_accelerate_base_code_print_one_process.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Code execution in all processes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "However there is code that must be executed in all processes, for example if we upload the checkpoints to the hub, so here we have two options, encapsulate the code in a function and decorate it with `accelerator.on_main_process`.\n",
                        "\n",
                        "``` python\n",
                        "@accelerator.on_main_process\n",
                        "def do_my_thing():\n",
                        "    \"Something done once per server\"\n",
                        "    do_thing_once()\n",
                        "```\n",
                        "\n",
                        "or put the code inside an `if accelerator.is_main_process`.\n",
                        "\n",
                        "``` python\n",
                        "if accelerator.is_main_process:\n",
                        "    repo.push_to_hub()\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we are training just to show the `accelerate` library and the model we are training is not good, there is no sense now to upload the checkpoints to the hub, so I am going to make an example with `print`s"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/06_accelerate_base_code_some_code_in_all_process.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_in_one_process(something):\n",
                        "    print(something)\n",
                        "\n",
                        "@accelerator.on_main_process\n",
                        "def print_in_all_processes(something):\n",
                        "    print(something)\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
                        "\n",
                        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_main_process:\n",
                        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it to see"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45000/45000 [00:03<00:00, 14518.44 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45000/45000 [00:03<00:00, 14368.77 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 16466.33 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 14806.14 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14253.33 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14337.07 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [02:00<00:00,  1.46it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.34it/s]\n",
                                    "Accuracy = 0.2092\n",
                                    "End of script with 0.2092 accuracy\n",
                                    "All process: Accuracy = 0.2092\n",
                                    "All process: End of script with 0.2092 accuracy\n",
                                    "CPU times: user 1.42 s, sys: 216 ms, total: 1.64 s\n",
                                    "Wall time: 2min 27s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Execution of code in the process X"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Finally we can specify in which process we want to execute code, for this we must create a function and decorate it with `@accelerator.on_process(process_index=0)`.\n",
                        "\n",
                        "``` python\n",
                        "@accelerator.on_process(process_index=0)\n",
                        "def do_my_thing():\n",
                        "    \"Something done on process index 0\"\n",
                        "    do_thing_on_index_zero()\n",
                        "```\n",
                        "\n",
                        "or decorate it with `@accelerator.on_local_process(local_process_idx=0)`.\n",
                        "\n",
                        "``` python\n",
                        "@accelerator.on_local_process(local_process_index=0)\n",
                        "def do_my_thing():\n",
                        "    \"Something done on process index 0 on each server\".\n",
                        "    do_thing_on_index_zero_on_each_server()\n",
                        "```\n",
                        "\n",
                        "Here I have put the process 0, but you can put any number"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 18,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/07_accelerate_base_code_some_code_in_some_process.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_in_one_process(something):\n",
                        "    print(something)\n",
                        "\n",
                        "@accelerator.on_main_process\n",
                        "def print_in_all_processes(something):\n",
                        "    print(something)\n",
                        "\n",
                        "@accelerator.on_process(process_index=0)\n",
                        "def print_in_process_0(something):\n",
                        "    print(\"Process 0: \" + something)\n",
                        "\n",
                        "@accelerator.on_local_process(local_process_index=1)\n",
                        "def print_in_process_1(something):\n",
                        "    print(\"Process 1: \" + something)\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
                        "\n",
                        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_main_process:\n",
                        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
                        "\n",
                        "print_in_process_0(\"End of process 0\")\n",
                        "print_in_process_1(\"End of process 1\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 15,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 15735.58 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14906.20 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [02:02<00:00,  1.44it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  3.27it/s]\n",
                                    "Process 1: End of process 1\n",
                                    "Accuracy = 0.2128\n",
                                    "End of script with 0.2128 accuracy\n",
                                    "All process: Accuracy = 0.2128\n",
                                    "All process: End of script with 0.2128 accuracy\n",
                                    "Process 0: End of process 0\n",
                                    "CPU times: user 1.42 s, sys: 295 ms, total: 1.71 s\n",
                                    "Wall time: 2min 37s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Synchronize processes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If we have code that must be executed in all processes, it is interesting to wait for it to finish in all processes before doing another task, so for this we use `accelerator.wait_for_everyone()`.\n",
                        "\n",
                        "To see this we are going to put a delay in one of the print functions in a process\n",
                        "\n",
                        "I've also put a break in the training loop so that he doesn't spend too much time training, which is not what we're interested in right now."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 22,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/08_accelerate_base_code_sync_all_process.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/09_accelerate_base_code_sync_all_process.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "import time\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_in_one_process(something):\n",
                        "    print(something)\n",
                        "\n",
                        "@accelerator.on_main_process\n",
                        "def print_in_all_processes(something):\n",
                        "    print(something)\n",
                        "\n",
                        "@accelerator.on_process(process_index=0)\n",
                        "def print_in_process_0(something):\n",
                        "    time.sleep(2)\n",
                        "    print(\"Process 0: \" + something)\n",
                        "\n",
                        "@accelerator.on_local_process(local_process_index=1)\n",
                        "def print_in_process_1(something):\n",
                        "    print(\"Process 1: \" + something)\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "        break\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_local_main_process:\n",
                        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
                        "\n",
                        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "if accelerator.is_main_process:\n",
                        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
                        "\n",
                        "print_in_one_process(\"Printing with delay in process 0\")\n",
                        "print_in_process_0(\"End of process 0\")\n",
                        "print_in_process_1(\"End of process 1\")\n",
                        "accelerator.wait_for_everyone()\n",
                        "\n",
                        "print_in_one_process(\"End of script\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 23,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 14218.23 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 14666.25 examples/s]\n",
                                    "  0%|                                                   | 0/176 [00:00<?, ?it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.58it/s]\n",
                                    "Process 1: End of process 1\n",
                                    "Accuracy = 0.212\n",
                                    "End of script with 0.212 accuracy\n",
                                    "All process: Accuracy = 0.212\n",
                                    "All process: End of script with 0.212 accuracy\n",
                                    "Printing with delay in process 0\n",
                                    "Process 0: End of process 0\n",
                                    "End of script\n"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate launch accelerate_scripts/09_accelerate_base_code_sync_all_process.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As you can see first we have printed `Process 1: End of process 1` and then the rest, this is because the rest of the prints are made either in process 0 or in all processes, so until the 2 seconds delay we have set is not finished the rest of the code is not executed."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Save and load the state dict"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "When we train, we sometimes save the state so that we can continue at a later time.\n",
                        "\n",
                        "To save the state we will have to use the `save_state()` and `load_state()` methods."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 66,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/09_accelerate_save_and_load_checkpoints.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/10_accelerate_save_and_load_checkpoints.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_something(something):\n",
                        "    print(something)\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "\n",
                        "    # Guardamos los pesos\n",
                        "    accelerator.save_state(\"accelerate_scripts/checkpoints\")\n",
                        "\n",
                        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
                        "\n",
                        "# Cargamos los pesos\n",
                        "accelerator.load_state(\"accelerate_scripts/checkpoints\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 67,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [01:58<00:00,  1.48it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.40it/s]\n",
                                    "Accuracy = 0.2142\n"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate launch accelerate_scripts/10_accelerate_save_and_load_checkpoints.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Save the model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "When the `prepare` method was used, the model was wrapped in order to save it to the necessary devices. So when saving it we have to use the `save_model` method which first unwraps it and then saves it. Also if we use the `safe_serialization=True` parameter the model will be saved as a `safe tensor`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Writing accelerate_scripts/11_accelerate_save_model.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/11_accelerate_save_model.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_something(something):\n",
                        "    print(something)\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "\n",
                        "    # Guardamos el modelo\n",
                        "    accelerator.wait_for_everyone()\n",
                        "    accelerator.save_model(model, \"accelerate_scripts/model\", safe_serialization=True)\n",
                        "\n",
                        "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 78,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [01:58<00:00,  1.48it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.35it/s]\n",
                                    "Accuracy = 0.214\n"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate launch accelerate_scripts/11_accelerate_save_model.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Save the `pretrained` model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In models that use the `transformers` library we must save the model with the `save_pretrained` method to be able to load it with the `from_pretrained` method. Before saving it we must unwrap it with the `unwrap_model` method."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 79,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Writing accelerate_scripts/11_accelerate_save_pretrained.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/12_accelerate_save_pretrained.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "@accelerator.on_local_main_process\n",
                        "def print_something(something):\n",
                        "    print(something)\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "\n",
                        "    # Guardamos el modelo pretrained\n",
                        "    unwrapped_model = accelerator.unwrap_model(model)\n",
                        "    unwrapped_model.save_pretrained(\n",
                        "        \"accelerate_scripts/model_pretrained\",\n",
                        "        is_main_process=accelerator.is_main_process,\n",
                        "        save_function=accelerator.save,\n",
                        "    )\n",
                        "\n",
                        "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 80,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45000/45000 [00:02<00:00, 15152.47 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45000/45000 [00:02<00:00, 15119.13 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 12724.70 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 12397.49 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 15247.21 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 15138.03 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [01:59<00:00,  1.48it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.37it/s]\n",
                                    "Accuracy = 0.21\n"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate launch accelerate_scripts/12_accelerate_save_pretrained.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we could load it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 82,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of RobertaModel were not initialized from the model checkpoint at accelerate_scripts/model_pretrained and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import AutoModel\n",
                        "\n",
                        "checkpoints = \"accelerate_scripts/model_pretrained\"\n",
                        "tokenizer = AutoModel.from_pretrained(checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Training on notebooks"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "So far we have seen how to run scripts, but if you want to run the code on a notebook, we can write the same code as before, but encapsulated in a function"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "First we import the libraries"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "import time\n",
                        "# from accelerate import Accelerator"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we create the function"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def train_code(batch_size: int = 64):\n",
                        "    from accelerate import Accelerator\n",
                        "    accelerator = Accelerator()\n",
                        "\n",
                        "    dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "    num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "    max_len = 130\n",
                        "\n",
                        "    checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "    tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "    def tokenize_function(dataset):\n",
                        "        return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "    tokenized_dataset = {\n",
                        "        \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "        \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "        \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    }\n",
                        "    tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "    tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "    tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "    BS = batch_size\n",
                        "    dataloader = {\n",
                        "        \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "        \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "        \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "    }\n",
                        "\n",
                        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "    model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "    loss_function = torch.nn.CrossEntropyLoss()\n",
                        "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "    metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "    EPOCHS = 1\n",
                        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "    device = accelerator.device\n",
                        "\n",
                        "    # model.to(device)\n",
                        "    model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "    for i in range(EPOCHS):\n",
                        "        model.train()\n",
                        "        progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "        for batch in progress_bar_train:\n",
                        "            optimizer.zero_grad()\n",
                        "\n",
                        "            input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "            labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "            loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "            # loss.backward()\n",
                        "            accelerator.backward(loss)\n",
                        "            optimizer.step()\n",
                        "\n",
                        "        model.eval()\n",
                        "        progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "        for batch in progress_bar_validation:\n",
                        "            input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "            labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "            with torch.no_grad():\n",
                        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "            predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "            # Recopilamos las predicciones de todos los dispositivos\n",
                        "            predictions = accelerator.gather_for_metrics(predictions)\n",
                        "            labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "            accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "        accuracy = metric.compute()\n",
                        "        \n",
                        "    accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In order to run the training on the notebook we use the `notebook_launcher` function, to which we pass the function we want to run, the arguments of that function and the number of GPUs on which we are going to train with the variable `num_processes`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Launching training on 2 GPUs.\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [02:01<00:00,  1.45it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  3.31it/s]\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Accuracy = 0.2112\n"
                              ]
                        }
                  ],
                  "source": [
                        "from accelerate import notebook_launcher\n",
                        "\n",
                        "args = (128,)\n",
                        "notebook_launcher(train_code, args, num_processes=2)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Training in FP16"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "When we first set up `accelerate` it asked us `Do you wish to use FP16 or BF16 (mixed precision)?` and we said no, so now we are going to say yes, we want to use FP16."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "So far we have trained in FP32, which means that each weight of the model is a 32-bit floating point number, and now we are going to use a 16-bit floating point number, that is, the model will occupy less space. So two things will happen, we will be able to use a larger batch size and it will also be faster."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "First we re-launch `accelerate config` and we will tell it that we want FP16"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "--------------------------------------------------------------------------------\n",
                                    "In which compute environment are you running?\n",
                                    "This machine\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "multi-GPU\n",
                                    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
                                    "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
                                    "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
                                    "Do you want to use DeepSpeed? [yes/NO]: no\n",
                                    "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
                                    "Do you want to use Megatron-LM ? [yes/NO]: no\n",
                                    "How many GPU(s) should be used for distributed training? [1]:2\n",
                                    "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "Do you wish to use FP16 or BF16 (mixed precision)?\n",
                                    "fp16\n",
                                    "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate config"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we create a script to train, with the same batch size as before, to see if it takes less time to train"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/12_accelerate_base_code_fp16_bs128.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/13_accelerate_base_code_fp16_bs128.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 128\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it and see how long it takes"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 14983.76 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14315.47 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [01:01<00:00,  2.88it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  6.84it/s]\n",
                                    "Accuracy = 0.2094\n",
                                    "CPU times: user 812 ms, sys: 163 ms, total: 976 ms\n",
                                    "Wall time: 1min 27s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/13_accelerate_base_code_fp16_bs128.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "When we ran this training in FP32 it took about 2 minutes and a half, and now it takes about 1 minute and a half. Let's see if now instead of training with a batch size of 128, we do it with a batch size of 256."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/14_accelerate_base_code_fp16_bs256.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 256\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 7,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 15390.30 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 14990.92 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:54<00:00,  1.62it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.45it/s]\n",
                                    "Accuracy = 0.2236\n",
                                    "CPU times: user 670 ms, sys: 91.6 ms, total: 761 ms\n",
                                    "Wall time: 1min 12s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "It has dropped only about 15 seconds"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### BF16 Training"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Before we have trained in FP16 and now we are going to train in BF16, what is the difference?\n",
                        "\n",
                        "![FP32_FP16_BF16](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/FP32_FP16_BF16.webp)\n",
                        "\n",
                        "As we can see in the picture, while FP16 compared to FP32 has fewer bits in the mantissa and exponent, which makes its range much smaller, BF16 compared to FP32 has the same number of bits in the exponent but fewer in the mantissa, which makes BF16 have the same range of numbers as FP32, but it is less accurate.\n",
                        "\n",
                        "This is beneficial because in FP16 some calculations could give very high numbers, which in FP16 format could not be represented. In addition there are certain HW devices that are optimized for this format."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As before, we execute `accelerate config` and indicate that we want BF16."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "--------------------------------------------------------------------------------\n",
                                    "In which compute environment are you running?\n",
                                    "This machine\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "multi-GPU\n",
                                    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
                                    "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
                                    "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
                                    "Do you want to use DeepSpeed? [yes/NO]: no\n",
                                    "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
                                    "Do you want to use Megatron-LM ? [yes/NO]: no\n",
                                    "How many GPU(s) should be used for distributed training? [1]:2\n",
                                    "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "Do you wish to use FP16 or BF16 (mixed precision)?\n",
                                    "bf16\n",
                                    "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate config"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we run the last script we created, i.e. with a batch size of 256"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14814.95 examples/s]\n",
                                    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14506.83 examples/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:51<00:00,  1.70it/s]\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.21it/s]\n",
                                    "Accuracy = 0.2112\n",
                                    "CPU times: user 688 ms, sys: 144 ms, total: 832 ms\n",
                                    "Wall time: 1min 17s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "It took a similar time to what it took before, which is normal, since we have trained a model with 16-bit weights, just like before."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Training in FP8"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we are going to train in FP8 format, which as its name suggests, is a floating point format, where each weight has 8 bits, so we run `accelerate config` to tell it that we want FP8"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "--------------------------------------------------------------------------------\n",
                                    "In which compute environment are you running?\n",
                                    "This machine\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "multi-GPU\n",
                                    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
                                    "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
                                    "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
                                    "Do you want to use DeepSpeed? [yes/NO]: no\n",
                                    "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
                                    "Do you want to use Megatron-LM ? [yes/NO]: no\n",
                                    "How many GPU(s) should be used for distributed training? [1]:2\n",
                                    "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
                                    "--------------------------------------------------------------------------------\n",
                                    "Do you wish to use FP16 or BF16 (mixed precision)?\n",
                                    "fp8\n",
                                    "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
                              ]
                        }
                  ],
                  "source": [
                        "!accelerate config"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we execute the last script, the batch size of 256"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 11,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Traceback (most recent call last):\n",
                                    "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
                                    "    accelerator = Accelerator()\n",
                                    "                  ^^^^^^^^^^^^^\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
                                    "    self.state = AcceleratorState(\n",
                                    "                 ^^^^^^^^^^^^^^^^^\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
                                    "    raise ValueError(\n",
                                    "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
                                    "Traceback (most recent call last):\n",
                                    "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
                                    "    accelerator = Accelerator()\n",
                                    "                  ^^^^^^^^^^^^^\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
                                    "    self.state = AcceleratorState(\n",
                                    "                 ^^^^^^^^^^^^^^^^^\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
                                    "    raise ValueError(\n",
                                    "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
                                    "[2024-05-13 21:40:56,455] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 501480) of binary: /home/wallabot/miniconda3/envs/nlp/bin/python\n",
                                    "Traceback (most recent call last):\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/bin/accelerate\", line 8, in <module>\n",
                                    "    sys.exit(main())\n",
                                    "             ^^^^^^\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
                                    "    args.func(args)\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
                                    "    multi_gpu_launcher(args)\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
                                    "    distrib_run.run(args)\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
                                    "    elastic_launch(\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
                                    "    return launch_agent(self._config, self._entrypoint, list(args))\n",
                                    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
                                    "    raise ChildFailedError(\n",
                                    "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
                                    "============================================================\n",
                                    "accelerate_scripts/13_accelerate_base_code_fp16_bs256.py FAILED\n",
                                    "------------------------------------------------------------\n",
                                    "Failures:\n",
                                    "[1]:\n",
                                    "  time      : 2024-05-13_21:40:56\n",
                                    "  host      : wallabot\n",
                                    "  rank      : 1 (local_rank: 1)\n",
                                    "  exitcode  : 1 (pid: 501481)\n",
                                    "  error_file: <N/A>\n",
                                    "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
                                    "------------------------------------------------------------\n",
                                    "Root Cause (first observed failure):\n",
                                    "[0]:\n",
                                    "  time      : 2024-05-13_21:40:56\n",
                                    "  host      : wallabot\n",
                                    "  rank      : 0 (local_rank: 0)\n",
                                    "  exitcode  : 1 (pid: 501480)\n",
                                    "  error_file: <N/A>\n",
                                    "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
                                    "============================================================\n",
                                    "CPU times: user 65.1 ms, sys: 14.5 ms, total: 79.6 ms\n",
                                    "Wall time: 7.24 s\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As the weights are now 8 bits and occupy half of the memory, we will increase the batch size to 512."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Writing accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader\n",
                        "from torch.optim import Adam\n",
                        "from datasets import load_dataset\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import evaluate\n",
                        "import tqdm\n",
                        "\n",
                        "# Importamos e inicializamos Accelerator\n",
                        "from accelerate import Accelerator\n",
                        "accelerator = Accelerator()\n",
                        "\n",
                        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
                        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
                        "max_len = 130\n",
                        "\n",
                        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "\n",
                        "def tokenize_function(dataset):\n",
                        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                        "tokenized_dataset = {\n",
                        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
                        "}\n",
                        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
                        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
                        "\n",
                        "BS = 512\n",
                        "dataloader = {\n",
                        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
                        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
                        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
                        "}\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
                        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
                        "\n",
                        "loss_function = torch.nn.CrossEntropyLoss()\n",
                        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
                        "metric = evaluate.load(\"accuracy\")\n",
                        "\n",
                        "EPOCHS = 1\n",
                        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "device = accelerator.device\n",
                        "\n",
                        "# model.to(device)\n",
                        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
                        "\n",
                        "for i in range(EPOCHS):\n",
                        "    model.train()\n",
                        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_train:\n",
                        "        optimizer.zero_grad()\n",
                        "\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        loss = loss_function(outputs['logits'], labels)\n",
                        "\n",
                        "        # loss.backward()\n",
                        "        accelerator.backward(loss)\n",
                        "        optimizer.step()\n",
                        "\n",
                        "    model.eval()\n",
                        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
                        "    for batch in progress_bar_validation:\n",
                        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
                        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
                        "        labels = batch[\"label\"]#.to(device)\n",
                        "\n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
                        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
                        "        # Recopilamos las predicciones de todos los dispositivos\n",
                        "        predictions = accelerator.gather_for_metrics(predictions)\n",
                        "        labels = accelerator.gather_for_metrics(labels)\n",
                        "\n",
                        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
                        "    accuracy = metric.compute()\n",
                        "    \n",
                        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We run it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "%%time\n",
                        "\n",
                        "!accelerate launch accelerate_scripts/15_accelerate_base_code_fp8_bs512.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Model inference"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Using the Hugging Face Ecosystem"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's see how to do large model inference with the `transformers` library of hugging face."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Inference with `pipeline`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If we use the Hugging Face ecosystem it is very simple, since everything is produced underneath without us having to do much. In the case of using `pipeline`, which is the easiest way to do inference with the `transformers` library, we simply have to tell it the model we want to use and very important, pass `device_map=\"auto\"`. This will cause `accelerate` to distribute the model among the different GPUs, CPU RAM or hard disk if necessary."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "There are more possible values for `device_map`, which we will see later, but for now stay with `\"auto\"`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We are going to use the `Llama3 8B` model, which as its name indicates is a model of about 8 billion parameters, as each parameter by default is in FP32 format, which corresponds to 4 bytes (32 bits), that means that if we multiply 8 billion parameters by 4 bytes, we would need a GPU with about 32 GB of VRAM.\n",
                        "\n",
                        "In my case I have 2 GPUs with 24 GB of VRAM, so it would not fit on a single GPU. But thanks to put `device_map=\"auto\"`, accelerate will distribute the model between the two GPUs and I will be able to make the inference."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/09_inference_with_pipeline.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/16_inference_with_pipeline.py\n",
                        "\n",
                        "from transformers import pipeline\n",
                        "\n",
                        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
                        "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
                        "\n",
                        "prompt = \"Conoces accelerate de hugging face?\"\n",
                        "output = generator(prompt)\n",
                        "print(output)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we run it, but since pipeline uses accelerate below, we don't need to run it with `accelerate launch script.py` but with `python script.py` will do."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.27s/it]\n",
                                    "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                                    "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                                    "[{'generated_text': 'Conoces accelerate de hugging face? Â¿QuÃ© es el modelo de lenguaje de transformers y cÃ³mo se utiliza en el marco de hugging face? Â¿CÃ³mo puedo utilizar modelos de lenguaje de transformers en mi aplicaciÃ³n? Â¿QuÃ© son los tokenizers y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo crear un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los datasets y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar datasets para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los checkpoints y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los evaluadores y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los pre-trainados y cÃ³mo se utilizan en el marco de hugging face? Â¿CÃ³mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? Â¿QuÃ© son los finetuning'}]\n"
                              ]
                        }
                  ],
                  "source": [
                        "!python accelerate_scripts/16_inference_with_pipeline.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As you can see, it did not answer, but kept asking questions. This is because Llama3 is a language model that predicts the next token, so with the prompt that I have passed it, it has considered that the next best tokens are those that correspond to more questions. Which makes sense, because there are times when people have doubts about a topic and generates many questions, so to answer the question we have to condition it a little bit"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/10_inference_with_pipeline_condition.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/17_inference_with_pipeline_condition.py\n",
                        "\n",
                        "from transformers import pipeline\n",
                        "\n",
                        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
                        "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
                        "\n",
                        "prompt = \"Conoces accelerate de hugging face?\"\n",
                        "messages = [\n",
                        "    {\n",
                        "        \"role\": \"system\",\n",
                        "        \"content\": \"Eres un chatbot amigable que siempre intenta solucionar las dudas\",\n",
                        "    },\n",
                        "    {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
                        "]\n",
                        "output = generator(messages)\n",
                        "print(output[0]['generated_text'][-1])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As you can see, a message has been generated with roles, conditioning the model and with the following prompt"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.41s/it]\n",
                                    "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                                    "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                                    "{'role': 'assistant', 'content': 'Â¡Hola!\\n\\nSÃ­, conozco Accelerate de Hugging Face. Accelerate es una biblioteca de Python desarrollada por Hugging Face que se enfoca en simplificar y acelerar el entrenamiento y la evaluaciÃ³n de modelos de lenguaje en diferentes dispositivos y entornos.\\n\\nCon Accelerate, puedes entrenar modelos de lenguaje en diferentes plataformas y dispositivos, como GPUs, TPUs, CPUs y servidores, sin necesidad de cambiar el cÃ³digo de tu modelo. Esto te permite aprovechar al mÃ¡ximo la potencia de cÃ¡lculo de tus dispositivos y reducir el tiempo de entrenamiento.\\n\\nAccelerate tambiÃ©n ofrece varias caracterÃ­sticas adicionales, como:\\n\\n* Soporte para diferentes frameworks de machine learning, como TensorFlow, PyTorch y JAX.\\n* IntegraciÃ³n con diferentes sistemas de almacenamiento y procesamiento de datos, como Amazon S3 y Google Cloud Storage.\\n* Soporte para diferentes protocolos de comunicaciÃ³n, como HTTP y gRPC.\\n* Herramientas para monitorear y depurar tus modelos en tiempo real.\\n\\nEn resumen, Accelerate es una herramienta muy Ãºtil para desarrolladores de modelos de lenguaje que buscan simplificar y acelerar el proceso de entrenamiento y evaluaciÃ³n de sus modelos.\\n\\nÂ¿Tienes alguna pregunta especÃ­fica sobre Accelerate o necesitas ayuda para implementarlo en tu proyecto?'}\n"
                              ]
                        }
                  ],
                  "source": [
                        "!python accelerate_scripts/17_inference_with_pipeline_condition.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now the answer if it responds to our prompt"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Inference with `AutoClass`"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Finally we will see how to do the inference only with `AutoClass`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Overwriting accelerate_scripts/11_inference_with_autoclass.py\n"
                              ]
                        }
                  ],
                  "source": [
                        "%%writefile accelerate_scripts/18_inference_with_autoclass.py\n",
                        "\n",
                        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
                        "\n",
                        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
                        "\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints, device_map=\"auto\")\n",
                        "model = AutoModelForCausalLM.from_pretrained(checkpoints, device_map=\"auto\")\n",
                        "streamer = TextStreamer(tokenizer)\n",
                        "\n",
                        "prompt = \"Conoces accelerate de hugging face?\"\n",
                        "tokens_input = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
                        "\n",
                        "_ = model.generate(**tokens_input, streamer=streamer, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As you can see, the `streamer` object has been created and then passed to the `generate` method of the model. This is useful so that each word is printed as it is generated and you don't have to wait for all the output to be generated before printing it."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                                    "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.28s/it]\n",
                                    "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                                    "<|begin_of_text|>Conoces accelerate de hugging face? Si es asÃ­, puedes utilizar la biblioteca `transformers` de Hugging Face para crear un modelo de lenguaje que pueda predecir la siguiente palabra en una secuencia de texto.\n",
                                    "\n",
                                    "AquÃ­ te muestro un ejemplo de cÃ³mo hacerlo:\n",
                                    "```\n",
                                    "import pandas as pd\n",
                                    "import torch\n",
                                    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                                    "\n",
                                    "# Cargar el modelo y el tokenizador\n",
                                    "model_name = \"bert-base-uncased\"\n",
                                    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                                    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                                    "\n",
                                    "# Cargar el conjunto de datos\n",
                                    "train_df = pd.read_csv(\"train.csv\")\n",
                                    "test_df = pd.read_csv(\"test.csv\")\n",
                                    "\n",
                                    "# Preprocesar los datos\n",
                                    "train_texts = train_df[\"text\"]\n",
                                    "train_labels = train_df[\"label\"]\n",
                                    "test_texts = test_df[\"text\"]\n",
                                    "\n",
                                    "# Convertir los textos en entradas para el modelo\n",
                                    "train_encodings = tokenizer.batch_encode_plus(train_texts, \n",
                                    "                                              add_special_tokens=True, \n",
                                    "                                              max_length=512, \n",
                                    "                                              return_attention_mask=True, \n",
                                    "                                              return_tensors='pt')\n",
                                    "\n",
                                    "test_encodings = tokenizer.batch_encode_plus(test_texts, \n",
                                    "                                             add_special_tokens=True, \n",
                                    "                                             max_length=512, \n",
                                    "                                             return_attention_mask=True, \n",
                                    "                                             return_tensors='pt')\n",
                                    "\n",
                                    "# Crear un dataloader para entrenar el modelo\n",
                                    "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], \n",
                                    "                                               train_encodings[\"attention_mask\"], \n",
                                    "                                               torch.tensor(train_labels))\n",
                                    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                                    "\n",
                                    "# Entrenar el modelo\n",
                                    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                                    "model.to(device)\n",
                                    "criterion = torch.nn.CrossEntropyLoss()\n",
                                    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
                                    "\n",
                                    "for epoch in range(5):\n",
                                    "    model.train()\n",
                                    "    total_loss = 0\n",
                                    "    for batch in train_loader:\n",
                                    "        input_ids = batch[0].to(device)\n",
                                    "        attention_mask = batch[1].to(device)\n",
                                    "        labels = batch[2].to(device)\n",
                                    "        optimizer.zero_grad()\n",
                                    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                                    "        loss = criterion(outputs, labels)\n",
                                    "        loss.backward()\n",
                                    "        optimizer.step()\n",
                                    "        total_loss += loss.item()\n",
                                    "    print(f\"Epoch {epoch+1}, Loss: {total\n"
                              ]
                        }
                  ],
                  "source": [
                        "!python accelerate_scripts/18_inference_with_autoclass.py"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Use pytorch"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Normally the way to make inferences with pytorch is to create a model with the weights initialized randomly and then load a `state dict` with the weights of the pre-trained model, so to get that `state dict` we are going to make a little trick first and download them"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/maximo.fernandez/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
                                    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [02:48<00:00, 1.43MB/s] \n"
                              ]
                        }
                  ],
                  "source": [
                        "import torch\n",
                        "import torchvision.models as models\n",
                        "\n",
                        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
                        "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now that we have the `state dict` let's do inference as it is normally done in pytorch"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([1, 1000])"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "import torch\n",
                        "import torchvision.models as models\n",
                        "\n",
                        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     # Set device\n",
                        "\n",
                        "resnet152 = models.resnet152().to(device) # Create model with random weights and move to device\n",
                        "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device) # Load pretrained weights into device memory\n",
                        "resnet152.load_state_dict(state_dict) # Load this weights into the model\n",
                        "\n",
                        "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
                        "output = resnet152(input)\n",
                        "output.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let us explain what happened\n",
                        "\n",
                        " * When we did `resnet152 = models.resnet152().to(device)` a resnet152 with random weights was loaded into GPU memory.\n",
                        " * When we did `state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)` a dictionary with the trained weights was loaded into GPU memory.\n",
                        " * When we have done `resnet152.load_state_dict(state_dict)` these pre-trained weights have been assigned to the model.\n",
                        "\n",
                        "In other words, the model has been loaded twice in the GPU memory."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "You may be wondering why we have done this first.\n",
                        "\n",
                        "``` python\n",
                        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
                        "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')\n",
                        "```\n",
                        "\n",
                        "To then make\n",
                        "\n",
                        "``` python\n",
                        "resnet152 = models.resnet152().to(device)\n",
                        "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)\n",
                        "resnet152.load_state_dict(state_dict)\n",
                        "```\n",
                        "\n",
                        "And why don't we use directly\n",
                        "\n",
                        "```\n",
                        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
                        "```\n",
                        "\n",
                        "And we stop saving the `state dict` to load it later. Well, because Pytorch, by edbajo does the same thing that we have done. So to be able to see the whole process we have done in several lines what Pytorch does in one line"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "This way of working has worked well until now, as long as the models had a manageable size for user GPUs. But since the advent of LLMs this approach does not make sense.\n",
                        "\n",
                        "For example, a 6B model of parameters would occupy 24 GB of memory, and since it is loaded twice with this way of working, a 48 GB GPU would be required."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "So to fix this, the way to load a pre-trained Pytorch model is:\n",
                        " * Create an empty model with `init_empty_weights` that will not occupy RAM.\n",
                        " * Then load the weights with `load_checkpoint_and_dispatch` which will load a checkpoint inside the empty model and distribute the weights for each layer on all available devices (GPU, CPU RAM and hard disk), thanks to setting `device_map=\"auto\"`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([1, 1000])"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "import torch\n",
                        "import torchvision.models as models\n",
                        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
                        "\n",
                        "with init_empty_weights():\n",
                        "    resnet152 = models.resnet152()\n",
                        "\n",
                        "resnet152 = load_checkpoint_and_dispatch(resnet152, checkpoint='accelerate_scripts/resnet152_pretrained.pth', device_map=\"auto\")\n",
                        "\n",
                        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                        "\n",
                        "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
                        "output = resnet152(input)\n",
                        "output.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### How accelerate works below"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In this video you can see graphically how accelerate works below"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/MWCSGj9jEAo\" title=\"Accelerate Big Model Inference: How Does it Work?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Initialization of an empty model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Accelerate creates the skeleton of an empty model using `init_empty_weights` to occupy as little memory as possible."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "For example, let's see how much RAM I now have available on my computer"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total RAM: 31.24 GB, Available RAM: 22.62 GB, Used RAM: 7.82 GB\n"
                              ]
                        }
                  ],
                  "source": [
                        "import psutil\n",
                        "\n",
                        "def get_ram_info():\n",
                        "    ram = dict(psutil.virtual_memory()._asdict())\n",
                        "    print(f\"Total RAM: {(ram['total']/1024/1024/1024):.2f} GB, Available RAM: {(ram['available']/1024/1024/1024):.2f} GB, Used RAM: {(ram['used']/1024/1024/1024):.2f} GB\")\n",
                        "\n",
                        "get_ram_info()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "I have about 22 GB of RAM available\n",
                        "\n",
                        "Now let's try to create a model 5000x1000x1000 parameters, i.e. 5B parameters, if each parameter is in FP32, it means 20 GB of RAM."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "from torch import nn\n",
                        "\n",
                        "model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If we look at RAM again"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total RAM: 31.24 GB, Available RAM: 3.77 GB, Used RAM: 26.70 GB\n"
                              ]
                        }
                  ],
                  "source": [
                        "get_ram_info()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we can see now we only have 3 GB of RAM available."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now let's delete the model to free RAM"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total RAM: 31.24 GB, Available RAM: 22.44 GB, Used RAM: 8.03 GB\n"
                              ]
                        }
                  ],
                  "source": [
                        "del model\n",
                        "get_ram_info()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We again have about 22 GB of RAM available."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's now use `init_empty_weights` from `accelerate` and then we see the RAM"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total RAM: 31.24 GB, Available RAM: 22.32 GB, Used RAM: 8.16 GB\n"
                              ]
                        }
                  ],
                  "source": [
                        "from accelerate import init_empty_weights\n",
                        "\n",
                        "with init_empty_weights():\n",
                        "    model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])\n",
                        "\n",
                        "get_ram_info()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Before we had exactly 22.44 GB free and after creating the model with `init_empty_weights` we have 22.32 GB. The saving in RAM is enormous! Almost no RAM has been used to create the model.\n",
                        "\n",
                        "This is based on the metadevice introduced in PyTorch 1.9, so it is important that to use `accelerate` we have a later version of Pytorch."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Loading weights"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Once we have initialized the model we have to load the weights using `load_checkpoint_and_dispatch` which, as its name indicates, loads the weights and sends them to the device or devices required."
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "base",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.8"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
