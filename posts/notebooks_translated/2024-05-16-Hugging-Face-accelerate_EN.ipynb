{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face Accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Accelerate` is a Hugging Face library that allows running the same PyTorch code in any distributed setup by adding only four lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To install `accelerate` with `pip`, simply run:",
        "\n",
        "``` bash\n",
        "pip install accelerate",
        "```\n",
        "\n",
        "And with `conda`:",
        "\n",
        "``` bash\n",
        "conda install -c conda-forge accelerate",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In each environment where `accelerate` is installed, the first thing to do is to configure it. To do this, run in a terminal:",
        "\n",
        "``` bash\n",
        "`accelerate config`",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "no\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In my case, the answers have been",
        "* In which compute environment are you running?",
        "- [x] \"This machine\"",
        "- [_] \"AWS (Amazon SageMaker)\"",
        "> I want to set it up on my computer",
        "\n",
        "* What type of machine are you using?",
        "- [_] multi-CPU",
        "- [_] multi-XPU",
        "- [x] multi-GPU",
        "- [_] multi-NPU",
        "- [_] TPU",
        "> Since I have 2 GPUs and want to run distributed codes on them, I choose `multi-GPU`",
        " \n",
        "* How many different machines will you use (use more than 1 for multi-node training)? [1]:",
        "- 1",
        "> I choose `1` because I'm only going to run it on my computer",
        "\n",
        "* Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]:",
        "- no",
        "> With this option, you can choose to have `accelerate` check for errors during execution, but it would make it slower, so I choose `no`, and if there are any errors I change it to `yes`.",
        " \n",
        "* Do you wish to optimize your script with torch dynamo? [yes/NO]:",
        "- no",
        "\n",
        "* Do you want to use FullyShardedDataParallel? [yes/NO]:",
        "- no",
        " \n",
        "* Do you want to use Megatron-LM? [yes/NO]:",
        "- no",
        " \n",
        "* How many GPU(s) should be used for distributed training? [1]:",
        "- 2",
        "> I choose `2` because I have 2 GPUs",
        "\n",
        "* What GPU(s) (by id) should be used for training on this machine as a comma-separated list? [all]:",
        "- 0.1",
        "> I choose `0,1` because I want to use both GPUs",
        "\n",
        "* Do you wish to use FP16 or BF16 (mixed precision)?",
        "- [x] no",
        "- [_] fp16",
        "- [_] bf16",
        "- [_] fp8",
        "> For now I choose `no`, because to simplify the code when not using `accelerate` we are going to train in fp32, but ideally we would use fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The configuration will be saved in `~/.cache/huggingface/accelerate/default_config.yaml` and can be modified at any time. Let's see what's inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compute_environment: LOCAL_MACHINE\n",
            "debug: false\n",
            "distributed_type: MULTI_GPU\n",
            "downcast_bf16: 'no'\n",
            "gpu_ids: 0,1\n",
            "machine_rank: 0\n",
            "main_training_function: main\n",
            "mixed_precision: fp16\n",
            "num_machines: 1\n",
            "num_processes: 2\n",
            "rdzv_backend: static\n",
            "same_network: true\n",
            "tpu_env: []\n",
            "tpu_use_cluster: false\n",
            "tpu_use_sudo: false\n",
            "use_cpu: false\n"
          ]
        }
      ],
      "source": [
        "!cat ~/.cache/huggingface/accelerate/default_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another way to view the configuration we have is by running in a terminal:",
        "\n",
        "``` bash\n",
        "accelerate env",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Copy-and-paste the text below in your GitHub issue\n",
            "\n",
            "- `Accelerate` version: 0.28.0\n",
            "- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\n",
            "- Python version: 3.11.8\n",
            "- Numpy version: 1.26.4\n",
            "- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
            "- PyTorch XPU available: False\n",
            "- PyTorch NPU available: False\n",
            "- System RAM: 31.24 GB\n",
            "- GPU type: NVIDIA GeForce RTX 3090\n",
            "- `Accelerate` default config:\n",
            "\t- compute_environment: LOCAL_MACHINE\n",
            "\t- distributed_type: MULTI_GPU\n",
            "\t- mixed_precision: fp16\n",
            "\t- use_cpu: False\n",
            "\t- debug: False\n",
            "\t- num_processes: 2\n",
            "\t- machine_rank: 0\n",
            "\t- num_machines: 1\n",
            "\t- gpu_ids: 0,1\n",
            "\t- rdzv_backend: static\n",
            "\t- same_network: True\n",
            "\t- main_training_function: main\n",
            "\t- downcast_bf16: no\n",
            "\t- tpu_use_cluster: False\n",
            "\t- tpu_use_sudo: False\n",
            "\t- tpu_env: []\n"
          ]
        }
      ],
      "source": [
        "!accelerate env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have configured `accelerate` we can test if we have done it correctly by running in a terminal:",
        "\n",
        "``` bash\n",
        "accelerate test",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running:  accelerate-launch ~/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
            "stdout: **Initialization**\n",
            "stdout: Testing, testing. 1, 2, 3.\n",
            "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
            "stdout: Num processes: 2\n",
            "stdout: Process index: 0\n",
            "stdout: Local process index: 0\n",
            "stdout: Device: cuda:0\n",
            "stdout: \n",
            "stdout: Mixed precision type: fp16\n",
            "stdout: \n",
            "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
            "stdout: Num processes: 2\n",
            "stdout: Process index: 1\n",
            "stdout: Local process index: 1\n",
            "stdout: Device: cuda:1\n",
            "stdout: \n",
            "stdout: Mixed precision type: fp16\n",
            "stdout: \n",
            "stdout: \n",
            "stdout: **Test process execution**\n",
            "stdout: \n",
            "stdout: **Test split between processes as a list**\n",
            "stdout: \n",
            "stdout: **Test split between processes as a dict**\n",
            "stdout: \n",
            "stdout: **Test split between processes as a tensor**\n",
            "stdout: \n",
            "stdout: **Test random number generator synchronization**\n",
            "stdout: All rng are properly synched.\n",
            "stdout: \n",
            "stdout: **DataLoader integration test**\n",
            "stdout: 0 1 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\n",
            "stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
            "stdout: Non-shuffled dataloader passing.\n",
            "stdout: Shuffled dataloader passing.\n",
            "stdout: Non-shuffled central dataloader passing.\n",
            "stdout: Shuffled central dataloader passing.\n",
            "stdout: \n",
            "stdout: **Training integration test**\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
            "stdout: FP16 training check.\n",
            "stdout: FP16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: BF16 training check.\n",
            "stdout: BF16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: \n",
            "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: FP16 training check.\n",
            "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
            "stdout: FP16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: BF16 training check.\n",
            "stdout: BF16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: \n",
            "stdout: **Breakpoint trigger test**\n",
            "Test is a success! You are ready for your distributed training!\n"
          ]
        }
      ],
      "source": [
        "!accelerate test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that it ends by saying `Test is a success! You are ready for your distributed training!` so everything is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization of Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Base code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by creating a basic training code and then we'll optimize it to see how it's done and how it improves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's find a dataset. In my case, I will use the [tweet_eval](https://huggingface.co/datasets/tweet_eval) dataset, which is a tweet classification dataset. Specifically, I will download the `emoji` subset that classifies tweets with emojis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 45000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['\u2764', '\ud83d\ude0d', '\ud83d\ude02', '\ud83d\udc95', '\ud83d\udd25', '\ud83d\ude0a', '\ud83d\ude0e', '\u2728', '\ud83d\udc99', '\ud83d\ude18', '\ud83d\udcf7', '\ud83c\uddfa\ud83c\uddf8', '\u2600', '\ud83d\udc9c', '\ud83d\ude09', '\ud83d\udcaf', '\ud83d\ude01', '\ud83c\udf84', '\ud83d\udcf8', '\ud83d\ude1c'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tweet_eval', config_name='emoji', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=3808792, num_examples=45000, shard_lengths=None, dataset_name='tweet_eval'), 'test': SplitInfo(name='test', num_bytes=4262151, num_examples=50000, shard_lengths=None, dataset_name='tweet_eval'), 'validation': SplitInfo(name='validation', num_bytes=396704, num_examples=5000, shard_lengths=None, dataset_name='tweet_eval')}, download_checksums={'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/train-00000-of-00001.parquet': {'num_bytes': 2609973, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/test-00000-of-00001.parquet': {'num_bytes': 3047341, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/validation-00000-of-00001.parquet': {'num_bytes': 281994, 'checksum': None}}, download_size=5939308, post_processing_size=None, dataset_size=8467647, size_in_bytes=14406955)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\u2764', '\ud83d\ude0d', '\ud83d\ude02', '\ud83d\udc95', '\ud83d\udd25', '\ud83d\ude0a', '\ud83d\ude0e', '\u2728', '\ud83d\udc99', '\ud83d\ude18', '\ud83d\udcf7', '\ud83c\uddfa\ud83c\uddf8', '\u2600', '\ud83d\udc9c', '\ud83d\ude09', '\ud83d\udcaf', '\ud83d\ude01', '\ud83c\udf84', '\ud83d\udcf8', '\ud83d\ude1c']\n"
          ]
        }
      ],
      "source": [
        "print(dataset[\"train\"].info.features[\"label\"].names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the dataset has 20 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the maximum sequence of each split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(142, 139, 167)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len_train = 0\n",
        "max_len_val = 0\n",
        "max_len_test = 0\n",
        "\n",
        "split = \"train\"\n",
        "for i in range(len(dataset[split])):\n",
        "    len_i = len(dataset[split][i][\"text\"])\n",
        "    if len_i > max_len_train:\n",
        "        max_len_train = len_i\n",
        "split = \"validation\"\n",
        "for i in range(len(dataset[split])):\n",
        "    len_i = len(dataset[split][i][\"text\"])\n",
        "    if len_i > max_len_val:\n",
        "        max_len_val = len_i\n",
        "split = \"test\"\n",
        "for i in range(len(dataset[split])):\n",
        "    len_i = len(dataset[split][i][\"text\"])\n",
        "    if len_i > max_len_test:\n",
        "        max_len_test = len_i\n",
        "\n",
        "max_len_train, max_len_val, max_len_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we define the maximum sequence in general as 130 for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 130"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are interested in the tokenized dataset, not the raw sequences, so we create a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a tokenization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we tokenize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a83f90dc1d074012b5d099511986898e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c14557614545118c2ceb0a0ab6178c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f1c71ced24c4b1ba6e13e6c0ba0e7f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, now we have the tokens (`input_ids`) and the attention masks (`attention_mask`), but let's take a look at what kind of data we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list, list, int)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"]), type(tokenized_dataset[\"train\"][0][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Tensor, torch.Tensor, torch.Tensor)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "type(tokenized_dataset[\"train\"][0][\"label\"]), type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "BS = 64\n",
        "\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how the model is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at its last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=2, bias=True)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.out_proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768, 2)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.out_proj.in_features, model.classifier.out_proj.out_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have seen that our dataset has 20 classes, but this model is trained for 2 classes, so we need to modify the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=20, bias=True)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "model.classifier.out_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create a loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And lastly, a metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that everything is fine with a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = next(iter(dataloader[\"train\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 130]), torch.Size([64, 130]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we feed that sample to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 20])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(\"cuda\")\n",
        "ouputs = model(input_ids=sample[\"input_ids\"].to(\"cuda\"), attention_mask=sample[\"attention_mask\"].to(\"cuda\"))\n",
        "ouputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the model outputs 64 batches, which is fine because we set `BS = 20` and each one with 20 outputs, which is good because we modified the model to have an output of 20 values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain the one with the highest value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = torch.argmax(ouputs.logits, axis=-1)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.9990389347076416"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = loss_function(ouputs.logits, sample[\"label\"].to(\"cuda\"))\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.015625"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = metric.compute(predictions=predictions, references=sample[\"label\"])[\"accuracy\"]\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create a small training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "master_progress_bar = master_bar(range(epochs))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Script with the base code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In most of the `accelerate` documentation, it is explained how to use `accelerate` with scripts, so for now we will do it this way and at the end we will explain how to do it with a notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we are going to create a folder where we will save the scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir accelerate_scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we write the base code in a script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/01_code_base.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/01_code_base.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 64\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.2112                                                               \n",
            "CPU times: user 2.12 s, sys: 391 ms, total: 2.51 s\n",
            "Wall time: 3min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!python accelerate_scripts/01_code_base.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that on my computer it took about 3 and a half minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code with accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we replace some things",
        "\n",
        "* First, we import `Accelerator` and initialize it",
        "\n",
        "``` python\n",
        "from accelerate import Accelerator",
        "accelerator = Accelerator()",
        "```\n",
        "\n",
        "* We don't do the typical anymore",
        "\n",
        "``` python \n",
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "```\n",
        "\n",
        "* But we let `accelerate` choose the device.",
        "\n",
        "``` python\n",
        "device = accelerator.device",
        "```\n",
        "\n",
        "* We pass the relevant elements for training through the `prepare` method and no longer do `model.to(device)`",
        "\n",
        "``` python\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])",
        "```\n",
        "\n",
        "* We no longer send the data and model to the GPU with `.to(device)` since `accelerate` has taken care of it with the `prepare` method.",
        "\n",
        "* Instead of performing backpropagation with `loss.backward()`, we let `accelerate` handle it with",
        " \n",
        "``` python\n",
        "accelerator.backward(loss)",
        "```\n",
        "\n",
        "* When calculating the metric in the validation loop, we need to gather the values from all points, especially if we are doing distributed training, for this we do",
        "\n",
        "``` python\n",
        "predictions = accelerator.gather_for_metrics(predictions)",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/02_accelerate_base_code.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/02_accelerate_base_code.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 64\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "    print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "\n",
        "print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you notice, I've added these two lines `print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")` and the line `print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")`, I added them on purpose because they will reveal something very important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's run it. To execute the `accelerate` scripts, use the command `accelerate launch`",
        "\n",
        "``` bash\n",
        "accelerate launch script.py",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
            "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
            "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
            "Accuracy = 0.206\n",
            "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
            "Accuracy = 0.206\n",
            "CPU times: user 1.6 s, sys: 272 ms, total: 1.88 s\n",
            "Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/02_accelerate_base_code.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that before it took about 3 and a half minutes, and now it takes around 2 and a half minutes. Quite an improvement. Additionally, if we look at the `print`s, we can see that they have been printed twice.",
        "\n",
        "And how can this be? Well, because `accelerate` has parallelized the training across the two GPUs I have, so it was much faster.",
        "\n",
        "Moreover, when I ran the first script, that is, when I didn't use `accelerate`, the GPU was almost full, while when I ran the second one, that is, the one that uses `accelerate`, both GPUs were barely utilized, so we can increase the batch size to try to fill both. Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/03_accelerate_base_code_more_bs.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/03_accelerate_base_code_more_bs.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "\n",
        "print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I have removed the extra prints, as we have already seen that the code is running on both GPUs, and I increased the batch size from 64 to 128. Let's run it and see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.1052                                                               \n",
            "Accuracy = 0.1052\n",
            "CPU times: user 1.41 s, sys: 180 ms, total: 1.59 s\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/03_accelerate_base_code_more_bs.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Increasing the batch size has reduced the execution time by a few seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execution of processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Execution of code in a single process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have previously seen that the `print`s were printed twice, this is because `accelerate` creates as many processes as there are devices where the code is executed, in my case it creates two processes due to having two GPUs.",
        "\n",
        "However, not all code should be executed in all processes, for example, the `print`s slow down the code a lot, especially if they are executed multiple times, if checkpoints are saved, they would be saved twice, etc.",
        "\n",
        "To be able to execute part of the code in a single process, it has to be encapsulated in a function and decorated with `accelerator.on_local_main_process`. For example, in the following code you will see that I have created the following function",
        "\n",
        "``` python\n",
        "@accelerator.on_local_main_process",
        "def print_something(something):",
        "print(something)",
        "```\n",
        "\n",
        "Another option is to include the code within an `if accelerator.is_local_main_process` as in the following code",
        "\n",
        "``` python\n",
        "if accelerator.is_local_main_process:",
        "print(\"Something\")",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "\n",
        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it and see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.2098                                                               \n",
            "End of script with 0.2098 accuracy\n",
            "CPU times: user 1.38 s, sys: 197 ms, total: 1.58 s\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the print has only been executed once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, although not visible much, the progress bars run in each process.",
        "\n",
        "I haven't found a way to avoid this with the progress bars from `fastprogress`, but I have with those from `tqdm`, so I'm going to replace the `fastprogress` progress bars with `tqdm` ones, and to make them run in a single process, you need to add the argument `disable=not accelerator.is_local_main_process`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:01<00:00,  1.45it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.30it/s]\n",
            "Accuracy = 0.2166\n",
            "End of script with 0.2166 accuracy\n",
            "CPU times: user 1.33 s, sys: 195 ms, total: 1.52 s\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have shown an example of how to print in a single process, and this has been a way to execute processes in a single process. But if what you want is just to print in a single process, the `print` method from `accelerate` can be used. Let's see the same example as before with this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/06_accelerate_base_code_print_one_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/06_accelerate_base_code_print_one_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15433.52 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 11406.61 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15036.87 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14932.76 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14956.60 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:00<00:00,  1.46it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.33it/s]\n",
            "Accuracy = 0.2134\n",
            "End of script with 0.2134 accuracy\n",
            "CPU times: user 1.4 s, sys: 189 ms, total: 1.59 s\n",
            "Wall time: 2min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/06_accelerate_base_code_print_one_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code Execution in All Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, there is code that must run in all processes, for example, if we upload the checkpoints to the hub, so here we have two options: wrap the code in a function and decorate it with `accelerator.on_main_process`",
        "\n",
        "``` python\n",
        "@accelerator.on_main_process",
        "def do_my_thing():",
        "\"Something done once per server\"",
        "do_thing_once()",
        "```\n",
        "\n",
        "or put the code inside an `if accelerator.is_main_process`",
        "\n",
        "``` python\n",
        "if accelerator.is_main_process:",
        "repo.push_to_hub()",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are only doing training to showcase the `accelerate` library and the model we are training is not good, it doesn't make sense to upload the checkpoints to the hub right now, so I will do an example with `print`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/06_accelerate_base_code_some_code_in_all_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_in_one_process(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_main_process\n",
        "def print_in_all_processes(something):\n",
        "    print(something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it to see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:03<00:00, 14518.44 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:03<00:00, 14368.77 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 16466.33 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14806.14 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14253.33 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14337.07 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:00<00:00,  1.46it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.34it/s]\n",
            "Accuracy = 0.2092\n",
            "End of script with 0.2092 accuracy\n",
            "All process: Accuracy = 0.2092\n",
            "All process: End of script with 0.2092 accuracy\n",
            "CPU times: user 1.42 s, sys: 216 ms, total: 1.64 s\n",
            "Wall time: 2min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code Execution in Process X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can specify in which process we want to execute code. For this, we need to create a function and decorate it with `@accelerator.on_process(process_index=0)`",
        "\n",
        "``` python\n",
        "@accelerator.on_process(process_index=0)",
        "def do_my_thing():",
        "\"Something done on process index 0\"",
        "do_thing_on_index_zero()",
        "```\n",
        "\n",
        "or decorate it with `@accelerator.on_local_process(local_process_idx=0)`",
        "\n",
        "``` python\n",
        "@accelerator.on_local_process(local_process_index=0)",
        "def do_my_thing():",
        "\"Something done on process index 0 on each server\"",
        "do_thing_on_index_zero_on_each_server()",
        "```\n",
        "\n",
        "Here I have put process 0, but any number can be put"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/07_accelerate_base_code_some_code_in_some_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_in_one_process(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_main_process\n",
        "def print_in_all_processes(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_process(process_index=0)\n",
        "def print_in_process_0(something):\n",
        "    print(\"Process 0: \" + something)\n",
        "\n",
        "@accelerator.on_local_process(local_process_index=1)\n",
        "def print_in_process_1(something):\n",
        "    print(\"Process 1: \" + something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_process_0(\"End of process 0\")\n",
        "print_in_process_1(\"End of process 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 15735.58 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14906.20 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:02<00:00,  1.44it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.27it/s]\n",
            "Process 1: End of process 1\n",
            "Accuracy = 0.2128\n",
            "End of script with 0.2128 accuracy\n",
            "All process: Accuracy = 0.2128\n",
            "All process: End of script with 0.2128 accuracy\n",
            "Process 0: End of process 0\n",
            "CPU times: user 1.42 s, sys: 295 ms, total: 1.71 s\n",
            "Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Synchronizing Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have code that needs to run on all processes, it's interesting to wait for it to finish on all processes before doing another task, so for this we use `accelerator.wait_for_everyone()`",
        "\n",
        "To see it, we are going to introduce a delay in one of the print functions in a process.",
        "\n",
        "I've also added a break in the training loop so that it doesn't train for too long, which isn't what we're interested in right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/08_accelerate_base_code_sync_all_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/09_accelerate_base_code_sync_all_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_in_one_process(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_main_process\n",
        "def print_in_all_processes(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_process(process_index=0)\n",
        "def print_in_process_0(something):\n",
        "    time.sleep(2)\n",
        "    print(\"Process 0: \" + something)\n",
        "\n",
        "@accelerator.on_local_process(local_process_index=1)\n",
        "def print_in_process_1(something):\n",
        "    print(\"Process 1: \" + something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        break\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_one_process(\"Printing with delay in process 0\")\n",
        "print_in_process_0(\"End of process 0\")\n",
        "print_in_process_1(\"End of process 1\")\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "print_in_one_process(\"End of script\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14218.23 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14666.25 examples/s]\n",
            "  0%|                                                   | 0/176 [00:00<?, ?it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.58it/s]\n",
            "Process 1: End of process 1\n",
            "Accuracy = 0.212\n",
            "End of script with 0.212 accuracy\n",
            "All process: Accuracy = 0.212\n",
            "All process: End of script with 0.212 accuracy\n",
            "Printing with delay in process 0\n",
            "Process 0: End of process 0\n",
            "End of script\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/09_accelerate_base_code_sync_all_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen, first `Process 1: End of process 1` is printed and then the rest. This is because the remaining prints are either done in process 0 or in all processes, so until the 2-second delay we set is over, the rest of the code does not execute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save and load the state dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we train, sometimes we save the state so we can continue at another time.",
        "\n",
        "To save the state, we will have to use the methods `save_state()` and `load_state()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/09_accelerate_save_and_load_checkpoints.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/10_accelerate_save_and_load_checkpoints.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "\n",
        "    # Guardamos los pesos\n",
        "    accelerator.save_state(\"accelerate_scripts/checkpoints\")\n",
        "\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "# Cargamos los pesos\n",
        "accelerator.load_state(\"accelerate_scripts/checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:58<00:00,  1.48it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.40it/s]\n",
            "Accuracy = 0.2142\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/10_accelerate_save_and_load_checkpoints.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the `prepare` method was used, the model was wrapped to be able to save it to the necessary devices. Therefore, when saving it, we need to use the `save_model` method, which first unwraps it and then saves it. Additionally, if we use the parameter `safe_serialization=True`, the model will be saved as a `safe tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/11_accelerate_save_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/11_accelerate_save_model.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "\n",
        "    # Guardamos el modelo\n",
        "    accelerator.wait_for_everyone()\n",
        "    accelerator.save_model(model, \"accelerate_scripts/model\", safe_serialization=True)\n",
        "\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:58<00:00,  1.48it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.35it/s]\n",
            "Accuracy = 0.214\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/11_accelerate_save_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the `pretrained` model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In models that use the `transformers` library, we must save the model using the `save_pretrained` method to be able to load it with the `from_pretrained` method. Before saving it, you need to unwrap it using the `unwrap_model` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/11_accelerate_save_pretrained.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/12_accelerate_save_pretrained.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "\n",
        "    # Guardamos el modelo pretrained\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(\n",
        "        \"accelerate_scripts/model_pretrained\",\n",
        "        is_main_process=accelerator.is_main_process,\n",
        "        save_function=accelerator.save,\n",
        "    )\n",
        "\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15152.47 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15119.13 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 12724.70 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 12397.49 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 15247.21 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 15138.03 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:59<00:00,  1.48it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.37it/s]\n",
            "Accuracy = 0.21\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/12_accelerate_save_pretrained.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we could load it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at accelerate_scripts/model_pretrained and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoints = \"accelerate_scripts/model_pretrained\"\n",
        "tokenizer = AutoModel.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have seen how to run scripts, but if you want to run the code in a notebook, we can write the same code as before, but encapsulated in a function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "import time\n",
        "# from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_code(batch_size: int = 64):\n",
        "    from accelerate import Accelerator\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "    num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "    max_len = 130\n",
        "\n",
        "    checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "    def tokenize_function(dataset):\n",
        "        return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    tokenized_dataset = {\n",
        "        \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "        \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "        \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    }\n",
        "    tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "    tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "    tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "    BS = batch_size\n",
        "    dataloader = {\n",
        "        \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "        \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "        \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "    }\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "    model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    EPOCHS = 1\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = accelerator.device\n",
        "\n",
        "    # model.to(device)\n",
        "    model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "    for i in range(EPOCHS):\n",
        "        model.train()\n",
        "        progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "        for batch in progress_bar_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"]#.to(device)\n",
        "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "            labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "            # loss.backward()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "        for batch in progress_bar_validation:\n",
        "            input_ids = batch[\"input_ids\"]#.to(device)\n",
        "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "            labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "            # Recopilamos las predicciones de todos los dispositivos\n",
        "            predictions = accelerator.gather_for_metrics(predictions)\n",
        "            labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "            accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "        accuracy = metric.compute()\n",
        "        \n",
        "    accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the training in the notebook, we use the `notebook_launcher` function, to which we pass the function we want to execute, the arguments of that function, and the number of GPUs on which we will train using the `num_processes` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on 2 GPUs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:01<00:00,  1.45it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.2112\n"
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (128,)\n",
        "notebook_launcher(train_code, args, num_processes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training in FP16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we first set up `accelerate`, it asked us `Do you wish to use FP16 or BF16 (mixed precision)?` and we said no, so now we are going to tell it yes, that we want FP16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have trained in FP32, which means that each weight of the model is a 32-bit floating-point number, and now we are going to use a 16-bit floating-point number, meaning the model will take up less space. As a result, two things will happen: we will be able to use a larger batch size, and it will also be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we run `accelerate config` again and tell it that we want FP16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "fp16\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create a script to train, with the same batch size as before, to see if it takes less time to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/12_accelerate_base_code_fp16_bs128.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/13_accelerate_base_code_fp16_bs128.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it to see how long it takes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14983.76 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14315.47 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:01<00:00,  2.88it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02<00:00,  6.84it/s]\n",
            "Accuracy = 0.2094\n",
            "CPU times: user 812 ms, sys: 163 ms, total: 976 ms\n",
            "Wall time: 1min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/13_accelerate_base_code_fp16_bs128.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we ran this training in FP32 it took about 2 and a half minutes, and now roughly 1 and a half minutes. Let's see if instead of training with a batch size of 128, we do it with one of 256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/14_accelerate_base_code_fp16_bs256.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 256\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 15390.30 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14990.92 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 88/88 [00:54<00:00,  1.62it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  3.45it/s]\n",
            "Accuracy = 0.2236\n",
            "CPU times: user 670 ms, sys: 91.6 ms, total: 761 ms\n",
            "Wall time: 1min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It has only dropped by about 15 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training in BF16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have previously trained in FP16 and now we are going to do it in BF16, what is the difference?",
        "\n",
        "![FP32_FP16_BF16](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/FP32_FP16_BF16.webp)",
        "\n",
        "As we can see in the image, while FP16 compared to FP32 has fewer bits in the mantissa and the exponent, which makes its range much smaller, BF16 compared to FP32 has the same number of bits for the exponent but fewer in the mantissa, which means that BF16 has the same range of numbers as FP32, but is less precise.",
        "\n",
        "This is beneficial because in FP16 some calculations could result in very high numbers, which cannot be represented in the FP16 format. Additionally, there are certain HW devices that are optimized for this format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like before, we run `accelerate config` and indicate that we want BF16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "bf16\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run the last script we created, that is, with a batch size of 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14814.95 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14506.83 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 88/88 [00:51<00:00,  1.70it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:03<00:00,  3.21it/s]\n",
            "Accuracy = 0.2112\n",
            "CPU times: user 688 ms, sys: 144 ms, total: 832 ms\n",
            "Wall time: 1min 17s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It took a similar amount of time as before, which is normal since we trained a model with 16-bit weights, just like before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training in FP8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to train in FP8 format, which, as the name suggests, is a floating-point format where each weight has 8 bits, so we run `accelerate config` to tell it that we want FP8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "fp8\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run the last script, the one with a batch size of 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
            "    accelerator = Accelerator()\n",
            "                  ^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
            "    accelerator = Accelerator()\n",
            "                  ^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
            "[2024-05-13 21:40:56,455] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 501480) of binary: /home/wallabot/miniconda3/envs/nlp/bin/python\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
            "    args.func(args)\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
            "    elastic_launch(\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "accelerate_scripts/13_accelerate_base_code_fp16_bs256.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2024-05-13_21:40:56\n",
            "  host      : wallabot\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 501481)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-05-13_21:40:56\n",
            "  host      : wallabot\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 501480)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "CPU times: user 65.1 ms, sys: 14.5 ms, total: 79.6 ms\n",
            "Wall time: 7.24 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the weights are now 8-bit and take up half the memory, we will increase the batch size to 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 512\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/15_accelerate_base_code_fp8_bs512.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage of the Hugging Face Ecosystem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how to perform inference with large models using the `transformers` library from Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference with `pipeline`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we use the Hugging Face ecosystem, it's very simple, as everything happens under the hood without us having to do much. In the case of using `pipeline`, which is the easiest way to perform inference with the `transformers` library, we just need to specify the model we want to use and, importantly, pass `device_map=\"auto\"`. This will make `accelerate` distribute the model across different GPUs, CPU RAM, or hard drive if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are more possible values for `device_map`, which we will see later, but for now stick with `\"auto\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to use the `Llama3 8B` model, which, as its name suggests, is a model with around 8 billion parameters. Since each parameter by default is in FP32 format, which corresponds to 4 bytes (32 bits), this means that if we multiply 8 billion parameters by 4 bytes, we get that it would require a GPU with around 32 GB of VRAM.",
        "\n",
        "In my case, I have 2 GPUs with 24 GB of VRAM each, so it wouldn't fit into a single GPU. But thanks to setting `device_map=\"auto\"`, accelerate will distribute the model across both GPUs and I will be able to perform inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/09_inference_with_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/16_inference_with_pipeline.py\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
        "\n",
        "prompt = \"Conoces accelerate de hugging face?\"\n",
        "output = generator(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run it, only as the pipeline uses accelerate under the hood, we don't need to run it with `accelerate launch script.py` but instead with `python script.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.27s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "[{'generated_text': 'Conoces accelerate de hugging face? \u00bfQu\u00e9 es el modelo de lenguaje de transformers y c\u00f3mo se utiliza en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar modelos de lenguaje de transformers en mi aplicaci\u00f3n? \u00bfQu\u00e9 son los tokenizers y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo crear un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los datasets y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar datasets para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning'}]\n"
          ]
        }
      ],
      "source": [
        "!python accelerate_scripts/16_inference_with_pipeline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen, it has not responded, but has continued to ask questions. This is because Llama3 is a language model that predicts the next token, so with the prompt I passed it, it considered that the next best tokens were ones that correspond to more questions. This makes sense because sometimes people have doubts about a topic and generate many questions, so to get it to answer the question, we need to condition it a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/10_inference_with_pipeline_condition.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/17_inference_with_pipeline_condition.py\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
        "\n",
        "prompt = \"Conoces accelerate de hugging face?\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un chatbot amigable que siempre intenta solucionar las dudas\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
        "]\n",
        "output = generator(messages)\n",
        "print(output[0]['generated_text'][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, a message has been generated with roles, conditioning the model and with the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.41s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "{'role': 'assistant', 'content': '\u00a1Hola!\\n\\nS\u00ed, conozco Accelerate de Hugging Face. Accelerate es una biblioteca de Python desarrollada por Hugging Face que se enfoca en simplificar y acelerar el entrenamiento y la evaluaci\u00f3n de modelos de lenguaje en diferentes dispositivos y entornos.\\n\\nCon Accelerate, puedes entrenar modelos de lenguaje en diferentes plataformas y dispositivos, como GPUs, TPUs, CPUs y servidores, sin necesidad de cambiar el c\u00f3digo de tu modelo. Esto te permite aprovechar al m\u00e1ximo la potencia de c\u00e1lculo de tus dispositivos y reducir el tiempo de entrenamiento.\\n\\nAccelerate tambi\u00e9n ofrece varias caracter\u00edsticas adicionales, como:\\n\\n* Soporte para diferentes frameworks de machine learning, como TensorFlow, PyTorch y JAX.\\n* Integraci\u00f3n con diferentes sistemas de almacenamiento y procesamiento de datos, como Amazon S3 y Google Cloud Storage.\\n* Soporte para diferentes protocolos de comunicaci\u00f3n, como HTTP y gRPC.\\n* Herramientas para monitorear y depurar tus modelos en tiempo real.\\n\\nEn resumen, Accelerate es una herramienta muy \u00fatil para desarrolladores de modelos de lenguaje que buscan simplificar y acelerar el proceso de entrenamiento y evaluaci\u00f3n de sus modelos.\\n\\n\u00bfTienes alguna pregunta espec\u00edfica sobre Accelerate o necesitas ayuda para implementarlo en tu proyecto?'}\n"
          ]
        }
      ],
      "source": [
        "!python accelerate_scripts/17_inference_with_pipeline_condition.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the response does answer our prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference with `AutoClass`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, we will see how to perform inference using only `AutoClass`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/11_inference_with_autoclass.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/18_inference_with_autoclass.py\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints, device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoints, device_map=\"auto\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "prompt = \"Conoces accelerate de hugging face?\"\n",
        "tokens_input = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "_ = model.generate(**tokens_input, streamer=streamer, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen, the `streamer` object has been created and is then passed to the model's `generate` method. This is useful for printing each word as it is generated, rather than waiting for the entire output to be generated before printing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.28s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "<|begin_of_text|>Conoces accelerate de hugging face? Si es as\u00ed, puedes utilizar la biblioteca `transformers` de Hugging Face para crear un modelo de lenguaje que pueda predecir la siguiente palabra en una secuencia de texto.\n",
            "\n",
            "Aqu\u00ed te muestro un ejemplo de c\u00f3mo hacerlo:\n",
            "```\n",
            "import pandas as pd\n",
            "import torch\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "\n",
            "# Cargar el modelo y el tokenizador\n",
            "model_name = \"bert-base-uncased\"\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "\n",
            "# Cargar el conjunto de datos\n",
            "train_df = pd.read_csv(\"train.csv\")\n",
            "test_df = pd.read_csv(\"test.csv\")\n",
            "\n",
            "# Preprocesar los datos\n",
            "train_texts = train_df[\"text\"]\n",
            "train_labels = train_df[\"label\"]\n",
            "test_texts = test_df[\"text\"]\n",
            "\n",
            "# Convertir los textos en entradas para el modelo\n",
            "train_encodings = tokenizer.batch_encode_plus(train_texts, \n",
            "                                              add_special_tokens=True, \n",
            "                                              max_length=512, \n",
            "                                              return_attention_mask=True, \n",
            "                                              return_tensors='pt')\n",
            "\n",
            "test_encodings = tokenizer.batch_encode_plus(test_texts, \n",
            "                                             add_special_tokens=True, \n",
            "                                             max_length=512, \n",
            "                                             return_attention_mask=True, \n",
            "                                             return_tensors='pt')\n",
            "\n",
            "# Crear un dataloader para entrenar el modelo\n",
            "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], \n",
            "                                               train_encodings[\"attention_mask\"], \n",
            "                                               torch.tensor(train_labels))\n",
            "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
            "\n",
            "# Entrenar el modelo\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "model.to(device)\n",
            "criterion = torch.nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
            "\n",
            "for epoch in range(5):\n",
            "    model.train()\n",
            "    total_loss = 0\n",
            "    for batch in train_loader:\n",
            "        input_ids = batch[0].to(device)\n",
            "        attention_mask = batch[1].to(device)\n",
            "        labels = batch[2].to(device)\n",
            "        optimizer.zero_grad()\n",
            "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
            "        loss = criterion(outputs, labels)\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        total_loss += loss.item()\n",
            "    print(f\"Epoch {epoch+1}, Loss: {total\n"
          ]
        }
      ],
      "source": [
        "!python accelerate_scripts/18_inference_with_autoclass.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normally, the way to make inferences with PyTorch is to create a model with randomly initialized weights and then load a `state dict` with the pretrained model's weights. So, to get that `state dict`, we'll first take a small shortcut and download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/maximo.fernandez/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 230M/230M [02:48<00:00, 1.43MB/s] \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
        "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the `state dict`, we are going to perform inference as it is typically done in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     # Set device\n",
        "\n",
        "resnet152 = models.resnet152().to(device) # Create model with random weights and move to device\n",
        "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device) # Load pretrained weights into device memory\n",
        "resnet152.load_state_dict(state_dict) # Load this weights into the model\n",
        "\n",
        "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
        "output = resnet152(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's explain what has happened",
        "\n",
        "* When we did `resnet152 = models.resnet152().to(device)`, a ResNet152 with random weights was loaded into the GPU memory.",
        "* When we did `state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)`, a dictionary with the trained weights was loaded into the GPU memory.",
        "* When we did `resnet152.load_state_dict(state_dict)`, those pretrained weights were assigned to the model.",
        "\n",
        "That is, the model has been loaded twice into the GPU memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You might be wondering why we did first",
        "\n",
        "``` python\n",
        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)",
        "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')",
        "```\n",
        "\n",
        "To do later",
        "\n",
        "``` python\n",
        "resnet152 = models.resnet152().to(device)",
        "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)",
        "resnet152.load_state_dict(state_dict)",
        "```\n",
        "\n",
        "And why don't we use directly",
        "\n",
        "```\n",
        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)",
        "```\n",
        "\n",
        "And we stop saving the `state dict` to load it later. Well, because Pytorch does the same thing under the hood. So, to see the whole process, we have broken down into several lines what Pytorch does in one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This way of working has been effective so far, while models were manageable by user GPUs. But since the arrival of LLMs, this approach no longer makes sense.",
        "\n",
        "For example, a 6B parameter model would occupy 24 GB in memory, and since it is loaded twice with this way of working, you would need a 48 GB GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So to fix this, the way to load a pretrained model in PyTorch is:",
        "* Create an empty model with `init_empty_weights` that will not occupy RAM memory",
        "* Then load the weights with `load_checkpoint_and_dispatch` which will load a checkpoint into the empty model and distribute the weights for each layer across all available devices (GPU, CPU, RAM, and hard drive), thanks to setting `device_map=\"auto\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "with init_empty_weights():\n",
        "    resnet152 = models.resnet152()\n",
        "\n",
        "resnet152 = load_checkpoint_and_dispatch(resnet152, checkpoint='accelerate_scripts/resnet152_pretrained.pth', device_map=\"auto\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
        "output = resnet152(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How accelerate works under the hood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this video you can see graphically how accelerate works under the hood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/MWCSGj9jEAo\" title=\"Accelerate Big Model Inference: How Does it Work?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialization of an empty model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Accelerate` creates the skeleton of an empty model using `init_empty_weights` so that it occupies as little memory as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, let's see how much RAM I have available on my computer now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 22.62 GB, Used RAM: 7.82 GB\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "def get_ram_info():\n",
        "    ram = dict(psutil.virtual_memory()._asdict())\n",
        "    print(f\"Total RAM: {(ram['total']/1024/1024/1024):.2f} GB, Available RAM: {(ram['available']/1024/1024/1024):.2f} GB, Used RAM: {(ram['used']/1024/1024/1024):.2f} GB\")\n",
        "\n",
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I have about 22 GB of RAM available.",
        "\n",
        "Now let's try to create a model with 5000x1000x1000 parameters, that is, 5B parameters. If each parameter is in FP32, it would require 20 GB of RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we look at the RAM again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 3.77 GB, Used RAM: 26.70 GB\n"
          ]
        }
      ],
      "source": [
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, now we only have 3 GB of RAM available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to delete the model to free up RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 22.44 GB, Used RAM: 8.03 GB\n"
          ]
        }
      ],
      "source": [
        "del model\n",
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have about 22 GB of RAM available again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now use `init_empty_weights` from `accelerate` and then check the RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 22.32 GB, Used RAM: 8.16 GB\n"
          ]
        }
      ],
      "source": [
        "from accelerate import init_empty_weights\n",
        "\n",
        "with init_empty_weights():\n",
        "    model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])\n",
        "\n",
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We previously had exactly 22.44 GB free, and after creating the model with `init_empty_weights` we have 22.32 GB. The RAM savings are huge! Almost no RAM was used to create the model.",
        "\n",
        "This is based on the meta-device introduced in PyTorch 1.9, so it is important that to use `accelerate` we have a version of Pytorch later than that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading the weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have initialized the model, we need to load its weights, which we do through `load_checkpoint_and_dispatch`, which, as its name suggests, loads the weights and dispatches them to the necessary device or devices."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-05-16",
      "description_en": "Speed up your training with HuggingFace Accelerate",
      "description_es": "Acelera tus entrenos con HuggingFace Accelerate",
      "description_pt": "Acelere seus treinamentos com HuggingFace Accelerate",
      "end_url": "hugging-face-accelerate",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_accelerate.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_accelerate.webp",
      "keywords_en": "hugging face, accelerate, pytorch, deep learning, machine learning, transformers",
      "keywords_es": "hugging face, accelerate, pytorch, deep learning, machine learning, transformers",
      "keywords_pt": "hugging face, accelerate, pytorch, deep learning, machine learning, transformers",
      "title_en": "HuggingFace Accelerate",
      "title_es": "HuggingFace Accelerate",
      "title_pt": "HuggingFace Accelerate"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}