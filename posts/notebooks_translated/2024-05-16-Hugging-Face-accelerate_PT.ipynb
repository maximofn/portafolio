{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# Acelera√ß√£o do rosto do abra√ßo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O Accelerate √© uma biblioteca Hugging Face que permite executar o mesmo c√≥digo PyTorch em qualquer configura√ß√£o distribu√≠da, adicionando apenas quatro linhas de c√≥digo."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..\n",
"\n",
"## Instala√ß√£o"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para instalar o `accelerate` com o `pip`, basta executar:\n",
"\n",
"``` bash\n",
"pip install accelerate\n",
"```\n",
"\n",
"E com `conda`:\n",
"\n",
"``` bash\n",
"conda install -c conda-forge accelerate\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Configura√ß√£o"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Em todos os ambientes em que o `accelerate` est√° instalado, a primeira coisa a fazer √© configur√°-lo. Para isso, executamos em um terminal:\n",
"\n",
"``` bash\n",
"acelerar a configura√ß√£o\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "--------------------------------------------------------------------------------\n",
"In which compute environment are you running?\n",
"This machine\n",
"--------------------------------------------------------------------------------\n",
"multi-GPU\n",
"How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
"Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
"Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
"Do you want to use DeepSpeed? [yes/NO]: no\n",
"Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
"Do you want to use Megatron-LM ? [yes/NO]: no\n",
"How many GPU(s) should be used for distributed training? [1]:2\n",
"What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
"--------------------------------------------------------------------------------\n",
"Do you wish to use FP16 or BF16 (mixed precision)?\n",
"no\n",
"accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
      "!accelerate config"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "No meu caso, as respostas foram\n",
" * Em qual ambiente de computa√ß√£o voc√™ est√° executando?\n",
"    - [x] \"Esta m√°quina\"\n",
"    - [_] \"AWS (Amazon SageMaker)\" [_] \"AWS (Amazon SageMaker)\" [_] \"AWS (Amazon SageMaker)\"\n",
" > Quero configur√°-lo em meu computador\n",
"\n",
" * Que tipo de m√°quina voc√™ est√° usando?\n",
"    - [_] multi-CPU\n",
"    - [_] multi-XPU\n",
"    - x] multi-GPU\n",
"    - [_] multi-NPU\n",
"    - [_] TPU\n",
" > Como tenho 2 GPUs e quero executar c√≥digo distribu√≠do nelas, escolhi `multi-GPU`.\n",
" \n",
" * Quantas m√°quinas diferentes voc√™ usar√° (use mais de uma para treinamento com v√°rios n√≥s)? [1]:\n",
"    - 1\n",
" > Escolhi `1` porque s√≥ vou executar em meu computador.\n",
"\n",
" * As opera√ß√µes distribu√≠das devem ser verificadas durante a execu√ß√£o em busca de erros? Isso pode evitar problemas de tempo limite, mas ser√° mais lento. [yes/NO]:\n",
"    - n√£o\n",
" > Com essa op√ß√£o, voc√™ pode optar por fazer com que o `accelerate` verifique se h√° erros na execu√ß√£o, mas isso tornaria a execu√ß√£o mais lenta, ent√£o eu escolho `no` e, caso haja erros, altero para `yes`.\n",
" \n",
" * Deseja otimizar seu script com o torch dynamo? [yes/NO]:\n",
"    - n√£o\n",
"\n",
" * Voc√™ deseja usar o FullyShardedDataParallel? [yes/NO]:\n",
"    - n√£o\n",
" \n",
" * Voc√™ quer usar o Megatron-LM? [sim/n√£o]:\n",
"    - n√£o\n",
" \n",
" * Quantas GPUs devem ser usadas para treinamento distribu√≠do? [1]:\n",
"    \n",
" > Escolhi `2` porque tenho 2 GPUs\n",
"\n",
" * Quais GPUs (por id) devem ser usadas para treinamento nesta m√°quina como uma lista separada por v√≠rgulas? [all]:\n",
"    - 0,1\n",
" > Escolhi `0,1` porque quero usar as duas GPUs.\n",
"\n",
" * Voc√™ deseja usar FP16 ou BF16 (precis√£o mista)?\n",
"    - x] n√£o\n",
"    - [_] fp16\n",
"    - [_] bf16\n",
"    - [_] fp8\n",
" > No momento, escolhi `no`, pois, para simplificar o c√≥digo, quando n√£o estivermos usando o `accelerate`, treinaremos em fp32, mas o ideal seria usar fp16."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A configura√ß√£o ser√° armazenada em `~/.cache/huggingface/accelerate/default_config.yaml` e poder√° ser modificada a qualquer momento. Vamos ver o que h√° dentro dele"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "compute_environment: LOCAL_MACHINE\n",
"debug: false\n",
"distributed_type: MULTI_GPU\n",
"downcast_bf16: 'no'\n",
"gpu_ids: 0,1\n",
"machine_rank: 0\n",
"main_training_function: main\n",
"mixed_precision: fp16\n",
"num_machines: 1\n",
"num_processes: 2\n",
"rdzv_backend: static\n",
"same_network: true\n",
"tpu_env: []\n",
"tpu_use_cluster: false\n",
"tpu_use_sudo: false\n",
"use_cpu: false\n"
          ]
        }
      ],
      "source": [
      "!cat ~/.cache/huggingface/accelerate/default_config.yaml"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Outra maneira de ver a configura√ß√£o que temos √© execut√°-la em um terminal:\n",
"\n",
"``` bash\n",
"acelerar o ambiente\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n",
"Copy-and-paste the text below in your GitHub issue\n",
"\n",
"- `Accelerate` version: 0.28.0\n",
"- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\n",
"- Python version: 3.11.8\n",
"- Numpy version: 1.26.4\n",
"- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
"- PyTorch XPU available: False\n",
"- PyTorch NPU available: False\n",
"- System RAM: 31.24 GB\n",
"- GPU type: NVIDIA GeForce RTX 3090\n",
"- `Accelerate` default config:\n",
"\t- compute_environment: LOCAL_MACHINE\n",
"\t- distributed_type: MULTI_GPU\n",
"\t- mixed_precision: fp16\n",
"\t- use_cpu: False\n",
"\t- debug: False\n",
"\t- num_processes: 2\n",
"\t- machine_rank: 0\n",
"\t- num_machines: 1\n",
"\t- gpu_ids: 0,1\n",
"\t- rdzv_backend: static\n",
"\t- same_network: True\n",
"\t- main_training_function: main\n",
"\t- downcast_bf16: no\n",
"\t- tpu_use_cluster: False\n",
"\t- tpu_use_sudo: False\n",
"\t- tpu_env: []\n"
          ]
        }
      ],
      "source": [
      "!accelerate env"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois de configurar o `accelerate`, podemos testar se fizemos tudo certo executando-o em um terminal:\n",
"\n",
"``` bash\n",
"teste de acelera√ß√£o\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n",
"Running:  accelerate-launch ~/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
"stdout: **Initialization**\n",
"stdout: Testing, testing. 1, 2, 3.\n",
"stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
"stdout: Num processes: 2\n",
"stdout: Process index: 0\n",
"stdout: Local process index: 0\n",
"stdout: Device: cuda:0\n",
"stdout: \n",
"stdout: Mixed precision type: fp16\n",
"stdout: \n",
"stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
"stdout: Num processes: 2\n",
"stdout: Process index: 1\n",
"stdout: Local process index: 1\n",
"stdout: Device: cuda:1\n",
"stdout: \n",
"stdout: Mixed precision type: fp16\n",
"stdout: \n",
"stdout: \n",
"stdout: **Test process execution**\n",
"stdout: \n",
"stdout: **Test split between processes as a list**\n",
"stdout: \n",
"stdout: **Test split between processes as a dict**\n",
"stdout: \n",
"stdout: **Test split between processes as a tensor**\n",
"stdout: \n",
"stdout: **Test random number generator synchronization**\n",
"stdout: All rng are properly synched.\n",
"stdout: \n",
"stdout: **DataLoader integration test**\n",
"stdout: 0 1 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
"stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
"stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
"stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\n",
"stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
"stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
"stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
"stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
"stdout: Non-shuffled dataloader passing.\n",
"stdout: Shuffled dataloader passing.\n",
"stdout: Non-shuffled central dataloader passing.\n",
"stdout: Shuffled central dataloader passing.\n",
"stdout: \n",
"stdout: **Training integration test**\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
"stdout: FP16 training check.\n",
"stdout: FP16 training check.\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Keep fp32 wrapper check.\n",
"stdout: Keep fp32 wrapper check.\n",
"stdout: BF16 training check.\n",
"stdout: BF16 training check.\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: \n",
"stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: FP16 training check.\n",
"stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
"stdout: FP16 training check.\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Keep fp32 wrapper check.\n",
"stdout: Keep fp32 wrapper check.\n",
"stdout: BF16 training check.\n",
"stdout: BF16 training check.\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
"stdout: \n",
"stdout: **Breakpoint trigger test**\n",
"Test is a success! You are ready for your distributed training!\n"
          ]
        }
      ],
      "source": [
      "!accelerate test"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que ele termina dizendo `O teste foi um sucesso! Voc√™ est√° pronto para seu treinamento distribu√≠do, portanto, tudo est√° correto."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Treinamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Otimiza√ß√£o do treinamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### C√≥digo base"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, criaremos um c√≥digo de treinamento b√°sico e, em seguida, o otimizaremos para ver como ele √© feito e como melhora."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, vamos procurar um conjunto de dados; no meu caso, usarei o conjunto de dados [tweet_eval](https://huggingface.co/datasets/tweet_eval), que √© um conjunto de dados de classifica√ß√£o de tweets; em particular, farei o download do subconjunto `emoji`, que classifica tweets com emoticons."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['text', 'label'],\n",
"        num_rows: 45000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['text', 'label'],\n",
"        num_rows: 50000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['text', 'label'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['‚ù§', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tweet_eval', config_name='emoji', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=3808792, num_examples=45000, shard_lengths=None, dataset_name='tweet_eval'), 'test': SplitInfo(name='test', num_bytes=4262151, num_examples=50000, shard_lengths=None, dataset_name='tweet_eval'), 'validation': SplitInfo(name='validation', num_bytes=396704, num_examples=5000, shard_lengths=None, dataset_name='tweet_eval')}, download_checksums={'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/train-00000-of-00001.parquet': {'num_bytes': 2609973, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/test-00000-of-00001.parquet': {'num_bytes': 3047341, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/validation-00000-of-00001.parquet': {'num_bytes': 281994, 'checksum': None}}, download_size=5939308, post_processing_size=None, dataset_size=8467647, size_in_bytes=14406955)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset[\"train\"].info"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada nas classes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "['‚ù§', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú']\n"
          ]
        }
      ],
      "source": [
      "print(dataset[\"train\"].info.features[\"label\"].names)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E o n√∫mero de classes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "20"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"num_classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que o conjunto de dados tem 20 classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada na sequ√™ncia m√°xima de cada divis√£o"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(142, 139, 167)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "max_len_train = 0\n",
"max_len_val = 0\n",
"max_len_test = 0\n",
"\n",
"split = \"train\"\n",
"for i in range(len(dataset[split])):\n",
"    len_i = len(dataset[split][i][\"text\"])\n",
"    if len_i > max_len_train:\n",
"        max_len_train = len_i\n",
"split = \"validation\"\n",
"for i in range(len(dataset[split])):\n",
"    len_i = len(dataset[split][i][\"text\"])\n",
"    if len_i > max_len_val:\n",
"        max_len_val = len_i\n",
"split = \"test\"\n",
"for i in range(len(dataset[split])):\n",
"    len_i = len(dataset[split][i][\"text\"])\n",
"    if len_i > max_len_test:\n",
"        max_len_test = len_i\n",
"\n",
"max_len_train, max_len_val, max_len_test"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Portanto, definimos a sequ√™ncia m√°xima em geral como 130 para a tokeniza√ß√£o."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "max_len = 130"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Estamos interessados no conjunto de dados tokenizado, n√£o nas sequ√™ncias brutas, portanto, criamos um tokenizador"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos uma fun√ß√£o de tokeniza√ß√£o"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E agora vamos tokenizar o conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a83f90dc1d074012b5d099511986898e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c14557614545118c2ceb0a0ab6178c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f1c71ced24c4b1ba6e13e6c0ba0e7f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver agora, temos os tokens (`input_ids`) e as m√°scaras de aten√ß√£o (`attention_mask`), mas vamos ver que tipo de dados temos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(list, list, int)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"]), type(tokenized_dataset[\"train\"][0][\"label\"])"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Tensor, torch.Tensor, torch.Tensor)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"type(tokenized_dataset[\"train\"][0][\"label\"]), type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos um carregador de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"from torch.utils.data import DataLoader\n",
"BS = 64\n",
"\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Carregamos o modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos ver como √© o modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "RobertaForSequenceClassification(\n",
"  (roberta): RobertaModel(\n",
"    (embeddings): RobertaEmbeddings(\n",
"      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
"      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
"      (token_type_embeddings): Embedding(1, 768)\n",
"      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"      (dropout): Dropout(p=0.1, inplace=False)\n",
"    )\n",
"    (encoder): RobertaEncoder(\n",
"      (layer): ModuleList(\n",
"        (0-11): 12 x RobertaLayer(\n",
"          (attention): RobertaAttention(\n",
"            (self): RobertaSelfAttention(\n",
"              (query): Linear(in_features=768, out_features=768, bias=True)\n",
"              (key): Linear(in_features=768, out_features=768, bias=True)\n",
"              (value): Linear(in_features=768, out_features=768, bias=True)\n",
"              (dropout): Dropout(p=0.1, inplace=False)\n",
"            )\n",
"            (output): RobertaSelfOutput(\n",
"              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
"              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"              (dropout): Dropout(p=0.1, inplace=False)\n",
"            )\n",
"          )\n",
"          (intermediate): RobertaIntermediate(\n",
"            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
"            (intermediate_act_fn): GELUActivation()\n",
"          )\n",
"          (output): RobertaOutput(\n",
"            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
"            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"            (dropout): Dropout(p=0.1, inplace=False)\n",
"          )\n",
"        )\n",
"      )\n",
"    )\n",
"  )\n",
"  (classifier): RobertaClassificationHead(\n",
"    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
"    (dropout): Dropout(p=0.1, inplace=False)\n",
"    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
"  )\n",
")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em sua √∫ltima camada"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Linear(in_features=768, out_features=2, bias=True)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model.classifier.out_proj"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(768, 2)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model.classifier.out_proj.in_features, model.classifier.out_proj.out_features"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vimos que nosso conjunto de dados tem 20 classes, mas esse modelo foi treinado para 2 classes, portanto, temos que modificar a √∫ltima camada"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Linear(in_features=768, out_features=20, bias=True)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"model.classifier.out_proj"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora √©"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora criamos uma fun√ß√£o de perda"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "loss_function = torch.nn.CrossEntropyLoss()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Um otimizador"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.optim import Adam\n",
"\n",
"optimizer = Adam(model.parameters(), lr=5e-4)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E, finalmente, uma m√©trica"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "import evaluate\n",
"\n",
"metric = evaluate.load(\"accuracy\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos verificar se est√° tudo certo com uma amostra"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "sample = next(iter(dataloader[\"train\"]))"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([64, 130]), torch.Size([64, 130]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, colocamos essa amostra no modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "torch.Size([64, 20])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model.to(\"cuda\")\n",
"ouputs = model(input_ids=sample[\"input_ids\"].to(\"cuda\"), attention_mask=sample[\"attention_mask\"].to(\"cuda\"))\n",
"ouputs.logits.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que o modelo produz 64 lotes, o que √© bom, porque definimos `BS = 20` e cada um com 20 sa√≠das, o que √© bom porque alteramos o modelo para produzir 20 valores."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Obtemos aquele com o valor mais alto"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "torch.Size([64])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "predictions = torch.argmax(ouputs.logits, axis=-1)\n",
"predictions.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Obtemos a perda"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "2.9990389347076416"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "loss = loss_function(ouputs.logits, sample[\"label\"].to(\"cuda\"))\n",
"loss.item()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E a precis√£o"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "0.015625"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "accuracy = metric.compute(predictions=predictions, references=sample[\"label\"])[\"accuracy\"]\n",
"accuracy"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora podemos criar um pequeno loop de treinamento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"<style>\n",
"    /* Turns off some styling */\n",
"    progress {\n",
"        /* gets rid of default border in Firefox and Opera. */\n",
"        border: none;\n",
"        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
"        background-size: auto;\n",
"    }\n",
"    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
"        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
"    }\n",
"    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
"        background: #F44336;\n",
"    }\n",
"</style>\n"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/html": [],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "from fastprogress.fastprogress import master_bar, progress_bar\n",
"\n",
"epochs = 1\n",
"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"model.to(device)\n",
"\n",
"master_progress_bar = master_bar(range(epochs))\n",
"for i in master_progress_bar:\n",
"    model.train()\n",
"    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"].to(device)\n",
"        attention_mask = batch[\"attention_mask\"].to(device)\n",
"        labels = batch[\"label\"].to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"].to(device)\n",
"        attention_mask = batch[\"attention_mask\"].to(device)\n",
"        labels = batch[\"label\"].to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Script com a base de c√≥digo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A maior parte da documenta√ß√£o do `accelerate` explica como usar o `accelerate` com scripts, portanto, faremos isso por enquanto e explicaremos como fazer isso com um notebook no final."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, vamos criar uma pasta na qual salvaremos os scripts."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "!mkdir accelerate_scripts"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora escrevemos o c√≥digo base em um script"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/01_code_base.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/01_code_base.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"from fastprogress.fastprogress import master_bar, progress_bar\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 64\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"model.to(device)\n",
"\n",
"master_progress_bar = master_bar(range(EPOCHS))\n",
"for i in master_progress_bar:\n",
"    model.train()\n",
"    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"].to(device)\n",
"        attention_mask = batch[\"attention_mask\"].to(device)\n",
"        labels = batch[\"label\"].to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"].to(device)\n",
"        attention_mask = batch[\"attention_mask\"].to(device)\n",
"        labels = batch[\"label\"].to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
"print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E agora vamos execut√°-lo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Accuracy = 0.2112                                                               \n",
"CPU times: user 2.12 s, sys: 391 ms, total: 2.51 s\n",
"Wall time: 3min 36s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!python accelerate_scripts/01_code_base.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Em meu computador, levou cerca de 3 minutos e meio."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### C√≥digo com acelera√ß√£o"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora vamos substituir alguns itens\n",
"\n",
" * Primeiro, importamos o `Accelerator` e o inicializamos.\n",
"\n",
"``` python\n",
"from accelerate import Accelerator\n",
"acelerador = Acelerador()\n",
"```\n",
"\n",
" * N√£o fazemos mais o t√≠pico\n",
"\n",
"``` python\n",
"torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"```\n",
"\n",
" * Em vez disso, deixamos o `accelerate` escolher o dispositivo por meio de\n",
"\n",
"``` python\n",
"dispositivo = accelerator.device\n",
"```\n",
"\n",
" * Passamos os elementos relevantes para treinamento por meio do m√©todo `prepare` e n√£o fazemos mais `model.to(device)`.\n",
"\n",
"``` python\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = preprare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"```\n",
"\n",
" * N√£o enviamos mais os dados e o modelo para a GPU com `.to(device)`, pois o `accelerate` cuidou disso com o m√©todo `prepare`.\n",
"\n",
" * Em vez de fazer a retropropaga√ß√£o com `loss.backward()`, deixamos que o `accelerate` fa√ßa isso com `loss.backward()`.\n",
" \n",
"``` python\n",
"accelerator.backward(loss)\n",
"```\n",
"\n",
" * Ao calcular a m√©trica no loop de valida√ß√£o, precisamos coletar os valores de todos os pontos, caso estejamos fazendo um treinamento distribu√≠do.\n",
"\n",
"``` python\n",
"previs√µes = accelerator.gather_for_metrics(previs√µes)\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/02_accelerate_base_code.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/02_accelerate_base_code.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"from fastprogress.fastprogress import master_bar, progress_bar\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 64\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"master_progress_bar = master_bar(range(EPOCHS))\n",
"for i in master_progress_bar:\n",
"    model.train()\n",
"    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"    print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
"    \n",
"    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
"\n",
"print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se voc√™ notar que adicionei essas duas linhas `print(f \"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")` e a linha `print(f \"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")`, eu as adicionei de prop√≥sito porque elas revelar√£o algo muito importante"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora vamos execut√°-lo. Para executar os scripts do `accelerate`, usamos o comando `accelerate launch`.\n",
"\n",
"``` bash\n",
"acelerar o lan√ßamento do <script>.py\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
"End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
"End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
"Accuracy = 0.206\n",
"End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
"Accuracy = 0.206\n",
"CPU times: user 1.6 s, sys: 272 ms, total: 1.88 s\n",
"Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/02_accelerate_base_code.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que antes levava cerca de 3 minutos e meio e agora leva cerca de 2 minutos e meio. Uma melhora significativa. Al√©m disso, se observarmos as impress√µes, veremos que elas foram impressas duas vezes.\n",
"\n",
"E como isso √© poss√≠vel? Porque o `accelerate` paralelizou o treinamento nas duas GPUs que tenho, de modo que ele ficou muito mais r√°pido.\n",
"\n",
"Al√©m disso, quando executei o primeiro script, ou seja, quando n√£o usei o `accelerate`, a GPU estava quase cheia, enquanto que quando executei o segundo, ou seja, o que usava o `accelerate`, as duas GPUs foram muito pouco usadas, portanto, podemos aumentar o tamanho do lote para tentar preencher ambas."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/03_accelerate_base_code_more_bs.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/03_accelerate_base_code_more_bs.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"from fastprogress.fastprogress import master_bar, progress_bar\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"master_progress_bar = master_bar(range(EPOCHS))\n",
"for i in master_progress_bar:\n",
"    model.train()\n",
"    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
"\n",
"print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Removi as impress√µes extras, pois j√° vimos que o c√≥digo est√° sendo executado em ambas as GPUs e aumentei o tamanho do lote de 64 para 128."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Accuracy = 0.1052                                                               \n",
"Accuracy = 0.1052\n",
"CPU times: user 1.41 s, sys: 180 ms, total: 1.59 s\n",
"Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/03_accelerate_base_code_more_bs.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O aumento do tamanho do lote reduziu o tempo de execu√ß√£o em alguns segundos."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Execu√ß√£o do processo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Execu√ß√£o de c√≥digo em um √∫nico processo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Anteriormente, vimos que o `print` foi impresso duas vezes, isso ocorre porque o `accelerate` cria tantos processos quanto os dispositivos em que o c√≥digo √© executado; no meu caso, ele cria dois processos porque tenho duas GPUs.\n",
"\n",
"No entanto, nem todo c√≥digo deve ser executado em todos os processos, por exemplo, o `print` torna o c√≥digo muito lento para ser executado v√°rias vezes, se os pontos de verifica√ß√£o forem salvos, eles ser√£o salvos duas vezes etc.\n",
"\n",
"Para executar parte de um c√≥digo em um √∫nico processo, √© necess√°rio encapsul√°-lo em uma fun√ß√£o e decor√°-la com `accelerator.on_local_main_process`. Por exemplo, no c√≥digo a seguir, voc√™ ver√° que criei a seguinte fun√ß√£o\n",
"\n",
"``` python\n",
"@accelerator.on_local_main_process\n",
"def print_something(something):\n",
"    print(something)\n",
"```\n",
"\n",
"Outra op√ß√£o √© colocar o c√≥digo dentro de um `if accelerator.is_local_main_process`, como o c√≥digo a seguir\n",
"\n",
"``` python\n",
"se accelerator.is_local_main_process:\n",
"    print(\"Algo\")\n",
"```"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"from fastprogress.fastprogress import master_bar, progress_bar\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_something(something):\n",
"    print(something)\n",
"\n",
"master_progress_bar = master_bar(range(EPOCHS))\n",
"for i in master_progress_bar:\n",
"    model.train()\n",
"    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
"\n",
"# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
"print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_local_main_process:\n",
"    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos execut√°-lo e ver"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Accuracy = 0.2098                                                               \n",
"End of script with 0.2098 accuracy\n",
"CPU times: user 1.38 s, sys: 197 ms, total: 1.58 s\n",
"Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, a impress√£o foi feita apenas uma vez"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "No entanto, embora voc√™ n√£o veja muito, as barras de progresso s√£o executadas em cada processo.\n",
"\n",
"N√£o encontrei uma maneira de contornar isso com as barras de progresso `fastprogress`, mas encontrei com as barras de progresso `tqdm`, portanto, substituirei as barras de progresso `fastprogress` pelas barras de progresso `tqdm` e, para que sejam executadas em um √∫nico processo, adicionarei o argumento `disable=not accelerator.is_local_main_process`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_something(something):\n",
"    print(something)\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        # master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
"print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_local_main_process:\n",
"    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:01<00:00,  1.45it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.30it/s]\n",
"Accuracy = 0.2166\n",
"End of script with 0.2166 accuracy\n",
"CPU times: user 1.33 s, sys: 195 ms, total: 1.52 s\n",
"Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Mostramos um exemplo de como imprimir em um √∫nico processo, e essa foi uma maneira de executar processos em um √∫nico processo. Mas se voc√™ quiser imprimir em um √∫nico processo, poder√° usar o m√©todo `print` do `accelerate`. Vejamos o mesmo exemplo anterior com esse m√©todo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Writing accelerate_scripts/06_accelerate_base_code_print_one_process.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/06_accelerate_base_code_print_one_process.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"        # master_progress_bar.child.comment = f'loss: {loss}'\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
"accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_local_main_process:\n",
"    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15433.52 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 11406.61 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15036.87 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14932.76 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14956.60 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:00<00:00,  1.46it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.33it/s]\n",
"Accuracy = 0.2134\n",
"End of script with 0.2134 accuracy\n",
"CPU times: user 1.4 s, sys: 189 ms, total: 1.59 s\n",
"Wall time: 2min 27s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/06_accelerate_base_code_print_one_process.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Execu√ß√£o de c√≥digo em todos os processos"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "No entanto, h√° um c√≥digo que deve ser executado em todos os processos, por exemplo, se fizermos o upload dos pontos de verifica√ß√£o para o hub, portanto, temos duas op√ß√µes: encapsular o c√≥digo em uma fun√ß√£o e decor√°-lo com `accelerator.on_main_process`.\n",
"\n",
"``` python\n",
"@accelerator.on_main_process\n",
"def do_my_thing():\n",
"    \"Algo feito uma vez por servidor\n",
"    do_thing_once()\n",
"```\n",
"\n",
"ou colocar o c√≥digo dentro de um `if accelerator.is_main_process`.\n",
"\n",
"``` python\n",
"se accelerator.is_main_process:\n",
"    repo.push_to_hub()\n",
"```"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como estamos treinando apenas para mostrar a biblioteca `accelerate` e o modelo que estamos treinando n√£o √© bom, n√£o faz sentido carregar os pontos de verifica√ß√£o no hub, portanto, farei um exemplo com `print`s."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/06_accelerate_base_code_some_code_in_all_process.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_in_one_process(something):\n",
"    print(something)\n",
"\n",
"@accelerator.on_main_process\n",
"def print_in_all_processes(something):\n",
"    print(something)\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_local_main_process:\n",
"    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
"\n",
"print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_main_process:\n",
"    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos para ver"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:03<00:00, 14518.44 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:03<00:00, 14368.77 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 16466.33 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14806.14 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14253.33 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14337.07 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:00<00:00,  1.46it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.34it/s]\n",
"Accuracy = 0.2092\n",
"End of script with 0.2092 accuracy\n",
"All process: Accuracy = 0.2092\n",
"All process: End of script with 0.2092 accuracy\n",
"CPU times: user 1.42 s, sys: 216 ms, total: 1.64 s\n",
"Wall time: 2min 27s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Execu√ß√£o de c√≥digo no processo X"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Por fim, podemos especificar em qual processo queremos executar o c√≥digo. Para isso, precisamos criar uma fun√ß√£o e decor√°-la com `@accelerator.on_process(process_index=0)`.\n",
"\n",
"``` python\n",
"@accelerator.on_process(process_index=0)\n",
"def do_my_thing():\n",
"    \"Algo feito no √≠ndice de processo 0\".\n",
"    do_thing_on_index_zero()\n",
"```\n",
"\n",
"ou decor√°-lo com `@accelerator.on_local_process(local_process_idx=0)`.\n",
"\n",
"``` python\n",
"@accelerator.on_local_process(local_process_index=0)\n",
"def do_my_thing():\n",
"    \"Algo feito no √≠ndice de processo 0 em cada servidor\".\n",
"    do_thing_on_index_zero_on_each_server()\n",
"```\n",
"\n",
"Aqui eu coloquei o processo 0, mas voc√™ pode colocar qualquer n√∫mero."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/07_accelerate_base_code_some_code_in_some_process.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_in_one_process(something):\n",
"    print(something)\n",
"\n",
"@accelerator.on_main_process\n",
"def print_in_all_processes(something):\n",
"    print(something)\n",
"\n",
"@accelerator.on_process(process_index=0)\n",
"def print_in_process_0(something):\n",
"    print(\"Process 0: \" + something)\n",
"\n",
"@accelerator.on_local_process(local_process_index=1)\n",
"def print_in_process_1(something):\n",
"    print(\"Process 1: \" + something)\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_local_main_process:\n",
"    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
"\n",
"print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_main_process:\n",
"    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
"\n",
"print_in_process_0(\"End of process 0\")\n",
"print_in_process_1(\"End of process 1\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 15735.58 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14906.20 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:02<00:00,  1.44it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.27it/s]\n",
"Process 1: End of process 1\n",
"Accuracy = 0.2128\n",
"End of script with 0.2128 accuracy\n",
"All process: Accuracy = 0.2128\n",
"All process: End of script with 0.2128 accuracy\n",
"Process 0: End of process 0\n",
"CPU times: user 1.42 s, sys: 295 ms, total: 1.71 s\n",
"Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Sincronizar processos"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se tivermos um c√≥digo que precisa ser executado em todos os processos, √© interessante esperar que ele termine em todos os processos antes de executar outra tarefa, portanto, usamos `accelerator.wait_for_everyone()` para isso.\n",
"\n",
"Para ver isso, vamos colocar um atraso em uma das fun√ß√µes de impress√£o em um processo.\n",
"\n",
"Tamb√©m coloquei um intervalo no ciclo de treinamento para que ele n√£o passe muito tempo treinando, o que n√£o √© o que nos interessa no momento."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/08_accelerate_base_code_sync_all_process.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/09_accelerate_base_code_sync_all_process.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"import time\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_in_one_process(something):\n",
"    print(something)\n",
"\n",
"@accelerator.on_main_process\n",
"def print_in_all_processes(something):\n",
"    print(something)\n",
"\n",
"@accelerator.on_process(process_index=0)\n",
"def print_in_process_0(something):\n",
"    time.sleep(2)\n",
"    print(\"Process 0: \" + something)\n",
"\n",
"@accelerator.on_local_process(local_process_index=1)\n",
"def print_in_process_1(something):\n",
"    print(\"Process 1: \" + something)\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"        break\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_local_main_process:\n",
"    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
"\n",
"print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"if accelerator.is_main_process:\n",
"    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
"\n",
"print_in_one_process(\"Printing with delay in process 0\")\n",
"print_in_process_0(\"End of process 0\")\n",
"print_in_process_1(\"End of process 1\")\n",
"accelerator.wait_for_everyone()\n",
"\n",
"print_in_one_process(\"End of script\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14218.23 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14666.25 examples/s]\n",
"  0%|                                                   | 0/176 [00:00<?, ?it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.58it/s]\n",
"Process 1: End of process 1\n",
"Accuracy = 0.212\n",
"End of script with 0.212 accuracy\n",
"All process: Accuracy = 0.212\n",
"All process: End of script with 0.212 accuracy\n",
"Printing with delay in process 0\n",
"Process 0: End of process 0\n",
"End of script\n"
          ]
        }
      ],
      "source": [
      "!accelerate launch accelerate_scripts/09_accelerate_base_code_sync_all_process.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como voc√™ pode ver, primeiro imprimimos `Process 1: End of process 1` e depois o restante, porque o restante das impress√µes √© feito no processo 0 ou em todos os processos, portanto, at√© que o atraso de 2 segundos que colocamos n√£o seja conclu√≠do, o restante do c√≥digo n√£o √© executado."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Salvar e carregar o ditado de estado"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Quando treinamos, √†s vezes salvamos o estado para que possamos continuar em outro momento.\n",
"\n",
"Para salvar o estado, teremos que usar os m√©todos `save_state()` e `load_state()`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/09_accelerate_save_and_load_checkpoints.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/10_accelerate_save_and_load_checkpoints.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_something(something):\n",
"    print(something)\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"\n",
"    # Guardamos los pesos\n",
"    accelerator.save_state(\"accelerate_scripts/checkpoints\")\n",
"\n",
"print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
"\n",
"# Cargamos los pesos\n",
"accelerator.load_state(\"accelerate_scripts/checkpoints\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:58<00:00,  1.48it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.40it/s]\n",
"Accuracy = 0.2142\n"
          ]
        }
      ],
      "source": [
      "!accelerate launch accelerate_scripts/10_accelerate_save_and_load_checkpoints.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Salvar o modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Quando o m√©todo `prepare` foi usado, o modelo foi empacotado para que pudesse ser salvo nos dispositivos necess√°rios. Portanto, ao salv√°-lo, temos que usar o m√©todo `save_model` que primeiro o desembrulha e depois o salva. Al√©m disso, se usarmos o par√¢metro `safe_serialization=True`, o modelo ser√° salvo como um tensor `safe`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Writing accelerate_scripts/11_accelerate_save_model.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/11_accelerate_save_model.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_something(something):\n",
"    print(something)\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"\n",
"    # Guardamos el modelo\n",
"    accelerator.wait_for_everyone()\n",
"    accelerator.save_model(model, \"accelerate_scripts/model\", safe_serialization=True)\n",
"\n",
"print_something(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:58<00:00,  1.48it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.35it/s]\n",
"Accuracy = 0.214\n"
          ]
        }
      ],
      "source": [
      "!accelerate launch accelerate_scripts/11_accelerate_save_model.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Salvar o modelo \"pr√©-treinado"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nos modelos que usam a biblioteca `transformers`, devemos salvar o modelo com o m√©todo `save_pretrained` para carreg√°-lo com o m√©todo `from_pretrained`. Antes de salvar, o modelo deve ser desempacotado com o m√©todo `unwrap_model`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Writing accelerate_scripts/11_accelerate_save_pretrained.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/12_accelerate_save_pretrained.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"@accelerator.on_local_main_process\n",
"def print_something(something):\n",
"    print(something)\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"\n",
"    # Guardamos el modelo pretrained\n",
"    unwrapped_model = accelerator.unwrap_model(model)\n",
"    unwrapped_model.save_pretrained(\n",
"        \"accelerate_scripts/model_pretrained\",\n",
"        is_main_process=accelerator.is_main_process,\n",
"        save_function=accelerator.save,\n",
"    )\n",
"\n",
"print_something(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15152.47 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15119.13 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 12724.70 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 12397.49 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 15247.21 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 15138.03 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:59<00:00,  1.48it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.37it/s]\n",
"Accuracy = 0.21\n"
          ]
        }
      ],
      "source": [
      "!accelerate launch accelerate_scripts/12_accelerate_save_pretrained.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora podemos carreg√°-lo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of RobertaModel were not initialized from the model checkpoint at accelerate_scripts/model_pretrained and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModel\n",
"\n",
"checkpoints = \"accelerate_scripts/model_pretrained\"\n",
"tokenizer = AutoModel.from_pretrained(checkpoints)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento em notebooks"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "At√© agora, vimos como executar scripts, mas se quiser executar o c√≥digo em um notebook, podemos escrever o mesmo c√≥digo de antes, mas encapsulado em uma fun√ß√£o"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, importamos as bibliotecas"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"import time\n",
"# from accelerate import Accelerator"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora criamos a fun√ß√£o"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
      "def train_code(batch_size: int = 64):\n",
"    from accelerate import Accelerator\n",
"    accelerator = Accelerator()\n",
"\n",
"    dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"    num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"    max_len = 130\n",
"\n",
"    checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"    tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"    def tokenize_function(dataset):\n",
"        return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"    tokenized_dataset = {\n",
"        \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"        \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"        \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    }\n",
"    tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"    tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"    tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"    BS = batch_size\n",
"    dataloader = {\n",
"        \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"        \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"        \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"    }\n",
"\n",
"    model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"    model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"    loss_function = torch.nn.CrossEntropyLoss()\n",
"    optimizer = Adam(model.parameters(), lr=5e-4)\n",
"    metric = evaluate.load(\"accuracy\")\n",
"\n",
"    EPOCHS = 1\n",
"    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"    device = accelerator.device\n",
"\n",
"    # model.to(device)\n",
"    model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"    for i in range(EPOCHS):\n",
"        model.train()\n",
"        progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"        for batch in progress_bar_train:\n",
"            optimizer.zero_grad()\n",
"\n",
"            input_ids = batch[\"input_ids\"]#.to(device)\n",
"            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"            labels = batch[\"label\"]#.to(device)\n",
"\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"            loss = loss_function(outputs['logits'], labels)\n",
"\n",
"            # loss.backward()\n",
"            accelerator.backward(loss)\n",
"            optimizer.step()\n",
"\n",
"        model.eval()\n",
"        progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"        for batch in progress_bar_validation:\n",
"            input_ids = batch[\"input_ids\"]#.to(device)\n",
"            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"            labels = batch[\"label\"]#.to(device)\n",
"\n",
"            with torch.no_grad():\n",
"                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"            predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"            # Recopilamos las predicciones de todos los dispositivos\n",
"            predictions = accelerator.gather_for_metrics(predictions)\n",
"            labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"            accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"        accuracy = metric.compute()\n",
"        \n",
"    accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para executar o treinamento no notebook, usamos a fun√ß√£o `notebook_launcher`, para a qual passamos a fun√ß√£o que queremos executar, os argumentos dessa fun√ß√£o e o n√∫mero de GPUs nas quais vamos treinar com a vari√°vel `num_processes`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Launching training on 2 GPUs.\n"
          ]
        },
{
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:01<00:00,  1.45it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.31it/s]\n"
          ]
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Accuracy = 0.2112\n"
          ]
        }
      ],
      "source": [
      "from accelerate import notebook_launcher\n",
"\n",
"args = (128,)\n",
"notebook_launcher(train_code, args, num_processes=2)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento em FP16"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Quando configuramos o `accelerate` pela primeira vez, ele nos perguntou `Do you wish to use FP16 or BF16 (mixed precision)?` e dissemos que n√£o, ent√£o agora vamos dizer que sim, que queremos usar FP16."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "At√© agora, treinamos em FP32, o que significa que cada peso do modelo √© um n√∫mero de ponto flutuante de 32 bits, e agora vamos usar um n√∫mero de ponto flutuante de 16 bits, ou seja, o modelo ocupar√° menos espa√ßo. Portanto, duas coisas acontecer√£o: poderemos usar um tamanho de lote maior e ele tamb√©m ser√° mais r√°pido."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, reiniciamos o `accelerate config` e informamos que queremos o FP16."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "--------------------------------------------------------------------------------\n",
"In which compute environment are you running?\n",
"This machine\n",
"--------------------------------------------------------------------------------\n",
"multi-GPU\n",
"How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
"Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
"Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
"Do you want to use DeepSpeed? [yes/NO]: no\n",
"Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
"Do you want to use Megatron-LM ? [yes/NO]: no\n",
"How many GPU(s) should be used for distributed training? [1]:2\n",
"What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
"--------------------------------------------------------------------------------\n",
"Do you wish to use FP16 or BF16 (mixed precision)?\n",
"fp16\n",
"accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
      "!accelerate config"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, criamos um script para treinar, com o mesmo tamanho de lote de antes, para ver se o tempo de treinamento √© menor."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/12_accelerate_base_code_fp16_bs128.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/13_accelerate_base_code_fp16_bs128.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 128\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos execut√°-lo e ver quanto tempo leva"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14983.76 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14315.47 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:01<00:00,  2.88it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.84it/s]\n",
"Accuracy = 0.2094\n",
"CPU times: user 812 ms, sys: 163 ms, total: 976 ms\n",
"Wall time: 1min 27s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/13_accelerate_base_code_fp16_bs128.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Quando executamos esse treinamento em FP32, demorou cerca de 2 minutos e meio, e agora demora cerca de 1 minuto e meio. Vamos ver se agora, em vez de treinar com um tamanho de lote de 128, o fazemos com um tamanho de lote de 256."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/14_accelerate_base_code_fp16_bs256.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 256\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 15390.30 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14990.92 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [00:54<00:00,  1.62it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.45it/s]\n",
"Accuracy = 0.2236\n",
"CPU times: user 670 ms, sys: 91.6 ms, total: 761 ms\n",
"Wall time: 1min 12s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "A queda foi de apenas 15 segundos"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento em BF16"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Antes trein√°vamos no FP16 e agora vamos treinar no BF16, qual √© a diferen√ßa?\n",
"\n",
"![FP32_FP16_BF16](https://maximofn.com/wp-content/uploads/2024/05/FP32_FP16_BF16.webp)\n",
"\n",
"Como podemos ver na figura, enquanto o FP16 comparado ao FP32 tem menos bits na mantissa e no expoente, o que torna seu intervalo muito menor, o BF16 comparado ao FP32 tem o mesmo n√∫mero de bits no expoente, mas menos na mantissa, o que faz com que o BF16 tenha o mesmo intervalo de n√∫meros que o FP32, mas seja menos preciso.\n",
"\n",
"Isso √© vantajoso porque, no FP16, alguns c√°lculos podem gerar n√∫meros muito altos, que n√£o poderiam ser representados no formato FP16. Al√©m disso, h√° determinados dispositivos HW que s√£o otimizados para esse formato."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como antes, executamos o `accelerate config` e indicamos que queremos o BF16."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "--------------------------------------------------------------------------------\n",
"In which compute environment are you running?\n",
"This machine\n",
"--------------------------------------------------------------------------------\n",
"multi-GPU\n",
"How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
"Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
"Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
"Do you want to use DeepSpeed? [yes/NO]: no\n",
"Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
"Do you want to use Megatron-LM ? [yes/NO]: no\n",
"How many GPU(s) should be used for distributed training? [1]:2\n",
"What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
"--------------------------------------------------------------------------------\n",
"Do you wish to use FP16 or BF16 (mixed precision)?\n",
"bf16\n",
"accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
      "!accelerate config"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, executamos o √∫ltimo script que criamos, ou seja, com um tamanho de lote de 256"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14814.95 examples/s]\n",
"Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14506.83 examples/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [00:51<00:00,  1.70it/s]\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  3.21it/s]\n",
"Accuracy = 0.2112\n",
"CPU times: user 688 ms, sys: 144 ms, total: 832 ms\n",
"Wall time: 1min 17s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Levou um tempo semelhante ao anterior, o que √© normal, pois treinamos um modelo com pesos de 16 bits, como antes."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento em FP8"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora vamos treinar no formato FP8, que, como o nome sugere, √© um formato de ponto flutuante, em que cada peso tem 8 bits, portanto, executamos o `accelerate config` para informar que queremos o FP8."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "--------------------------------------------------------------------------------\n",
"In which compute environment are you running?\n",
"This machine\n",
"--------------------------------------------------------------------------------\n",
"multi-GPU\n",
"How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
"Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
"Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
"Do you want to use DeepSpeed? [yes/NO]: no\n",
"Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
"Do you want to use Megatron-LM ? [yes/NO]: no\n",
"How many GPU(s) should be used for distributed training? [1]:2\n",
"What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
"--------------------------------------------------------------------------------\n",
"Do you wish to use FP16 or BF16 (mixed precision)?\n",
"fp8\n",
"accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
      "!accelerate config"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, executamos o √∫ltimo script, o script de tamanho de lote de 256."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Traceback (most recent call last):\n",
"  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
"    accelerator = Accelerator()\n",
"                  ^^^^^^^^^^^^^\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
"    self.state = AcceleratorState(\n",
"                 ^^^^^^^^^^^^^^^^^\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
"    raise ValueError(\n",
"ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
"Traceback (most recent call last):\n",
"  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
"    accelerator = Accelerator()\n",
"                  ^^^^^^^^^^^^^\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
"    self.state = AcceleratorState(\n",
"                 ^^^^^^^^^^^^^^^^^\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
"    raise ValueError(\n",
"ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
"[2024-05-13 21:40:56,455] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 501480) of binary: /home/wallabot/miniconda3/envs/nlp/bin/python\n",
"Traceback (most recent call last):\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/bin/accelerate\", line 8, in <module>\n",
"    sys.exit(main())\n",
"             ^^^^^^\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
"    args.func(args)\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
"    multi_gpu_launcher(args)\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
"    distrib_run.run(args)\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
"    elastic_launch(\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
"    return launch_agent(self._config, self._entrypoint, list(args))\n",
"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
"  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
"    raise ChildFailedError(\n",
"torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
"============================================================\n",
"accelerate_scripts/13_accelerate_base_code_fp16_bs256.py FAILED\n",
"------------------------------------------------------------\n",
"Failures:\n",
"[1]:\n",
"  time      : 2024-05-13_21:40:56\n",
"  host      : wallabot\n",
"  rank      : 1 (local_rank: 1)\n",
"  exitcode  : 1 (pid: 501481)\n",
"  error_file: <N/A>\n",
"  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
"------------------------------------------------------------\n",
"Root Cause (first observed failure):\n",
"[0]:\n",
"  time      : 2024-05-13_21:40:56\n",
"  host      : wallabot\n",
"  rank      : 0 (local_rank: 0)\n",
"  exitcode  : 1 (pid: 501480)\n",
"  error_file: <N/A>\n",
"  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
"============================================================\n",
"CPU times: user 65.1 ms, sys: 14.5 ms, total: 79.6 ms\n",
"Wall time: 7.24 s\n"
          ]
        }
      ],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como os pesos agora s√£o de 8 bits e ocupam metade da mem√≥ria, aumentaremos o tamanho do lote para 512."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Writing accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n",
"\n",
"import torch\n",
"from torch.utils.data import DataLoader\n",
"from torch.optim import Adam\n",
"from datasets import load_dataset\n",
"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import evaluate\n",
"import tqdm\n",
"\n",
"# Importamos e inicializamos Accelerator\n",
"from accelerate import Accelerator\n",
"accelerator = Accelerator()\n",
"\n",
"dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
"num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
"max_len = 130\n",
"\n",
"checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"\n",
"def tokenize_function(dataset):\n",
"    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
"tokenized_dataset = {\n",
"    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
"}\n",
"tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
"tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
"\n",
"BS = 512\n",
"dataloader = {\n",
"    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
"    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
"    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
"}\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
"model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
"\n",
"loss_function = torch.nn.CrossEntropyLoss()\n",
"optimizer = Adam(model.parameters(), lr=5e-4)\n",
"metric = evaluate.load(\"accuracy\")\n",
"\n",
"EPOCHS = 1\n",
"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"device = accelerator.device\n",
"\n",
"# model.to(device)\n",
"model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
"\n",
"for i in range(EPOCHS):\n",
"    model.train()\n",
"    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_train:\n",
"        optimizer.zero_grad()\n",
"\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        loss = loss_function(outputs['logits'], labels)\n",
"\n",
"        # loss.backward()\n",
"        accelerator.backward(loss)\n",
"        optimizer.step()\n",
"\n",
"    model.eval()\n",
"    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
"    for batch in progress_bar_validation:\n",
"        input_ids = batch[\"input_ids\"]#.to(device)\n",
"        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
"        labels = batch[\"label\"]#.to(device)\n",
"\n",
"        with torch.no_grad():\n",
"            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
"        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
"        # Recopilamos las predicciones de todos los dispositivos\n",
"        predictions = accelerator.gather_for_metrics(predictions)\n",
"        labels = accelerator.gather_for_metrics(labels)\n",
"\n",
"        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
"    accuracy = metric.compute()\n",
"    \n",
"accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "N√≥s o executamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "%%time\n",
"\n",
"!accelerate launch accelerate_scripts/15_accelerate_base_code_fp8_bs512.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Infer√™ncia de modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Uso do ecossistema de rosto abra√ßado"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos ver como fazer a infer√™ncia de modelos grandes com a biblioteca de faces de abra√ßo `transformers`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Infer√™ncia com `pipeline`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se usarmos o ecossistema Hugging Face, √© muito simples, pois tudo √© produzido por baixo sem que tenhamos que fazer muito. No caso de usar o `pipeline`, que √© a maneira mais f√°cil de fazer infer√™ncia com a biblioteca `transformers`, basta informar o modelo que queremos usar e, o que √© muito importante, passar `device_map=\"auto\"`. Isso far√° com que o `accelerate` distribua o modelo entre as diferentes GPUs, a RAM da CPU ou o disco r√≠gido, se necess√°rio."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "H√° mais valores poss√≠veis para `device_map`, que veremos mais adiante, mas, por enquanto, mantenha o `\"auto\"`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos usar o modelo `Llama3 8B`, que, como o nome sugere, √© um modelo com cerca de 8 bilh√µes de par√¢metros, j√° que cada par√¢metro, por padr√£o, est√° no formato FP32, que corresponde a 4 bytes (32 bits), o que significa que, se multiplicarmos 8 bilh√µes de par√¢metros por 4 bytes, teremos uma GPU com cerca de 32 GB de VRAM.\n",
"\n",
"No meu caso, tenho duas GPUs com 24 GB de VRAM, portanto, n√£o caberia em uma √∫nica GPU. Mas ao definir `device_map=\"auto\"`, a acelera√ß√£o distribuir√° o modelo entre as duas GPUs e eu poderei realizar a infer√™ncia."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/09_inference_with_pipeline.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/16_inference_with_pipeline.py\n",
"\n",
"from transformers import pipeline\n",
"\n",
"checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
"generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
"\n",
"prompt = \"Conoces accelerate de hugging face?\"\n",
"output = generator(prompt)\n",
"print(output)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora vamos execut√°-lo, mas como o pipeline usa o accelerate abaixo, n√£o precisamos execut√°-lo com `accelerate launch script.py`, mas com `python script.py`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.27s/it]\n",
"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
"[{'generated_text': 'Conoces accelerate de hugging face? ¬øQu√© es el modelo de lenguaje de transformers y c√≥mo se utiliza en el marco de hugging face? ¬øC√≥mo puedo utilizar modelos de lenguaje de transformers en mi aplicaci√≥n? ¬øQu√© son los tokenizers y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo crear un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los datasets y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar datasets para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning'}]\n"
          ]
        }
      ],
      "source": [
      "!python accelerate_scripts/16_inference_with_pipeline.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como voc√™ pode ver, ele n√£o respondeu, mas continuou fazendo perguntas. Isso ocorre porque o Llama3 √© um modelo de linguagem que prev√™ o pr√≥ximo token, portanto, com o prompt que eu dei a ele, ele considerou que os pr√≥ximos melhores tokens s√£o aqueles que correspondem a mais perguntas. O que faz sentido, pois h√° momentos em que as pessoas t√™m d√∫vidas sobre um t√≥pico e isso gera muitas perguntas, portanto, para que ele responda √† pergunta, temos de condicion√°-lo um pouco."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/10_inference_with_pipeline_condition.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/17_inference_with_pipeline_condition.py\n",
"\n",
"from transformers import pipeline\n",
"\n",
"checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
"generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
"\n",
"prompt = \"Conoces accelerate de hugging face?\"\n",
"messages = [\n",
"    {\n",
"        \"role\": \"system\",\n",
"        \"content\": \"Eres un chatbot amigable que siempre intenta solucionar las dudas\",\n",
"    },\n",
"    {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
"]\n",
"output = generator(messages)\n",
"print(output[0]['generated_text'][-1])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como voc√™ pode ver, foi gerada uma mensagem com fun√ß√µes, condicionando o modelo e o prompt"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.41s/it]\n",
"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
"{'role': 'assistant', 'content': '¬°Hola!\\n\\nS√≠, conozco Accelerate de Hugging Face. Accelerate es una biblioteca de Python desarrollada por Hugging Face que se enfoca en simplificar y acelerar el entrenamiento y la evaluaci√≥n de modelos de lenguaje en diferentes dispositivos y entornos.\\n\\nCon Accelerate, puedes entrenar modelos de lenguaje en diferentes plataformas y dispositivos, como GPUs, TPUs, CPUs y servidores, sin necesidad de cambiar el c√≥digo de tu modelo. Esto te permite aprovechar al m√°ximo la potencia de c√°lculo de tus dispositivos y reducir el tiempo de entrenamiento.\\n\\nAccelerate tambi√©n ofrece varias caracter√≠sticas adicionales, como:\\n\\n* Soporte para diferentes frameworks de machine learning, como TensorFlow, PyTorch y JAX.\\n* Integraci√≥n con diferentes sistemas de almacenamiento y procesamiento de datos, como Amazon S3 y Google Cloud Storage.\\n* Soporte para diferentes protocolos de comunicaci√≥n, como HTTP y gRPC.\\n* Herramientas para monitorear y depurar tus modelos en tiempo real.\\n\\nEn resumen, Accelerate es una herramienta muy √∫til para desarrolladores de modelos de lenguaje que buscan simplificar y acelerar el proceso de entrenamiento y evaluaci√≥n de sus modelos.\\n\\n¬øTienes alguna pregunta espec√≠fica sobre Accelerate o necesitas ayuda para implementarlo en tu proyecto?'}\n"
          ]
        }
      ],
      "source": [
      "!python accelerate_scripts/17_inference_with_pipeline_condition.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora a resposta, se ela responder ao nosso prompt"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Infer√™ncia com `AutoClass`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Por fim, veremos como fazer a infer√™ncia somente com o `AutoClass`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Overwriting accelerate_scripts/11_inference_with_autoclass.py\n"
          ]
        }
      ],
      "source": [
      "%%writefile accelerate_scripts/18_inference_with_autoclass.py\n",
"\n",
"from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
"\n",
"checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints, device_map=\"auto\")\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints, device_map=\"auto\")\n",
"streamer = TextStreamer(tokenizer)\n",
"\n",
"prompt = \"Conoces accelerate de hugging face?\"\n",
"tokens_input = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
"\n",
"_ = model.generate(**tokens_input, streamer=streamer, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como voc√™ pode ver, o objeto `streamer` foi criado e, em seguida, √© passado para o m√©todo `generate` do modelo. Isso √© √∫til para que cada palavra seja impressa √† medida que √© gerada e voc√™ n√£o precise esperar que toda a sa√≠da seja gerada antes de imprimi-la."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
"Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.28s/it]\n",
"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
"<|begin_of_text|>Conoces accelerate de hugging face? Si es as√≠, puedes utilizar la biblioteca `transformers` de Hugging Face para crear un modelo de lenguaje que pueda predecir la siguiente palabra en una secuencia de texto.\n",
"\n",
"Aqu√≠ te muestro un ejemplo de c√≥mo hacerlo:\n",
"```\n",
"import pandas as pd\n",
"import torch\n",
"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
"\n",
"# Cargar el modelo y el tokenizador\n",
"model_name = \"bert-base-uncased\"\n",
"model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
"tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
"\n",
"# Cargar el conjunto de datos\n",
"train_df = pd.read_csv(\"train.csv\")\n",
"test_df = pd.read_csv(\"test.csv\")\n",
"\n",
"# Preprocesar los datos\n",
"train_texts = train_df[\"text\"]\n",
"train_labels = train_df[\"label\"]\n",
"test_texts = test_df[\"text\"]\n",
"\n",
"# Convertir los textos en entradas para el modelo\n",
"train_encodings = tokenizer.batch_encode_plus(train_texts, \n",
"                                              add_special_tokens=True, \n",
"                                              max_length=512, \n",
"                                              return_attention_mask=True, \n",
"                                              return_tensors='pt')\n",
"\n",
"test_encodings = tokenizer.batch_encode_plus(test_texts, \n",
"                                             add_special_tokens=True, \n",
"                                             max_length=512, \n",
"                                             return_attention_mask=True, \n",
"                                             return_tensors='pt')\n",
"\n",
"# Crear un dataloader para entrenar el modelo\n",
"train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], \n",
"                                               train_encodings[\"attention_mask\"], \n",
"                                               torch.tensor(train_labels))\n",
"train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
"\n",
"# Entrenar el modelo\n",
"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"model.to(device)\n",
"criterion = torch.nn.CrossEntropyLoss()\n",
"optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
"\n",
"for epoch in range(5):\n",
"    model.train()\n",
"    total_loss = 0\n",
"    for batch in train_loader:\n",
"        input_ids = batch[0].to(device)\n",
"        attention_mask = batch[1].to(device)\n",
"        labels = batch[2].to(device)\n",
"        optimizer.zero_grad()\n",
"        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
"        loss = criterion(outputs, labels)\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"        total_loss += loss.item()\n",
"    print(f\"Epoch {epoch+1}, Loss: {total\n"
          ]
        }
      ],
      "source": [
      "!python accelerate_scripts/18_inference_with_autoclass.py"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Use pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Normalmente, a maneira de fazer infer√™ncias com o pytorch √© criar um modelo com os pesos inicializados aleatoriamente e, em seguida, carregar um `state dict` com os pesos do modelo pr√©-treinado."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/maximo.fernandez/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 230M/230M [02:48<00:00, 1.43MB/s] \n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"import torchvision.models as models\n",
"\n",
"model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
"torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora que temos o `state dict`, vamos fazer a infer√™ncia como normalmente fazemos no pytorch."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "import torch\n",
"import torchvision.models as models\n",
"\n",
"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     # Set device\n",
"\n",
"resnet152 = models.resnet152().to(device) # Create model with random weights and move to device\n",
"state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device) # Load pretrained weights into device memory\n",
"resnet152.load_state_dict(state_dict) # Load this weights into the model\n",
"\n",
"input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
"output = resnet152(input)\n",
"output.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos explicar o que aconteceu\n",
"\n",
" * Quando usamos `resnet152 = models.resnet152().to(device)`, um resnet152 com pesos aleat√≥rios foi carregado na mem√≥ria da GPU.\n",
" * Quando fizemos `state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)`, um dicion√°rio com os pesos treinados foi carregado na mem√≥ria da GPU.\n",
" * Quando fizemos `resnet152.load_state_dict(state_dict)`, esses pesos pr√©-treinados foram atribu√≠dos ao modelo.\n",
"\n",
"Ou seja, o modelo foi carregado duas vezes na mem√≥ria da GPU."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Voc√™ pode estar se perguntando por que fizemos primeiro\n",
"\n",
"``` python\n",
"model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
"torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')\n",
"```\n",
"\n",
"Para ent√£o fazer\n",
"\n",
"``` python\n",
"resnet152 = models.resnet152().to(device)\n",
"state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)\n",
"resnet152.load_state_dict(state_dict)\n",
"```\n",
"\n",
"E por que n√£o usamos diretamente\n",
"\n",
"```\n",
"model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
"```\n",
"\n",
"E paramos de salvar o `state dict` para carreg√°-lo mais tarde. Bem, porque o Pytorch, por exemplo, faz a mesma coisa que n√≥s fizemos. Portanto, para poder ver todo o processo, fizemos em v√°rias linhas o que o Pytorch faz em uma linha"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Essa maneira de trabalhar funcionou bem at√© agora, desde que os modelos tivessem um tamanho gerenci√°vel para as GPUs dos usu√°rios. Por√©m, desde o advento dos LLMs, essa abordagem n√£o faz mais sentido.\n",
"\n",
"Por exemplo, um modelo de 6B par√¢metros ocuparia 24 GB de mem√≥ria e, como √© carregado duas vezes com essa forma de trabalho, seria necess√°ria uma GPU de 48 GB."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Portanto, para corrigir isso, a maneira de carregar um modelo pr√©-treinado do Pytorch √©:\n",
" * Crie um modelo vazio com `init_empty_weights` que n√£o ocupar√° a RAM.\n",
" * Em seguida, carregue os pesos com `load_checkpoint_and_dispatch`, que carregar√° um ponto de verifica√ß√£o dentro do modelo vazio e distribuir√° os pesos de cada camada em todos os dispositivos dispon√≠veis (GPU, CPU, RAM e disco r√≠gido), gra√ßas √† configura√ß√£o `device_map=\"auto\"`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "import torch\n",
"import torchvision.models as models\n",
"from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
"\n",
"with init_empty_weights():\n",
"    resnet152 = models.resnet152()\n",
"\n",
"resnet152 = load_checkpoint_and_dispatch(resnet152, checkpoint='accelerate_scripts/resnet152_pretrained.pth', device_map=\"auto\")\n",
"\n",
"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
"\n",
"input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
"output = resnet152(input)\n",
"output.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Como a acelera√ß√£o funciona abaixo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Neste v√≠deo, voc√™ pode ver graficamente como a acelera√ß√£o funciona por baixo."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/MWCSGj9jEAo\" title=\"Accelerate Big Model Inference: How Does it Work?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Inicializa√ß√£o de um modelo vazio"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O Accelerate cria o esqueleto de um modelo vazio usando `init_empty_weights` para que ele ocupe o m√≠nimo de mem√≥ria poss√≠vel."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Por exemplo, vamos ver quanta RAM eu tenho dispon√≠vel em meu computador."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Total RAM: 31.24 GB, Available RAM: 22.62 GB, Used RAM: 7.82 GB\n"
          ]
        }
      ],
      "source": [
      "import psutil\n",
"\n",
"def get_ram_info():\n",
"    ram = dict(psutil.virtual_memory()._asdict())\n",
"    print(f\"Total RAM: {(ram['total']/1024/1024/1024):.2f} GB, Available RAM: {(ram['available']/1024/1024/1024):.2f} GB, Used RAM: {(ram['used']/1024/1024/1024):.2f} GB\")\n",
"\n",
"get_ram_info()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Tenho cerca de 22 GB de RAM dispon√≠veis\n",
"\n",
"Agora vamos tentar criar um modelo de 5000x1000x1000 par√¢metros, ou seja, 5B par√¢metros, se cada par√¢metro estiver em FP32, o que significa 20 GB de RAM."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"from torch import nn\n",
"\n",
"model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se olharmos para a RAM novamente"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Total RAM: 31.24 GB, Available RAM: 3.77 GB, Used RAM: 26.70 GB\n"
          ]
        }
      ],
      "source": [
      "get_ram_info()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, agora temos apenas 3 GB de RAM dispon√≠veis."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, vamos excluir o modelo para liberar a RAM"
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Total RAM: 31.24 GB, Available RAM: 22.44 GB, Used RAM: 8.03 GB\n"
          ]
        }
      ],
      "source": [
      "del model\n",
"get_ram_info()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Voltamos a ter cerca de 22 GB de RAM dispon√≠veis."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos agora usar o `init_empty_weights` do `accelerate` e, em seguida, examinar a RAM."
      ]
    },
{
      "cell_type": "code",
      "execution_count": "None",
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Total RAM: 31.24 GB, Available RAM: 22.32 GB, Used RAM: 8.16 GB\n"
          ]
        }
      ],
      "source": [
      "from accelerate import init_empty_weights\n",
"\n",
"with init_empty_weights():\n",
"    model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])\n",
"\n",
"get_ram_info()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Antes, t√≠nhamos exatamente 22,44 GB livres e, depois de criar o modelo com `init_empty_weights`, temos 22,32 GB. A economia de RAM √© enorme! Quase nenhuma RAM foi usada para criar o modelo.\n",
"\n",
"Isso se baseia no metadevice introduzido no PyTorch 1.9, portanto, √© importante que, para usar o `accelerate`, tenhamos uma vers√£o mais recente do Pytorch."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "#### Carregamento de pesos"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois de inicializar o modelo, precisamos carregar os pesos, o que √© feito pelo `load_checkpoint_and_dispatch` que, como o nome sugere, carrega os pesos e os envia para os dispositivos necess√°rios."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
