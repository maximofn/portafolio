{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face Acelerar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Accelerate` \u00e9 uma biblioteca da Hugging Face que permite executar o mesmo c\u00f3digo PyTorch em qualquer configura\u00e7\u00e3o distribu\u00edda adicionando apenas quatro linhas de c\u00f3digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instala\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para instalar `accelerate` com `pip`, simplesmente execute:",
        "\n",
        "``` bash\n",
        "pip install accelerate",
        "```\n",
        "\n",
        "E com `conda`:",
        "\n",
        "``` bash\n",
        "conda install -c conda-forge accelerate",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configura\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em cada ambiente em que o `accelerate` seja instalado, a primeira coisa que precisa ser feita \u00e9 configur\u00e1-lo. Para isso, executamos no terminal:",
        "\n",
        "``` bash\n",
        "`accelerate config`",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "no\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No entendi completamente sua solicita\u00e7\u00e3o. Vou traduzir o texto fornecido para portugu\u00eas, mantendo a estrutura e estilo do markdown original:\n\nEm meu caso, as respostas foram",
        "* Em qual ambiente de computa\u00e7\u00e3o voc\u00ea est\u00e1 executando?",
        "- [x] \"Esta m\u00e1quina\"",
        "- [_] \"AWS (Amazon SageMaker)\"",
        "> Quero configur\u00e1-lo no meu computador",
        "\n",
        "* Qual tipo de m\u00e1quina voc\u00ea est\u00e1 usando?",
        "- [_] multi-CPU",
        "- [_] multi-XPU",
        "- [x] multi-GPU",
        "- [_] multi-NPU",
        "- [_] TPU",
        "> Como tenho 2 GPUs e quero executar c\u00f3digos distribu\u00eddos entre elas, escolho `multi-GPU`",
        " \n",
        "* Quantas m\u00e1quinas diferentes voc\u00ea usar\u00e1 (use mais de 1 para treinamento multin\u00f3)? [1]:",
        "- 1",
        "> Escolho `1` porque s\u00f3 vou executar no meu computador",
        "\n",
        "* Deve-se verificar opera\u00e7\u00f5es distribu\u00eddas em execu\u00e7\u00e3o para erros? Isso pode evitar problemas de tempo limite, mas ser\u00e1 mais lento. [sim/N\u00c3O]:",
        "- n\u00e3o",
        "> Com essa op\u00e7\u00e3o, pode-se escolher que o `accelerate` verifique erros durante a execu\u00e7\u00e3o, mas isso faria com que fosse mais lento, ent\u00e3o escolho `no`, e em caso de erros mudo para `yes`.",
        " \n",
        "* Deseja otimizar seu script com torch dynamo? [sim/N\u00c3O]:",
        "- n\u00e3o",
        "\n",
        "* Voc\u00ea quer usar FullyShardedDataParallel? [sim/N\u00c3O]:",
        "- n\u00e3o",
        " \n",
        "* Voc\u00ea quer usar o Megatron-LM? [sim/N\u00c3O]:",
        "- n\u00e3o",
        " \n",
        "* Quantas GPUs devem ser usadas para treinamento distribu\u00eddo? [1]:",
        "- 2",
        "> Escolho `2` porque tenho 2 GPUs",
        "\n",
        "* Quais GPUs (por ID) devem ser usadas para treinamento nesta m\u00e1quina como uma lista separada por v\u00edrgulas? [todas]:",
        "- 0,1",
        "> Escolho `0,1` porque quero usar as duas GPUs",
        "\n",
        "* Voc\u00ea deseja usar FP16 ou BF16 (precis\u00e3o mista)?",
        "- [x] n\u00e3o",
        "- [_] fp16",
        "- [_] bf16",
        "- [_] fp8",
        "> Por enquanto escolho `no`, pois para simplificar o c\u00f3digo quando n\u00e3o uso `acelerate` vamos treinar em fp32, mas o ideal seria usar fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A configura\u00e7\u00e3o ser\u00e1 salva em `~/.cache/huggingface/accelerate/default_config.yaml` e pode ser modificada a qualquer momento. Vamos ver o que h\u00e1 dentro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compute_environment: LOCAL_MACHINE\n",
            "debug: false\n",
            "distributed_type: MULTI_GPU\n",
            "downcast_bf16: 'no'\n",
            "gpu_ids: 0,1\n",
            "machine_rank: 0\n",
            "main_training_function: main\n",
            "mixed_precision: fp16\n",
            "num_machines: 1\n",
            "num_processes: 2\n",
            "rdzv_backend: static\n",
            "same_network: true\n",
            "tpu_env: []\n",
            "tpu_use_cluster: false\n",
            "tpu_use_sudo: false\n",
            "use_cpu: false\n"
          ]
        }
      ],
      "source": [
        "!cat ~/.cache/huggingface/accelerate/default_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outra forma de ver a configura\u00e7\u00e3o que temos \u00e9 executando em um terminal:",
        "\n",
        "``` bash\n",
        "acelerar env",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Copy-and-paste the text below in your GitHub issue\n",
            "\n",
            "- `Accelerate` version: 0.28.0\n",
            "- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\n",
            "- Python version: 3.11.8\n",
            "- Numpy version: 1.26.4\n",
            "- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
            "- PyTorch XPU available: False\n",
            "- PyTorch NPU available: False\n",
            "- System RAM: 31.24 GB\n",
            "- GPU type: NVIDIA GeForce RTX 3090\n",
            "- `Accelerate` default config:\n",
            "\t- compute_environment: LOCAL_MACHINE\n",
            "\t- distributed_type: MULTI_GPU\n",
            "\t- mixed_precision: fp16\n",
            "\t- use_cpu: False\n",
            "\t- debug: False\n",
            "\t- num_processes: 2\n",
            "\t- machine_rank: 0\n",
            "\t- num_machines: 1\n",
            "\t- gpu_ids: 0,1\n",
            "\t- rdzv_backend: static\n",
            "\t- same_network: True\n",
            "\t- main_training_function: main\n",
            "\t- downcast_bf16: no\n",
            "\t- tpu_use_cluster: False\n",
            "\t- tpu_use_sudo: False\n",
            "\t- tpu_env: []\n"
          ]
        }
      ],
      "source": [
        "!accelerate env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que configuramos o `accelerate`, podemos testar se fizemos tudo corretamente executando no terminal:",
        "\n",
        "``` bash\n",
        "acelerar teste",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running:  accelerate-launch ~/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
            "stdout: **Initialization**\n",
            "stdout: Testing, testing. 1, 2, 3.\n",
            "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
            "stdout: Num processes: 2\n",
            "stdout: Process index: 0\n",
            "stdout: Local process index: 0\n",
            "stdout: Device: cuda:0\n",
            "stdout: \n",
            "stdout: Mixed precision type: fp16\n",
            "stdout: \n",
            "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
            "stdout: Num processes: 2\n",
            "stdout: Process index: 1\n",
            "stdout: Local process index: 1\n",
            "stdout: Device: cuda:1\n",
            "stdout: \n",
            "stdout: Mixed precision type: fp16\n",
            "stdout: \n",
            "stdout: \n",
            "stdout: **Test process execution**\n",
            "stdout: \n",
            "stdout: **Test split between processes as a list**\n",
            "stdout: \n",
            "stdout: **Test split between processes as a dict**\n",
            "stdout: \n",
            "stdout: **Test split between processes as a tensor**\n",
            "stdout: \n",
            "stdout: **Test random number generator synchronization**\n",
            "stdout: All rng are properly synched.\n",
            "stdout: \n",
            "stdout: **DataLoader integration test**\n",
            "stdout: 0 1 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\n",
            "stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
            "stdout: Non-shuffled dataloader passing.\n",
            "stdout: Shuffled dataloader passing.\n",
            "stdout: Non-shuffled central dataloader passing.\n",
            "stdout: Shuffled central dataloader passing.\n",
            "stdout: \n",
            "stdout: **Training integration test**\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
            "stdout: FP16 training check.\n",
            "stdout: FP16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: BF16 training check.\n",
            "stdout: BF16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: \n",
            "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: FP16 training check.\n",
            "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
            "stdout: FP16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: Keep fp32 wrapper check.\n",
            "stdout: BF16 training check.\n",
            "stdout: BF16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: \n",
            "stdout: **Breakpoint trigger test**\n",
            "Test is a success! You are ready for your distributed training!\n"
          ]
        }
      ],
      "source": [
        "!accelerate test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que termina dizendo `Test is a success! You are ready for your distributed training!` por isso tudo est\u00e1 correto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Otimiza\u00e7\u00e3o do treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C\u00f3digo base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a fazer primeiro um c\u00f3digo de treinamento b\u00e1sico e depois otimiz\u00e1-lo para ver como \u00e9 feito e como melhora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro vamos buscar um dataset. No meu caso, vou usar o dataset [tweet_eval](https://huggingface.co/datasets/tweet_eval), que \u00e9 um dataset de classifica\u00e7\u00e3o de tweets. Especificamente, vou baixar o subset `emoji`, que classifica os tweets com emoticons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 45000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['\u2764', '\ud83d\ude0d', '\ud83d\ude02', '\ud83d\udc95', '\ud83d\udd25', '\ud83d\ude0a', '\ud83d\ude0e', '\u2728', '\ud83d\udc99', '\ud83d\ude18', '\ud83d\udcf7', '\ud83c\uddfa\ud83c\uddf8', '\u2600', '\ud83d\udc9c', '\ud83d\ude09', '\ud83d\udcaf', '\ud83d\ude01', '\ud83c\udf84', '\ud83d\udcf8', '\ud83d\ude1c'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tweet_eval', config_name='emoji', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=3808792, num_examples=45000, shard_lengths=None, dataset_name='tweet_eval'), 'test': SplitInfo(name='test', num_bytes=4262151, num_examples=50000, shard_lengths=None, dataset_name='tweet_eval'), 'validation': SplitInfo(name='validation', num_bytes=396704, num_examples=5000, shard_lengths=None, dataset_name='tweet_eval')}, download_checksums={'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/train-00000-of-00001.parquet': {'num_bytes': 2609973, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/test-00000-of-00001.parquet': {'num_bytes': 3047341, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/validation-00000-of-00001.parquet': {'num_bytes': 281994, 'checksum': None}}, download_size=5939308, post_processing_size=None, dataset_size=8467647, size_in_bytes=14406955)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver as classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\u2764', '\ud83d\ude0d', '\ud83d\ude02', '\ud83d\udc95', '\ud83d\udd25', '\ud83d\ude0a', '\ud83d\ude0e', '\u2728', '\ud83d\udc99', '\ud83d\ude18', '\ud83d\udcf7', '\ud83c\uddfa\ud83c\uddf8', '\u2600', '\ud83d\udc9c', '\ud83d\ude09', '\ud83d\udcaf', '\ud83d\ude01', '\ud83c\udf84', '\ud83d\udcf8', '\ud83d\ude1c']\n"
          ]
        }
      ],
      "source": [
        "print(dataset[\"train\"].info.features[\"label\"].names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E o n\u00famero de classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que o conjunto de dados tem 20 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver a sequ\u00eancia m\u00e1xima de cada split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(142, 139, 167)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len_train = 0\n",
        "max_len_val = 0\n",
        "max_len_test = 0\n",
        "\n",
        "split = \"train\"\n",
        "for i in range(len(dataset[split])):\n",
        "    len_i = len(dataset[split][i][\"text\"])\n",
        "    if len_i > max_len_train:\n",
        "        max_len_train = len_i\n",
        "split = \"validation\"\n",
        "for i in range(len(dataset[split])):\n",
        "    len_i = len(dataset[split][i][\"text\"])\n",
        "    if len_i > max_len_val:\n",
        "        max_len_val = len_i\n",
        "split = \"test\"\n",
        "for i in range(len(dataset[split])):\n",
        "    len_i = len(dataset[split][i][\"text\"])\n",
        "    if len_i > max_len_test:\n",
        "        max_len_test = len_i\n",
        "\n",
        "max_len_train, max_len_val, max_len_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ent\u00e3o definimos a sequ\u00eancia m\u00e1xima em geral como 130 para a tokeniza\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 130"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A n\u00f3s interessa o conjunto de dados tokenizado, n\u00e3o as sequ\u00eancias brutas, ent\u00e3o criamos um tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma fun\u00e7\u00e3o de tokeniza\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E agora tokenizamos o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a83f90dc1d074012b5d099511986898e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c14557614545118c2ceb0a0ab6178c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f1c71ced24c4b1ba6e13e6c0ba0e7f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, agora temos os tokens (`input_ids`) e as m\u00e1scaras de aten\u00e7\u00e3o (`attention_mask`), mas vamos ver que tipo de dados temos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list, list, int)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"]), type(tokenized_dataset[\"train\"][0][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Tensor, torch.Tensor, torch.Tensor)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "type(tokenized_dataset[\"train\"][0][\"label\"]), type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos um DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "BS = 64\n",
        "\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Carregamos o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como \u00e9 o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver sua \u00faltima camada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=2, bias=True)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.out_proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768, 2)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.out_proj.in_features, model.classifier.out_proj.out_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vimos que nosso conjunto de dados possui 20 classes, mas este modelo foi treinado para 2 classes, ent\u00e3o precisamos modificar a \u00faltima camada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=20, bias=True)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "model.classifier.out_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos uma fun\u00e7\u00e3o de perda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Um otimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E por \u00faltimo, uma m\u00e9trica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a verificar se tudo est\u00e1 bem com uma amostra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = next(iter(dataloader[\"train\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 130]), torch.Size([64, 130]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora essa amostra \u00e9 inserida no modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 20])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(\"cuda\")\n",
        "ouputs = model(input_ids=sample[\"input_ids\"].to(\"cuda\"), attention_mask=sample[\"attention_mask\"].to(\"cuda\"))\n",
        "ouputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que o modelo gera 64 batches, o que est\u00e1 correto, pois configuramos `BS = 20` e cada um com 20 sa\u00eddas, o que est\u00e1 correto porque alteramos o modelo para que tenha a sa\u00edda de 20 valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos a de maior valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = torch.argmax(ouputs.logits, axis=-1)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos a loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.9990389347076416"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = loss_function(ouputs.logits, sample[\"label\"].to(\"cuda\"))\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E a precis\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.015625"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = metric.compute(predictions=predictions, references=sample[\"label\"])[\"accuracy\"]\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 podemos criar um pequeno loop de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "master_progress_bar = master_bar(range(epochs))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Script com o c\u00f3digo base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na maioria da documenta\u00e7\u00e3o do `accelerate`, \u00e9 explicado como usar o `accelerate` com scripts, ent\u00e3o por enquanto vamos fazer assim e no final explicaremos como faz\u00ea-lo com um notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro vamos a criar uma pasta na qual vamos guardar os scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir accelerate_scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora escrevemos o c\u00f3digo base em um script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/01_code_base.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/01_code_base.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 64\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E agora o executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.2112                                                               \n",
            "CPU times: user 2.12 s, sys: 391 ms, total: 2.51 s\n",
            "Wall time: 3min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!python accelerate_scripts/01_code_base.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que no meu computador levou cerca de 3 minutos e meio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C\u00f3digo com accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora substitu\u00edmos algumas coisas",
        "\n",
        "* Em primeiro lugar, importamos `Accelerator` e o inicializamos",
        "\n",
        "``` python\n",
        "from accelerate import Accelerator",
        "acelerador = Acelerador()",
        "```\n",
        "\n",
        "* J\u00e1 n\u00e3o fazemos o t\u00edpico",
        "\n",
        "``` python \n",
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "```\n",
        "\n",
        "* Mas deixamos que seja `acelerate` o respons\u00e1vel por escolher o dispositivo atrav\u00e9s",
        "\n",
        "``` python\n",
        "dispositivo = acelerador.dispositivo",
        "```\n",
        "\n",
        "* Passamos os elementos relevantes para o treinamento pelo m\u00e9todo `prepare` e n\u00e3o fazemos mais `model.to(device)`",
        "\n",
        "``` python\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = preprare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])",
        "```\n",
        "\n",
        "* J\u00e1 n\u00e3o enviamos os dados e o modelo para a GPU com `.to(device)`, pois o `accelerate` cuidou disso com o m\u00e9todo `prepare`",
        "\n",
        "* Em vez de fazer o backpropagation com `loss.backward()`, deixamos que `accelerate` fa\u00e7a isso com",
        " \n",
        "``` python\n",
        "accelerator.backward(loss)",
        "```\n",
        "\n",
        "* Na hora de calcular a m\u00e9trica no loop de valida\u00e7\u00e3o, precisamos coletar os valores de todos os pontos, em caso de estar fazendo um treinamento distribu\u00eddo, para isso fazemos",
        "\n",
        "``` python\n",
        "predictions = accelerator.gather_for_metrics(predictions)",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/02_accelerate_base_code.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/02_accelerate_base_code.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 64\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "    print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "\n",
        "print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se voc\u00ea notar, adicionei estas duas linhas `print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")` e a linha `print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")`, adicionei propositalmente porque nos revelar\u00e3o algo muito importante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora o executamos, para executar os scripts de `accelerate` faz-se com o comando `accelerate launch`",
        "\n",
        "``` bash\n",
        "accelerate launch script.py",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
            "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
            "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
            "Accuracy = 0.206\n",
            "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
            "Accuracy = 0.206\n",
            "CPU times: user 1.6 s, sys: 272 ms, total: 1.88 s\n",
            "Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/02_accelerate_base_code.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que antes demorava uns 3 minutos e meio e agora demora mais ou menos 2 minutos e meio. Bastante melhoria. Al\u00e9m disso, se observarmos os `print`s, podemos ver que foram impressos duas vezes.",
        "\n",
        "E isso como pode ser? Bem, porque o `accelerate` paralelizou o treinamento nas duas GPUs que tenho, por isso foi muito mais r\u00e1pido.",
        "\n",
        "Al\u00e9m disso, quando executei o primeiro script, ou seja, quando n\u00e3o usei `accelerate`, a GPU estava quase cheia, enquanto que quando executei o segundo, ou seja, o que usa `accelerate`, as duas GPUs estavam muito pouco utilizadas, portanto podemos aumentar o batch size para tentar preencher as duas, vamos nisso!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/03_accelerate_base_code_more_bs.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/03_accelerate_base_code_more_bs.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "\n",
        "print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removi os prints extras, pois j\u00e1 vimos que o c\u00f3digo est\u00e1 sendo executado nas duas GPUs e aumentei o batch size de 64 para 128. Vamos executar para ver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.1052                                                               \n",
            "Accuracy = 0.1052\n",
            "CPU times: user 1.41 s, sys: 180 ms, total: 1.59 s\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/03_accelerate_base_code_more_bs.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aumentando o tamanho do batch reduziu alguns segundos o tempo de execu\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execu\u00e7\u00e3o de processos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Execu\u00e7\u00e3o de c\u00f3digo em um \u00fanico processo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes vimos que os `print`s eram impressos duas vezes, isso ocorre porque o `accelerate` cria tantos processos quantos dispositivos onde o c\u00f3digo \u00e9 executado, no meu caso ele cria dois processos por ter duas GPUs.",
        "\n",
        "No entanto, nem todo o c\u00f3digo deve ser executado em todos os processos, por exemplo, os `print`s, desaceleram muito o c\u00f3digo, como para execut\u00e1-lo v\u00e1rias vezes, se forem salvos os checkpoints, seriam salvos duas vezes, etc.",
        "\n",
        "Para poder executar parte de um c\u00f3digo em um \u00fanico processo, tem que ser encapsulada em uma fun\u00e7\u00e3o e decorada com `accelerator.on_local_main_process`. Por exemplo, no seguinte c\u00f3digo voc\u00ea vai ver que criei a seguinte fun\u00e7\u00e3o",
        "\n",
        "``` python\n",
        "@accelerator.on_local_main_process",
        "def print_something(something):",
        "print(algo)",
        "```\n",
        "\n",
        "Outra op\u00e7\u00e3o \u00e9 incluir o c\u00f3digo dentro de um `if accelerator.is_local_main_process` como no seguinte c\u00f3digo",
        "\n",
        "``` python\n",
        "if accelerator.is_local_main_process:",
        "print(\"Algo\")",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "master_progress_bar = master_bar(range(EPOCHS))\n",
        "for i in master_progress_bar:\n",
        "    model.train()\n",
        "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
        "\n",
        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos execut\u00e1-lo para ver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.2098                                                               \n",
            "End of script with 0.2098 accuracy\n",
            "CPU times: user 1.38 s, sys: 197 ms, total: 1.58 s\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora o print foi impresso apenas uma vez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No entanto, embora n\u00e3o seja muito vis\u00edvel, as barras de progresso s\u00e3o executadas em cada processo.",
        "\n",
        "N\u00e3o encontrei uma maneira de evitar isso com as barras de progresso do `fastprogress`, mas sim com as do `tqdm`, ent\u00e3o vou substituir as barras de progresso do `fastprogress` pelas do `tqdm` e para que se executem em um \u00fanico processo \u00e9 necess\u00e1rio adicionar o argumento `disable=not accelerator.is_local_main_process`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:01<00:00,  1.45it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.30it/s]\n",
            "Accuracy = 0.2166\n",
            "End of script with 0.2166 accuracy\n",
            "CPU times: user 1.33 s, sys: 195 ms, total: 1.52 s\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N\u00f3s mostramos um exemplo de como imprimir em um \u00fanico processo, e isso foi uma maneira de executar processos em um \u00fanico processo. Mas se o que voc\u00ea quer \u00e9 apenas imprimir em um \u00fanico processo, pode-se usar o m\u00e9todo `print` do `accelerate`. Vamos ver o mesmo exemplo de antes com esse m\u00e9todo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/06_accelerate_base_code_print_one_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/06_accelerate_base_code_print_one_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15433.52 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 11406.61 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15036.87 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14932.76 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14956.60 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:00<00:00,  1.46it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.33it/s]\n",
            "Accuracy = 0.2134\n",
            "End of script with 0.2134 accuracy\n",
            "CPU times: user 1.4 s, sys: 189 ms, total: 1.59 s\n",
            "Wall time: 2min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/06_accelerate_base_code_print_one_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Execu\u00e7\u00e3o de c\u00f3digo em todos os processos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No entanto, h\u00e1 c\u00f3digo que deve ser executado em todos os processos, por exemplo, se subirmos os checkpoints para o hub, ent\u00e3o aqui temos duas op\u00e7\u00f5es: encapsular o c\u00f3digo em uma fun\u00e7\u00e3o e decor\u00e1-la com `accelerator.on_main_process`",
        "\n",
        "``` python\n",
        "@accelerator.on_main_process",
        "def fazer_minha_cozinha():",
        "\"Algo feito uma vez por servidor\"",
        "do_thing_once()",
        "```\n",
        "\n",
        "ou colocar o c\u00f3digo dentro de um `if accelerator.is_main_process`",
        "\n",
        "``` python\n",
        "if accelerator.is_main_process:",
        "repo.push_to_hub()",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como estamos fazendo treinamentos apenas para mostrar a biblioteca `accelerate` e o modelo que estamos treinando n\u00e3o \u00e9 bom, n\u00e3o faz sentido agora fazer upload dos checkpoints para o hub, ent\u00e3o vou fazer um exemplo com `print`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/06_accelerate_base_code_some_code_in_all_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_in_one_process(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_main_process\n",
        "def print_in_all_processes(something):\n",
        "    print(something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos execut\u00e1-lo para ver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:03<00:00, 14518.44 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:03<00:00, 14368.77 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 16466.33 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14806.14 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14253.33 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14337.07 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:00<00:00,  1.46it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.34it/s]\n",
            "Accuracy = 0.2092\n",
            "End of script with 0.2092 accuracy\n",
            "All process: Accuracy = 0.2092\n",
            "All process: End of script with 0.2092 accuracy\n",
            "CPU times: user 1.42 s, sys: 216 ms, total: 1.64 s\n",
            "Wall time: 2min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Execu\u00e7\u00e3o de c\u00f3digo no processo X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, podemos especificar em qual processo queremos executar o c\u00f3digo. Para isso, \u00e9 necess\u00e1rio criar uma fun\u00e7\u00e3o e decor\u00e1-la com `@accelerator.on_process(process_index=0)`",
        "\n",
        "``` python\n",
        "@accelerator.on_process(process_index=0)",
        "def fazer_minha_coisa():",
        "\"Algo feito no \u00edndice do processo 0\"",
        "do_thing_on_index_zero()",
        "```\n",
        "\n",
        "ou decor\u00e1-la com `@accelerator.on_local_process(local_process_idx=0)`",
        "\n",
        "``` python\n",
        "@accelerator.on_local_process(local_process_index=0)",
        "def fazer_minha_coisa():",
        "\"Algo feito no \u00edndice do processo 0 em cada servidor\"",
        "do_thing_on_index_zero_on_each_server()",
        "```\n",
        "\n",
        "Aqui coloquei o processo 0, mas pode ser qualquer n\u00famero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/07_accelerate_base_code_some_code_in_some_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_in_one_process(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_main_process\n",
        "def print_in_all_processes(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_process(process_index=0)\n",
        "def print_in_process_0(something):\n",
        "    print(\"Process 0: \" + something)\n",
        "\n",
        "@accelerator.on_local_process(local_process_index=1)\n",
        "def print_in_process_1(something):\n",
        "    print(\"Process 1: \" + something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_process_0(\"End of process 0\")\n",
        "print_in_process_1(\"End of process 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 15735.58 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14906.20 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:02<00:00,  1.44it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.27it/s]\n",
            "Process 1: End of process 1\n",
            "Accuracy = 0.2128\n",
            "End of script with 0.2128 accuracy\n",
            "All process: Accuracy = 0.2128\n",
            "All process: End of script with 0.2128 accuracy\n",
            "Process 0: End of process 0\n",
            "CPU times: user 1.42 s, sys: 295 ms, total: 1.71 s\n",
            "Wall time: 2min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sincronizar processos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se temos c\u00f3digo que deve ser executado em todos os processos, \u00e9 interessante esperar que termine em todos os processos antes de fazer outra tarefa, ent\u00e3o para isso usamos `accelerator.wait_for_everyone()`",
        "\n",
        "Para v\u00ea-lo, vamos introduzir um atraso em uma das fun\u00e7\u00f5es de impress\u00e3o em um processo.",
        "\n",
        "Al\u00e9m disso, coloquei um break no loop de treinamento para que n\u00e3o fique treinando por muito tempo, o que n\u00e3o \u00e9 o que nos interessa agora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/08_accelerate_base_code_sync_all_process.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/09_accelerate_base_code_sync_all_process.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_in_one_process(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_main_process\n",
        "def print_in_all_processes(something):\n",
        "    print(something)\n",
        "\n",
        "@accelerator.on_process(process_index=0)\n",
        "def print_in_process_0(something):\n",
        "    time.sleep(2)\n",
        "    print(\"Process 0: \" + something)\n",
        "\n",
        "@accelerator.on_local_process(local_process_index=1)\n",
        "def print_in_process_1(something):\n",
        "    print(\"Process 1: \" + something)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        break\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
        "\n",
        "print_in_one_process(\"Printing with delay in process 0\")\n",
        "print_in_process_0(\"End of process 0\")\n",
        "print_in_process_1(\"End of process 1\")\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "print_in_one_process(\"End of script\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14218.23 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14666.25 examples/s]\n",
            "  0%|                                                   | 0/176 [00:00<?, ?it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.58it/s]\n",
            "Process 1: End of process 1\n",
            "Accuracy = 0.212\n",
            "End of script with 0.212 accuracy\n",
            "All process: Accuracy = 0.212\n",
            "All process: End of script with 0.212 accuracy\n",
            "Printing with delay in process 0\n",
            "Process 0: End of process 0\n",
            "End of script\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/09_accelerate_base_code_sync_all_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como pode ser visto primeiro foi impresso `Process 1: End of process 1` e depois o resto, isso \u00e9 porque o resto dos prints s\u00e3o feitos ou no processo 0 ou em todos os processos, ent\u00e3o at\u00e9 que n\u00e3o termine o atraso de 2 segundos que colocamos, o resto do c\u00f3digo n\u00e3o \u00e9 executado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Salvar e carregar o state dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando treinamos, \u00e0s vezes salvamos o estado para poder continuar em outro momento",
        "\n",
        "Para salvar o estado teremos que usar os m\u00e9todos `save_state()` e `load_state()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/09_accelerate_save_and_load_checkpoints.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/10_accelerate_save_and_load_checkpoints.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "\n",
        "    # Guardamos los pesos\n",
        "    accelerator.save_state(\"accelerate_scripts/checkpoints\")\n",
        "\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
        "\n",
        "# Cargamos los pesos\n",
        "accelerator.load_state(\"accelerate_scripts/checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:58<00:00,  1.48it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.40it/s]\n",
            "Accuracy = 0.2142\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/10_accelerate_save_and_load_checkpoints.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Salvar o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando o m\u00e9todo `prepare` foi usado, o modelo foi embrulhado para poder ser salvo nos dispositivos necess\u00e1rios. Portanto, na hora de salv\u00e1-lo, temos que usar o m\u00e9todo `save_model`, que primeiro o desembrulha e depois o salva. Al\u00e9m disso, se usarmos o par\u00e2metro `safe_serialization=True`, o modelo ser\u00e1 salvo como um `safe tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/11_accelerate_save_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/11_accelerate_save_model.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "\n",
        "    # Guardamos el modelo\n",
        "    accelerator.wait_for_everyone()\n",
        "    accelerator.save_model(model, \"accelerate_scripts/model\", safe_serialization=True)\n",
        "\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:58<00:00,  1.48it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.35it/s]\n",
            "Accuracy = 0.214\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/11_accelerate_save_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Salvar o modelo `pretrained`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em modelos que usam a biblioteca `transformers` devemos salvar o modelo com o m\u00e9todo `save_pretrained` para poder carreg\u00e1-lo com o m\u00e9todo `from_pretrained`. Antes de salv\u00e1-lo, \u00e9 necess\u00e1rio desembrulh\u00e1-lo com o m\u00e9todo `unwrap_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/11_accelerate_save_pretrained.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/12_accelerate_save_pretrained.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@accelerator.on_local_main_process\n",
        "def print_something(something):\n",
        "    print(something)\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "\n",
        "    # Guardamos el modelo pretrained\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(\n",
        "        \"accelerate_scripts/model_pretrained\",\n",
        "        is_main_process=accelerator.is_main_process,\n",
        "        save_function=accelerator.save,\n",
        "    )\n",
        "\n",
        "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15152.47 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:02<00:00, 15119.13 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 12724.70 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 12397.49 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 15247.21 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 15138.03 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:59<00:00,  1.48it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.37it/s]\n",
            "Accuracy = 0.21\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch accelerate_scripts/12_accelerate_save_pretrained.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora poder\u00edamos carreg\u00e1-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at accelerate_scripts/model_pretrained and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoints = \"accelerate_scripts/model_pretrained\"\n",
        "tokenizer = AutoModel.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento em notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At\u00e9 agora vimos como executar scripts, mas se voc\u00ea quiser executar o c\u00f3digo em um notebook, podemos escrever o mesmo c\u00f3digo anterior, mas encapsulado em uma fun\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro importamos as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "import time\n",
        "# from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos a fun\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_code(batch_size: int = 64):\n",
        "    from accelerate import Accelerator\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "    num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "    max_len = 130\n",
        "\n",
        "    checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "    def tokenize_function(dataset):\n",
        "        return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    tokenized_dataset = {\n",
        "        \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "        \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "        \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    }\n",
        "    tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "    tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "    tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "    BS = batch_size\n",
        "    dataloader = {\n",
        "        \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "        \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "        \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "    }\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "    model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    EPOCHS = 1\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = accelerator.device\n",
        "\n",
        "    # model.to(device)\n",
        "    model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "    for i in range(EPOCHS):\n",
        "        model.train()\n",
        "        progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "        for batch in progress_bar_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"]#.to(device)\n",
        "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "            labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "            # loss.backward()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "        for batch in progress_bar_validation:\n",
        "            input_ids = batch[\"input_ids\"]#.to(device)\n",
        "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "            labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "            # Recopilamos las predicciones de todos los dispositivos\n",
        "            predictions = accelerator.gather_for_metrics(predictions)\n",
        "            labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "            accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "        accuracy = metric.compute()\n",
        "        \n",
        "    accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder executar o treinamento no notebook usamos a fun\u00e7\u00e3o `notebook_launcher`, \u00e0 qual passamos a fun\u00e7\u00e3o que queremos executar, os argumentos dessa fun\u00e7\u00e3o e o n\u00famero de GPUs nas quais vamos treinar com a vari\u00e1vel `num_processes`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on 2 GPUs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [02:01<00:00,  1.45it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 0.2112\n"
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (128,)\n",
        "notebook_launcher(train_code, args, num_processes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento em FP16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando configuramos o `accelerate` no in\u00edcio, ele nos perguntou `Deseja usar FP16 ou BF16 (precis\u00e3o mista)?` e dissemos que n\u00e3o, ent\u00e3o agora vamos dizer que sim, que queremos em FP16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At\u00e9 agora treinamos em FP32, o que significa que cada peso do modelo \u00e9 um n\u00famero de ponto flutuante de 32 bits, e agora vamos usar um n\u00famero de ponto flutuante de 16 bits, ou seja, o modelo vai ocupar menos. Portanto, duas coisas v\u00e3o acontecer, poderemos usar um batch size maior e al\u00e9m disso ser\u00e1 mais r\u00e1pido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro, voltamos a executar `accelerate config` e vamos dizer que queremos FP16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "fp16\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos um script para treinar, com o mesmo tamanho de lote de antes, para ver se leva menos tempo para treinar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/12_accelerate_base_code_fp16_bs128.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/13_accelerate_base_code_fp16_bs128.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 128\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Executamo-lo para ver quanto demora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14983.76 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14315.47 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 176/176 [01:01<00:00,  2.88it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02<00:00,  6.84it/s]\n",
            "Accuracy = 0.2094\n",
            "CPU times: user 812 ms, sys: 163 ms, total: 976 ms\n",
            "Wall time: 1min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/13_accelerate_base_code_fp16_bs128.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando executamos este treinamento em FP32 levou cerca de 2 minutos e meio, e agora mais ou menos 1 minuto e meio. Vamos ver se agora, em vez de treinar com um batch size de 128, fazemos isso com um de 256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/14_accelerate_base_code_fp16_bs256.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 256\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 15390.30 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 14990.92 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 88/88 [00:54<00:00,  1.62it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  3.45it/s]\n",
            "Accuracy = 0.2236\n",
            "CPU times: user 670 ms, sys: 91.6 ms, total: 761 ms\n",
            "Wall time: 1min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baixou apenas uns 15 segundos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento em BF16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes treinamos em FP16 e agora vamos fazer isso em BF16, qual \u00e9 a diferen\u00e7a?",
        "\n",
        "![FP32_FP16_BF16](https://images.maximofn.com/FP32_FP16_BF16.webp)",
        "\n",
        "Como podemos ver na imagem, enquanto o FP16 em compara\u00e7\u00e3o com o FP32 tem menos bits na mantissa e no expoente, o que faz com que seu intervalo seja muito menor, o BF16 em compara\u00e7\u00e3o com o FP32 tem o mesmo n\u00famero de bits do expoente mas menos na mantissa, o que faz com que o BF16 tenha o mesmo intervalo de n\u00fameros que o FP32, mas \u00e9 menos preciso.",
        "\n",
        "Isso \u00e9 ben\u00e9fico porque em FP16 alguns c\u00e1lculos podem resultar em n\u00fameros muito altos, que n\u00e3o poderiam ser representados no formato FP16. Al\u00e9m disso, existem certos dispositivos HW que est\u00e3o otimizados para esse formato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assim como antes, executamos `accelerate config` e indicamos que queremos BF16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "bf16\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora executamos o \u00faltimo script que hav\u00edamos criado, ou seja, com um batch size de 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14814.95 examples/s]\n",
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:03<00:00, 14506.83 examples/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 88/88 [00:51<00:00,  1.70it/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:03<00:00,  3.21it/s]\n",
            "Accuracy = 0.2112\n",
            "CPU times: user 688 ms, sys: 144 ms, total: 832 ms\n",
            "Wall time: 1min 17s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Levou um tempo semelhante ao que levou anteriormente, o que \u00e9 normal, j\u00e1 que treinamos um modelo com pesos de 16 bits, assim como antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento em FP8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos treinar no formato FP8, que como o nome sugere, \u00e9 um formato de ponto flutuante onde cada peso tem 8 bits, ent\u00e3o executamos `accelerate config` para informar que queremos FP8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "In which compute environment are you running?\n",
            "This machine\n",
            "--------------------------------------------------------------------------------\n",
            "multi-GPU\n",
            "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
            "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
            "Do you want to use Megatron-LM ? [yes/NO]: no\n",
            "How many GPU(s) should be used for distributed training? [1]:2\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
            "--------------------------------------------------------------------------------\n",
            "Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "fp8\n",
            "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora executamos o \u00faltimo script, o de batch size de 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
            "    accelerator = Accelerator()\n",
            "                  ^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
            "    accelerator = Accelerator()\n",
            "                  ^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
            "[2024-05-13 21:40:56,455] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 501480) of binary: /home/wallabot/miniconda3/envs/nlp/bin/python\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
            "    args.func(args)\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
            "    elastic_launch(\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "accelerate_scripts/13_accelerate_base_code_fp16_bs256.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2024-05-13_21:40:56\n",
            "  host      : wallabot\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 501481)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-05-13_21:40:56\n",
            "  host      : wallabot\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 501480)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "CPU times: user 65.1 ms, sys: 14.5 ms, total: 79.6 ms\n",
            "Wall time: 7.24 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como os pesos agora s\u00e3o de 8 bits e ocupam metade da mem\u00f3ria, vamos aumentar o batch size para 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import tqdm\n",
        "\n",
        "# Importamos e inicializamos Accelerator\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
        "max_len = 130\n",
        "\n",
        "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokenized_dataset = {\n",
        "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
        "}\n",
        "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
        "\n",
        "BS = 512\n",
        "dataloader = {\n",
        "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
        "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
        "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
        "}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
        "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "EPOCHS = 1\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = accelerator.device\n",
        "\n",
        "# model.to(device)\n",
        "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_function(outputs['logits'], labels)\n",
        "\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
        "    for batch in progress_bar_validation:\n",
        "        input_ids = batch[\"input_ids\"]#.to(device)\n",
        "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
        "        labels = batch[\"label\"]#.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
        "        # Recopilamos las predicciones de todos los dispositivos\n",
        "        predictions = accelerator.gather_for_metrics(predictions)\n",
        "        labels = accelerator.gather_for_metrics(labels)\n",
        "\n",
        "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
        "    accuracy = metric.compute()\n",
        "    \n",
        "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O executamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "!accelerate launch accelerate_scripts/15_accelerate_base_code_fp8_bs512.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Infer\u00eancia de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uso do ecossistema da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como fazer infer\u00eancia de grandes modelos com a biblioteca `transformers` do hugging face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Infer\u00eancia com `pipeline`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se usarmos o ecossistema da Hugging Face \u00e9 muito simples, pois tudo acontece em segundo plano sem precisarmos fazer muito. No caso de usar `pipeline`, que \u00e9 a maneira mais f\u00e1cil de fazer infer\u00eancia com a biblioteca `transformers`, basta dizermos qual modelo queremos usar e, muito importante, passar `device_map=\"auto\"`. Isso far\u00e1 com que, em segundo plano, o `accelerate` distribua o modelo entre as diferentes GPUs, RAM da CPU ou disco r\u00edgido, se necess\u00e1rio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H\u00e1 mais valores poss\u00edveis para `device_map`, que veremos mais adiante, mas por enquanto fique com `\"auto\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos usar o modelo `Llama3 8B`, que como seu nome indica \u00e9 um modelo de cerca de 8 bilh\u00f5es de par\u00e2metros. Como cada par\u00e2metro por padr\u00e3o est\u00e1 no formato FP32, que corresponde a 4 bytes (32 bits), isso significa que se multiplicarmos 8 bilh\u00f5es de par\u00e2metros por 4 bytes, teremos que uma GPU com cerca de 32 GB de VRAM seria necess\u00e1ria.",
        "\n",
        "No meu caso, tenho 2 GPUs com 24 GB de VRAM cada uma, ent\u00e3o n\u00e3o caberia em uma \u00fanica GPU. Mas gra\u00e7as a colocar `device_map=\"auto\"`, o accelerate distribuir\u00e1 o modelo entre as duas GPUs e poderei realizar a infer\u00eancia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/09_inference_with_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/16_inference_with_pipeline.py\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
        "\n",
        "prompt = \"Conoces accelerate de hugging face?\"\n",
        "output = generator(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora o executamos, s\u00f3 que como pipeline ele usa internamente o `accelerate`, n\u00e3o precisamos execut\u00e1-lo com `accelerate launch script.py`, basta usar `python script.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.27s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "[{'generated_text': 'Conoces accelerate de hugging face? \u00bfQu\u00e9 es el modelo de lenguaje de transformers y c\u00f3mo se utiliza en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar modelos de lenguaje de transformers en mi aplicaci\u00f3n? \u00bfQu\u00e9 son los tokenizers y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo crear un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los datasets y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar datasets para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los checkpoints y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los evaluadores y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los pre-trainados y c\u00f3mo se utilizan en el marco de hugging face? \u00bfC\u00f3mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? \u00bfQu\u00e9 son los finetuning'}]\n"
          ]
        }
      ],
      "source": [
        "!python accelerate_scripts/16_inference_with_pipeline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como pode ver, n\u00e3o respondeu, mas continuou fazendo perguntas. Isso ocorre porque o Llama3 \u00e9 um modelo de linguagem que prev\u00ea o pr\u00f3ximo token, ent\u00e3o com o prompt que passei, ele considerou que os pr\u00f3ximos melhores tokens eram aqueles correspondentes a mais perguntas. O que faz sentido, pois \u00e0s vezes as pessoas t\u00eam d\u00favidas sobre um t\u00f3pico e geram muitas perguntas, ent\u00e3o para que nos responda \u00e0 pergunta, \u00e9 necess\u00e1rio condicion\u00e1-lo um pouco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/10_inference_with_pipeline_condition.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/17_inference_with_pipeline_condition.py\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
        "\n",
        "prompt = \"Conoces accelerate de hugging face?\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un chatbot amigable que siempre intenta solucionar las dudas\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
        "]\n",
        "output = generator(messages)\n",
        "print(output[0]['generated_text'][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como voc\u00ea pode ver, foi gerado uma mensagem com pap\u00e9is, condicionando o modelo e com o prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.41s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "{'role': 'assistant', 'content': '\u00a1Hola!\\n\\nS\u00ed, conozco Accelerate de Hugging Face. Accelerate es una biblioteca de Python desarrollada por Hugging Face que se enfoca en simplificar y acelerar el entrenamiento y la evaluaci\u00f3n de modelos de lenguaje en diferentes dispositivos y entornos.\\n\\nCon Accelerate, puedes entrenar modelos de lenguaje en diferentes plataformas y dispositivos, como GPUs, TPUs, CPUs y servidores, sin necesidad de cambiar el c\u00f3digo de tu modelo. Esto te permite aprovechar al m\u00e1ximo la potencia de c\u00e1lculo de tus dispositivos y reducir el tiempo de entrenamiento.\\n\\nAccelerate tambi\u00e9n ofrece varias caracter\u00edsticas adicionales, como:\\n\\n* Soporte para diferentes frameworks de machine learning, como TensorFlow, PyTorch y JAX.\\n* Integraci\u00f3n con diferentes sistemas de almacenamiento y procesamiento de datos, como Amazon S3 y Google Cloud Storage.\\n* Soporte para diferentes protocolos de comunicaci\u00f3n, como HTTP y gRPC.\\n* Herramientas para monitorear y depurar tus modelos en tiempo real.\\n\\nEn resumen, Accelerate es una herramienta muy \u00fatil para desarrolladores de modelos de lenguaje que buscan simplificar y acelerar el proceso de entrenamiento y evaluaci\u00f3n de sus modelos.\\n\\n\u00bfTienes alguna pregunta espec\u00edfica sobre Accelerate o necesitas ayuda para implementarlo en tu proyecto?'}\n"
          ]
        }
      ],
      "source": [
        "!python accelerate_scripts/17_inference_with_pipeline_condition.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora a resposta sim responde ao nosso prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Infer\u00eancia com `AutoClass`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, vamos ver como fazer a infer\u00eancia apenas com `AutoClass`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting accelerate_scripts/11_inference_with_autoclass.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile accelerate_scripts/18_inference_with_autoclass.py\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints, device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoints, device_map=\"auto\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "prompt = \"Conoces accelerate de hugging face?\"\n",
        "tokens_input = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "_ = model.generate(**tokens_input, streamer=streamer, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como pode ser visto, foi criado o objeto `streamer` que depois \u00e9 passado ao m\u00e9todo `generate` do modelo. Isso \u00e9 \u00fatil para que cada palavra seja impressa \u00e0 medida que \u00e9 gerada, e n\u00e3o seja necess\u00e1rio esperar at\u00e9 que toda a sa\u00edda seja gerada para imprimi-la."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.28s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "<|begin_of_text|>Conoces accelerate de hugging face? Si es as\u00ed, puedes utilizar la biblioteca `transformers` de Hugging Face para crear un modelo de lenguaje que pueda predecir la siguiente palabra en una secuencia de texto.\n",
            "\n",
            "Aqu\u00ed te muestro un ejemplo de c\u00f3mo hacerlo:\n",
            "```\n",
            "import pandas as pd\n",
            "import torch\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "\n",
            "# Cargar el modelo y el tokenizador\n",
            "model_name = \"bert-base-uncased\"\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "\n",
            "# Cargar el conjunto de datos\n",
            "train_df = pd.read_csv(\"train.csv\")\n",
            "test_df = pd.read_csv(\"test.csv\")\n",
            "\n",
            "# Preprocesar los datos\n",
            "train_texts = train_df[\"text\"]\n",
            "train_labels = train_df[\"label\"]\n",
            "test_texts = test_df[\"text\"]\n",
            "\n",
            "# Convertir los textos en entradas para el modelo\n",
            "train_encodings = tokenizer.batch_encode_plus(train_texts, \n",
            "                                              add_special_tokens=True, \n",
            "                                              max_length=512, \n",
            "                                              return_attention_mask=True, \n",
            "                                              return_tensors='pt')\n",
            "\n",
            "test_encodings = tokenizer.batch_encode_plus(test_texts, \n",
            "                                             add_special_tokens=True, \n",
            "                                             max_length=512, \n",
            "                                             return_attention_mask=True, \n",
            "                                             return_tensors='pt')\n",
            "\n",
            "# Crear un dataloader para entrenar el modelo\n",
            "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], \n",
            "                                               train_encodings[\"attention_mask\"], \n",
            "                                               torch.tensor(train_labels))\n",
            "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
            "\n",
            "# Entrenar el modelo\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "model.to(device)\n",
            "criterion = torch.nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
            "\n",
            "for epoch in range(5):\n",
            "    model.train()\n",
            "    total_loss = 0\n",
            "    for batch in train_loader:\n",
            "        input_ids = batch[0].to(device)\n",
            "        attention_mask = batch[1].to(device)\n",
            "        labels = batch[2].to(device)\n",
            "        optimizer.zero_grad()\n",
            "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
            "        loss = criterion(outputs, labels)\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        total_loss += loss.item()\n",
            "    print(f\"Epoch {epoch+1}, Loss: {total\n"
          ]
        }
      ],
      "source": [
        "!python accelerate_scripts/18_inference_with_autoclass.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uso PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalmente, a maneira de fazer infer\u00eancias com o PyTorch \u00e9 criar um modelo com os pesos inicializados aleatoriamente e, em seguida, carregar um `state dict` com os pesos do modelo pr\u00e9-treinado. Ent\u00e3o, para obter esse `state dict`, vamos primeiro fazer uma pequena trapa\u00e7a e baix\u00e1-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/maximo.fernandez/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 230M/230M [02:48<00:00, 1.43MB/s] \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
        "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos o `state dict`, vamos fazer infer\u00eancia como \u00e9 feito normalmente no PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     # Set device\n",
        "\n",
        "resnet152 = models.resnet152().to(device) # Create model with random weights and move to device\n",
        "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device) # Load pretrained weights into device memory\n",
        "resnet152.load_state_dict(state_dict) # Load this weights into the model\n",
        "\n",
        "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
        "output = resnet152(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a explicar o que aconteceu",
        "\n",
        "* Quando fizemos `resnet152 = models.resnet152().to(device)` carregamos uma ResNet152 com pesos aleat\u00f3rios na mem\u00f3ria da GPU",
        "* Quando fizemos `state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)` foi carregado um dicion\u00e1rio com os pesos treinados na mem\u00f3ria da GPU",
        "* Quando fizemos `resnet152.load_state_dict(state_dict)`, esses pesos pr\u00e9-treinados foram atribu\u00eddos ao modelo.",
        "\n",
        "Isto significa que o modelo foi carregado duas vezes na mem\u00f3ria da GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voc\u00ea pode estar se perguntando por que fizemos primeiro",
        "\n",
        "``` python\n",
        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)",
        "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')",
        "```\n",
        "\n",
        "Para fazer depois",
        "\n",
        "``` python\n",
        "resnet152 = models.resnet152().to(device)",
        "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)",
        "resnet152.load_state_dict(state_dict)",
        "```\n",
        "\n",
        "E por que n\u00e3o usamos diretamente",
        "\n",
        "```\n",
        "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)",
        "```\n",
        "\n",
        "E deixamos de salvar o `state dict` para carreg\u00e1-lo depois. Bem, \u00e9 porque o Pytorch, internamente, faz a mesma coisa que fizemos. Ent\u00e3o, para poder ver todo o processo, fizemos em v\u00e1rias linhas o que o Pytorch faz em uma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta maneira de trabalhar funcionou bem at\u00e9 agora, enquanto os modelos tinham um tamanho gerenci\u00e1vel pelas GPUs dos usu\u00e1rios. Mas desde a chegada dos LLMs, esta abordagem n\u00e3o faz mais sentido.",
        "\n",
        "Por exemplo, um modelo de 6B de par\u00e2metros ocuparia na mem\u00f3ria 24 GB, e como \u00e9 carregado duas vezes com esse m\u00e9todo de trabalho, seria necess\u00e1rio ter uma GPU de 48 GB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ent\u00e3o, para corrigir isso, a maneira de carregar um modelo pr\u00e9-treinado do PyTorch \u00e9:",
        "* Criar um modelo vazio com `init_empty_weights` que n\u00e3o ocupar\u00e1 mem\u00f3ria RAM",
        "* Em seguida, carregue os pesos com `load_checkpoint_and_dispatch`, que carregar\u00e1 um ponto de checkpoint dentro do modelo vazio e distribuir\u00e1 os pesos para cada camada em todos os dispositivos dispon\u00edveis (GPU, CPU, RAM e disco r\u00edgido), gra\u00e7as a colocar `device_map=\"auto\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "with init_empty_weights():\n",
        "    resnet152 = models.resnet152()\n",
        "\n",
        "resnet152 = load_checkpoint_and_dispatch(resnet152, checkpoint='accelerate_scripts/resnet152_pretrained.pth', device_map=\"auto\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
        "output = resnet152(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Como funciona o accelerate por baixo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste v\u00eddeo \u00e9 poss\u00edvel ver graficamente como o accelerate funciona por baixo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/MWCSGj9jEAo\" title=\"Acelerar a Infer\u00eancia de Modelos Grandes: Como Funciona?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inicializa\u00e7\u00e3o de um modelo vazio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Accelerate` cria o esqueleto de um modelo vazio usando `init_empty_weights` para ocupar a menor quantidade de mem\u00f3ria poss\u00edvel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por exemplo, vejamos quanto de RAM tenho dispon\u00edvel no meu computador agora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 22.62 GB, Used RAM: 7.82 GB\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "def get_ram_info():\n",
        "    ram = dict(psutil.virtual_memory()._asdict())\n",
        "    print(f\"Total RAM: {(ram['total']/1024/1024/1024):.2f} GB, Available RAM: {(ram['available']/1024/1024/1024):.2f} GB, Used RAM: {(ram['used']/1024/1024/1024):.2f} GB\")\n",
        "\n",
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenho cerca de 22 GB de RAM dispon\u00edveis.",
        "\n",
        "Agora vamos tentar criar um modelo 5000x1000x1000 par\u00e2metros, ou seja, de 5B de par\u00e2metros, se cada par\u00e2metro estiver em FP32, isso implica 20 GB de RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se voltarmos a ver a RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 3.77 GB, Used RAM: 26.70 GB\n"
          ]
        }
      ],
      "source": [
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, agora temos apenas 3 GB de RAM dispon\u00edveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos a eliminar o modelo para liberar RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 22.44 GB, Used RAM: 8.03 GB\n"
          ]
        }
      ],
      "source": [
        "del model\n",
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos novamente cerca de 22 GB de RAM dispon\u00edveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos agora usar `init_empty_weights` de `accelerate` e depois verificamos a RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 31.24 GB, Available RAM: 22.32 GB, Used RAM: 8.16 GB\n"
          ]
        }
      ],
      "source": [
        "from accelerate import init_empty_weights\n",
        "\n",
        "with init_empty_weights():\n",
        "    model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])\n",
        "\n",
        "get_ram_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes t\u00ednhamos exatamente 22,44 GB livres e ap\u00f3s criar o modelo com `init_empty_weights` temos 22,32 GB. A economia de RAM \u00e9 enorme! Quase n\u00e3o foi utilizada RAM para criar o modelo.",
        "\n",
        "Isso se baseia no metadispositivo introduzido no PyTorch 1.9, portanto \u00e9 importante ter uma vers\u00e3o do Pytorch posterior para usar o `accelerate`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Carregamento dos pesos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez inicializado o modelo, temos que carregar os pesos, o que fazemos atrav\u00e9s de `load_checkpoint_and_dispatch`, que, como o nome sugere, carrega os pesos e os envia para o dispositivo ou dispositivos necess\u00e1rios."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-05-16",
      "description_en": "Speed up your training with HuggingFace Accelerate",
      "description_es": "Acelera tus entrenos con HuggingFace Accelerate",
      "description_pt": "Acelere seus treinamentos com HuggingFace Accelerate",
      "end_url": "hugging-face-accelerate",
      "image": "https://images.maximofn.com/huggingface_accelerate.webp",
      "image_hover_path": "https://images.maximofn.com/huggingface_accelerate.webp",
      "keywords_en": "hugging face, accelerate, pytorch, deep learning, machine learning, transformers",
      "keywords_es": "hugging face, accelerate, pytorch, deep learning, machine learning, transformers",
      "keywords_pt": "hugging face, accelerate, pytorch, deep learning, machine learning, transformers",
      "title_en": "HuggingFace Accelerate",
      "title_es": "HuggingFace Accelerate",
      "title_pt": "HuggingFace Accelerate"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}