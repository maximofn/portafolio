{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# Otimização do rosto de abraços"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Optimum é uma extensão da biblioteca [Transformers] (https://maximofn.com/hugging-face-transformers/) que fornece um conjunto de ferramentas de otimização de desempenho para modelos de treinamento e inferência, em hardware específico, com eficiência máxima.\n",
                        "\n",
                        "O ecossistema de IA está evoluindo rapidamente e cada vez mais hardware especializado está surgindo todos os dias, juntamente com suas próprias otimizações. Portanto, o `Optimum` permite que os usuários utilizem eficientemente qualquer um desses HW com a mesma facilidade dos [Transformers] (https://maximofn.com/hugging-face-transformers/)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..\n",
                        "\n",
                        "A otimização para as seguintes plataformas HW é possível com o `Optimun`:\n",
                        "\n",
                        " * Nvidia\n",
                        " * AMD\n",
                        " * Intel\n",
                        " * AWS\n",
                        " * TPU\n",
                        " * Havana\n",
                        " * FuriosaAI\n",
                        "\n",
                        "Além disso, ele oferece aceleração para as seguintes integrações de código aberto\n",
                        "\n",
                        " * Tempo de execução do ONNX\n",
                        " * Exportadores: exportam dados do Pytorch ou do TensorFlow para diferentes formatos, como ONNX ou TFLite.\n",
                        " * Melhor transformador\n",
                        " * Tocha FX"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Instalação"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para instalar o `Optimum`, basta executar:\n",
                        "\n",
                        "``` bash\n",
                        "pip install optimum\n",
                        "```\n",
                        "\n",
                        "Mas se quiser instalá-lo com suporte para todas as plataformas HW, você pode fazer o seguinte\n",
                        "\n",
                        "|Accelerator |Instalação|\n",
                        "|---|---|\n",
                        "|ONNX Runtime |`pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]`|\n",
                        "|Intel Neural Compressor |`pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]`|\n",
                        "|OpenVINO |`pip install --upgrade --upgrade-strategy eager optimum[openvino]`|\n",
                        "|NVIDIA TensorRT-LLM |`docker run -it --gpus all --ipc host huggingface/optimum-nvidia`|\n",
                        "|AMD Instinct GPUs e Ryzen AI NPU |`pip install --upgrade --upgrade-strategy eager optimum[amd]`|\n",
                        "|AWS Trainum & Inferentia |`pip install --upgrade --upgrade-strategy eager optimum[neuronx]`|\n",
                        "|Havana Gaudi Processor (HPU) |`pip install --upgrade --upgrade-strategy eager optimum [habana]`|\n",
                        "|FuriosaAI |`pip install --upgrade --upgrade-strategy eager optimum[furiosa]`|\n",
                        "\n",
                        "Os sinalizadores `--upgrade --upgrade-strategy eager` são necessários para garantir que os diferentes pacotes sejam atualizados para a versão mais recente possível."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como a maioria das pessoas usa o Pytorch em GPUs Nvidia e, especialmente, como eu tenho uma Nvidia, esta postagem falará apenas sobre o uso do `Optimun` com GPUs Nvidia e Pytorch."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## BeterTransformer"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "O BetterTransformer é uma otimização nativa do PyTorch para aumentar a velocidade de 1,25 a 4 vezes na inferência de modelo baseada no Transformer."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "BetterTransformer é uma API que permite que você aproveite os recursos modernos de hardware para acelerar o treinamento e a inferência de modelos de transformadores no PyTorch, usando implementações mais eficientes e com atenção ao \"caminho rápido\" da versão nativa `nn.TransformerEncoderLayer` do `nn.TransformerEncoderLayer`.\n",
                        "\n",
                        "O BetterTransformer usa dois tipos de acelerações:\n",
                        "\n",
                        " 1. `Flash Attention`: é uma implementação do `attention` que usa o `sparse` para reduzir a complexidade computacional. A atenção é uma das operações mais caras nos modelos de transformadores, e o `Flash Attention` a torna mais eficiente.\n",
                        " 2. `Memory-Efficient Attention`: essa é outra implementação de atenção que usa a função `scaled_dot_product_attention` do PyTorch. Essa função é mais eficiente em termos de memória do que a implementação padrão de atenção do PyTorch.\n",
                        "\n",
                        "Além disso, a versão 2.0 do PyTorch inclui um operador de atenção de produto de ponto escalonado (SDPA) nativo como parte do `torch.nn.functional`.\n",
                        "\n",
                        "O Optimmun fornece essa funcionalidade com a biblioteca `Transformers`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Inferência com modelo automático"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Primeiro, vamos ver como seria a inferência normal com `Transformers` e `Automodel`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
                                    ]
                              },
                              "execution_count": 1,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
                        "\n",
                        "tokenizer.pad_token = tokenizer.eos_token\n",
                        "\n",
                        "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
                        "output_tokens = model.generate(**input_tokens, max_length=50)\n",
                        "\n",
                        "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
                        "sentence_output"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, veremos como ele seria otimizado com o `BetterTransformer` e o `Optimun`.\n",
                        "\n",
                        "O que temos que fazer é converter o modelo usando o método `transform` do `BeterTransformer`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
                                    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
                                    ]
                              },
                              "execution_count": 2,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                        "from optimum.bettertransformer import BetterTransformer\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
                        "\n",
                        "# Convert the model to a BetterTransformer model\n",
                        "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
                        "\n",
                        "tokenizer.pad_token = tokenizer.eos_token\n",
                        "\n",
                        "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
                        "output_tokens = model.generate(**input_tokens, max_length=50)\n",
                        "\n",
                        "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
                        "sentence_output"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Inferências com o pipeline"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como antes, primeiro vemos como seria a inferência normal com `Transformers` e `Pipeline`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'score': 0.05116177722811699,\n",
                                          "  'token': 8422,\n",
                                          "  'token_str': 'stanford',\n",
                                          "  'sequence': 'i am a student at stanford university.'},\n",
                                          " {'score': 0.04033993184566498,\n",
                                          "  'token': 5765,\n",
                                          "  'token_str': 'harvard',\n",
                                          "  'sequence': 'i am a student at harvard university.'},\n",
                                          " {'score': 0.03990468755364418,\n",
                                          "  'token': 7996,\n",
                                          "  'token_str': 'yale',\n",
                                          "  'sequence': 'i am a student at yale university.'},\n",
                                          " {'score': 0.0361952930688858,\n",
                                          "  'token': 10921,\n",
                                          "  'token_str': 'cornell',\n",
                                          "  'sequence': 'i am a student at cornell university.'},\n",
                                          " {'score': 0.03303057327866554,\n",
                                          "  'token': 9173,\n",
                                          "  'token_str': 'princeton',\n",
                                          "  'sequence': 'i am a student at princeton university.'}]"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from transformers import pipeline\n",
                        "\n",
                        "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\")\n",
                        "pipe(\"I am a student at [MASK] University.\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora veremos como otimizá-lo, de modo que usaremos `pipeline` do `Optimun`, em vez de `Transformers`. Também devemos indicar que queremos usar o `bettertransformer` como acelerador."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
                                    "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
                                    "  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'score': 0.05116180703043938,\n",
                                          "  'token': 8422,\n",
                                          "  'token_str': 'stanford',\n",
                                          "  'sequence': 'i am a student at stanford university.'},\n",
                                          " {'score': 0.040340032428503036,\n",
                                          "  'token': 5765,\n",
                                          "  'token_str': 'harvard',\n",
                                          "  'sequence': 'i am a student at harvard university.'},\n",
                                          " {'score': 0.039904672652482986,\n",
                                          "  'token': 7996,\n",
                                          "  'token_str': 'yale',\n",
                                          "  'sequence': 'i am a student at yale university.'},\n",
                                          " {'score': 0.036195311695337296,\n",
                                          "  'token': 10921,\n",
                                          "  'token_str': 'cornell',\n",
                                          "  'sequence': 'i am a student at cornell university.'},\n",
                                          " {'score': 0.03303062543272972,\n",
                                          "  'token': 9173,\n",
                                          "  'token_str': 'princeton',\n",
                                          "  'sequence': 'i am a student at princeton university.'}]"
                                    ]
                              },
                              "execution_count": 4,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from optimum.pipelines import pipeline\n",
                        "\n",
                        "# Use the BetterTransformer pipeline\n",
                        "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\", accelerator=\"bettertransformer\")\n",
                        "pipe(\"I am a student at [MASK] University.\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Treinamento"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para o treinamento com `Optimun`, fazemos o mesmo que com a inferência Automodel, convertemos o modelo usando o método `transform` do `BeterTransformer`.\n",
                        "\n",
                        "Quando terminamos o treinamento, convertemos o modelo novamente usando o método `reverse` do `BeterTransformer` para obter o modelo original de volta, para que possamos salvá-lo e carregá-lo no hub Hugging Face."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                        "from optimum.bettertransformer import BetterTransformer\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
                        "\n",
                        "# Convert the model to a BetterTransformer model\n",
                        "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
                        "\n",
                        "##############################################################################\n",
                        "# do your training here\n",
                        "##############################################################################\n",
                        "\n",
                        "# Convert the model back to a Hugging Face model\n",
                        "model_hf = BetterTransformer.reverse(model)\n",
                        "\n",
                        "model_hf.save_pretrained(\"fine_tuned_model\")\n",
                        "model_hf.push_to_hub(\"fine_tuned_model\")"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "base",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.10.12"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
