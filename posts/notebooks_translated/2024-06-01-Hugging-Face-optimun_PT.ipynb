{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face \u00d3timo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Optimun` \u00e9 uma extens\u00e3o da biblioteca [Transformers](https://maximofn.com/hugging-face-transformers/) que fornece um conjunto de ferramentas de otimiza\u00e7\u00e3o de desempenho para treinar e inferir modelos, em hardware espec\u00edfico, com a m\u00e1xima efici\u00eancia.",
        "\n",
        "O ecossistema de IA evolui rapidamente e a cada dia surge mais hardware especializado junto com suas pr\u00f3prias otimiza\u00e7\u00f5es. Portanto, `Optimum` permite aos usu\u00e1rios utilizar eficientemente qualquer deste HW com a mesma facilidade que [Transformers](https://maximofn.com/hugging-face-transformers/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Optimun` permite a otimiza\u00e7\u00e3o para as seguintes plataformas HW:",
        "\n",
        "* Nvidia",
        "* AMD",
        "* Intel",
        "* AWS",
        "* TPU",
        "* Havana",
        "* FuriosaIA",
        "\n",
        "Al\u00e9m disso, oferece acelera\u00e7\u00e3o para as seguintes integra\u00e7\u00f5es open source",
        "\n",
        "* Tempo de execu\u00e7\u00e3o do ONNX",
        "* Exportadores: Exportar modelos Pytorch ou TensorFlow para diferentes formatos como ONNX ou TFLite",
        "* BetterTransformer",
        "* Torch FX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instala\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para instalar `Optimun` simplesmente execute:",
        "\n",
        "```bash\n",
        "pip install optimum",
        "```\n",
        "\n",
        "Mas se quiser instalar com suporte para todas as plataformas HW, pode ser feito assim",
        "\n",
        "|Acelerador|Instala\u00e7\u00e3o|",
        "|---|---|",
        "|ONNX Runtime\t|`pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]`|",
        "|Intel Neural Compressor\t|`pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]`|",
        "|OpenVINO\t|`pip install --upgrade --upgrade-strategy eager optimum[openvino]`|",
        "|NVIDIA TensorRT-LLM\t|`docker run -it --gpus all --ipc host huggingface/optimum-nvidia`|",
        "|GPUs AMD Instinct e NPU Ryzen AI|`pip install --upgrade --upgrade-strategy eager optimum[amd]`|",
        "|AWS Trainum & Inferentia\t|`pip install --upgrade --upgrade-strategy eager optimum[neuronx]`|",
        "|Processador Habana Gaudi (HPU)|`pip install --upgrade --upgrade-strategy eager optimum[habana]`|",
        "|FuriosaAI\t|`pip install --upgrade --upgrade-strategy eager optimum[furiosa]`|",
        "\n",
        "Os flags `--upgrade --upgrade-strategy eager` s\u00e3o necess\u00e1rios para garantir que os diferentes pacotes sejam atualizados para a vers\u00e3o mais recente poss\u00edvel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como a maioria das pessoas usa o Pytorch em GPUs da Nvidia, e principalmente, como eu tenho uma Nvidia, este post vai falar apenas sobre o uso do `Optimun` com GPUs da Nvidia e Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BeterTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BetterTransformer \u00e9 uma otimiza\u00e7\u00e3o nativa do PyTorch para obter um aumento de velocidade de x1,25 a x4 na infer\u00eancia de modelos baseados em Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BetterTransformer \u00e9 uma API que permite aproveitar as caracter\u00edsticas de hardware modernas para acelerar o treinamento e a infer\u00eancia de modelos de transformers no PyTorch, utilizando implementa\u00e7\u00f5es de aten\u00e7\u00e3o mais eficientes e `fast path` da vers\u00e3o nativa de `nn.TransformerEncoderLayer`.",
        "\n",
        "BetterTransformer usa dois tipos de acelera\u00e7\u00f5es:",
        "\n",
        "1. `Flash Attention`: Esta \u00e9 uma implementa\u00e7\u00e3o da `attention` que utiliza `sparse` para reduzir a complexidade computacional. A aten\u00e7\u00e3o \u00e9 uma das opera\u00e7\u00f5es mais custosas nos modelos de transformers, e `Flash Attention` torna-a mais eficiente.",
        "2. `Aten\u00e7\u00e3o Eficiente em Mem\u00f3ria`: Esta \u00e9 outra implementa\u00e7\u00e3o da aten\u00e7\u00e3o que utiliza a fun\u00e7\u00e3o `scaled_dot_product_attention` do PyTorch. Essa fun\u00e7\u00e3o \u00e9 mais eficiente em termos de mem\u00f3ria do que a implementa\u00e7\u00e3o padr\u00e3o da aten\u00e7\u00e3o no PyTorch.",
        "\n",
        "Al\u00e9m disso, a vers\u00e3o 2.0 do PyTorch inclui um operador de aten\u00e7\u00e3o de produtos ponto escalado (SDPA) nativo como parte de `torch.nn.functional`",
        "\n",
        "`Optimun` fornece esta funcionalidade com a biblioteca `Transformers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Infer\u00eancia com Automodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro vamos a ver como seria a infer\u00eancia normal com `Transformers` e `Automodel`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "checkpoint = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "output_tokens = model.generate(**input_tokens, max_length=50)\n",
        "\n",
        "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "sentence_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora veremos como isso seria otimizado com `BetterTransformer` e `Optimun`",
        "\n",
        "O que temos que fazer \u00e9 converter o modelo usando o m\u00e9todo `transform` de `BetterTransformer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from optimum.bettertransformer import BetterTransformer\n",
        "\n",
        "checkpoint = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
        "\n",
        "# Convert the model to a BetterTransformer model\n",
        "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "output_tokens = model.generate(**input_tokens, max_length=50)\n",
        "\n",
        "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "sentence_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Infer\u00eancia com Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assim como antes, primeiro vemos como seria a infer\u00eancia normal com `Transformers` e `Pipeline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.05116177722811699,\n",
              "  'token': 8422,\n",
              "  'token_str': 'stanford',\n",
              "  'sequence': 'i am a student at stanford university.'},\n",
              " {'score': 0.04033993184566498,\n",
              "  'token': 5765,\n",
              "  'token_str': 'harvard',\n",
              "  'sequence': 'i am a student at harvard university.'},\n",
              " {'score': 0.03990468755364418,\n",
              "  'token': 7996,\n",
              "  'token_str': 'yale',\n",
              "  'sequence': 'i am a student at yale university.'},\n",
              " {'score': 0.0361952930688858,\n",
              "  'token': 10921,\n",
              "  'token_str': 'cornell',\n",
              "  'sequence': 'i am a student at cornell university.'},\n",
              " {'score': 0.03303057327866554,\n",
              "  'token': 9173,\n",
              "  'token_str': 'princeton',\n",
              "  'sequence': 'i am a student at princeton university.'}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\")\n",
        "pipe(\"I am a student at [MASK] University.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vemos como otimiz\u00e1-lo, para isso usamos `pipeline` de `Optimun`, em vez do de `Transformers`. Al\u00e9m disso, \u00e9 necess\u00e1rio indicar que queremos usar `bettertransformer` como acelerador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
            "  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.05116180703043938,\n",
              "  'token': 8422,\n",
              "  'token_str': 'stanford',\n",
              "  'sequence': 'i am a student at stanford university.'},\n",
              " {'score': 0.040340032428503036,\n",
              "  'token': 5765,\n",
              "  'token_str': 'harvard',\n",
              "  'sequence': 'i am a student at harvard university.'},\n",
              " {'score': 0.039904672652482986,\n",
              "  'token': 7996,\n",
              "  'token_str': 'yale',\n",
              "  'sequence': 'i am a student at yale university.'},\n",
              " {'score': 0.036195311695337296,\n",
              "  'token': 10921,\n",
              "  'token_str': 'cornell',\n",
              "  'sequence': 'i am a student at cornell university.'},\n",
              " {'score': 0.03303062543272972,\n",
              "  'token': 9173,\n",
              "  'token_str': 'princeton',\n",
              "  'sequence': 'i am a student at princeton university.'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from optimum.pipelines import pipeline\n",
        "\n",
        "# Use the BetterTransformer pipeline\n",
        "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\", accelerator=\"bettertransformer\")\n",
        "pipe(\"I am a student at [MASK] University.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para o treinamento com `Optimun` fazemos o mesmo que com a infer\u00eancia com Automodel, convertemos o modelo por meio do m\u00e9todo `transform` de `BeterTransformer`.",
        "\n",
        "Quando terminamos o treinamento, voltamos a converter o modelo usando o m\u00e9todo `reverse` de `BetterTransformer` para recuperar o modelo original e assim poder salv\u00e1-lo e envi\u00e1-lo para o hub da Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from optimum.bettertransformer import BetterTransformer\n",
        "\n",
        "checkpoint = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
        "\n",
        "# Convert the model to a BetterTransformer model\n",
        "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
        "\n",
        "##############################################################################\n",
        "# do your training here\n",
        "##############################################################################\n",
        "\n",
        "# Convert the model back to a Hugging Face model\n",
        "model_hf = BetterTransformer.reverse(model)\n",
        "\n",
        "model_hf.save_pretrained(\"fine_tuned_model\")\n",
        "model_hf.push_to_hub(\"fine_tuned_model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-01",
      "description_en": "Attention, slow PyTorch models! \ud83d\udc0c Optimun, the Hugging Face library, comes to the rescue to speed up your workouts and inferences. With Optimun, you can forget about speed issues and enjoy more speed and efficiency \ud83d\udd52\ufe0f. And best of all, it's PyTorch compatible - go on, give your models a boost with Optimun! \ud83d\udcbb",
      "description_es": "\u00a1Atenci\u00f3n, modelos de PyTorch lentos! \ud83d\udc0c Optimun, la librer\u00eda de Hugging Face, viene al rescate para acelerar tus entrenamientos e inferencias. Con Optimun, puedes olvidarte de los problemas de velocidad y disfrutar de m\u00e1s velocidad y eficiencia \ud83d\udd52\ufe0f. Y lo mejor de todo, es compatible con PyTorch. \u00a1Vamos, dale un boost a tus modelos con Optimun! \ud83d\udcbb",
      "description_pt": "Aten\u00e7\u00e3o, modelos PyTorch lentos! \ud83d\udc0c Optimun, a biblioteca Hugging Face, vem em seu socorro para acelerar seu treinamento e suas infer\u00eancias. Com o Optimun, voc\u00ea pode esquecer os problemas de velocidade e aproveitar mais velocidade e efici\u00eancia \ud83d\udd52\ufe0f. E o melhor de tudo \u00e9 que ela \u00e9 compat\u00edvel com o PyTorch - v\u00e1 em frente, d\u00ea um impulso aos seus modelos com a Optimun! \ud83d\udcbb",
      "end_url": "hugging-face-optimun",
      "image": "https://images.maximofn.com/huggingface_optimun.webp",
      "image_hover_path": "https://images.maximofn.com/huggingface_optimun.webp",
      "keywords_en": "hugging face, optimun, pytorch, transformers, training, inference, speed, efficiency",
      "keywords_es": "hugging face, optimun, pytorch, transformers, entrenamiento, inferencia, velocidad, eficiencia",
      "keywords_pt": "hugging face, optimun, pytorch, transformers, treinamento, infer\u00eancia, velocidade, efici\u00eancia",
      "title_en": "Hugging Face Optimun",
      "title_es": "Hugging Face Optimun",
      "title_pt": "Hugging Face Optimun"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}