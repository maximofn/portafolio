{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BPE tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `BPE` (Byte Pair Encoding - byte pair encoding) algorithm is a data compression technique used to create a subword vocabulary from a text corpus. This algorithm is based on the frequency of byte pairs in the text. It gained popularity because it was used as a tokenizer by LLMs such as GPT, GPT-2, RoBERTa, BART, and DeBERTa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we have a text corpus that only contains the following words `hug, pug, pun, bun, and hugs`, the first step is to create a vocabulary with all the characters present in the corpus, in our case it will be `b, g, h, n, p, s, u`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "Number of initial corpus tokens: 7\n"
          ]
        }
      ],
      "source": [
        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
        "\n",
        "# Concatenate all the words in the corpus\n",
        "initial_corpus_tokens = \"\"\n",
        "for word in corpus_words:\n",
        "    initial_corpus_tokens += word\n",
        "\n",
        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
        "initial_corpus_tokens = set(initial_corpus_tokens)\n",
        "\n",
        "print(f\"Corpus words: {corpus_words}\")\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"Number of initial corpus tokens: {len(initial_corpus_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's assume this is our corpus of sentences; it's a made-up corpus, it doesn't make sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"hug hug hug pun pun bun hugs\",\n",
        "    \"hug hug pug pug pun pun hugs\",\n",
        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
        "    \"pug pun pun pun bun hugs\",\n",
        "    \"hug hug hug pun bun bun hugs\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's count the number of times each word appears in the corpus, to check that what we put before is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of hug: 10\n",
            "Number of pug: 5\n",
            "Number of pun: 12\n",
            "Number of bun: 4\n",
            "Number of hugs: 5\n"
          ]
        }
      ],
      "source": [
        "num_hug = 0\n",
        "num_pug = 0\n",
        "num_pun = 0\n",
        "num_bun = 0\n",
        "num_hugs = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        if word == \"hug\":\n",
        "            num_hug += 1\n",
        "        elif word == \"pug\":\n",
        "            num_pug += 1\n",
        "        elif word == \"pun\":\n",
        "            num_pun += 1\n",
        "        elif word == \"bun\":\n",
        "            num_bun += 1\n",
        "        elif word == \"hugs\":\n",
        "            num_hugs += 1\n",
        "\n",
        "print(f\"Number of hug: {num_hug}\")\n",
        "print(f\"Number of pug: {num_pug}\")\n",
        "print(f\"Number of pun: {num_pun}\")\n",
        "print(f\"Number of bun: {num_bun}\")\n",
        "print(f\"Number of hugs: {num_hugs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything we had discussed is fine, we can continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a dictionary with the tokens of each word and the number of times they appear in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = {\n",
        "    \"hug\":\n",
        "        {\n",
        "            \"count\": num_hug,\n",
        "            \"tokens\": [character for character in \"hug\"],\n",
        "        },\n",
        "    \"pug\":\n",
        "        {\n",
        "            \"count\": num_pug,\n",
        "            \"tokens\": [character for character in \"pug\"],\n",
        "        },\n",
        "    \"pun\":\n",
        "        {\n",
        "            \"count\": num_pun,\n",
        "            \"tokens\": [character for character in \"pun\"],\n",
        "        },\n",
        "    \"bun\":\n",
        "        {\n",
        "            \"count\": num_bun,\n",
        "            \"tokens\": [character for character in \"bun\"],\n",
        "        },\n",
        "    \"hugs\":\n",
        "        {\n",
        "            \"count\": num_hugs,\n",
        "            \"tokens\": [character for character in \"hugs\"],\n",
        "        },\n",
        "}\n",
        "\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to find the pair of consecutive tokens that appears most frequently in the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List of consecutive tokens: ['hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'bu', 'bu', 'bu', 'bu', 'un', 'un', 'un', 'un', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'gs', 'gs', 'gs', 'gs', 'gs']\n",
            "Dictionary of consecutive tokens: {'hu': 15, 'ug': 20, 'pu': 17, 'un': 16, 'bu': 4, 'gs': 5}\n",
            "Consecutive token with maximum frequency: ug\n"
          ]
        }
      ],
      "source": [
        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "\n",
        "list_consecutive_tokens = []\n",
        "for i, key in enumerate(dict_keys):\n",
        "    # Get the tokens of the word\n",
        "    number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
        "\n",
        "    # Get consecituve tokens\n",
        "    for j in range(number_of_toneks_of_word-1):\n",
        "        # Get consecutive tokens\n",
        "        consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
        "        # Append the consecutive tokens to the list the number of times the word appears\n",
        "        for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
        "            list_consecutive_tokens.append(consecutive_tokens)\n",
        "# Print the list of consecutive tokens\n",
        "print(f\"List of consecutive tokens: {list_consecutive_tokens}\")\n",
        "\n",
        "# Get consecutive tokens with maximum frequency\n",
        "dict_consecutive_tokens = {}\n",
        "for token in list_consecutive_tokens:\n",
        "    # Check if the token is already in the dictionary\n",
        "    if token in dict_consecutive_tokens:\n",
        "        # Increment the count of the token\n",
        "        dict_consecutive_tokens[token] += 1\n",
        "    \n",
        "    # If the token is not in the dictionary\n",
        "    else:\n",
        "        # Add the token to the dictionary\n",
        "        dict_consecutive_tokens[token] = 1\n",
        "# Print the dictionary of consecutive tokens\n",
        "print(f\"Dictionary of consecutive tokens: {dict_consecutive_tokens}\")\n",
        "\n",
        "# Get the consecutive token with maximum frequency\n",
        "max_consecutive_token = None\n",
        "while True:\n",
        "    # Get the token with maximum frequency\n",
        "    consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
        "\n",
        "    # Check if the token is already in the list of tokens\n",
        "    if consecutive_token in initial_corpus_tokens:\n",
        "        # Remove token from the dictionary\n",
        "        dict_consecutive_tokens.pop(consecutive_token)\n",
        "\n",
        "    # If the token is not in the list of tokens\n",
        "    else:\n",
        "        # Assign the token to the max_consecutive_token\n",
        "        max_consecutive_token = consecutive_token\n",
        "        break\n",
        "\n",
        "# Print the consecutive token with maximum frequency\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have obtained the pair of tokens that appear most frequently. Let's encapsulate this in a function because we are going to use it more times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, list_corpus_tokens):\n",
        "    dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "\n",
        "    list_consecutive_tokens = []\n",
        "    for i, key in enumerate(dict_keys):\n",
        "        # Get the tokens of the word\n",
        "        number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
        "\n",
        "        # Get consecituve tokens\n",
        "        for j in range(number_of_toneks_of_word-1):\n",
        "            # Get consecutive tokens\n",
        "            consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
        "            # Append the consecutive tokens to the list\n",
        "            for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
        "                list_consecutive_tokens.append(consecutive_tokens)\n",
        "\n",
        "    # Get consecutive tokens with maximum frequency\n",
        "    dict_consecutive_tokens = {}\n",
        "    for token in list_consecutive_tokens:\n",
        "        # Check if the token is already in the dictionary\n",
        "        if token in dict_consecutive_tokens:\n",
        "            # Increment the count of the token\n",
        "            dict_consecutive_tokens[token] += 1\n",
        "        \n",
        "        # If the token is not in the dictionary\n",
        "        else:\n",
        "            # Add the token to the dictionary\n",
        "            dict_consecutive_tokens[token] = 1\n",
        "\n",
        "    # Get the consecutive token with maximum frequency\n",
        "    max_consecutive_token = None\n",
        "    while True:\n",
        "        # Get the token with maximum frequency\n",
        "        consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
        "\n",
        "        # Check if the token is already in the list of tokens\n",
        "        if consecutive_token in list_corpus_tokens:\n",
        "            # Remove token from the dictionary\n",
        "            dict_consecutive_tokens.pop(consecutive_token)\n",
        "\n",
        "        # If the token is not in the list of tokens\n",
        "        else:\n",
        "            # Assign the token to the max_consecutive_token\n",
        "            max_consecutive_token = consecutive_token\n",
        "            break\n",
        "\n",
        "    return max_consecutive_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check that we get the same as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: ug\n"
          ]
        }
      ],
      "source": [
        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, initial_corpus_tokens)\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now our token corpus can be modified by adding the `ug` token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
          ]
        }
      ],
      "source": [
        "# new_corpus_tokens = initial_corpus_tokens + max_consecutive_token\n",
        "new_corpus_tokens = initial_corpus_tokens.copy()\n",
        "new_corpus_tokens.add(max_consecutive_token)\n",
        "\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also put this in a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens):\n",
        "    new_corpus_tokens = initial_corpus_tokens.copy()\n",
        "    new_corpus_tokens.add(max_consecutive_token)\n",
        "    return new_corpus_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will check again that we get the same as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
          ]
        }
      ],
      "source": [
        "new_corpus_tokens = get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens)\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to modify the dictionary where the words, the tokens, and the number of times they appear are listed with the new token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token ug is in the word hug\n",
            "New tokens of the word hug: ['h', 'u', 'g', 'ug']\n",
            "Token ug is in the word pug\n",
            "New tokens of the word pug: ['p', 'u', 'g', 'ug']\n",
            "Token ug is in the word hugs\n",
            "New tokens of the word hugs: ['h', 'u', 'g', 's', 'ug']\n",
            "Initial tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}\n",
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
        "\n",
        "for key in dict_keys:\n",
        "    # Check if the new token is in the word\n",
        "    if max_consecutive_token in key:\n",
        "        print(f\"Token {max_consecutive_token} is in the word {key}\")\n",
        "\n",
        "        # Add the new token to the word tokens\n",
        "        dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
        "\n",
        "        print(f\"New tokens of the word {key}: {dict_tokens_by_word_appearance_tmp[key]['tokens']}\")\n",
        "\n",
        "print(f\"Initial tokens by word appearance: {dict_tokens_by_word_appearance}\")\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put this into a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token):\n",
        "    dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
        "    dict_keys = dict_tokens_by_word_appearance_tmp.keys()\n",
        "\n",
        "    for key in dict_keys:\n",
        "        # Check if the new token is in the word\n",
        "        if max_consecutive_token in key:\n",
        "            # Add the new token to the word tokens\n",
        "            dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
        "\n",
        "    return dict_tokens_by_word_appearance_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check that it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, in the first iteration we have moved from a token corpus of `s, g, h, u, n, p, b` to the new token corpus `h, u, n, p, s, g, b, ug`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now perform a second iteration, obtaining the pair of consecutive tokens that appear most frequently in the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: pu\n"
          ]
        }
      ],
      "source": [
        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, new_corpus_tokens)\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain the new token corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n",
            "New corpus tokens: {'p', 'n', 'pu', 'u', 's', 'h', 'ug', 'g', 'b'}\n"
          ]
        }
      ],
      "source": [
        "corpus_tokens = get_new_corpus_tokens(max_consecutive_token, new_corpus_tokens)\n",
        "print(f\"Initial corpus tokens: {new_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we get the new dictionary in which the words, the tokens, and the number of times they appear are listed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can continue until we have a corpus of tokens of the size we want, let's create a corpus of 15 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: un\n",
            "New corpus tokens: {'p', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}\n",
            "\n",
            "Consecutive token with maximum frequency: hu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: gug\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: ughu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: npu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: puun\n",
            "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "len_corpus_tokens = 15\n",
        "\n",
        "while len(corpus_tokens) < len_corpus_tokens:\n",
        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
        "    print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")\n",
        "\n",
        "    # If there are no more consecutive tokens break the loop\n",
        "    if max_consecutive_token is None:\n",
        "        break\n",
        "\n",
        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
        "    print(f\"New corpus tokens: {corpus_tokens}\")\n",
        "\n",
        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "    print(f\"New tokens by word appearance: {dict_tokens_by_word_appearance}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have seen how the BPE tokenizer is trained, let's train it from scratch to reinforce our knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "Number of initial corpus tokens: 7\n"
          ]
        }
      ],
      "source": [
        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
        "\n",
        "# Concatenate all the words in the corpus\n",
        "initial_corpus_tokens = \"\"\n",
        "for word in corpus_words:\n",
        "    initial_corpus_tokens += word\n",
        "\n",
        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
        "corpus_tokens = set(initial_corpus_tokens)\n",
        "\n",
        "print(f\"Corpus words: {corpus_words}\")\n",
        "print(f\"Initial corpus tokens: {corpus_tokens}\")\n",
        "print(f\"Number of initial corpus tokens: {len(corpus_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"hug hug hug pun pun bun hugs\",\n",
        "    \"hug hug pug pug pun pun hugs\",\n",
        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
        "    \"pug pun pun pun bun hugs\",\n",
        "    \"hug hug hug pun bun bun hugs\",\n",
        "]\n",
        "\n",
        "num_hug = 0\n",
        "num_pug = 0\n",
        "num_pun = 0\n",
        "num_bun = 0\n",
        "num_hugs = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        if word == \"hug\":\n",
        "            num_hug += 1\n",
        "        elif word == \"pug\":\n",
        "            num_pug += 1\n",
        "        elif word == \"pun\":\n",
        "            num_pun += 1\n",
        "        elif word == \"bun\":\n",
        "            num_bun += 1\n",
        "        elif word == \"hugs\":\n",
        "            num_hugs += 1\n",
        "\n",
        "dict_tokens_by_word_appearance = {\n",
        "    \"hug\":\n",
        "        {\n",
        "            \"count\": num_hug,\n",
        "            \"tokens\": [character for character in \"hug\"],\n",
        "        },\n",
        "    \"pug\":\n",
        "        {\n",
        "            \"count\": num_pug,\n",
        "            \"tokens\": [character for character in \"pug\"],\n",
        "        },\n",
        "    \"pun\":\n",
        "        {\n",
        "            \"count\": num_pun,\n",
        "            \"tokens\": [character for character in \"pun\"],\n",
        "        },\n",
        "    \"bun\":\n",
        "        {\n",
        "            \"count\": num_bun,\n",
        "            \"tokens\": [character for character in \"bun\"],\n",
        "        },\n",
        "    \"hugs\":\n",
        "        {\n",
        "            \"count\": num_hugs,\n",
        "            \"tokens\": [character for character in \"hugs\"],\n",
        "        },\n",
        "}\n",
        "\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We trained it from scratch until we obtained a corpus of 15 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: (7) {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: (15) {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n"
          ]
        }
      ],
      "source": [
        "len_corpus_tokens = 15\n",
        "print(f\"Initial corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")\n",
        "\n",
        "while len(corpus_tokens) < len_corpus_tokens:\n",
        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
        "\n",
        "    # If there are no more consecutive tokens break the loop\n",
        "    if max_consecutive_token is None:\n",
        "        break\n",
        "\n",
        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
        "\n",
        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "\n",
        "print(f\"New corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we now wanted to tokenize, we would first have to create a vocabulary, that is, assign an ID to each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'p': 0,\n",
              " 'hu': 1,\n",
              " 'sug': 2,\n",
              " 'npu': 3,\n",
              " 'ugpu': 4,\n",
              " 'gug': 5,\n",
              " 'u': 6,\n",
              " 'ug': 7,\n",
              " 'ughu': 8,\n",
              " 'n': 9,\n",
              " 'pu': 10,\n",
              " 'un': 11,\n",
              " 'puun': 12,\n",
              " 's': 13,\n",
              " 'h': 14,\n",
              " 'gs': 15,\n",
              " 'g': 16,\n",
              " 'b': 17}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {}\n",
        "for i, token in enumerate(corpus_tokens):\n",
        "    vocab[token] = i\n",
        "\n",
        "print(f\"Vocabulary: \")\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put it in a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vocabulary(corpus_tokens):\n",
        "    vocab = {}\n",
        "    for i, token in enumerate(corpus_tokens):\n",
        "        vocab[token] = i\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check that it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'p': 0,\n",
              " 'hu': 1,\n",
              " 'sug': 2,\n",
              " 'npu': 3,\n",
              " 'ugpu': 4,\n",
              " 'gug': 5,\n",
              " 'u': 6,\n",
              " 'ug': 7,\n",
              " 'ughu': 8,\n",
              " 'n': 9,\n",
              " 'pu': 10,\n",
              " 'un': 11,\n",
              " 'puun': 12,\n",
              " 's': 13,\n",
              " 'h': 14,\n",
              " 'gs': 15,\n",
              " 'g': 16,\n",
              " 'b': 17}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = get_vocabulary(corpus_tokens)\n",
        "print(f\"Vocabulary: \")\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we now want to tokenize the word `bug` we can do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of tokens: 4\n",
            "Prefix: bug\n",
            "Prefix: bug\n",
            "Prefix: bu\n",
            "Prefix: b\n",
            "prefix b is in the vocabulary\n",
            "Prefix: ug\n",
            "prefix ug is in the vocabulary\n",
            "Tokens: ['b', 'ug']\n"
          ]
        }
      ],
      "source": [
        "word = 'bug'\n",
        "\n",
        "# Get the maximum length of tokens\n",
        "max_len = max(len(token) for token in vocab)\n",
        "print(f\"Maximum length of tokens: {max_len}\")\n",
        "\n",
        "# Create a empty list of tokens\n",
        "tokens = []\n",
        "while len(word) > 0:\n",
        "    # Flag to check if the token is found\n",
        "    found = False\n",
        "\n",
        "    # Iterate over the maximum length of tokens from max_len to 0\n",
        "    for i in range(max_len, 0, -1):\n",
        "        # Get the prefix of the word\n",
        "        prefix = word[:i]\n",
        "        print(f\"Prefix: {prefix}\")\n",
        "\n",
        "        # Check if the prefix is in the vocabulary\n",
        "        if prefix in vocab:\n",
        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
        "            tokens.append(prefix)\n",
        "            word = word[i:]\n",
        "            found = True\n",
        "            break\n",
        "    \n",
        "    # if not found:\n",
        "    #     tokens.append('<UNK>')\n",
        "    #     word = word[1:]\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But if we now want to tokenize the word `mug` we couldn't because the character `m` is not in the vocabulary, so we tokenize it with the `<UNK>` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of tokens: 4\n",
            "Prefix: mug\n",
            "Prefix: mug\n",
            "Prefix: mu\n",
            "Prefix: m\n",
            "Prefix: ug\n",
            "prefix ug is in the vocabulary\n",
            "Tokens: ['<UNK>', 'ug']\n"
          ]
        }
      ],
      "source": [
        "word = 'mug'\n",
        "\n",
        "# Get the maximum length of tokens\n",
        "max_len = max(len(token) for token in vocab)\n",
        "print(f\"Maximum length of tokens: {max_len}\")\n",
        "\n",
        "# Create a empty list of tokens\n",
        "tokens = []\n",
        "while len(word) > 0:\n",
        "    # Flag to check if the token is found\n",
        "    found = False\n",
        "\n",
        "    # Iterate over the maximum length of tokens from max_len to 0\n",
        "    for i in range(max_len, 0, -1):\n",
        "        # Get the prefix of the word\n",
        "        prefix = word[:i]\n",
        "        print(f\"Prefix: {prefix}\")\n",
        "\n",
        "        # Check if the prefix is in the vocabulary\n",
        "        if prefix in vocab:\n",
        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
        "            tokens.append(prefix)\n",
        "            word = word[i:]\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        tokens.append('<UNK>')\n",
        "        word = word[1:]\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put it in a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_word(word, vocab):\n",
        "    # Get the maximum length of tokens\n",
        "    max_len = max(len(token) for token in vocab)\n",
        "\n",
        "    # Create a empty list of tokens\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        # Flag to check if the token is found\n",
        "        found = False\n",
        "\n",
        "        # Iterate over the maximum length of tokens from max_len to 0\n",
        "        for i in range(max_len, 0, -1):\n",
        "            # Get the prefix of the word\n",
        "            prefix = word[:i]\n",
        "\n",
        "            # Check if the prefix is in the vocabulary\n",
        "            if prefix in vocab:\n",
        "                tokens.append(prefix)\n",
        "                word = word[i:]\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            tokens.append('<UNK>')\n",
        "            word = word[1:]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check that it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization of the word 'bug': ['b', 'ug']\n",
            "Tokenization of the word 'mug': ['<UNK>', 'ug']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokenization of the word 'bug': {tokenize_word('bug', vocab)}\")\n",
        "print(f\"Tokenization of the word 'mug': {tokenize_word('mug', vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token Viewer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we know how a BPE tokenizer works, let's see using the visualizer [the-tokenizer-playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground) what the tokens of any sentence would look like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe>",
        "src=\"https://xenova-the-tokenizer-playground.static.hf.space\"",
        "frameborder=\"0\"",
        "width=\"850\"",
        "height=\"450\"",
        "></iframe>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-04",
      "description_en": "\ud83d\udd0d Discover the secret of tokenization! \ud83d\udd11 I reveal you the mysteries of BPE (Byte Pair Encoding) tokenization, one of the most popular and effective methods to split text into tokens. Learn how to to tokenize with BPE! \ud83d\udcbb Read my post and discover the tips and tricks to master tokenization with BPE! \ud83d\udcc4",
      "description_es": "\ud83d\udd0d \u00a1Descubre el secreto de la tokenizaci\u00f3n! \ud83d\udd11 Te revelo los misterios del tokenizador BPE (Byte Pair Encoding), uno de los m\u00e1s populares y efectivos m\u00e9todos para dividir el texto en tokens. \u00a1Aprende c\u00f3mo tokenizar con BPE! \ud83d\udcbb \u00a1Lee mi post y descubre los trucos y consejos para dominar la tokenizaci\u00f3n con BPE! \ud83d\udcc4",
      "description_pt": "\ud83d\udd0d Descubra o segredo da tokeniza\u00e7\u00e3o! \ud83d\udd11 Revelo os mist\u00e9rios da tokeniza\u00e7\u00e3o BPE (Byte Pair Encoding), um dos m\u00e9todos mais populares e eficazes para dividir o texto em tokens. Aprenda a tokenizar com BPE! \ud83d\udcbb Leia minha postagem e descubra as dicas e os truques para dominar a tokeniza\u00e7\u00e3o com BPE! \ud83d\udcc4",
      "end_url": "bpe",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/BPE_tokenizer.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/BPE_tokenizer.webp",
      "keywords_en": "bpe tokenizer, byte pair encoding, tokenization, nlp, natural language processing, text processing",
      "keywords_es": "tokenizador bpe, byte pair encoding, tokenizaci\u00f3n, pnl, procesamiento de lenguaje natural, procesamiento de texto",
      "keywords_pt": "tokenizador bpe, byte pair encoding, tokeniza\u00e7\u00e3o, pnl, processamento de linguagem natural, processamento de texto",
      "title_en": "BPE tokenizer",
      "title_es": "BPE tokenizer",
      "title_pt": "BPE tokenizer"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}