{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# Tokenizador BPE"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação.."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "O tokenizador `BPE` (Byte Pair Encoding) é um algoritmo de compactação de dados usado para criar um vocabulário de subpalavras a partir de um corpus de texto. Esse algoritmo é baseado na frequência de pares de bytes no texto. Ele se tornou popular porque foi usado como um tokenizador por LLMs como GPT, GPT-2, RoBERTa, BART e DeBERTa."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Algoritmo de treinamento"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Suponha que tenhamos um corpus de texto contendo apenas as seguintes palavras `hug, pug, pun, bun e hugs`, a primeira etapa é criar um vocabulário com todos os caracteres presentes no corpus, que no nosso caso será `b, g, h, n, p, s, u`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 22,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
                                    "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
                                    "Number of initial corpus tokens: 7\n"
                              ]
                        }
                  ],
                  "source": [
                        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
                        "\n",
                        "# Concatenate all the words in the corpus\n",
                        "initial_corpus_tokens = \"\"\n",
                        "for word in corpus_words:\n",
                        "    initial_corpus_tokens += word\n",
                        "\n",
                        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
                        "initial_corpus_tokens = set(initial_corpus_tokens)\n",
                        "\n",
                        "print(f\"Corpus words: {corpus_words}\")\n",
                        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
                        "print(f\"Number of initial corpus tokens: {len(initial_corpus_tokens)}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, suponha que esse seja o nosso corpus de sentenças, é um corpus inventado, não faz sentido."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 23,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "corpus = [\n",
                        "    \"hug hug hug pun pun bun hugs\",\n",
                        "    \"hug hug pug pug pun pun hugs\",\n",
                        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
                        "    \"pug pun pun pun bun hugs\",\n",
                        "    \"hug hug hug pun bun bun hugs\",\n",
                        "]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos contar o número de vezes que cada palavra aparece no corpus, para verificar se o que colocamos antes está correto."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 24,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Number of hug: 10\n",
                                    "Number of pug: 5\n",
                                    "Number of pun: 12\n",
                                    "Number of bun: 4\n",
                                    "Number of hugs: 5\n"
                              ]
                        }
                  ],
                  "source": [
                        "num_hug = 0\n",
                        "num_pug = 0\n",
                        "num_pun = 0\n",
                        "num_bun = 0\n",
                        "num_hugs = 0\n",
                        "\n",
                        "for sentence in corpus:\n",
                        "    words = sentence.split(\" \")\n",
                        "    for word in words:\n",
                        "        if word == \"hug\":\n",
                        "            num_hug += 1\n",
                        "        elif word == \"pug\":\n",
                        "            num_pug += 1\n",
                        "        elif word == \"pun\":\n",
                        "            num_pun += 1\n",
                        "        elif word == \"bun\":\n",
                        "            num_bun += 1\n",
                        "        elif word == \"hugs\":\n",
                        "            num_hugs += 1\n",
                        "\n",
                        "print(f\"Number of hug: {num_hug}\")\n",
                        "print(f\"Number of pug: {num_pug}\")\n",
                        "print(f\"Number of pun: {num_pun}\")\n",
                        "print(f\"Number of bun: {num_bun}\")\n",
                        "print(f\"Number of hugs: {num_hugs}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Tudo o que havíamos contado está bem, podemos continuar."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criaremos um dicionário com os tokens de cada palavra e o número de vezes que ela aparece no corpus."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 25,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
                                          " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
                                          " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
                                          " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
                                          " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
                                    ]
                              },
                              "execution_count": 25,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "dict_tokens_by_word_appearance = {\n",
                        "    \"hug\":\n",
                        "        {\n",
                        "            \"count\": num_hug,\n",
                        "            \"tokens\": [character for character in \"hug\"],\n",
                        "        },\n",
                        "    \"pug\":\n",
                        "        {\n",
                        "            \"count\": num_pug,\n",
                        "            \"tokens\": [character for character in \"pug\"],\n",
                        "        },\n",
                        "    \"pun\":\n",
                        "        {\n",
                        "            \"count\": num_pun,\n",
                        "            \"tokens\": [character for character in \"pun\"],\n",
                        "        },\n",
                        "    \"bun\":\n",
                        "        {\n",
                        "            \"count\": num_bun,\n",
                        "            \"tokens\": [character for character in \"bun\"],\n",
                        "        },\n",
                        "    \"hugs\":\n",
                        "        {\n",
                        "            \"count\": num_hugs,\n",
                        "            \"tokens\": [character for character in \"hugs\"],\n",
                        "        },\n",
                        "}\n",
                        "\n",
                        "dict_tokens_by_word_appearance"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora vamos procurar o par de tokens consecutivos que aparece com mais frequência no dicionário."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 26,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "List of consecutive tokens: ['hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'bu', 'bu', 'bu', 'bu', 'un', 'un', 'un', 'un', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'gs', 'gs', 'gs', 'gs', 'gs']\n",
                                    "Dictionary of consecutive tokens: {'hu': 15, 'ug': 20, 'pu': 17, 'un': 16, 'bu': 4, 'gs': 5}\n",
                                    "Consecutive token with maximum frequency: ug\n"
                              ]
                        }
                  ],
                  "source": [
                        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
                        "\n",
                        "list_consecutive_tokens = []\n",
                        "for i, key in enumerate(dict_keys):\n",
                        "    # Get the tokens of the word\n",
                        "    number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
                        "\n",
                        "    # Get consecituve tokens\n",
                        "    for j in range(number_of_toneks_of_word-1):\n",
                        "        # Get consecutive tokens\n",
                        "        consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
                        "        # Append the consecutive tokens to the list the number of times the word appears\n",
                        "        for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
                        "            list_consecutive_tokens.append(consecutive_tokens)\n",
                        "# Print the list of consecutive tokens\n",
                        "print(f\"List of consecutive tokens: {list_consecutive_tokens}\")\n",
                        "\n",
                        "# Get consecutive tokens with maximum frequency\n",
                        "dict_consecutive_tokens = {}\n",
                        "for token in list_consecutive_tokens:\n",
                        "    # Check if the token is already in the dictionary\n",
                        "    if token in dict_consecutive_tokens:\n",
                        "        # Increment the count of the token\n",
                        "        dict_consecutive_tokens[token] += 1\n",
                        "    \n",
                        "    # If the token is not in the dictionary\n",
                        "    else:\n",
                        "        # Add the token to the dictionary\n",
                        "        dict_consecutive_tokens[token] = 1\n",
                        "# Print the dictionary of consecutive tokens\n",
                        "print(f\"Dictionary of consecutive tokens: {dict_consecutive_tokens}\")\n",
                        "\n",
                        "# Get the consecutive token with maximum frequency\n",
                        "max_consecutive_token = None\n",
                        "while True:\n",
                        "    # Get the token with maximum frequency\n",
                        "    consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
                        "\n",
                        "    # Check if the token is already in the list of tokens\n",
                        "    if consecutive_token in initial_corpus_tokens:\n",
                        "        # Remove token from the dictionary\n",
                        "        dict_consecutive_tokens.pop(consecutive_token)\n",
                        "\n",
                        "    # If the token is not in the list of tokens\n",
                        "    else:\n",
                        "        # Assign the token to the max_consecutive_token\n",
                        "        max_consecutive_token = consecutive_token\n",
                        "        break\n",
                        "\n",
                        "# Print the consecutive token with maximum frequency\n",
                        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Obtivemos o par de tokens que aparece com mais frequência. Vamos encapsular isso em uma função porque vamos usá-la com mais frequência"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 27,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, list_corpus_tokens):\n",
                        "    dict_keys = dict_tokens_by_word_appearance.keys()\n",
                        "\n",
                        "    list_consecutive_tokens = []\n",
                        "    for i, key in enumerate(dict_keys):\n",
                        "        # Get the tokens of the word\n",
                        "        number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
                        "\n",
                        "        # Get consecituve tokens\n",
                        "        for j in range(number_of_toneks_of_word-1):\n",
                        "            # Get consecutive tokens\n",
                        "            consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
                        "            # Append the consecutive tokens to the list\n",
                        "            for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
                        "                list_consecutive_tokens.append(consecutive_tokens)\n",
                        "\n",
                        "    # Get consecutive tokens with maximum frequency\n",
                        "    dict_consecutive_tokens = {}\n",
                        "    for token in list_consecutive_tokens:\n",
                        "        # Check if the token is already in the dictionary\n",
                        "        if token in dict_consecutive_tokens:\n",
                        "            # Increment the count of the token\n",
                        "            dict_consecutive_tokens[token] += 1\n",
                        "        \n",
                        "        # If the token is not in the dictionary\n",
                        "        else:\n",
                        "            # Add the token to the dictionary\n",
                        "            dict_consecutive_tokens[token] = 1\n",
                        "\n",
                        "    # Get the consecutive token with maximum frequency\n",
                        "    max_consecutive_token = None\n",
                        "    while True:\n",
                        "        # Get the token with maximum frequency\n",
                        "        consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
                        "\n",
                        "        # Check if the token is already in the list of tokens\n",
                        "        if consecutive_token in list_corpus_tokens:\n",
                        "            # Remove token from the dictionary\n",
                        "            dict_consecutive_tokens.pop(consecutive_token)\n",
                        "\n",
                        "        # If the token is not in the list of tokens\n",
                        "        else:\n",
                        "            # Assign the token to the max_consecutive_token\n",
                        "            max_consecutive_token = consecutive_token\n",
                        "            break\n",
                        "\n",
                        "    return max_consecutive_token"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Verificamos se o resultado é o mesmo de antes"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 28,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Consecutive token with maximum frequency: ug\n"
                              ]
                        }
                  ],
                  "source": [
                        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, initial_corpus_tokens)\n",
                        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos que sim"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, nosso corpus de token pode ser modificado com a adição do token `ug`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 29,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
                                    "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
                              ]
                        }
                  ],
                  "source": [
                        "# new_corpus_tokens = initial_corpus_tokens + max_consecutive_token\n",
                        "new_corpus_tokens = initial_corpus_tokens.copy()\n",
                        "new_corpus_tokens.add(max_consecutive_token)\n",
                        "\n",
                        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
                        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Colocamos isso também em uma função"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 30,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens):\n",
                        "    new_corpus_tokens = initial_corpus_tokens.copy()\n",
                        "    new_corpus_tokens.add(max_consecutive_token)\n",
                        "    return new_corpus_tokens"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Verificaremos novamente se estamos recebendo o mesmo que antes."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 31,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
                                    "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
                              ]
                        }
                  ],
                  "source": [
                        "new_corpus_tokens = get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens)\n",
                        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
                        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos que sim"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora vamos modificar o dicionário no qual aparecem as palavras, os tokens e o número de vezes que eles aparecem com o novo token."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 32,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Token ug is in the word hug\n",
                                    "New tokens of the word hug: ['h', 'u', 'g', 'ug']\n",
                                    "Token ug is in the word pug\n",
                                    "New tokens of the word pug: ['p', 'u', 'g', 'ug']\n",
                                    "Token ug is in the word hugs\n",
                                    "New tokens of the word hugs: ['h', 'u', 'g', 's', 'ug']\n",
                                    "Initial tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}\n",
                                    "New tokens by word appearance: \n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
                                          " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
                                          " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
                                          " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
                                          " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
                                    ]
                              },
                              "execution_count": 32,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "import copy\n",
                        "\n",
                        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
                        "dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
                        "\n",
                        "for key in dict_keys:\n",
                        "    # Check if the new token is in the word\n",
                        "    if max_consecutive_token in key:\n",
                        "        print(f\"Token {max_consecutive_token} is in the word {key}\")\n",
                        "\n",
                        "        # Add the new token to the word tokens\n",
                        "        dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
                        "\n",
                        "        print(f\"New tokens of the word {key}: {dict_tokens_by_word_appearance_tmp[key]['tokens']}\")\n",
                        "\n",
                        "print(f\"Initial tokens by word appearance: {dict_tokens_by_word_appearance}\")\n",
                        "print(f\"New tokens by word appearance: \")\n",
                        "dict_tokens_by_word_appearance_tmp"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Colocamos isso em uma função"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 33,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token):\n",
                        "    dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
                        "    dict_keys = dict_tokens_by_word_appearance_tmp.keys()\n",
                        "\n",
                        "    for key in dict_keys:\n",
                        "        # Check if the new token is in the word\n",
                        "        if max_consecutive_token in key:\n",
                        "            # Add the new token to the word tokens\n",
                        "            dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
                        "\n",
                        "    return dict_tokens_by_word_appearance_tmp"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Verificamos se está tudo bem"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 34,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "New tokens by word appearance: \n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
                                          " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
                                          " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
                                          " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
                                          " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
                                    ]
                              },
                              "execution_count": 34,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
                        "print(f\"New tokens by word appearance: \")\n",
                        "dict_tokens_by_word_appearance"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Em resumo, em uma primeira iteração, passamos de um corpus de tokens `s, g, h, u, n, p, b` para o novo corpus de tokens `h, u, n, p, s, g, b, ug`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora realizamos uma segunda iteração, obtendo o par de tokens consecutivos que aparecem com mais frequência no dicionário."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 35,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Consecutive token with maximum frequency: pu\n"
                              ]
                        }
                  ],
                  "source": [
                        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, new_corpus_tokens)\n",
                        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Obtemos o novo corpus de tokens"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 36,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Initial corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n",
                                    "New corpus tokens: {'p', 'n', 'pu', 'u', 's', 'h', 'ug', 'g', 'b'}\n"
                              ]
                        }
                  ],
                  "source": [
                        "corpus_tokens = get_new_corpus_tokens(max_consecutive_token, new_corpus_tokens)\n",
                        "print(f\"Initial corpus tokens: {new_corpus_tokens}\")\n",
                        "print(f\"New corpus tokens: {corpus_tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "E temos o novo dicionário mostrando as palavras, os tokens e o número de vezes que eles aparecem."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 37,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "New tokens by word appearance: \n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
                                          " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']},\n",
                                          " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu']},\n",
                                          " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
                                          " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
                                    ]
                              },
                              "execution_count": 37,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
                        "print(f\"New tokens by word appearance: \")\n",
                        "dict_tokens_by_word_appearance"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, podemos continuar até termos um corpus de tokens com o tamanho desejado. Vamos criar um corpus de 15 tokens."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 38,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Consecutive token with maximum frequency: un\n",
                                    "New corpus tokens: {'p', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
                                    "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}\n",
                                    "\n",
                                    "Consecutive token with maximum frequency: hu\n",
                                    "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
                                    "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
                                    "\n",
                                    "Consecutive token with maximum frequency: gug\n",
                                    "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
                                    "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
                                    "\n",
                                    "Consecutive token with maximum frequency: ughu\n",
                                    "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
                                    "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
                                    "\n",
                                    "Consecutive token with maximum frequency: npu\n",
                                    "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
                                    "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
                                    "\n",
                                    "Consecutive token with maximum frequency: puun\n",
                                    "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
                                    "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
                                    "\n"
                              ]
                        }
                  ],
                  "source": [
                        "len_corpus_tokens = 15\n",
                        "\n",
                        "while len(corpus_tokens) < len_corpus_tokens:\n",
                        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
                        "    print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")\n",
                        "\n",
                        "    # If there are no more consecutive tokens break the loop\n",
                        "    if max_consecutive_token is None:\n",
                        "        break\n",
                        "\n",
                        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
                        "    print(f\"New corpus tokens: {corpus_tokens}\")\n",
                        "\n",
                        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
                        "    print(f\"New tokens by word appearance: {dict_tokens_by_word_appearance}\\n\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora que já vimos como treinar o tokenizador BPE, vamos treiná-lo do zero para consolidar nosso conhecimento."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 46,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
                                    "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
                                    "Number of initial corpus tokens: 7\n"
                              ]
                        }
                  ],
                  "source": [
                        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
                        "\n",
                        "# Concatenate all the words in the corpus\n",
                        "initial_corpus_tokens = \"\"\n",
                        "for word in corpus_words:\n",
                        "    initial_corpus_tokens += word\n",
                        "\n",
                        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
                        "corpus_tokens = set(initial_corpus_tokens)\n",
                        "\n",
                        "print(f\"Corpus words: {corpus_words}\")\n",
                        "print(f\"Initial corpus tokens: {corpus_tokens}\")\n",
                        "print(f\"Number of initial corpus tokens: {len(corpus_tokens)}\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 47,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
                                          " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
                                          " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
                                          " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
                                          " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
                                    ]
                              },
                              "execution_count": 47,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "corpus = [\n",
                        "    \"hug hug hug pun pun bun hugs\",\n",
                        "    \"hug hug pug pug pun pun hugs\",\n",
                        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
                        "    \"pug pun pun pun bun hugs\",\n",
                        "    \"hug hug hug pun bun bun hugs\",\n",
                        "]\n",
                        "\n",
                        "num_hug = 0\n",
                        "num_pug = 0\n",
                        "num_pun = 0\n",
                        "num_bun = 0\n",
                        "num_hugs = 0\n",
                        "\n",
                        "for sentence in corpus:\n",
                        "    words = sentence.split(\" \")\n",
                        "    for word in words:\n",
                        "        if word == \"hug\":\n",
                        "            num_hug += 1\n",
                        "        elif word == \"pug\":\n",
                        "            num_pug += 1\n",
                        "        elif word == \"pun\":\n",
                        "            num_pun += 1\n",
                        "        elif word == \"bun\":\n",
                        "            num_bun += 1\n",
                        "        elif word == \"hugs\":\n",
                        "            num_hugs += 1\n",
                        "\n",
                        "dict_tokens_by_word_appearance = {\n",
                        "    \"hug\":\n",
                        "        {\n",
                        "            \"count\": num_hug,\n",
                        "            \"tokens\": [character for character in \"hug\"],\n",
                        "        },\n",
                        "    \"pug\":\n",
                        "        {\n",
                        "            \"count\": num_pug,\n",
                        "            \"tokens\": [character for character in \"pug\"],\n",
                        "        },\n",
                        "    \"pun\":\n",
                        "        {\n",
                        "            \"count\": num_pun,\n",
                        "            \"tokens\": [character for character in \"pun\"],\n",
                        "        },\n",
                        "    \"bun\":\n",
                        "        {\n",
                        "            \"count\": num_bun,\n",
                        "            \"tokens\": [character for character in \"bun\"],\n",
                        "        },\n",
                        "    \"hugs\":\n",
                        "        {\n",
                        "            \"count\": num_hugs,\n",
                        "            \"tokens\": [character for character in \"hugs\"],\n",
                        "        },\n",
                        "}\n",
                        "\n",
                        "dict_tokens_by_word_appearance"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nós o treinamos do zero para obter um corpus de 15 tokens."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 48,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Initial corpus tokens: (7) {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
                                    "New corpus tokens: (15) {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n"
                              ]
                        }
                  ],
                  "source": [
                        "len_corpus_tokens = 15\n",
                        "print(f\"Initial corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")\n",
                        "\n",
                        "while len(corpus_tokens) < len_corpus_tokens:\n",
                        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
                        "\n",
                        "    # If there are no more consecutive tokens break the loop\n",
                        "    if max_consecutive_token is None:\n",
                        "        break\n",
                        "\n",
                        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
                        "\n",
                        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
                        "\n",
                        "print(f\"New corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Tokenização"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se agora quiséssemos tokenizar, primeiro teríamos que criar um vocabulário, ou seja, atribuir um ID a cada token."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 54,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Vocabulary: \n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'p': 0,\n",
                                          " 'hu': 1,\n",
                                          " 'sug': 2,\n",
                                          " 'npu': 3,\n",
                                          " 'ugpu': 4,\n",
                                          " 'gug': 5,\n",
                                          " 'u': 6,\n",
                                          " 'ug': 7,\n",
                                          " 'ughu': 8,\n",
                                          " 'n': 9,\n",
                                          " 'pu': 10,\n",
                                          " 'un': 11,\n",
                                          " 'puun': 12,\n",
                                          " 's': 13,\n",
                                          " 'h': 14,\n",
                                          " 'gs': 15,\n",
                                          " 'g': 16,\n",
                                          " 'b': 17}"
                                    ]
                              },
                              "execution_count": 54,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "vocab = {}\n",
                        "for i, token in enumerate(corpus_tokens):\n",
                        "    vocab[token] = i\n",
                        "\n",
                        "print(f\"Vocabulary: \")\n",
                        "vocab"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nós o colocamos em uma função"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 55,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def get_vocabulary(corpus_tokens):\n",
                        "    vocab = {}\n",
                        "    for i, token in enumerate(corpus_tokens):\n",
                        "        vocab[token] = i\n",
                        "    return vocab"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Compromissos corretos"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 56,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Vocabulary: \n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'p': 0,\n",
                                          " 'hu': 1,\n",
                                          " 'sug': 2,\n",
                                          " 'npu': 3,\n",
                                          " 'ugpu': 4,\n",
                                          " 'gug': 5,\n",
                                          " 'u': 6,\n",
                                          " 'ug': 7,\n",
                                          " 'ughu': 8,\n",
                                          " 'n': 9,\n",
                                          " 'pu': 10,\n",
                                          " 'un': 11,\n",
                                          " 'puun': 12,\n",
                                          " 's': 13,\n",
                                          " 'h': 14,\n",
                                          " 'gs': 15,\n",
                                          " 'g': 16,\n",
                                          " 'b': 17}"
                                    ]
                              },
                              "execution_count": 56,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "vocab = get_vocabulary(corpus_tokens)\n",
                        "print(f\"Vocabulary: \")\n",
                        "vocab"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se agora quisermos tokenizar a palavra `bug`, podemos fazer o seguinte"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 63,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Maximum length of tokens: 4\n",
                                    "Prefix: bug\n",
                                    "Prefix: bug\n",
                                    "Prefix: bu\n",
                                    "Prefix: b\n",
                                    "prefix b is in the vocabulary\n",
                                    "Prefix: ug\n",
                                    "prefix ug is in the vocabulary\n",
                                    "Tokens: ['b', 'ug']\n"
                              ]
                        }
                  ],
                  "source": [
                        "word = 'bug'\n",
                        "\n",
                        "# Get the maximum length of tokens\n",
                        "max_len = max(len(token) for token in vocab)\n",
                        "print(f\"Maximum length of tokens: {max_len}\")\n",
                        "\n",
                        "# Create a empty list of tokens\n",
                        "tokens = []\n",
                        "while len(word) > 0:\n",
                        "    # Flag to check if the token is found\n",
                        "    found = False\n",
                        "\n",
                        "    # Iterate over the maximum length of tokens from max_len to 0\n",
                        "    for i in range(max_len, 0, -1):\n",
                        "        # Get the prefix of the word\n",
                        "        prefix = word[:i]\n",
                        "        print(f\"Prefix: {prefix}\")\n",
                        "\n",
                        "        # Check if the prefix is in the vocabulary\n",
                        "        if prefix in vocab:\n",
                        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
                        "            tokens.append(prefix)\n",
                        "            word = word[i:]\n",
                        "            found = True\n",
                        "            break\n",
                        "    \n",
                        "    # if not found:\n",
                        "    #     tokens.append('<UNK>')\n",
                        "    #     word = word[1:]\n",
                        "\n",
                        "print(f\"Tokens: {tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Mas se agora quisermos tokenizar a palavra `mug`, não poderemos, porque o caractere `m` não está no vocabulário, então o tokenizaremos com o token `<UNK>`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 70,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Maximum length of tokens: 4\n",
                                    "Prefix: mug\n",
                                    "Prefix: mug\n",
                                    "Prefix: mu\n",
                                    "Prefix: m\n",
                                    "Prefix: ug\n",
                                    "prefix ug is in the vocabulary\n",
                                    "Tokens: ['<UNK>', 'ug']\n"
                              ]
                        }
                  ],
                  "source": [
                        "word = 'mug'\n",
                        "\n",
                        "# Get the maximum length of tokens\n",
                        "max_len = max(len(token) for token in vocab)\n",
                        "print(f\"Maximum length of tokens: {max_len}\")\n",
                        "\n",
                        "# Create a empty list of tokens\n",
                        "tokens = []\n",
                        "while len(word) > 0:\n",
                        "    # Flag to check if the token is found\n",
                        "    found = False\n",
                        "\n",
                        "    # Iterate over the maximum length of tokens from max_len to 0\n",
                        "    for i in range(max_len, 0, -1):\n",
                        "        # Get the prefix of the word\n",
                        "        prefix = word[:i]\n",
                        "        print(f\"Prefix: {prefix}\")\n",
                        "\n",
                        "        # Check if the prefix is in the vocabulary\n",
                        "        if prefix in vocab:\n",
                        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
                        "            tokens.append(prefix)\n",
                        "            word = word[i:]\n",
                        "            found = True\n",
                        "            break\n",
                        "\n",
                        "    if not found:\n",
                        "        tokens.append('<UNK>')\n",
                        "        word = word[1:]\n",
                        "\n",
                        "print(f\"Tokens: {tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nós o colocamos em uma função"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 71,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def tokenize_word(word, vocab):\n",
                        "    # Get the maximum length of tokens\n",
                        "    max_len = max(len(token) for token in vocab)\n",
                        "\n",
                        "    # Create a empty list of tokens\n",
                        "    tokens = []\n",
                        "    while len(word) > 0:\n",
                        "        # Flag to check if the token is found\n",
                        "        found = False\n",
                        "\n",
                        "        # Iterate over the maximum length of tokens from max_len to 0\n",
                        "        for i in range(max_len, 0, -1):\n",
                        "            # Get the prefix of the word\n",
                        "            prefix = word[:i]\n",
                        "\n",
                        "            # Check if the prefix is in the vocabulary\n",
                        "            if prefix in vocab:\n",
                        "                tokens.append(prefix)\n",
                        "                word = word[i:]\n",
                        "                found = True\n",
                        "                break\n",
                        "\n",
                        "        if not found:\n",
                        "            tokens.append('<UNK>')\n",
                        "            word = word[1:]\n",
                        "\n",
                        "    return tokens"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Verificamos se está tudo bem"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 72,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Tokenization of the word 'bug': ['b', 'ug']\n",
                                    "Tokenization of the word 'mug': ['<UNK>', 'ug']\n"
                              ]
                        }
                  ],
                  "source": [
                        "print(f\"Tokenization of the word 'bug': {tokenize_word('bug', vocab)}\")\n",
                        "print(f\"Tokenization of the word 'mug': {tokenize_word('mug', vocab)}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Visualizador de tokens"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora que sabemos como funciona um tokenizador BPE, vamos ver como o visualizador [the-tokenizer-playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground) se pareceria com os tokens de qualquer frase"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "<iframe\n",
                        "\tsrc=\"https://xenova-the-tokenizer-playground.static.hf.space\"\n",
                        "\tframeborder=\"0\"\n",
                        "\twidth=\"850\"\n",
                        "\theight=\"450\"\n",
                        "></iframe>"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "base",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.8"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
