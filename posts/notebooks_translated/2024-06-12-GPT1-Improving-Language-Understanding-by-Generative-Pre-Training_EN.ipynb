{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT1 - Improving Language Understanding by Generative Pre-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) is the GPT1 paper. Before reading the post, it's necessary to set the context: before GPT, language models were based on recurrent neural networks (RNNs), which worked relatively well for specific tasks but could not reuse pre-training for fine-tuning for other tasks. Additionally, they had limited memory, so if very long sentences were input, they wouldn't remember the beginning of the sentence very well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before discussing the architecture of GPT-1, let's recall how the architecture of transformers worked.",
        "\n",
        "![transformer architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT1 is a model based on the decoders of transformers, so since we don't have an encoder, the architecture with a single decoder looks like this:",
        "\n",
        "![decoder architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer_decoder_only-scaled.webp)",
        "\n",
        "The attention mechanism between the encoder and decoder sentence is removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the GPT-1 paper, they propose the following architecture",
        "\n",
        "![gpt1 architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_architecture.webp)",
        "\n",
        "This corresponds to the decoder of a transformer as we have seen before, executed 12 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most interesting ideas from the paper are:",
        "\n",
        "* The model is trained on a large corpus of text without supervision. This results in the creation of a language model. A high-capacity language model is created from a large corpus of text.",
        "* Then fine-tuning is performed on supervised NLP tasks with labeled datasets. A fine adjustment is made in the target task with supervision. Additionally, when evaluating the model on the supervised task, it is not only evaluated for that task, but also for how well it predicts the next token, which helps improve the generalization of the supervised model and makes the model converge faster.",
        "* Although we have already mentioned it, the paper states that the transformer architecture is used, as RNNs were being used for language models up until that point. This led to an improvement in which what was learned during the first training (unsupervised training on the text corpus) is easier to transfer to supervised tasks. In other words, thanks to the use of transformers, it became possible to train on an entire text corpus and then perform fine-tunings on supervised tasks.",
        "* They evaluated the model on four types of language understanding tasks:",
        "* Natural language inference",
        "* Answers to questions",
        "* Semantic similarity",
        "* Text classification.",
        "* The general model (the one trained on the entire text corpus without supervision) outperforms discriminatively trained RNN models that use architectures specifically designed for each task, significantly improving the state of the art in 9 out of the 12 tasks studied. They also analyze the \"zero-shot\" behaviors of the pre-trained model in four different environments and demonstrated that it acquires useful linguistic knowledge for subsequent tasks.",
        "* In recent years, researchers had demonstrated the benefits of using embeddings, which are trained on unlabeled corpora, to improve performance across a variety of tasks. However, these approaches primarily transfer information at the word level, while the use of transformers trained on large unsupervised text corpora captures higher-level semantics at the phrase level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how to generate text with a pretrained GPT1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, you need to install `ftfy` and `spacy` via",
        "\n",
        "```bash\n",
        "pip install ftfy spacy",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once installed, you should download the language model of spacy that you want to use. For example, to download the English model, you can run:",
        "\n",
        "```bash\n",
        "python -m spacy download en_core_web_sm",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate text, we are going to use the model from the [GPT1](https://huggingface.co/openai-community/openai-gpt) repository of Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you notice, we have imported `OpenAIGPTTokenizer` and `AutoTokenizer`. This is because in the [model card](https://huggingface.co/openai-community/openai-gpt) of GPT1 it is indicated that `OpenAIGPTTokenizer` should be used, but in the library's [transformers](https://maximofn.com/hugging-face-transformers/) post we explain that `AutoTokenizer` should be used to load the tokenizer. So, let's try both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokens: \n",
            "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "input auto tokens: \n",
            "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "ckeckpoints = \"openai-community/openai-gpt\"\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(ckeckpoints)\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "\n",
        "input_tokens = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
        "input_auto_tokens = auto_tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
        "\n",
        "print(f\"input tokens: \\n{input_tokens}\")\n",
        "print(f\"input auto tokens: \\n{input_auto_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen with the two tokenizers, the same tokens are obtained. So, to make the code more general, so that if the checkpoints change, there is no need to change the code, we will use `AutoTokenizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create the device, the tokenizer, and the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have instantiated the model, let's see how many parameters it has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 117M\n"
          ]
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {round(params/1e6)}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the era of trillions of parameters, we can see that GPT1 only had 117 million parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the input tokens for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Hello, my dog is cute and\"\n",
        "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "input_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We pass them to the model to generate the output tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output tokens: \n",
            "tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,\n",
            "           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "output_tokens = model.generate(**input_tokens)\n",
        "\n",
        "print(f\"output tokens: \\n{output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We decode the tokens to obtain the output sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded output: \n",
            "hello, my dog is cute and i'm going to take him for a walk. \" \n",
            " \"\n"
          ]
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded output: \\n{decoded_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have already managed to generate text with GPT1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate text token by token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Greedy search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have used `model.generate` to generate the output tokens all at once, but we are going to see how to generate them one by one. For this, instead of using `model.generate`, we will use `model`, which actually calls the `model.forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
              "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
              "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
              "         ...,\n",
              "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
              "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
              "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that it outputs many data points, first let's look at the keys of the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['logits'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case we only have the logits of the model, let's check their size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 40478])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits = outputs.logits\n",
        "\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how many tokens we had at the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tokens.input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well, at the output we have the same number of logits as at the input. This is normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain the logits from the last position of the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40478])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nex_token_logits = logits[0,-1]\n",
        "\n",
        "nex_token_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a total of 40478 logits, meaning there is a vocabulary of 40478 tokens and we need to determine which token has the highest probability. To do this, we first calculate the softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40478])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "\n",
        "softmax_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1898, device='cuda:0', grad_fn=<MaxBackward0>),\n",
              " tensor(249, device='cuda:0'))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "\n",
        "next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have obtained the following token, now we decode it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(next_token_id.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtained the following token using the greedy method, that is, the token with the highest probability. But we already saw in the post about the transformers library, the [ways to generate text](https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto) such as sampling, top-k, top-p, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's put everything into a function and see what comes out if we generate a few tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_next_greedy_token(input_sentence, tokenizer, model, device):\n",
        "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**input_tokens)\n",
        "    logits = outputs.logits\n",
        "    nex_token_logits = logits[0,-1]\n",
        "    softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "    next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "    return next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_greedy_text(input_sentence, tokenizer, model, device, max_length=20):\n",
        "    generated_text = input_sentence\n",
        "    for _ in range(max_length):\n",
        "        next_token_prob, next_token_id = generate_next_greedy_token(generated_text, tokenizer, model, device)\n",
        "        generated_text += tokenizer.decode(next_token_id.item())\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello, my dog is cute andi.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_greedy_text(\"Hello, my dog is cute and\", tokenizer, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is quite repetitive, as was already seen in the [ways of generating texts](https://maximofn.com/hugging-face-transformers/#Ways-of-text-generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tuning GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculation of the Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before starting to fine-tune GPT1, let's look at something. Previously, when we obtained the model's output, we did this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
              "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
              "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
              "         ...,\n",
              "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
              "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
              "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen that we get `loss=None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we will need the loss to perform fine-tuning, let's see how to obtain it.",
        "\n",
        "If we go to the documentation of the [forward](https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward) method of `OpenAIGPTLMHeadModel`, we can see that it states that the output is an object of type `transformers.modeling_outputs.CausalLMOutput`. So, if we go to the documentation of [transformers.modeling_outputs.CausalLMOutput](https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput), we can see that it states that it returns `loss` if `labels` are passed to the `forward` method.",
        "\n",
        "If we go to the source code of the [forward](https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544) method, we see this block of code",
        "\n",
        "```python\n",
        "loss = None",
        "if labels is not None:",
        "# Shift so that tokens < n predict n",
        "shift_logits = lm_logits[..., :-1, :].contiguous()",
        "shift_labels = labels[..., 1:].contiguous()",
        "# Flatten the tokens",
        "loss_fct = CrossEntropyLoss()",
        "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))",
        "```\n",
        "\n",
        "That is, the `loss` is calculated as follows",
        "\n",
        "* Logits and labels shift: The first part is to shift the logits (`lm_logits`) and the labels (`labels`) so that the `tokens < n` predict `n`, meaning from position `n` it predicts the next token based on the previous ones.",
        "* CrossEntropyLoss: An instance of the loss function `CrossEntropyLoss()` is created.",
        "* Flatten tokens: Next, the logits and labels are flattened using `view(-1, shift_logits.size(-1))` and `view(-1)`, respectively. This is done to ensure that the logits and labels have the same shape for the loss function.",
        "* Calculation of the loss: Finally, the loss is calculated using the `CrossEntropyLoss()` function with the flattened logits and flattened labels as inputs.",
        "\n",
        "In summary, the `loss` is calculated as the cross-entropy loss between the shifted and flattened logits and the shifted and flattened labels.",
        "\n",
        "Therefore, if we pass the labels to the `forward` method, it will return the `loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.2607, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens, labels=input_tokens.input_ids)\n",
        "\n",
        "outputs.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the training, we are going to use an English jokes dataset [short-jokes-dataset](https://huggingface.co/datasets/Maximofn/short-jokes-dataset), which is a dataset with 231 thousand English jokes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'Joke'],\n",
              "        num_rows: 231657\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
        "jokes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ID': 1,\n",
              " 'Joke': '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jokes[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First let's see how the training would be done with pure Pytorch",
        "\n",
        "> We restart the notebook to avoid issues with GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import OpenAIGPTLMHeadModel, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ckeckpoints = \"openai-community/openai-gpt\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pytorch dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a Dataset class in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.joke = \"JOKE: \"\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset[\"train\"])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = self.joke + self.dataset[\"train\"][item][\"Joke\"] + self.end_of_text_token\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        return sentence, tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We instantiate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = JokesDataset(jokes, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo<|endoftext|>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 30]), torch.Size([1, 30]))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence, tokens = dataset[5]\n",
        "print(sentence)\n",
        "tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create a Pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 1\n",
        "joke_dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences, tokens = next(iter(joke_dataloader))\n",
        "len(sentences), tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:31<00:00, 334.88it/s, loss=2.88, lr=2.93e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:30<00:00, 335.27it/s, loss=2.49, lr=5.87e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 2 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:17<00:00, 341.75it/s, loss=2.57, lr=8.81e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 3 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:18<00:00, 341.27it/s, loss=2.41, lr=1.18e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 4 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:19<00:00, 341.04it/s, loss=2.49, lr=1.47e-5]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import tqdm\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 500\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "proc_seq_count = 0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_jokes_tens = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    progress_bar = tqdm.tqdm(joke_dataloader, desc=\"Training\")\n",
        "    \n",
        "    for sample in progress_bar:\n",
        "\n",
        "        sentence, tokens = sample\n",
        "        \n",
        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
        "        joke_tens = tokens.input_ids[0].to(device)\n",
        "\n",
        "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        # The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            # The next joke does not fit in so we process the sequence and leave the last joke \n",
        "            # as the start for next sequence \n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                #Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "        ################## Sequence ready, process it trough the model ##################\n",
        "            \n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        if batch_count == 10:\n",
        "            batch_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how well the model tells jokes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "joke : what do you call a group of people who are not afraid of the dark? a group\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"JOKE:\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that you pass it a sequence with the word `joke` and it returns a joke. But if you return another sequence, it does not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "my dog is cute and i'm not sure if i should be offended or not. \" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"My dog is cute and\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-12",
      "description_en": "Unlock the power of language!!!! \ud83d\udca5 In my last post, I take you by the hand through the GPT-1 paper, explaining in a clear and concise way how this pioneer model in natural language processing works. And not only that! I also show you how to fine-tuning the model so you can adapt it to your specific needs \ud83d\udcca Don't miss the opportunity to learn about one of the most influential models in history! \ud83d\ude80 Read my post and find out how you can improve your artificial intelligence skills! \ud83d\udcc4",
      "description_es": "\u00a1\u00a1\u00a1Desbloquea el poder del lenguaje!!! \ud83d\udca5 En mi \u00faltimo post, te llevo de la mano a trav\u00e9s del paper de GPT-1, explicando de manera clara y concisa c\u00f3mo funciona este modelo pionero en el procesamiento de lenguaje natural. \u00a1Y no solo eso! Tambi\u00e9n te muestro c\u00f3mo hacer un fine-tuning del modelo para que puedas adaptarlo a tus necesidades espec\u00edficas \ud83d\udcca. \u00a1No te pierdas la oportunidad de aprender sobre uno de los modelos m\u00e1s influyentes de la historia! \ud83d\ude80 \u00a1Lee mi post y descubre c\u00f3mo puedes mejorar tus habilidades en inteligencia artificial! \ud83d\udcc4",
      "description_pt": "Desbloqueie o poder da linguagem!!!! \ud83d\udca5 Em minha \u00faltima postagem, apresentei o artigo GPT-1, explicando de forma clara e concisa como funciona esse modelo pioneiro no processamento de linguagem natural. E n\u00e3o \u00e9 s\u00f3 isso! Tamb\u00e9m mostro como fazer o ajuste fino do modelo para que voc\u00ea possa adapt\u00e1-lo \u00e0s suas necessidades espec\u00edficas \ud83d\udcca N\u00e3o perca a oportunidade de conhecer um dos modelos mais influentes da hist\u00f3ria! \ud83d\ude80 Leia minha postagem e descubra como voc\u00ea pode melhorar suas habilidades de intelig\u00eancia artificial! \ud83d\udcc4",
      "end_url": "gpt1",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_thumnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_thumnail.webp",
      "keywords_en": "gpt1, nlp, transformers, fine-tuning, language model, hugging face, pytorch",
      "keywords_es": "gpt1, nlp, transformers, fine-tuning, modelo de lenguaje, hugging face, pytorch",
      "keywords_pt": "gpt1, nlp, transformers, ajuste fino, modelo de linguagem, hugging face, pytorch",
      "title_en": "GPT1 \u2013 Improving Language Understanding by Generative Pre-Training",
      "title_es": "GPT1 \u2013 Improving Language Understanding by Generative Pre-Training",
      "title_pt": "GPT1 \u2013 Improving Language Understanding by Generative Pre-Training"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}