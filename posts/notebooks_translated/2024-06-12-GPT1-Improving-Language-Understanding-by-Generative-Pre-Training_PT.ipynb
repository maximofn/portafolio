{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# GPT1 - Aprimorando a compreensão da linguagem por meio de pré-treinamento generativo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Papel"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..\n",
                        "\n",
                        "[Improving Language Understanding by Generative Pre-Training] (https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) é o artigo GPT1. Antes de ler a postagem, é necessário se colocar na situação: antes do GPT, os modelos de linguagem eram baseados em redes recorrentes (RNN), que eram redes que funcionavam relativamente bem para tarefas específicas, mas com as quais não era possível reutilizar o pré-treinamento para fazer um ajuste fino para outras tarefas. Elas também não tinham muita memória, portanto, se você colocasse frases muito longas nelas, elas não se lembravam muito bem do início da frase."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Arquitetura"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Antes de falarmos sobre a arquitetura do GPT1, vamos nos lembrar de como era a arquitetura dos Transformers.\n",
                        "\n",
                        "![arquitetura do transformador](https://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "O GPT1 é um modelo baseado nos decodificadores de transformador, portanto, como não temos um codificador, a arquitetura de um único decodificador é a seguinte\n",
                        "\n",
                        "![arquitetura do decodificador](https://maximofn.com/wp-content/uploads/2024/06/transformer_decoder_only-scaled.webp)\n",
                        "\n",
                        "O mecanismo de atenção entre a sentença do codificador e do decodificador é eliminado."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "No documento GPT1, eles propõem a seguinte arquitetura\n",
                        "\n",
                        "![arquitetura gpt1](https://maximofn.com/wp-content/uploads/2024/06/GPT1_architecture.webp)\n",
                        "\n",
                        "O que corresponde ao decodificador de um transformador, como vimos anteriormente, executado 12 vezes."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Resumo do artigo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As ideias mais interessantes do artigo são:\n",
                        "\n",
                        " * O modelo é treinado em um grande corpus de texto não supervisionado. Isso cria um modelo de linguagem. Um modelo de linguagem de alta capacidade é criado em um grande corpus de texto.\n",
                        " * O ajuste fino é então realizado em tarefas supervisionadas de NLP com conjuntos de dados rotulados. O ajuste fino é realizado em uma tarefa-alvo supervisionada. Além disso, quando o modelo é avaliado na tarefa supervisionada, ele não é avaliado apenas nessa tarefa, mas em quão bem ele prevê o próximo token, o que ajuda a aprimorar a generalização do modelo supervisionado e faz com que o modelo converse mais rapidamente.\n",
                        " * Embora já tenhamos mencionado isso, o documento diz que a arquitetura do transformador é usada, já que até aquele momento os RNNs eram usados para os modelos de linguagem. Isso levou a uma melhoria no sentido de que o que foi aprendido no primeiro treinamento (treinamento no corpus de texto não supervisionado) é mais fácil de transferir para tarefas supervisionadas. Ou seja, graças ao uso de transformadores, foi possível treinar em um corpus inteiro de texto e depois fazer o ajuste fino em tarefas supervisionadas.\n",
                        " * Eles testaram o modelo em quatro tipos de tarefas de compreensão de linguagem:\n",
                        "    * Inferência de linguagem natural\n",
                        "    * Resposta às perguntas\n",
                        "    * Similaridade semântica\n",
                        "    * Classificação de textos.\n",
                        " * O modelo geral (aquele treinado em todo o corpus de texto não supervisionado) supera os modelos RNN treinados de forma discriminatória que empregam arquiteturas específicas de tarefas, melhorando significativamente o estado da arte em 9 das 12 tarefas estudadas. Eles também analisaram os comportamentos de \"disparo zero\" do modelo pré-treinado em quatro ambientes diferentes e mostraram que ele adquire conhecimento linguístico útil para tarefas subsequentes.\n",
                        " * Nos últimos anos, os pesquisadores demonstraram os benefícios do uso de embeddings, que são treinados em corpora não rotulados, para melhorar o desempenho em várias tarefas. No entanto, essas abordagens transferem informações principalmente no nível da palavra, enquanto o uso de transformadores treinados em grandes corpora de texto não supervisionados captura a semântica de nível superior, no nível da frase."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Geração de texto"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos ver como gerar texto com um GPT1 pré-treinado."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Primeiro, você precisa instalar o `ftfy` e o `spacy` via\n",
                        "\n",
                        "````bash\n",
                        "pip install ftfy spacy\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Depois de instalado, você deve fazer o download do modelo de idioma spacy que deseja usar. Por exemplo, para fazer o download do modelo em inglês, você pode executar:\n",
                        "\n",
                        "````bash\n",
                        "python -m spacy download en_core_web_sm\n",
                        "```"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para gerar texto, usaremos o modelo do repositório [GPT1](https://huggingface.co/openai-community/openai-gpt) do Hugging Face."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Importamos as bibliotecas"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, AutoTokenizer"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se você notar, importamos o `OpenAIGPTTokenizer` e o `AutoTokenizer`. Isso ocorre porque no [model card](https://huggingface.co/openai-community/openai-gpt) do GPT1 diz para usar o `OpenAIGPTTokenizer`, mas na postagem da biblioteca [transformers](https://maximofn.com/hugging-face-transformers/) explicamos que você deve usar o `AutoTokenizer` para carregar o tokenizador. Então, vamos tentar os dois"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "input tokens: \n",
                                    "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
                                    "input auto tokens: \n",
                                    "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
                              ]
                        }
                  ],
                  "source": [
                        "ckeckpoints = \"openai-community/openai-gpt\"\n",
                        "tokenizer = OpenAIGPTTokenizer.from_pretrained(ckeckpoints)\n",
                        "auto_tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
                        "\n",
                        "input_tokens = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
                        "input_auto_tokens = auto_tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
                        "\n",
                        "print(f\"input tokens: \\n{input_tokens}\")\n",
                        "print(f\"input auto tokens: \\n{input_auto_tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como você pode ver, com os dois tokenizadores você obtém os mesmos tokens. Portanto, para tornar o código mais geral, de modo que, se você alterar os pontos de verificação, não precisará alterar o código, vamos usar o `AutoTokenizer`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Em seguida, criamos o dispositivo, o tokenizador e o modelo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "\n",
                        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
                        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints).to(device)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como instanciamos o modelo, vamos ver quantos parâmetros ele tem"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Number of parameters: 117M\n"
                              ]
                        }
                  ],
                  "source": [
                        "params = sum(p.numel() for p in model.parameters())\n",
                        "print(f\"Number of parameters: {round(params/1e6)}M\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Na era de bilhões de parâmetros, podemos ver que o GPT1 tinha apenas 117 milhões de parâmetros."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos os tokens de entrada para o modelo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
                                    ]
                              },
                              "execution_count": 4,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "input_sentence = \"Hello, my dog is cute and\"\n",
                        "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
                        "\n",
                        "input_tokens"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nós os passamos para o modelo para gerar os tokens de saída."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "output tokens: \n",
                                    "tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,\n",
                                    "           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],\n",
                                    "       device='cuda:0')\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
                                    "  warnings.warn(\n"
                              ]
                        }
                  ],
                  "source": [
                        "output_tokens = model.generate(**input_tokens)\n",
                        "\n",
                        "print(f\"output tokens: \\n{output_tokens}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Decodificamos os tokens para obter a declaração de saída"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "decoded output: \n",
                                    "hello, my dog is cute and i'm going to take him for a walk. \" \n",
                                    " \"\n"
                              ]
                        }
                  ],
                  "source": [
                        "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
                        "\n",
                        "print(f\"decoded output: \\n{decoded_output}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Já conseguimos gerar texto com o GPT1"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Gerar token para o texto do token"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Greedy search"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Usamos o `model.generate` para gerar os tokens de saída de uma só vez, mas vamos ver como gerá-los um a um. Para fazer isso, em vez de usar `model.generate`, usaremos `model`, que, na verdade, chama o método `model.forward`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 7,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
                                          "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
                                          "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
                                          "         ...,\n",
                                          "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
                                          "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
                                          "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
                                          "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
                                    ]
                              },
                              "execution_count": 7,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "outputs = model(**input_tokens)\n",
                        "\n",
                        "outputs"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos que isso gera muitos dados, mas primeiro vamos dar uma olhada nas chaves de saída."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "odict_keys(['logits'])"
                                    ]
                              },
                              "execution_count": 8,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "outputs.keys()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nesse caso, temos apenas os logits do modelo, vamos ver seu tamanho."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([1, 7, 40478])"
                                    ]
                              },
                              "execution_count": 9,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "logits = outputs.logits\n",
                        "\n",
                        "logits.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos ver quantos tokens tínhamos na entrada."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([1, 7])"
                                    ]
                              },
                              "execution_count": 10,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "input_tokens.input_ids.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Bem, temos o mesmo número de logits na saída e na entrada. Isso é normal"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Obtemos os logits da última posição da saída"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 11,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([40478])"
                                    ]
                              },
                              "execution_count": 11,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "nex_token_logits = logits[0,-1]\n",
                        "\n",
                        "nex_token_logits.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Há um total de 40478 logits, ou seja, há um vocabulário de 40478 tokens e temos que ver qual token tem a maior probabilidade."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([40478])"
                                    ]
                              },
                              "execution_count": 12,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
                        "\n",
                        "softmax_logits.shape"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(tensor(0.1898, device='cuda:0', grad_fn=<MaxBackward0>),\n",
                                          " tensor(249, device='cuda:0'))"
                                    ]
                              },
                              "execution_count": 13,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
                        "\n",
                        "next_token_prob, next_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Obtivemos o seguinte token, agora vamos decodificá-lo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 14,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "'i'"
                                    ]
                              },
                              "execution_count": 14,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "tokenizer.decode(next_token_id.item())"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Obtivemos o seguinte token usando o método guloso, ou seja, o token com a maior probabilidade. Mas já vimos na postagem sobre a biblioteca de transformadores, as [formas de gerar textos] (https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto) que podem ser feitas por amostragem, top-k, top-p, etc."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos colocar tudo em uma função e ver o que acontece se gerarmos alguns tokens."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 15,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def generate_next_greedy_token(input_sentence, tokenizer, model, device):\n",
                        "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
                        "    outputs = model(**input_tokens)\n",
                        "    logits = outputs.logits\n",
                        "    nex_token_logits = logits[0,-1]\n",
                        "    softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
                        "    next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
                        "    return next_token_prob, next_token_id"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 16,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def generate_greedy_text(input_sentence, tokenizer, model, device, max_length=20):\n",
                        "    generated_text = input_sentence\n",
                        "    for _ in range(max_length):\n",
                        "        next_token_prob, next_token_id = generate_next_greedy_token(generated_text, tokenizer, model, device)\n",
                        "        generated_text += tokenizer.decode(next_token_id.item())\n",
                        "    return generated_text"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora vamos gerar o texto"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 17,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "'Hello, my dog is cute andi.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
                                    ]
                              },
                              "execution_count": 17,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "generate_greedy_text(\"Hello, my dog is cute and\", tokenizer, model, device)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "O resultado é bastante repetitivo, como já foi visto em [ways to generate text] (https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Ajuste fino do GPT"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Cálculo da perda"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Antes de começarmos a fazer o ajuste fino do GPT1, vamos dar uma olhada em um aspecto. Antes, quando costumávamos obter a saída do modelo, fazíamos o seguinte"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 19,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
                                          "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
                                          "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
                                          "         ...,\n",
                                          "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
                                          "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
                                          "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
                                          "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
                                    ]
                              },
                              "execution_count": 19,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "outputs = model(**input_tokens)\n",
                        "\n",
                        "outputs"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Você pode ver que obtemos `loss=None`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 20,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "None\n"
                              ]
                        }
                  ],
                  "source": [
                        "print(outputs.loss)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como precisaremos da perda para fazer o ajuste fino, vamos ver como obtê-la.\n",
                        "\n",
                        "Se consultarmos a documentação do método [forward](https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward) de `OpenAIGPTLMHeadModel`, veremos que ele diz que a saída retorna um objeto do tipo `transformers.modeling_outputs.CausalLMOutput`, portanto, se consultarmos a documentação de [transformers.modeling_outputs.CausalLMOutput](https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput), veremos que ele diz que retorna `loss` se `labels` for passado para o método `forward`.\n",
                        "\n",
                        "Se acessarmos o código-fonte do método [forward](https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544), veremos este bloco de código\n",
                        "\n",
                        "````python\n",
                        "        perda = Nenhuma\n",
                        "        se labels não for None:\n",
                        "            # Deslocamento de modo que os tokens < n prevejam n\n",
                        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
                        "            shift_labels = labels[..., 1:].contiguous()\n",
                        "            # Achatar os tokens\n",
                        "            loss_fct = CrossEntropyLoss()\n",
                        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
                        "```\n",
                        "\n",
                        "Em outras palavras, a \"perda\" é calculada da seguinte forma\n",
                        "\n",
                        " * Deslocamento de logits e rótulos: a primeira parte é deslocar logits (`lm_logits`) e rótulos (`labels`) para que `tokens < n` prevejam `n`, ou seja, a partir de uma posição `n`, o próximo token é previsto a partir dos anteriores.\n",
                        " * CrossEntropyLoss: é criada uma instância da função de perda `CrossEntropyLoss()`.\n",
                        " * Achatar tokens: os logits e os rótulos são achatados usando `view(-1, shift_logits.size(-1))` e `view(-1)`, respectivamente. Isso é feito para que os logits e os rótulos tenham a mesma forma para a função de perda.\n",
                        " * Cálculo da perda: finalmente, a perda é calculada usando a função de perda `CrossEntropyLoss()` com os logits achatados e os rótulos achatados como entradas.\n",
                        "\n",
                        "Em resumo, a \"perda\" é calculada como a perda de entropia cruzada entre os logits deslocados e achatados e os rótulos deslocados e achatados.\n",
                        "\n",
                        "Portanto, se passarmos os rótulos para o método `forward`, ele retornará a `perda`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 21,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "tensor(4.2607, device='cuda:0', grad_fn=<NllLossBackward0>)"
                                    ]
                              },
                              "execution_count": 21,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "outputs = model(**input_tokens, labels=input_tokens.input_ids)\n",
                        "\n",
                        "outputs.loss"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Conjunto de dados"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para o treinamento, usaremos um conjunto de dados de piadas em inglês [short-jokes-dataset] (https://huggingface.co/datasets/Maximofn/short-jokes-dataset), que é um conjunto de dados com 231 mil piadas em inglês."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Baixamos o conjunto de dados"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 22,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetDict({\n",
                                          "    train: Dataset({\n",
                                          "        features: ['ID', 'Joke'],\n",
                                          "        num_rows: 231657\n",
                                          "    })\n",
                                          "})"
                                    ]
                              },
                              "execution_count": 22,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from datasets import load_dataset\n",
                        "\n",
                        "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
                        "jokes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos dar uma olhada nisso"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 23,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'ID': 1,\n",
                                          " 'Joke': '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"'}"
                                    ]
                              },
                              "execution_count": 23,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "jokes[\"train\"][0]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Treinamento Pytorch"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Primeiro, vamos dar uma olhada em como o treinamento puro do Pytorch seria feito.\n",
                        "\n",
                        "> Reinicie o notebook para que não haja problemas de memória da GPU"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 24,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "from transformers import OpenAIGPTLMHeadModel, AutoTokenizer\n",
                        "\n",
                        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "\n",
                        "ckeckpoints = \"openai-community/openai-gpt\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
                        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints)\n",
                        "\n",
                        "model = model.to(device)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Conjunto de dados Pytorch"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criar uma classe de conjunto de dados do Pytorch"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 25,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from torch.utils.data import Dataset\n",
                        "\n",
                        "class JokesDataset(Dataset):\n",
                        "    def __init__(self, dataset, tokenizer):\n",
                        "        self.dataset = dataset\n",
                        "        self.joke = \"JOKE: \"\n",
                        "        self.end_of_text_token = \"<|endoftext|>\"\n",
                        "        self.tokenizer = tokenizer\n",
                        "        \n",
                        "    def __len__(self):\n",
                        "        return len(self.dataset[\"train\"])\n",
                        "\n",
                        "    def __getitem__(self, item):\n",
                        "        sentence = self.joke + self.dataset[\"train\"][item][\"Joke\"] + self.end_of_text_token\n",
                        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
                        "        return sentence, tokens"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nós o instanciamos"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 26,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "dataset = JokesDataset(jokes, tokenizer=tokenizer)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aqui está um exemplo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 27,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo<|endoftext|>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "(torch.Size([1, 30]), torch.Size([1, 30]))"
                                    ]
                              },
                              "execution_count": 27,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "sentence, tokens = dataset[5]\n",
                        "print(sentence)\n",
                        "tokens.input_ids.shape, tokens.attention_mask.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Dataloader"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, criamos um carregador de dados do Pytorch"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 28,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from torch.utils.data import DataLoader\n",
                        "\n",
                        "BS = 1\n",
                        "joke_dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos um lote"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 29,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))"
                                    ]
                              },
                              "execution_count": 29,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "sentences, tokens = next(iter(joke_dataloader))\n",
                        "len(sentences), tokens.input_ids.shape, tokens.attention_mask.shape"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Treinamento"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 30,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
                                    "  warnings.warn(\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "EPOCH 0 started==============================\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Training: 100%|██████████| 231657/231657 [11:31<00:00, 334.88it/s, loss=2.88, lr=2.93e-6]\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "EPOCH 1 started==============================\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Training: 100%|██████████| 231657/231657 [11:30<00:00, 335.27it/s, loss=2.49, lr=5.87e-6]\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "EPOCH 2 started==============================\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Training: 100%|██████████| 231657/231657 [11:17<00:00, 341.75it/s, loss=2.57, lr=8.81e-6]\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "EPOCH 3 started==============================\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Training: 100%|██████████| 231657/231657 [11:18<00:00, 341.27it/s, loss=2.41, lr=1.18e-5]\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "EPOCH 4 started==============================\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Training: 100%|██████████| 231657/231657 [11:19<00:00, 341.04it/s, loss=2.49, lr=1.47e-5]\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
                        "import tqdm\n",
                        "\n",
                        "BATCH_SIZE = 32\n",
                        "EPOCHS = 5\n",
                        "LEARNING_RATE = 3e-5\n",
                        "WARMUP_STEPS = 5000\n",
                        "MAX_SEQ_LEN = 500\n",
                        "\n",
                        "model.train()\n",
                        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
                        "proc_seq_count = 0\n",
                        "batch_count = 0\n",
                        "\n",
                        "tmp_jokes_tens = None\n",
                        "\n",
                        "for epoch in range(EPOCHS):\n",
                        "    \n",
                        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
                        "    progress_bar = tqdm.tqdm(joke_dataloader, desc=\"Training\")\n",
                        "    \n",
                        "    for sample in progress_bar:\n",
                        "\n",
                        "        sentence, tokens = sample\n",
                        "        \n",
                        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
                        "        joke_tens = tokens.input_ids[0].to(device)\n",
                        "\n",
                        "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
                        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
                        "            continue\n",
                        "        \n",
                        "        # The first joke sequence in the sequence\n",
                        "        if not torch.is_tensor(tmp_jokes_tens):\n",
                        "            tmp_jokes_tens = joke_tens\n",
                        "            continue\n",
                        "        else:\n",
                        "            # The next joke does not fit in so we process the sequence and leave the last joke \n",
                        "            # as the start for next sequence \n",
                        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
                        "                work_jokes_tens = tmp_jokes_tens\n",
                        "                tmp_jokes_tens = joke_tens\n",
                        "            else:\n",
                        "                #Add the joke to sequence, continue and try to add more\n",
                        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
                        "                continue\n",
                        "        ################## Sequence ready, process it trough the model ##################\n",
                        "            \n",
                        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
                        "        loss = outputs.loss\n",
                        "        loss.backward()\n",
                        "                       \n",
                        "        proc_seq_count = proc_seq_count + 1\n",
                        "        if proc_seq_count == BATCH_SIZE:\n",
                        "            proc_seq_count = 0    \n",
                        "            batch_count += 1\n",
                        "            optimizer.step()\n",
                        "            scheduler.step() \n",
                        "            optimizer.zero_grad()\n",
                        "            model.zero_grad()\n",
                        "\n",
                        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
                        "        if batch_count == 10:\n",
                        "            batch_count = 0"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Inferência"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos ver como o modelo faz piadas."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 35,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "decoded joke: \n",
                                    "joke : what do you call a group of people who are not afraid of the dark? a group\n"
                              ]
                        }
                  ],
                  "source": [
                        "sentence_joke = \"JOKE:\"\n",
                        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
                        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
                        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
                        "\n",
                        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Você pode ver que você passa uma sequência com a palavra `joke` e ele retorna uma piada. Mas se você retornar outra string, ela não retornará"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 39,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "decoded joke: \n",
                                    "my dog is cute and i'm not sure if i should be offended or not. \" \n",
                                    "\n"
                              ]
                        }
                  ],
                  "source": [
                        "sentence_joke = \"My dog is cute and\"\n",
                        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
                        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
                        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
                        "\n",
                        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "base",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.7"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
