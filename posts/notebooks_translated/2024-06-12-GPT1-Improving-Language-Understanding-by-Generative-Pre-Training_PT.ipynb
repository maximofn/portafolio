{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT1 - Melhorando a Compreens\u00e3o de Linguagem por Pr\u00e9-Treinamento Gerativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Artigo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Melhorando a Compreens\u00e3o de Linguagem por Pr\u00e9-Treinamento Gerativo](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) \u00e9 o paper do GPT1. Antes de ler o post, \u00e9 necess\u00e1rio que voc\u00ea se situe: antes do GPT, os modelos de linguagem estavam baseados em redes recorrentes (RNN), que eram redes que funcionavam relativamente bem para tarefas espec\u00edficas, mas com as quais n\u00e3o era poss\u00edvel reutilizar o pr\u00e9-treinamento para fazer um fine tuning para outras tarefas. Al\u00e9m disso, elas n\u00e3o tinham muita mem\u00f3ria, ent\u00e3o se fossem alimentadas com frases muito longas, n\u00e3o lembravam muito bem o in\u00edcio da frase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arquitetura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de falar da arquitetura do GPT1, vamos lembrar como era a arquitetura dos transformers.",
        "\n",
        "![arquitetura do transformer](https://images.maximofn.com/transformer-scaled.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT1 \u00e9 um modelo baseado nos decodificadores dos transformers, ent\u00e3o, como n\u00e3o temos codificador, a arquitetura de um \u00fanico decodificador fica da seguinte maneira",
        "\n",
        "![decoder architecture](https://images.maximofn.com/transformer_decoder_only-scaled.webp)",
        "\n",
        "O mecanismo de aten\u00e7\u00e3o entre a senten\u00e7a do encoder e do decoder \u00e9 removido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No o artigo do GPT1 prop\u00f5em a seguinte arquitetura",
        "\n",
        "![arquitetura do gpt1](https://images.maximofn.com/GPT1_architecture.webp)",
        "\n",
        "Que corresponde ao decoder de um transformer como vimos anteriormente, executado 12 vezes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do artigo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As ideias mais interessantes do paper s\u00e3o:",
        "\n",
        "* O modelo \u00e9 treinado em um grande corpus de texto sem supervis\u00e3o. Com isso, cria-se um modelo de linguagem. Cria-se um modelo de linguagem de alta capacidade em um grande corpus de texto.",
        "* Em seguida, \u00e9 feito um fine-tuning em tarefas de NLP supervisionadas com conjuntos de dados rotulados. Realiza-se um ajuste fino em uma tarefa objetivo com supervis\u00e3o. Al\u00e9m disso, quando o modelo \u00e9 avaliado na tarefa supervisionada, n\u00e3o \u00e9 avaliado apenas por essa tarefa, mas tamb\u00e9m pelo qu\u00e3o bem ele prev\u00ea o pr\u00f3ximo token, o que ajuda a melhorar a generaliza\u00e7\u00e3o do modelo supervisionado e faz com que o modelo convirja mais rapidamente.",
        "* Embora j\u00e1 tenhamos mencionado, no artigo diz que \u00e9 utilizada a arquitetura transformer, pois at\u00e9 aquele momento eram usadas RNN para os modelos de linguagem. Isso resultou em uma melhoria na qual o aprendizado do primeiro treinamento (treinamento no corpus de texto sem supervis\u00e3o) \u00e9 mais f\u00e1cil de transferir para tarefas supervisionadas. Ou seja, gra\u00e7as ao uso de transformers, foi poss\u00edvel realizar um treinamento em todo um corpus de texto e, posteriormente, ajustes finos em tarefas supervisionadas.",
        "* Avaliaram o modelo em quatro tipos de tarefas de compreens\u00e3o da linguagem:",
        "* Infer\u00eancia da linguagem natural",
        "* Resposta a perguntas",
        "* Similaridade sem\u00e2ntica",
        "* Classifica\u00e7\u00e3o de textos.",
        "* O modelo geral (treinado em todo o corpus de texto sem supervis\u00e3o) supera os modelos RNN treinados discriminativamente que utilizam arquiteturas projetadas especificamente para cada tarefa, melhorando significativamente o estado da arte em 9 das 12 tarefas estudadas. Eles tamb\u00e9m analisaram os comportamentos de \"tiro zero\" do modelo pr\u00e9-treinado em quatro ambientes diferentes e demonstraram que ele adquire um conhecimento lingu\u00edstico \u00fatil para as tarefas subsequentes.",
        "* Nos \u00faltimos anos, os pesquisadores haviam demonstrado os benef\u00edcios de utilizar embeddings, que s\u00e3o treinados em corpora n\u00e3o anotados, para melhorar o desempenho em uma variedade de tarefas. No entanto, essas abordagens transferem principalmente informa\u00e7\u00f5es a n\u00edvel de palavra, enquanto o uso de transformers treinados em grandes corpora de texto sem supervis\u00e3o captura a sem\u00e2ntica de n\u00edvel superior, a n\u00edvel de frase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gera\u00e7\u00e3o de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como gerar texto com um GPT1 pr\u00e9-treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro \u00e9 necess\u00e1rio instalar `ftfy` e `spacy` atrav\u00e9s de",
        "\n",
        "```bash\n",
        "pip install ftfy spacy",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez instaladas, voc\u00ea deve baixar o modelo de linguagem do spaCy que deseja usar. Por exemplo, para baixar o modelo em ingl\u00eas, voc\u00ea pode executar:",
        "\n",
        "```bash\n",
        "python -m spacy download en_core_web_sm",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para gerar texto vamos utilizar o modelo do reposit\u00f3rio [GPT1](https://huggingface.co/openai-community/openai-gpt) da Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importamos as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se voc\u00ea reparar, importamos `OpenAIGPTTokenizer` e `AutoTokenizer`. Isso \u00e9 porque na [model card](https://huggingface.co/openai-community/openai-gpt) do GPT1 indica-se que se deve usar `OpenAIGPTTokenizer`, mas no post da biblioteca [transformers](https://maximofn.com/hugging-face-transformers/) explicamos que se deve usar `AutoTokenizer` para carregar o tokenizador. Ent\u00e3o, vamos testar os dois."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokens: \n",
            "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "input auto tokens: \n",
            "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "ckeckpoints = \"openai-community/openai-gpt\"\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(ckeckpoints)\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "\n",
        "input_tokens = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
        "input_auto_tokens = auto_tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
        "\n",
        "print(f\"input tokens: \\n{input_tokens}\")\n",
        "print(f\"input auto tokens: \\n{input_auto_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como pode ser visto com os dois tokenizadores, s\u00e3o obtidos os mesmos tokens. Ent\u00e3o, para que o c\u00f3digo seja mais geral, de forma que se os checkpoints forem alterados, n\u00e3o seja necess\u00e1rio alterar o c\u00f3digo, vamos utilizar `AutoTokenizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos ent\u00e3o o dispositivo, o tokenizador e o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como instanciamos o modelo, vamos a ver quantos par\u00e2metros ele tem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 117M\n"
          ]
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {round(params/1e6)}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na era dos trilh\u00f5es de par\u00e2metros, podemos ver que o GPT1 tinha apenas 117 milh\u00f5es de par\u00e2metros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos os tokens de entrada para o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Hello, my dog is cute and\"\n",
        "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "input_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passamo-los ao modelo para gerar os tokens de sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output tokens: \n",
            "tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,\n",
            "           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "output_tokens = model.generate(**input_tokens)\n",
        "\n",
        "print(f\"output tokens: \\n{output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decodificamos os tokens para obter a frase de sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded output: \n",
            "hello, my dog is cute and i'm going to take him for a walk. \" \n",
            " \"\n"
          ]
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded output: \\n{decoded_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 conseguimos gerar texto com GPT1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gerar texto token a token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Busca gulosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N\u00f3s usamos `model.generate` para gerar os tokens de sa\u00edda de uma s\u00f3 vez, mas vamos ver como ger\u00e1-los um a um. Para isso, em vez de usar `model.generate` vamos usar `model`, que na verdade chama o m\u00e9todo `model.forward`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
              "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
              "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
              "         ...,\n",
              "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
              "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
              "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que sa\u00ed muitos dados, primeiro vamos ver as keys da sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['logits'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste caso, temos apenas os logits do modelo, vamos verificar seu tamanho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 40478])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits = outputs.logits\n",
        "\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver quantos tokens t\u00ednhamos na entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tokens.input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bem, na sa\u00edda temos o mesmo n\u00famero de logits que na entrada. Isso \u00e9 normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos os logits da \u00faltima posi\u00e7\u00e3o da sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40478])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nex_token_logits = logits[0,-1]\n",
        "\n",
        "nex_token_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H\u00e1 um total de 40478 logits, ou seja, h\u00e1 um vocabul\u00e1rio de 40478 tokens e temos que ver qual \u00e9 o token com a maior probabilidade. Para isso, primeiro calculamos a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40478])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "\n",
        "softmax_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1898, device='cuda:0', grad_fn=<MaxBackward0>),\n",
              " tensor(249, device='cuda:0'))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "\n",
        "next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtivemos o seguinte token, agora vamos decodific\u00e1-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(next_token_id.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtivemos o seguinte token atrav\u00e9s do m\u00e9todo greedy, ou seja, o token com maior probabilidade. Mas j\u00e1 vimos no post da biblioteca transformers as [formas de gerar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-genera%C3%A7%C3%A3o-de-texto) que podem ser feitas sampling, top-k, top-p, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a colocar tudo dentro de uma fun\u00e7\u00e3o e ver o que sai se gerarmos alguns tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_next_greedy_token(input_sentence, tokenizer, model, device):\n",
        "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**input_tokens)\n",
        "    logits = outputs.logits\n",
        "    nex_token_logits = logits[0,-1]\n",
        "    softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "    next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "    return next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_greedy_text(input_sentence, tokenizer, model, device, max_length=20):\n",
        "    generated_text = input_sentence\n",
        "    for _ in range(max_length):\n",
        "        next_token_prob, next_token_id = generate_next_greedy_token(generated_text, tokenizer, model, device)\n",
        "        generated_text += tokenizer.decode(next_token_id.item())\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora geramos texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello, my dog is cute andi.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_greedy_text(\"Hello, my dog is cute and\", tokenizer, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A sa\u00edda \u00e9 bastante repetitiva, como j\u00e1 foi visto nas [formas de gerar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-gera%C3%A7%C3%A3o-de-texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ajuste fino do GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C\u00e1lculo da perda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de come\u00e7ar a fazer o fine tuning do GPT1, vamos ver uma coisa. Antes, quando obt\u00ednhamos a sa\u00edda do modelo, faz\u00edamos isso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
              "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
              "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
              "         ...,\n",
              "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
              "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
              "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode-se ver que obtemos `loss=None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vamos a precisar da loss para fazer o fine tuning, vamos a ver como obt\u00ea-la.",
        "\n",
        "Se n\u00f3s formos \u00e0 documenta\u00e7\u00e3o do m\u00e9todo [forward](https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward) de `OpenAIGPTLMHeadModel`, podemos ver que diz que na sa\u00edda retorna um objeto do tipo `transformers.modeling_outputs.CausalLMOutput`, ent\u00e3o se formos \u00e0 documenta\u00e7\u00e3o de [transformers.modeling_outputs.CausalLMOutput](https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput), podemos ver que diz que retorna `loss` se for passado `labels` para o m\u00e9todo `forward`.",
        "\n",
        "Se n\u00f3s formos \u00e0 fonte do c\u00f3digo do m\u00e9todo [forward](https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544), vemos este bloco de c\u00f3digo",
        "\n",
        "```python\n",
        "perda = None",
        "se labels n\u00e3o for None:",
        "# Deslocar para que os tokens < n prevejam n",
        "shift_logits = lm_logits[..., :-1, :].contiguous()",
        "shift_labels = labels[..., 1:].contiguous()",
        "# Aplanar os tokens",
        "loss_fct = CrossEntropyLoss()",
        "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))",
        "```\n",
        "\n",
        "Isso significa que a `loss` \u00e9 calculada da seguinte maneira",
        "\n",
        "* Shift de logits e labels: A primeira parte \u00e9 deslocar os logits (`lm_logits`) e as etiquetas (`labels`) para que os `tokens < n` prevejam `n`, ou seja, a partir de uma posi\u00e7\u00e3o `n` se prev\u00ea o pr\u00f3ximo token com base nos anteriores.",
        "* CrossEntropyLoss: Cria-se uma inst\u00e2ncia da fun\u00e7\u00e3o de perda `CrossEntropyLoss()`.",
        "* Achatando tokens: A seguir, os logits e as etiquetas s\u00e3o aplanados utilizando `view(-1, shift_logits.size(-1))` e `view(-1)`, respectivamente. Isso \u00e9 feito para que os logits e as etiquetas tenham a mesma forma para a fun\u00e7\u00e3o de perda.",
        "* C\u00e1lculo da perda: Finalmente, calcula-se a perda utilizando a fun\u00e7\u00e3o de perda `CrossEntropyLoss()` com os logits achatados e as etiquetas achatadas como entradas.",
        "\n",
        "Em resumo, a `loss` \u00e9 calculada como a perda de entropia cruzada entre os logits deslocados e achatados e as labels deslocadas e achatadas.",
        "\n",
        "Portanto, se passarmos os labels para o m\u00e9todo `forward`, ele retornar\u00e1 a `loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.2607, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens, labels=input_tokens.input_ids)\n",
        "\n",
        "outputs.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para o treinamento, vamos usar um dataset de piadas em ingl\u00eas [short-jokes-dataset](https://huggingface.co/datasets/Maximofn/short-jokes-dataset), que \u00e9 um dataset com 231 mil piadas em ingl\u00eas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baixamos o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'Joke'],\n",
              "        num_rows: 231657\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
        "jokes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos v\u00ea-lo um pouco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ID': 1,\n",
              " 'Joke': '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jokes[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento com Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro vamos ver como seria o treinamento com puro Pytorch",
        "\n",
        "> Reiniciamos o notebook para que n\u00e3o haja problemas com a mem\u00f3ria da GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import OpenAIGPTLMHeadModel, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ckeckpoints = \"openai-community/openai-gpt\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conjunto de dados do Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma classe Dataset do Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.joke = \"JOKE: \"\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset[\"train\"])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = self.joke + self.dataset[\"train\"][item][\"Joke\"] + self.end_of_text_token\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        return sentence, tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A instanciamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = JokesDataset(jokes, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo<|endoftext|>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 30]), torch.Size([1, 30]))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence, tokens = dataset[5]\n",
        "print(sentence)\n",
        "tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos agora um dataloader do Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 1\n",
        "joke_dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos um lote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences, tokens = next(iter(joke_dataloader))\n",
        "len(sentences), tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:31<00:00, 334.88it/s, loss=2.88, lr=2.93e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:30<00:00, 335.27it/s, loss=2.49, lr=5.87e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 2 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:17<00:00, 341.75it/s, loss=2.57, lr=8.81e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 3 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:18<00:00, 341.27it/s, loss=2.41, lr=1.18e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 4 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [11:19<00:00, 341.04it/s, loss=2.49, lr=1.47e-5]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import tqdm\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 500\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "proc_seq_count = 0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_jokes_tens = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    progress_bar = tqdm.tqdm(joke_dataloader, desc=\"Training\")\n",
        "    \n",
        "    for sample in progress_bar:\n",
        "\n",
        "        sentence, tokens = sample\n",
        "        \n",
        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
        "        joke_tens = tokens.input_ids[0].to(device)\n",
        "\n",
        "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        # The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            # The next joke does not fit in so we process the sequence and leave the last joke \n",
        "            # as the start for next sequence \n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                #Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "        ################## Sequence ready, process it trough the model ##################\n",
        "            \n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        if batch_count == 10:\n",
        "            batch_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Infer\u00eancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como o modelo faz piadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "joke : what do you call a group of people who are not afraid of the dark? a group\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"JOKE:\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode-se ver que voc\u00ea passa uma sequ\u00eancia com a palavra `joke` e ele retorna uma piada. Mas se voc\u00ea retornar outra sequ\u00eancia n\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "my dog is cute and i'm not sure if i should be offended or not. \" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"My dog is cute and\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-12",
      "description_en": "Unlock the power of language!!!! \ud83d\udca5 In my last post, I take you by the hand through the GPT-1 paper, explaining in a clear and concise way how this pioneer model in natural language processing works. And not only that! I also show you how to fine-tuning the model so you can adapt it to your specific needs \ud83d\udcca Don't miss the opportunity to learn about one of the most influential models in history! \ud83d\ude80 Read my post and find out how you can improve your artificial intelligence skills! \ud83d\udcc4",
      "description_es": "\u00a1\u00a1\u00a1Desbloquea el poder del lenguaje!!! \ud83d\udca5 En mi \u00faltimo post, te llevo de la mano a trav\u00e9s del paper de GPT-1, explicando de manera clara y concisa c\u00f3mo funciona este modelo pionero en el procesamiento de lenguaje natural. \u00a1Y no solo eso! Tambi\u00e9n te muestro c\u00f3mo hacer un fine-tuning del modelo para que puedas adaptarlo a tus necesidades espec\u00edficas \ud83d\udcca. \u00a1No te pierdas la oportunidad de aprender sobre uno de los modelos m\u00e1s influyentes de la historia! \ud83d\ude80 \u00a1Lee mi post y descubre c\u00f3mo puedes mejorar tus habilidades en inteligencia artificial! \ud83d\udcc4",
      "description_pt": "Desbloqueie o poder da linguagem!!!! \ud83d\udca5 Em minha \u00faltima postagem, apresentei o artigo GPT-1, explicando de forma clara e concisa como funciona esse modelo pioneiro no processamento de linguagem natural. E n\u00e3o \u00e9 s\u00f3 isso! Tamb\u00e9m mostro como fazer o ajuste fino do modelo para que voc\u00ea possa adapt\u00e1-lo \u00e0s suas necessidades espec\u00edficas \ud83d\udcca N\u00e3o perca a oportunidade de conhecer um dos modelos mais influentes da hist\u00f3ria! \ud83d\ude80 Leia minha postagem e descubra como voc\u00ea pode melhorar suas habilidades de intelig\u00eancia artificial! \ud83d\udcc4",
      "end_url": "gpt1",
      "image": "https://images.maximofn.com/GPT1_thumnail.webp",
      "image_hover_path": "https://images.maximofn.com/GPT1_thumnail.webp",
      "keywords_en": "gpt1, nlp, transformers, fine-tuning, language model, hugging face, pytorch",
      "keywords_es": "gpt1, nlp, transformers, fine-tuning, modelo de lenguaje, hugging face, pytorch",
      "keywords_pt": "gpt1, nlp, transformers, ajuste fino, modelo de linguagem, hugging face, pytorch",
      "title_en": "GPT1 \u2013 Improving Language Understanding by Generative Pre-Training",
      "title_es": "GPT1 \u2013 Improving Language Understanding by Generative Pre-Training",
      "title_pt": "GPT1 \u2013 Improving Language Understanding by Generative Pre-Training"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}