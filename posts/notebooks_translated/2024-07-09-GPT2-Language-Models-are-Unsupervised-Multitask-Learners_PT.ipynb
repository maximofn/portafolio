{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 - Modelos de Linguagem s\u00e3o Aprendizes Multitarefa N\u00e3o Supervisionados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Artigo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) \u00e9 o paper do GPT-2. Esta \u00e9 a segunda vers\u00e3o do modelo [GPT-1](https://maximofn.com/gpt1/) que j\u00e1 vimos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arquitetura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de falar sobre a arquitetura do GPT-2, vamos lembrar como era a arquitetura do GPT-1.",
        "\n",
        "![arquitetura do gpt1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_architecture.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em GPT-2 \u00e9 utilizada uma arquitetura baseada em transformers, assim como [GPT-1](https://maximofn.com/gpt1/), com os seguintes tamanhos",
        "\n",
        "|Par\u00e2metros|Camadas|d_model|",
        "|---|---|---|",
        "|117M| 12| 768|",
        "| 345M| 24| 1024|",
        "| 762M| 36| 1280|",
        "| 1542M| 48| 1600|",
        "\n",
        "O modelo mais pequeno \u00e9 equivalente ao GPT original, e o segundo mais pequeno \u00e9 equivalente ao modelo maior do BERT. O modelo maior tem mais de uma ordem de magnitude mais par\u00e2metros que o GPT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al\u00e9m disso, foram realizadas as seguintes modifica\u00e7\u00f5es na arquitetura",
        "\n",
        "* Adiciona-se uma camada de normaliza\u00e7\u00e3o antes do bloco de aten\u00e7\u00e3o. Isso pode ajudar a estabilizar o treinamento do modelo e melhorar sua capacidade de aprender representa\u00e7\u00f5es mais profundas. Ao normalizar as entradas de cada bloco, reduz-se a variabilidade nas sa\u00eddas e facilita-se o treinamento do modelo",
        "* Foi adicionada uma normaliza\u00e7\u00e3o adicional ap\u00f3s o bloco de auto-aten\u00e7\u00e3o final. Isso pode ajudar a reduzir a variabilidade nas sa\u00eddas do modelo e melhorar sua estabilidade.",
        "* Na maioria dos modelos, os pesos das camadas s\u00e3o inicializados aleatoriamente, seguindo uma distribui\u00e7\u00e3o normal ou uniforme. No entanto, no caso do GPT-2, os autores decidiram utilizar uma inicializa\u00e7\u00e3o modificada que leva em conta a profundidade do modelo. A ideia por tr\u00e1s dessa inicializa\u00e7\u00e3o modificada \u00e9 que, \u00e0 medida que o modelo se torna mais profundo, o sinal que flui atrav\u00e9s das camadas residuais vai enfraquecendo. Isso ocorre porque cada camada residual \u00e9 somada \u00e0 entrada original, o que pode fazer com que o sinal v\u00e1 atenuando com a profundidade do modelo. Para contrariar esse efeito, decidiram escalar os pesos das camadas residuais na inicializa\u00e7\u00e3o por um fator de 1/\u221aN, onde N \u00e9 o n\u00famero de camadas residuais. Isso significa que, \u00e0 medida que o modelo se torna mais profundo, os pesos das camadas residuais ficam menores. Este truque de inicializa\u00e7\u00e3o pode ajudar a estabilizar o treinamento do modelo e a melhorar sua capacidade de aprender representa\u00e7\u00f5es mais profundas. Ao escalar os pesos das camadas residuais, reduz-se a variabilidade nas sa\u00eddas de cada camada e facilita-se o fluxo do sinal atrav\u00e9s do modelo. Em resumo, a inicializa\u00e7\u00e3o modificada no GPT-2 \u00e9 utilizada para contrariar o efeito de atenua\u00e7\u00e3o do sinal nas camadas residuais, o que ajuda a estabilizar o treinamento do modelo e a melhorar sua capacidade de aprender representa\u00e7\u00f5es mais profundas.",
        "* O tamanho do vocabul\u00e1rio expandiu-se para 50.257. Isso significa que o modelo pode aprender a representar um conjunto mais amplo de palavras e tokens.",
        "* O tamanho do contexto foi aumentado de 512 para 1024 tokens. Isso permite que o modelo leve em conta um contexto mais amplo ao gerar texto.",
        "\n",
        "![GPT1 vs GPT-2 architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_vs_GPT2_architecture.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do artigo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As ideias mais interessantes do paper s\u00e3o:",
        "\n",
        "* Para o pr\u00e9-treinamento do modelo, pensaram em usar uma fonte de texto diversa e quase ilimitada, como web scraping de Common Crawl. No entanto, descobriram que havia texto de qualidade muito baixa. Ent\u00e3o, usaram o dataset WebText, que tamb\u00e9m provinha de web scraping, mas com um filtro de qualidade, como a quantidade de links de sa\u00edda do Reddit, etc. Al\u00e9m disso, removeram o texto proveniente da Wikipedia, pois poderia estar repetido em outras p\u00e1ginas.",
        "* Utilizaram um tokenizador BPE que j\u00e1 explicamos em um [post](https://maximofn.com/bpe/) anterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gera\u00e7\u00e3o de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como gerar texto com um GPT-2 pr\u00e9-treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para gerar texto vamos utilizar o modelo do reposit\u00f3rio [GPT-2](https://huggingface.co/openai-community/gpt2) da Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gera\u00e7\u00e3o de texto com pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com esse modelo j\u00e1 podemos usar o pipeline de transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output 1: Hello, I'm a language model, and I want to change the way you read\n",
            "\n",
            "A little in today's post I want to talk about\n",
            "Output 2: Hello, I'm a language model, with two roles: the language model and the lexicographer-semantics expert. The language models are going\n",
            "Output 3: Hello, I'm a language model, and this is your brain. Here is your brain, and all this data that's stored in there, that\n",
            "Output 4: Hello, I'm a language model, and I like to talk... I want to help you talk to your customers\n",
            "\n",
            "Are you using language model\n",
            "Output 5: Hello, I'm a language model, I'm gonna tell you about what type of language you're using. We all know a language like this,\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "checkpoints = \"openai-community/gpt2-xl\"\n",
        "generator = pipeline('text-generation', model=checkpoints)\n",
        "output = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
        "for i, o in enumerate(output):\n",
        "    print(f\"Output {i+1}: {o['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gera\u00e7\u00e3o de texto com automodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mas se quisermos utilizar `Automodel`, podemos fazer o seguinte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, AutoTokenizer\n",
        "\n",
        "checkpoints = \"openai-community/gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoints)\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assim como com [GPT-1](https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto) podemos importar `GPT2Tokenizer` e `AutoTokenizer`. Isso \u00e9 porque na [model card](https://huggingface.co/openai-community/gpt2) do GPT-2 indica-se que se use `GPT2Tokenizer`, mas no post da biblioteca [transformers](https://maximofn.com/hugging-face-transformers/) explicamos que deve-se usar `AutoTokenizer` para carregar o tokenizador. Ent\u00e3o vamos testar os dois."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokens: \n",
            "{'input_ids': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "input auto tokens: \n",
            "{'input_ids': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "checkpoints = \"openai-community/gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoints)\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "\n",
        "input_tokens = tokenizer(\"Hello, I'm a language model,\", return_tensors=\"pt\")\n",
        "input_auto_tokens = auto_tokenizer(\"Hello, I'm a language model,\", return_tensors=\"pt\")\n",
        "\n",
        "print(f\"input tokens: \\n{input_tokens}\")\n",
        "print(f\"input auto tokens: \\n{input_auto_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como pode ser visto com os dois tokenizadores, s\u00e3o obtidos os mesmos tokens. Assim, para que o c\u00f3digo seja mais geral, de maneira que, se forem alterados os checkpoints, n\u00e3o seja necess\u00e1rio alterar o c\u00f3digo, vamos utilizar `AutoTokenizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos ent\u00e3o o dispositivo, o tokenizador e o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "checkpoints = \"openai-community/gpt2-xl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoints).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como instanciamos o modelo, vamos a ver quantos par\u00e2metros ele tem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 1558M\n"
          ]
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {round(params/1e6)}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, carregamos o modelo com 1,5B de par\u00e2metros, mas se quis\u00e9ssemos carregar os outros modelos ter\u00edamos que fazer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters of small model: 124M\n",
            "Number of parameters of medium model: 355M\n",
            "Number of parameters of large model: 774M\n",
            "Number of parameters of xl model: 1558M\n"
          ]
        }
      ],
      "source": [
        "checkpoints_small = \"openai-community/gpt2\"\n",
        "model_small = GPT2LMHeadModel.from_pretrained(checkpoints_small)\n",
        "print(f\"Number of parameters of small model: {round(sum(p.numel() for p in model_small.parameters())/1e6)}M\")\n",
        "\n",
        "checkpoints_medium = \"openai-community/gpt2-medium\"\n",
        "model_medium = GPT2LMHeadModel.from_pretrained(checkpoints_medium)\n",
        "print(f\"Number of parameters of medium model: {round(sum(p.numel() for p in model_medium.parameters())/1e6)}M\")\n",
        "\n",
        "checkpoints_large = \"openai-community/gpt2-large\"\n",
        "model_large = GPT2LMHeadModel.from_pretrained(checkpoints_large)\n",
        "print(f\"Number of parameters of large model: {round(sum(p.numel() for p in model_large.parameters())/1e6)}M\")\n",
        "\n",
        "checkpoints_xl = \"openai-community/gpt2-xl\"\n",
        "model_xl = GPT2LMHeadModel.from_pretrained(checkpoints_xl)\n",
        "print(f\"Number of parameters of xl model: {round(sum(p.numel() for p in model_xl.parameters())/1e6)}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos os tokens de entrada para o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]],\n",
              "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Hello, I'm a language model,\"\n",
        "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "input_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passamos isso ao modelo para gerar os tokens de sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output tokens: \n",
            "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   314,\n",
            "          1101,  1016,   284,  1037,   345,   351,   534,  1917,    13,   198]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "output_tokens = model.generate(**input_tokens)\n",
        "\n",
        "print(f\"output tokens: \\n{output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decodificamos os tokens para obter a frase de sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded output: \n",
            "Hello, I'm a language model, and I'm going to help you with your problem.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded output: \\n{decoded_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 conseguimos gerar texto com o GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gerar texto token a token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Busca gulosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N\u00f3s usamos `model.generate` para gerar os tokens de sa\u00edda de uma s\u00f3 vez, mas vamos ver como ger\u00e1-los um a um. Para isso, em vez de usar `model.generate` vamos usar `model`, que na verdade chama o m\u00e9todo `model.forward`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],\n",
              "         [ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],\n",
              "         [ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],\n",
              "         ...,\n",
              "         [ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],\n",
              "         [ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],\n",
              "         [ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],\n",
              "          [ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],\n",
              "          [-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],\n",
              "          ...,\n",
              "          [-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],\n",
              "          [-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],\n",
              "          [ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],\n",
              "\n",
              "         [[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],\n",
              "          [-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],\n",
              "          [ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],\n",
              "          ...,\n",
              "          [-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],\n",
              "          [-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],\n",
              "          [ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],\n",
              "\n",
              "         [[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],\n",
              "          [ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],\n",
              "          ...,\n",
              "          [ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],\n",
              "          [ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],\n",
              "          [ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],\n",
              "       device='cuda:0', grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que sa\u00ed muitos dados, primeiro vamos ver as chaves da sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['logits', 'past_key_values'])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste caso, temos apenas os logits do modelo, vamos verificar seu tamanho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 50257])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits = outputs.logits\n",
        "\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver quantos tokens t\u00ednhamos na entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tokens.input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bem, na sa\u00edda temos o mesmo n\u00famero de logits que na entrada. Isso \u00e9 normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos os logits da \u00faltima posi\u00e7\u00e3o da sa\u00edda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nex_token_logits = logits[0,-1]\n",
        "\n",
        "nex_token_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H\u00e1 um total de 50257 logits, ou seja, h\u00e1 um vocabul\u00e1rio de 50257 tokens e temos que ver qual \u00e9 o token com a maior probabilidade, para isso primeiro calculamos a softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "\n",
        "softmax_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que calculamos a softmax, obtemos o token mais prov\u00e1vel procurando aquele que tenha a maior probabilidade, ou seja, aquele que tenha o maior valor ap\u00f3s a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1732, device='cuda:0', grad_fn=<MaxBackward0>),\n",
              " tensor(290, device='cuda:0'))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "\n",
        "next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtivemos o seguinte token, agora vamos decodific\u00e1-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' and'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(next_token_id.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtivemos o seguinte token atrav\u00e9s do m\u00e9todo greedy, ou seja, o token com a maior probabilidade. Mas j\u00e1 vimos no post da biblioteca transformers as [formas de gerar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-genera\u00e7\u00e3o-de-texto) que podem ser feitas `sampling`, `top-k`, `top-p`, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a colocar tudo dentro de uma fun\u00e7\u00e3o e ver o que sai se gerarmos alguns tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_next_greedy_token(input_sentence, tokenizer, model, device):\n",
        "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**input_tokens)\n",
        "    logits = outputs.logits\n",
        "    nex_token_logits = logits[0,-1]\n",
        "    softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "    next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "    return next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_greedy_text(input_sentence, tokenizer, model, device, max_length=20):\n",
        "    generated_text = input_sentence\n",
        "    for _ in range(max_length):\n",
        "        next_token_prob, next_token_id = generate_next_greedy_token(generated_text, tokenizer, model, device)\n",
        "        generated_text += tokenizer.decode(next_token_id.item())\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora geramos texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello, I'm a language model, and I'm going to help you with your problem.\\n\\n\\nI'm going to help you\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_greedy_text(\"Hello, I'm a language model,\", tokenizer, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A sa\u00edda \u00e9 bastante repetitiva, como j\u00e1 foi visto nas [formas de gerar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-gera\u00e7\u00e3o-de-texto). Mas ainda assim, \u00e9 uma sa\u00edda melhor do que a obtida com [GPT-1](https://maximofn.com/gpt1/#Gera\u00e7\u00e3o-de-texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arquitetura dos modelos dispon\u00edveis na Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se formos \u00e0 documenta\u00e7\u00e3o do Hugging Face de [GPT2](https://huggingface.co/docs/transformers/en/model_doc/gpt2) podemos ver que temos as op\u00e7\u00f5es `GPT2Model`, `GPT2LMHeadModel`, `GPT2ForSequenceClassification`, `GPT2ForQuestionAnswering`, `GPT2ForTokenClassification`. Vamos v\u00ea-los"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "ckeckpoints = \"openai-community/gpt2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT2Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este \u00e9 o modelo base, ou seja, o decodificador do transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (h): ModuleList(\n",
              "    (0-11): 12 x GPT2Block(\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): GPT2Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): GPT2MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (act): NewGELUActivation()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2Model\n",
        "model = GPT2Model.from_pretrained(ckeckpoints)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se pode ver na sa\u00edda um tensor de dimens\u00e3o 768, que \u00e9 a dimens\u00e3o dos embeddings do modelo pequeno. Se tiv\u00e9ssemos usado o modelo `openai-community/gpt2-xl`, ter\u00edamos obtido uma sa\u00edda de 1600.",
        "\n",
        "De acordo com a tarefa que se deseja realizar, agora seria necess\u00e1rio adicionar mais camadas.",
        "\n",
        "Podemos adicion\u00e1-las manualmente, mas os pesos dessas camadas ser\u00e3o inicializados aleatoriamente. Enquanto isso, se usarmos os modelos da Hugging Face com essas camadas, os pesos est\u00e3o pr\u00e9-treinados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u00c9 o que usamos anteriormente para gerar texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel.from_pretrained(ckeckpoints)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como pode ser visto, \u00e9 o mesmo modelo que antes, apenas com uma camada linear adicionada no final, com uma entrada de 768 (os embeddings) e uma sa\u00edda de 50257, que corresponde ao tamanho do vocabul\u00e1rio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT2ForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta op\u00e7\u00e3o \u00e9 para classificar sequ\u00eancias de texto, neste caso temos que especificar com `num_labels` o n\u00famero de classes que queremos classificar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2ForSequenceClassification\n",
        "model = GPT2ForSequenceClassification.from_pretrained(ckeckpoints, num_labels=5)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, em vez de ter uma sa\u00edda de 50257, temos uma sa\u00edda de 5, que \u00e9 o n\u00famero que introduzimos em `num_labels` e \u00e9 o n\u00famero de classes que queremos classificar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT2ParaRespostaDePerguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No post de [transformers](https://maximofn.com/hugging-face-transformers/) explicamos que, neste modo, passa-se um contexto ao modelo e uma pergunta sobre o contexto e ele retorna a resposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2ForQuestionAnswering(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2ForQuestionAnswering\n",
        "model = GPT2ForQuestionAnswering.from_pretrained(ckeckpoints)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que na sa\u00edda nos d\u00e1 um tensor de duas dimens\u00f5es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT2ForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb\u00e9m no post de [transformers](https://maximofn.com/hugging-face-transformers/) contamos o que era token classification, explicamos que classificava a qual categoria correspondia cada token. Temos que passar o n\u00famero de classes que queremos classificar com `num_labels`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2ForTokenClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2ForTokenClassification\n",
        "model = GPT2ForTokenClassification.from_pretrained(ckeckpoints, num_labels=5)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na sa\u00edda obtemos as cinco classes que especificamos com `num_labels`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ajuste fino do GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ajuste fino para gera\u00e7\u00e3o de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro vamos ver como seria o treinamento com puro Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C\u00e1lculo da perda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de come\u00e7ar a fazer o fine tuning do GPT-2, vamos ver uma coisa. Antes, quando obt\u00ednhamos a sa\u00edda do modelo, faz\u00edamos isso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],\n",
              "         [ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],\n",
              "         [ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],\n",
              "         ...,\n",
              "         [ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],\n",
              "         [ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],\n",
              "         [ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],\n",
              "          [ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],\n",
              "          [-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],\n",
              "          ...,\n",
              "          [-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],\n",
              "          [-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],\n",
              "          [ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],\n",
              "\n",
              "         [[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],\n",
              "          [-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],\n",
              "          [ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],\n",
              "          ...,\n",
              "          [-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],\n",
              "          [-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],\n",
              "          [ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],\n",
              "\n",
              "         [[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],\n",
              "          [ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],\n",
              "          ...,\n",
              "          [ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],\n",
              "          [ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],\n",
              "          [ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],\n",
              "       device='cuda:0', grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode-se ver que obtemos `loss=None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vamos a precisar da loss para fazer o fine tuning, vamos a ver como obt\u00ea-la.",
        "\n",
        "Se n\u00f3s formos \u00e0 documenta\u00e7\u00e3o do m\u00e9todo [forward](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel.forward) de `GPT2LMHeadModel`, podemos ver que diz que na sa\u00edda retorna um objeto do tipo `transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`, ent\u00e3o se formos \u00e0 documenta\u00e7\u00e3o de [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions), podemos ver que diz que retorna `loss` se for passado `labels` para o m\u00e9todo `forward`.",
        "\n",
        "Se n\u00f3s f\u00f4ssemos \u00e0 fonte do c\u00f3digo do m\u00e9todo [forward](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1277), ver\u00edamos este bloco de c\u00f3digo",
        "\n",
        "```python\n",
        "perda = None",
        "se labels n\u00e3o for None:",
        "# mover r\u00f3tulos para o dispositivo correto para habilitar o paralelismo do modelo",
        "labels = labels.to(lm_logits.device)",
        "# Desloque para que os tokens < n prevejam n",
        "shift_logits = lm_logits[..., :-1, :].contiguous()",
        "shift_labels = labels[..., 1:].contiguous()",
        "# Aplanar os tokens",
        "loss_fct = CrossEntropyLoss()",
        "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))",
        "```\n",
        "\n",
        "Isso significa que a `loss` \u00e9 calculada da seguinte maneira",
        "\n",
        "* Shift de logits e labels: A primeira parte \u00e9 deslocar os logits (`lm_logits`) e as etiquetas (`labels`) para que os `tokens < n` prevejam `n`, ou seja, a partir de uma posi\u00e7\u00e3o `n` se prev\u00ea o pr\u00f3ximo token com base nos anteriores.",
        "* CrossEntropyLoss: Cria-se uma inst\u00e2ncia da fun\u00e7\u00e3o de perda `CrossEntropyLoss()`.",
        "* Aplanar tokens: A seguir, os logits e as etiquetas s\u00e3o aplainados utilizando `view(-1, shift_logits.size(-1))` e `view(-1)`, respectivamente. Isso \u00e9 feito para que os logits e as etiquetas tenham a mesma forma para a fun\u00e7\u00e3o de perda.",
        "* C\u00e1lculo da perda: Finalmente, calcula-se a perda utilizando a fun\u00e7\u00e3o de perda `CrossEntropyLoss()` com os logits achatados e as etiquetas achatadas como entradas.",
        "\n",
        "Em resumo, a `loss` \u00e9 calculada como a perda de entropia cruzada entre os logits deslocados e achatados e as labels deslocadas e achatadas.",
        "\n",
        "Portanto, se passarmos os labels para o m\u00e9todo `forward`, ele retornar\u00e1 a `loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.8028, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens, labels=input_tokens.input_ids)\n",
        "\n",
        "outputs.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para o treinamento vamos usar um dataset de piadas em ingl\u00eas [short-jokes-dataset](https://huggingface.co/datasets/Maximofn/short-jokes-dataset), que \u00e9 um dataset com 231 mil piadas em ingl\u00eas.",
        "\n",
        "> Reiniciamos o notebook para que n\u00e3o haja problemas com a mem\u00f3ria da GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baixamos o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'Joke'],\n",
              "        num_rows: 231657\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
        "jokes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos v\u00ea-lo um pouco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ID': 1,\n",
              " 'Joke': '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jokes[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inst\u00e2ncia do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder usar o modelo `xl`, ou seja, o de 1,5B de par\u00e2metros, eu o passo para FP16 para n\u00e3o ficar sem mem\u00f3ria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ckeckpoints = \"openai-community/gpt2-xl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = GPT2LMHeadModel.from_pretrained(ckeckpoints)\n",
        "\n",
        "model = model.half().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conjunto de dados do Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma classe Dataset do Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.joke = \"JOKE: \"\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset[\"train\"])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = self.joke + self.dataset[\"train\"][item][\"Joke\"] + self.end_of_text_token\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        return sentence, tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A instanciamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = JokesDataset(jokes, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo<|endoftext|>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 22]), torch.Size([1, 22]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence, tokens = dataset[5]\n",
        "print(sentence)\n",
        "tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos agora um DataLoader do Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 1\n",
        "joke_dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos um lote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, torch.Size([1, 1, 36]), torch.Size([1, 1, 36]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences, tokens = next(iter(joke_dataloader))\n",
        "len(sentences), tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/231657 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [32:29<00:00, 118.83it/s, loss=3.1, lr=2.31e-7] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [32:34<00:00, 118.55it/s, loss=2.19, lr=4.62e-7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 2 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [32:36<00:00, 118.42it/s, loss=2.42, lr=6.93e-7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 3 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [32:23<00:00, 119.18it/s, loss=2.16, lr=9.25e-7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 4 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 231657/231657 [32:22<00:00, 119.25it/s, loss=2.1, lr=1.16e-6] \n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import tqdm\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-6\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 500\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "proc_seq_count = 0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_jokes_tens = None\n",
        "\n",
        "losses = []\n",
        "lrs = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    progress_bar = tqdm.tqdm(joke_dataloader, desc=\"Training\")\n",
        "    \n",
        "    for sample in progress_bar:\n",
        "\n",
        "        sentence, tokens = sample\n",
        "        \n",
        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
        "        joke_tens = tokens.input_ids[0].to(device)\n",
        "\n",
        "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        # The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            # The next joke does not fit in so we process the sequence and leave the last joke \n",
        "            # as the start for next sequence \n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                #Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "        ################## Sequence ready, process it trough the model ##################\n",
        "            \n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "                    \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        losses.append(loss.item())\n",
        "        lrs.append(scheduler.get_last_lr()[0])\n",
        "        if batch_count == 10:\n",
        "            batch_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "losses_np = np.array(losses)\n",
        "lrs_np = np.array(lrs)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(losses_np, label='loss')\n",
        "plt.plot(lrs_np, label='learning rate')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Infer\u00eancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver como o modelo faz piadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "JOKE:!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"JOKE:\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode-se ver que voc\u00ea passa uma sequ\u00eancia com a palavra `joke` e ele retorna uma piada. Mas se voc\u00ea retornar outra sequ\u00eancia n\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "My dog is cute and!!!!!!!!!!!!!!!\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"My dog is cute and\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ajuste fino do GPT-2 para classifica\u00e7\u00e3o de senten\u00e7as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos fazer um treinamento com as bibliotecas do Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a usar o conjunto de dados `imdb` de classifica\u00e7\u00e3o de senten\u00e7as em positivas e negativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos v\u00ea-lo um pouco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='imdb', config_name='plain_text', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name='imdb')}, download_checksums={'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet': {'num_bytes': 20979968, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet': {'num_bytes': 20470363, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet': {'num_bytes': 41996509, 'checksum': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver as funcionalidades que este conjunto de dados possui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].info.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O conjunto de dados cont\u00e9m strings e classes. Al\u00e9m disso, existem dois tipos de classes, `pos` e `neg`. Vamos criar uma vari\u00e1vel com o n\u00famero de classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_clases = len(dataset[\"train\"].unique(\"label\"))\n",
        "num_clases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "checkpoints = \"openai-community/gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoints, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos um tokenizador, podemos tokenizar o conjunto de dados, pois o modelo s\u00f3 entende tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2ForSequenceClassification\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(checkpoints, num_labels=num_clases).half()\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avalia\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma m\u00e9trica de avalia\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Treinador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o treinador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Treinamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "xMx9AyTZxsrf",
        "outputId": "4665ef22-52f7-48ab-aa1a-9ae02f282850"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4689/4689 1:27:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.379400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4689, training_loss=0.04045845954294626, metrics={'train_runtime': 5271.3532, 'train_samples_per_second': 14.228, 'train_steps_per_second': 0.89, 'total_flos': 3.91945125888e+16, 'train_loss': 0.04045845954294626, 'epoch': 3.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Infer\u00eancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testamos o modelo ap\u00f3s trein\u00e1-lo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_sentiment(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs)\n",
        "    prediction = outputs.logits.argmax(-1).item()\n",
        "    return \"positive\" if prediction == 1 else \"negative\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "negative\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I hate this movie!\"\n",
        "print(get_sentiment(sentence))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "maximofn": {
      "date": "2024-07-09",
      "description_en": "Unlock the power of text generation with GPT-2, the latest open model from OpenAI \ud83d\udcb8! \ud83d\ude80 In this post, I take you by the hand through the architecture behind this model, and show you how to fine-tune it \ud83d\ude1c, code included. Read more and find out how GPT-2 can make your words more interesting than a human's \ud83d\udcac (or at least, than a bored human's) \ud83d\ude09",
      "description_es": "\u00a1Desbloquea el poder de la generaci\u00f3n de texto con GPT-2, el \u00faltimo modelo open de OpenAI \ud83d\udcb8! \ud83d\ude80 En este post, te llevo de la mano a trav\u00e9s de la arquitectura detr\u00e1s de este modelo, y te muestro c\u00f3mo fine-tunearlo \ud83d\ude1c, con c\u00f3digo incluido. \u00a1Lee m\u00e1s y descubre c\u00f3mo GPT-2 puede hacer que tus palabras sean m\u00e1s interesantes que las de un humano \ud83d\udcac (o al menos, que las de un humano aburrido) \ud83d\ude09",
      "description_pt": "Libere o poder da gera\u00e7\u00e3o de texto com o GPT-2, o mais recente modelo aberto da OpenAI \ud83d\udcb8! \ud83d\ude80 Nesta postagem, eu o conduzo pela arquitetura por tr\u00e1s desse modelo e mostro como ajust\u00e1-lo \ud83d\ude1c, incluindo o c\u00f3digo. Leia mais e descubra como o GPT-2 pode tornar suas palavras mais interessantes do que as de um ser humano \ud83d\udcac (ou, pelo menos, de um ser humano entediado) \ud83d\ude09",
      "end_url": "gpt2",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT2.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT2.webp",
      "keywords_en": "gpt2, openai, text generation, fine-tuning, nlp, natural language processing",
      "keywords_es": "gpt2, openai, generaci\u00f3n de texto, fine-tuning, nlp, procesamiento de lenguaje natural",
      "keywords_pt": "gpt2, openai, gera\u00e7\u00e3o de texto, ajuste fino, nlp, processamento de linguagem natural",
      "title_en": "GPT-2 \u2013 Language Models are Unsupervised Multitask Learners",
      "title_es": "GPT-2 \u2013 Language Models are Unsupervised Multitask Learners",
      "title_pt": "GPT-2 \u2013 Language Models are Unsupervised Multitask Learners"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}