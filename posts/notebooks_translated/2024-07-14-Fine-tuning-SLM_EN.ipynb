{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# Fine tuning SMLs with Hugging Face"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In this post we are going to see how to fine tune small language models, we are going to see how to fine tune text classification and text generation. First we are going to see how to do it with the Hugging Face libraries, since Hugging Face has become a very important player in the AI ecosystem right now.\n",
"\n",
"But although Hugging Face libraries are very important and useful, it is very important to know how the training is actually done and what is going on underneath, so let's repeat the training for classification and text generation but with Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.\n",
"\n",
"## Fine tuning for text classification with Hugging Face"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Login"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To be able to upload the training result to the hub we must first log in, for this we need a token\n",
"\n",
"To create a token, go to the [setings/tokens](https://huggingface.co/settings/tokens) page of your account, you will see something like this\n",
"\n",
"![User-Access-Token-dark](http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png)\n",
"\n",
"Click on `New token` and a window will appear to create a new token.\n",
"\n",
"![new-token-dark](http://maximofn.com/wp-content/uploads/2024/03/new-token-dark.png)\n",
"\n",
"We name the token and create it with the `write` role, or with the `Fine-grained` role, which allows us to select exactly what permissions the token will have.\n",
"\n",
"Once created, we copy and paste it below"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from huggingface_hub import notebook_login\n",
"notebook_login()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we download a dataset, in this case we are going to download one of reviews from [Amazon](https://huggingface.co/datasets/mteb/amazon_reviews_multi)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text'],\n",
"        num_rows: 200000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text'],\n",
"        num_rows: 5000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that you have a training set with 200,000 samples, a validation set with 5,000 samples and a test set with 5,000 samples."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see an example of the training set"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'id': 'en_0907914',\n",
" 'text': 'Mixed with fir it’s passable\\n\\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',\n",
" 'label': 3,\n",
" 'label_text': '3'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from random import randint\n",
"\n",
"idx = randint(0, len(dataset['train']) - 1)\n",
"dataset['train'][idx]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that it has the review in the `text` field and the score given by the user in the `label` field."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we are going to make a text classification model, we need to know how many classes we are going to have"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "5"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "num_classes = len(dataset['train'].unique('label'))\n",
"num_classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We are going to have 5 classes, now we are going to see the minimum value of these classes to know if the score starts at 0 or 1. For this we use the `unique` method"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'train': [0, 1, 2, 3, 4],\n",
" 'validation': [0, 1, 2, 3, 4],\n",
" 'test': [0, 1, 2, 3, 4]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset.unique('label')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The minimum value will be 0"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To train, the labels have to be in a field called `labels`, while in our dataset it is in a field called `label`, so we create the new field `lables` with the same value as `label`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a function that does what we want"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
      "def set_labels(example):\n",
"    example['labels'] = example['label']\n",
"    return example"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We apply the function to the dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
      "dataset = dataset.map(set_labels)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see how the dataset looks like"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'id': 'en_0907914',\n",
" 'text': 'Mixed with fir it’s passable\\n\\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',\n",
" 'label': 3,\n",
" 'label_text': '3',\n",
" 'labels': 3}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset['train'][idx]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokenizer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As in the dataset we have the reviews in text, we need to tokenize them in order to put the tokens into the model."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"openai-community/gpt2\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we create a function to tokenize the text. We are going to make it so that all sentences have the same length, so that the tokenizer will truncate when necessary and add padding tokens when necessary. We also tell it to return pytorch tensors."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We make the length of each statement 768 tokens because we are using the small GPT2 model, which as we saw in the [GPT2](https://maximofn.com/gpt2/#Arquitectura) post has an embedding dimension of 768 tokens."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's try tokenizing a text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize_function(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][idx])\n",
"Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2989\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2970\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2971\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2986\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2987\u001b[0m     )\n\u001b[1;32m   2988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2990\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2991\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2992\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2993\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2994\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2995\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2996\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2997\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2998\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2999\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3000\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3001\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3002\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3003\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3004\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3005\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3006\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3008\u001b[0m     )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3053\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3053\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3054\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3055\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   3056\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3057\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3058\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3059\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3060\u001b[0m )\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   3063\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3064\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3081\u001b[0m )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2792\u001b[0m     )\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2796\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2797\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2801\u001b[0m ):\n",
"\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ],
      "source": [
      "tokens = tokenize_function(dataset['train'][idx])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We get an error because the GPT2 tokenizer does not have a token for padding and asks us to assign one, it also suggests us to do `tokenizer.pad_token = tokenizer.eos_token`, so we do it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We test again the tokenization function"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1, 768]), torch.Size([1, 768]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens = tokenize_function(dataset['train'][idx])\n",
"tokens['input_ids'].shape, tokens['attention_mask'].shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now that we have checked that the function tokenizes well, we apply this function to the dataset, but we also apply it in batches so that it runs faster\n",
"\n",
"We also take advantage of this and eliminate the columns we do not need"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now let's see how the dataset looks like"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
"        num_rows: 200000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
"        num_rows: 5000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can see that we have the fields 'labels', 'input_ids' and 'attention_mask', which is what we are interested in training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We instantiate a model for sequence classification and we indicate the number of classes we have"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It tells us that the weights of the `score` layer have been initialized randomly and that we have to retrain them, let's see why this happens"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "The GPT2 model would look like this"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoModelForCausalLM\n",
"\n",
"casual_model = AutoModelForCausalLM.from_pretrained(checkpoint)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "While the GPT2 model for generating text is as follows"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at its architecture"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "GPT2LMHeadModel(\n",
"  (transformer): GPT2Model(\n",
"    (wte): Embedding(50257, 768)\n",
"    (wpe): Embedding(1024, 768)\n",
"    (drop): Dropout(p=0.1, inplace=False)\n",
"    (h): ModuleList(\n",
"      (0-11): 12 x GPT2Block(\n",
"        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (attn): GPT2Attention(\n",
"          (c_attn): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
"          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (mlp): GPT2MLP(\n",
"          (c_fc): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (act): NewGELUActivation()\n",
"          (dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"      )\n",
"    )\n",
"    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"  )\n",
"  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "casual_model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And now the architecture of the model we are going to use to classify the reviews"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "GPT2ForSequenceClassification(\n",
"  (transformer): GPT2Model(\n",
"    (wte): Embedding(50257, 768)\n",
"    (wpe): Embedding(1024, 768)\n",
"    (drop): Dropout(p=0.1, inplace=False)\n",
"    (h): ModuleList(\n",
"      (0-11): 12 x GPT2Block(\n",
"        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (attn): GPT2Attention(\n",
"          (c_attn): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
"          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (mlp): GPT2MLP(\n",
"          (c_fc): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (act): NewGELUActivation()\n",
"          (dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"      )\n",
"    )\n",
"    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"  )\n",
"  (score): Linear(in_features=768, out_features=5, bias=False)\n",
")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "There are two things to mention here\n",
"\n",
" + The first is that in both, the first layer has dimensions of 50257x768, which corresponds to 50257 possible tokens of the GPT2 vocabulary and 768 dimensions of the embedding, so we have done well to tokenize the reviews with a size of 768 tokens.\n",
" + The second is that the `casual` model (the text generation model) has at the end a `Linear` layer that generates 50257 values, that is, it is in charge of predicting the next token and gives a value to each possible token. While the classification model has a `Linear` layer that only generates 5 values, one for each class, which will give us the probability that the review belongs to each class.\n",
"\n",
"That is why we got the message that the weights of the `score` layer had been initialized randomly, because the transformers library has removed the `Linear` layer of 768x50257 and added a `Linear` layer of 768x5, it has initialized it with random values and we have to train it for our particular problem."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We delete the casual model, because we are not going to use it."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
      "del casual_model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Trainer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's now configure the training arguments"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import TrainingArguments\n",
"\n",
"metric_name = \"accuracy\"\n",
"model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
"LR = 2e-5\n",
"BS_TRAIN = 28\n",
"BS_EVAL = 40\n",
"EPOCHS = 3\n",
"WEIGHT_DECAY = 0.01\n",
"\n",
"training_args = TrainingArguments(\n",
"    model_name,\n",
"    eval_strategy=\"epoch\",\n",
"    save_strategy=\"epoch\",\n",
"    learning_rate=LR,\n",
"    per_device_train_batch_size=BS_TRAIN,\n",
"    per_device_eval_batch_size=BS_EVAL,\n",
"    num_train_epochs=EPOCHS,\n",
"    weight_decay=WEIGHT_DECAY,\n",
"    lr_scheduler_type=\"cosine\",\n",
"    warmup_ratio = 0.1,\n",
"    fp16=True,\n",
"    load_best_model_at_end=True,\n",
"    metric_for_best_model=metric_name,\n",
"    push_to_hub=True,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Define a metric for the validation dataloader"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
      "import numpy as np\n",
"from evaluate import load\n",
"\n",
"metric = load(\"accuracy\")\n",
"\n",
"def compute_metrics(eval_pred):\n",
"    print(eval_pred)\n",
"    predictions, labels = eval_pred\n",
"    predictions = np.argmax(predictions, axis=1)\n",
"    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We now define the trainer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import Trainer\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=dataset['train'],\n",
"    eval_dataset=dataset['validation'],\n",
"    tokenizer=tokenizer,\n",
"    compute_metrics=compute_metrics,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We train"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51caae4c5a874c81938cee530d7f2017",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "  0%|          | 0/600000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "ename": "ValueError",
          "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label', 'labels']",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1876\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1877\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1878\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1879\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1880\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1881\u001b[0m     )\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m pad_without_fast_tokenizer_warning(\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m    273\u001b[0m         features,\n\u001b[1;32m    274\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[1;32m    275\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m    276\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    277\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_tensors,\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3299\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3297\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3302\u001b[0m     )\n\u001b[1;32m   3304\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
"\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label', 'labels']"
          ]
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We get an error again because the model does not have a padding token assigned, so as with the tokenizer we assign it to it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
      "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We recreate the trainer arguments with the new model, which now has a padding token, the trainer and train again."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "nLb8ZZxgmWB9",
        "outputId": "b0a7c6f5-1b10-43a6-cc2d-75d1dbf88714"
      },
      "outputs": [],
      "source": [
      "training_args = TrainingArguments(\n",
"    model_name,\n",
"    eval_strategy=\"epoch\",\n",
"    save_strategy=\"epoch\",\n",
"    learning_rate=LR,\n",
"    per_device_train_batch_size=BS_TRAIN,\n",
"    per_device_eval_batch_size=BS_EVAL,\n",
"    num_train_epochs=EPOCHS,\n",
"    weight_decay=WEIGHT_DECAY,\n",
"    lr_scheduler_type=\"cosine\",\n",
"    warmup_ratio = 0.1,\n",
"    fp16=True,\n",
"    load_best_model_at_end=True,\n",
"    metric_for_best_model=metric_name,\n",
"    push_to_hub=True,\n",
"    logging_dir=\"./runs\",\n",
")\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=dataset['train'],\n",
"    eval_dataset=dataset['validation'],\n",
"    tokenizer=tokenizer,\n",
"    compute_metrics=compute_metrics,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now that we have seen that all is well, we can train."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "nLb8ZZxgmWB9",
        "outputId": "b0a7c6f5-1b10-43a6-cc2d-75d1dbf88714"
      },
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='2250' max='21429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [ 2250/21429 45:38 < 6:29:27, 0.82 it/s, Epoch 0.31/3]\n",
"    </div>\n",
"    <table border=\"1\" class=\"dataframe\">\n",
"  <thead>\n",
" <tr style=\"text-align: left;\">\n",
"      <th>Epoch</th>\n",
"      <th>Training Loss</th>\n",
"      <th>Validation Loss</th>\n",
"    </tr>\n",
"  </thead>\n",
"  <tbody>\n",
"  </tbody>\n",
"</table><p>"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='21429' max='21429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [21429/21429 7:19:25, Epoch 3/3]\n",
"    </div>\n",
"    <table border=\"1\" class=\"dataframe\">\n",
"  <thead>\n",
" <tr style=\"text-align: left;\">\n",
"      <th>Epoch</th>\n",
"      <th>Training Loss</th>\n",
"      <th>Validation Loss</th>\n",
"      <th>Accuracy</th>\n",
"    </tr>\n",
"  </thead>\n",
"  <tbody>\n",
"    <tr>\n",
"      <td>1</td>\n",
"      <td>0.807400</td>\n",
"      <td>0.820341</td>\n",
"      <td>0.652000</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>2</td>\n",
"      <td>0.751900</td>\n",
"      <td>0.802189</td>\n",
"      <td>0.654600</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>3</td>\n",
"      <td>0.718100</td>\n",
"      <td>0.810221</td>\n",
"      <td>0.657800</td>\n",
"    </tr>\n",
"  </tbody>\n",
"</table><p>"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "<transformers.trainer_utils.EvalPrediction object at 0x782767ea1450>\n",
"<transformers.trainer_utils.EvalPrediction object at 0x782767eeefe0>\n",
"<transformers.trainer_utils.EvalPrediction object at 0x782767eecfd0>\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "TrainOutput(global_step=21429, training_loss=0.7846888848762739, metrics={'train_runtime': 26367.7801, 'train_samples_per_second': 22.755, 'train_steps_per_second': 0.813, 'total_flos': 2.35173445632e+17, 'train_loss': 0.7846888848762739, 'epoch': 3.0})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Evaluation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once trained we evaluate on the test dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "OsCTTRUcmWB9",
        "outputId": "d9d816a8-1559-4774-c274-60047692e4bc"
      },
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [125/125 01:15]\n",
"    </div>\n",
"    "
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "<transformers.trainer_utils.EvalPrediction object at 0x7826ddfded40>\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "{'eval_loss': 0.7973636984825134,\n",
" 'eval_accuracy': 0.6626,\n",
" 'eval_runtime': 76.3016,\n",
" 'eval_samples_per_second': 65.529,\n",
" 'eval_steps_per_second': 1.638,\n",
" 'epoch': 3.0}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "trainer.evaluate(eval_dataset=dataset['test'])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Publish the model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now that we have our model trained, we can share it with the world, so first we create a model card."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "trainer.create_model_card()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And now we can publish it. As the first thing we have done is to log in with the huggingface hub, we can upload it to our hub without any problem."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "trainer.push_to_hub()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model use"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We clean as much as possible"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"import gc\n",
"\n",
"\n",
"def clear_hardwares():\n",
"    torch.clear_autocast_cache()\n",
"    torch.cuda.ipc_collect()\n",
"    torch.cuda.empty_cache()\n",
"    gc.collect()\n",
"\n",
"\n",
"clear_hardwares()\n",
"clear_hardwares()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we have uploaded the model to our hub we can download it and use it."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import pipeline\n",
"\n",
"user = \"maximofn\"\n",
"checkpoints = f\"{user}/{model_name}\"\n",
"task = \"text-classification\"\n",
"classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now if we want to return the probability of all classes, we simply use the classifier we just instantiated, with the parameter `top_k=None`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_4', 'score': 0.8253807425498962},\n",
" {'label': 'LABEL_3', 'score': 0.15411493182182312},\n",
" {'label': 'LABEL_2', 'score': 0.013907806016504765},\n",
" {'label': 'LABEL_0', 'score': 0.003939222544431686},\n",
" {'label': 'LABEL_1', 'score': 0.0026572425849735737}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "labels = classifier(\"I love this product\", top_k=None)\n",
"labels"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If we only want the class with the highest probability we do the same but with the parameter `top_k=1`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_4', 'score': 0.8253807425498962}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "label = classifier(\"I love this product\", top_k=1)\n",
"label"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "And if we want n classes we do the same but with the parameter `top_k=n`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_4', 'score': 0.8253807425498962},\n",
" {'label': 'LABEL_3', 'score': 0.15411493182182312}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "two_labels = classifier(\"I love this product\", top_k=2)\n",
"two_labels"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We can also test the model with Automodel and AutoTokenizer."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import torch\n",
"\n",
"model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
"user = \"maximofn\"\n",
"checkpoint = f\"{user}/{model_name}\"\n",
"num_classes = 5\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes).half().eval().to(\"cuda\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[0.003963470458984375,\n",
" 0.0026721954345703125,\n",
" 0.01397705078125,\n",
" 0.154541015625,\n",
" 0.82470703125]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens = tokenizer.encode(\"I love this product\", return_tensors=\"pt\").to(model.device)\n",
"with torch.no_grad():\n",
"    output = model(tokens)\n",
"logits = output.logits\n",
"lables = torch.softmax(logits, dim=1).cpu().numpy().tolist()\n",
"lables[0]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If you want to test the model further you can see it in [Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Fine tuning for text generation with Hugging Face"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To make sure I don't have VRAM memory problems, I reboot the notebook."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Login"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In order to upload the training result to the hub we must first log in, for this we need a token\n",
"\n",
"To create a token, go to the [setings/tokens](https://huggingface.co/settings/tokens) page of your account, you will see something like this\n",
"\n",
"![User-Access-Token-dark](http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png)\n",
"\n",
"Click on `New token` and a window will appear to create a new token.\n",
"\n",
"![new-token-dark](http://maximofn.com/wp-content/uploads/2024/03/new-token-dark.png)\n",
"\n",
"We name the token and create it with the `write` role, or with the `Fine-grained` role, which allows us to select exactly what permissions the token will have.\n",
"\n",
"Once created, we copy and paste it below"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from huggingface_hub import notebook_login\n",
"notebook_login()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's use a dataset of [English jokes](https://huggingface.co/datasets/Maximofn/short-jokes-dataset)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['ID', 'Joke'],\n",
"        num_rows: 231657\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
"jokes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's take a look at it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['ID', 'Joke'],\n",
"        num_rows: 231657\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "jokes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that it is a single training set of more than 200 thousand jokes. So later we will have to divide it into training and evaluation."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see a sample"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'ID': 198387,\n",
" 'Joke': 'My hot dislexic co-worker said she had an important massage to give me in her office... When I got there, she told me it can wait until I put on some clothes.'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from random import randint\n",
"\n",
"idx = randint(0, len(jokes['train']) - 1)\n",
"jokes['train'][idx]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that it has an ID of the joke that does not interest us at all and the joke itself."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In case you have low GPU memory I will make a subset of the dataset, choose the percentage of jokes you want to use."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Dataset({\n",
"    features: ['ID', 'Joke'],\n",
"    num_rows: 231657\n",
"})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "percent_of_train_dataset = 1    # If you want 50% of the dataset, set this to 0.5\n",
"\n",
"subset_dataset = jokes[\"train\"].select(range(int(len(jokes[\"train\"]) * percent_of_train_dataset)))\n",
"subset_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We now divide the subset into a training set and a validation set."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583\n"
          ]
        }
      ],
      "source": [
      "percent_of_train_dataset = 0.90\n",
"\n",
"split_dataset = subset_dataset.train_test_split(train_size=int(subset_dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
"train_dataset = split_dataset[\"train\"]\n",
"validation_test_dataset = split_dataset[\"test\"]\n",
"\n",
"split_dataset = validation_test_dataset.train_test_split(train_size=int(validation_test_dataset.num_rows * 0.5), seed=19, shuffle=False)\n",
"validation_dataset = split_dataset[\"train\"]\n",
"test_dataset = split_dataset[\"test\"]\n",
"\n",
"print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(validation_dataset)}. Size of the test set: {len(test_dataset)}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokenizer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instantiate the tokenizer. We instantiate the padding token of the tokenizer so that it does not give us error as before."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoints = \"openai-community/gpt2\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"tokenizer.padding_side = \"right\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We are going to add two new tokens for joke start and joke end to have more control"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Added 2 tokens\n"
          ]
        }
      ],
      "source": [
      "new_tokens = ['<SJ>', '<EJ>']   # Start and end of joke tokens\n",
"\n",
"num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
"print(f\"Added {num_added_tokens} tokens\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a function to add the new tokens to the sentences"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
      "joke_column = \"Joke\"\n",
"\n",
"def format_joke(example):\n",
"    example[joke_column] = '<SJ> ' + example['Joke'] + ' <EJ>'\n",
"    return example"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Select the columns we do not need"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "['ID']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "remove_columns = [column for column in train_dataset.column_names if column != joke_column]\n",
"remove_columns"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Format the dataset and delete the columns that are not needed"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "train_dataset = train_dataset.map(format_joke, remove_columns=remove_columns)\n",
"validation_dataset = validation_dataset.map(format_joke, remove_columns=remove_columns)\n",
"test_dataset = test_dataset.map(format_joke, remove_columns=remove_columns)\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we create a function to tokenize the jokes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[joke_column], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We tokenize the dataset and delete the column with the text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we instantiate the model for text generation and assign the end of string token to the pading token"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoModelForCausalLM\n",
"\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
"model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see the size of the model vocabulary"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "50257"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "vocab_size = model.config.vocab_size\n",
"vocab_size"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "It has 50257 tokens, which is the size of the GPT2 vocabulary. But since we said that we were going to create two new tokens with the start of joke and the end of joke, we added them to the model"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Old vocab size: 50257. New vocab size: 50259. Added 2 tokens\n"
          ]
        }
      ],
      "source": [
      "model.resize_token_embeddings(len(tokenizer))\n",
"\n",
"new_vocab_size = model.config.vocab_size\n",
"print(f\"Old vocab size: {vocab_size}. New vocab size: {new_vocab_size}. Added {new_vocab_size - vocab_size} tokens\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Two new tokens have been added"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We configure the training parameters"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import TrainingArguments\n",
"\n",
"metric_name = \"accuracy\"\n",
"model_name = \"GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM\"\n",
"output_dir = f\"./training_results\"\n",
"LR = 2e-5\n",
"BS_TRAIN = 28\n",
"BS_EVAL = 32\n",
"EPOCHS = 3\n",
"WEIGHT_DECAY = 0.01\n",
"WARMUP_STEPS = 100\n",
"\n",
"training_args = TrainingArguments(\n",
"    model_name,\n",
"    eval_strategy=\"epoch\",\n",
"    save_strategy=\"epoch\",\n",
"    learning_rate=LR,\n",
"    per_device_train_batch_size=BS_TRAIN,\n",
"    per_device_eval_batch_size=BS_EVAL,\n",
"    warmup_steps=WARMUP_STEPS,\n",
"    num_train_epochs=EPOCHS,\n",
"    weight_decay=WEIGHT_DECAY,\n",
"    lr_scheduler_type=\"cosine\",\n",
"    warmup_ratio = 0.1,\n",
"    fp16=True,\n",
"    load_best_model_at_end=True,\n",
"    # metric_for_best_model=metric_name,\n",
"    push_to_hub=True,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we do not use `metric_for_best_model`, after defining the trainer we explain why"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We define the trainer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import Trainer\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=train_dataset,\n",
"    eval_dataset=validation_dataset,\n",
"    tokenizer=tokenizer,\n",
"    # compute_metrics=compute_metrics,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "In this case we do not pass a `compute_metrics` function, if it is not passed, during the evaluation the `loss` will be used to evaluate the model. That is why when defining the arguments we do not define `metric_for_best_model`, because we are not going to use a metric to evaluate the model, but the `loss`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We train"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3497c5690f2946c6936f028f7d1f881d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "  0%|          | 0/625473 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "ename": "ValueError",
          "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3282\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3284\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3285\u001b[0m         )\n\u001b[1;32m   3286\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3287\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
"\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
          ]
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see, it gives us an error, it tells us that the model does not return the value of the loss, which is key to be able to train, let's see why."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First, let's see what an example of the dataset looks like."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'input_ids': [50257,\n",
"  4162,\n",
"  750,\n",
"  262,\n",
"  18757,\n",
"  6451,\n",
"  2245,\n",
"  2491,\n",
"  30,\n",
"  4362,\n",
"  340,\n",
"  373,\n",
"  734,\n",
"  10032,\n",
"  13,\n",
"  220,\n",
"  50258,\n",
"  50256,\n",
"  50256,\n",
"  ...,\n",
"  50256,\n",
"  50256,\n",
"  50256],\n",
" 'attention_mask': [1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  0,\n",
"  0,\n",
"  0,\n",
"  ...,\n",
"  0,\n",
"  0,\n",
"  0]}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "idx = randint(0, len(train_dataset) - 1)\n",
"sample = train_dataset[idx]\n",
"sample"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see, we have a dictionary with the `input_ids` and the `attention_mask`, if we pass it to the model we obtain this"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "None\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"\n",
"output = model(\n",
"    input_ids=torch.Tensor(sample[\"input_ids\"]).long().unsqueeze(0).to(model.device),\n",
"    attention_mask=torch.Tensor(sample[\"attention_mask\"]).long().unsqueeze(0).to(model.device),\n",
")\n",
"print(output.loss)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see it does not return the value of the loss because it is waiting for a value for `labels`, which we have not passed it. In the previous example, in which we did fine tuning for text classification, we said that the labels had to be passed to a field of the dataset called `labels`, but in this case we do not have that field in the dataset.\n",
"\n",
"If we now assign the `lables` to the `input_ids` and look again at the `input_ids` loss"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "tensor(102.1873, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"\n",
"output = model(\n",
"    input_ids=torch.Tensor(sample[\"input_ids\"]).long().unsqueeze(0).to(model.device),\n",
"    attention_mask=torch.Tensor(sample[\"attention_mask\"]).long().unsqueeze(0).to(model.device),\n",
"    labels=torch.Tensor(sample[\"input_ids\"]).long().unsqueeze(0).to(model.device)\n",
")\n",
"print(output.loss)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we get a `loss`.\n",
"\n",
"So we have two options, add a `labels` field to the dataset, with the values of `input_ids` or use a function of the `transformers` library called `data_collator`, in this case we will use `DataCollatorForLanguageModeling`. Let's take a look at it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import DataCollatorForLanguageModeling\n",
"\n",
"my_data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We pass the `sample` sample through this `data_collator`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "collated_sample = my_data_collator([sample]).to(model.device)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see how the output is"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input_ids (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,\n",
"           340,   373,   734, 10032,    13,   220, 50258, 50256, ..., 50256, 50256]],\n",
"       device='cuda:0')\n",
"attention_mask (torch.Size([1, 768])): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ..., 0, 0]],\n",
"       device='cuda:0')\n",
"labels (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,\n",
"           340,   373,   734, 10032,    13,   220, 50258,  -100,  ...,  -100,  -100]],\n",
"       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
      "for key, value in collated_sample.items():\n",
"    print(f\"{key} ({value.shape}): {value}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As you can see, the `data_collator` has created a `labels` field and assigned it the values of `input_ids`. The tokens that are masked have been assigned the value -100. This is because when we define the `data_collator` we pass the parameter `mlm=False`, which means that we are not doing `Masked Language Modeling`, but `Language Modeling`, that is why it does not mask any original token."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see if now we get a `loss` with this `data_collator`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor(102.7181, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output = model(**collated_sample)\n",
"output.loss"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "So we redefine the `trainer` with the `data_collator` and we train again"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import DataCollatorForLanguageModeling\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=train_dataset,\n",
"    eval_dataset=validation_dataset,\n",
"    tokenizer=tokenizer,\n",
"    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='22341' max='22341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [22341/22341 2:33:28, Epoch 3/3]\n",
"    </div>\n",
"    <table border=\"1\" class=\"dataframe\">\n",
"  <thead>\n",
" <tr style=\"text-align: left;\">\n",
"      <th>Epoch</th>\n",
"      <th>Training Loss</th>\n",
"      <th>Validation Loss</th>\n",
"    </tr>\n",
"  </thead>\n",
"  <tbody>\n",
"    <tr>\n",
"      <td>1</td>\n",
"      <td>3.386600</td>\n",
"      <td>3.258979</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>2</td>\n",
"      <td>3.259900</td>\n",
"      <td>3.199673</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>3</td>\n",
"      <td>3.212600</td>\n",
"      <td>3.192009</td>\n",
"    </tr>\n",
"  </tbody>\n",
"</table><p>"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "TrainOutput(global_step=22341, training_loss=3.505178199598342, metrics={'train_runtime': 9209.5353, 'train_samples_per_second': 67.916, 'train_steps_per_second': 2.426, 'total_flos': 2.45146666696704e+17, 'train_loss': 3.505178199598342, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Evaluation"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Once the model is trained we evaluate the model on the test dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='362' max='362' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [362/362 01:04]\n",
"    </div>\n",
"    "
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/plain": [
            "{'eval_loss': 3.201305866241455,\n",
" 'eval_runtime': 65.0033,\n",
" 'eval_samples_per_second': 178.191,\n",
" 'eval_steps_per_second': 5.569,\n",
" 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "trainer.evaluate(eval_dataset=test_dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Publish the model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the model card"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "trainer.create_model_card()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We publish it"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "706edd6325984537ad8202c3de937995",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "events.out.tfevents.1720875425.8de3af1b431d.6946.1:   0%|          | 0.00/364 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
            "CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM/commit/d107b3bb0e02076483238f9975697761015ec390', commit_message='End of training', commit_description='', oid='d107b3bb0e02076483238f9975697761015ec390', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "trainer.push_to_hub()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model use"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We clean as much as possible"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"import gc\n",
"\n",
"\n",
"def clear_hardwares():\n",
"    torch.clear_autocast_cache()\n",
"    torch.cuda.ipc_collect()\n",
"    torch.cuda.empty_cache()\n",
"    gc.collect()\n",
"\n",
"\n",
"clear_hardwares()\n",
"clear_hardwares()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Download the model and tokenizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"user = \"maximofn\"\n",
"checkpoints = f\"{user}/{model_name}\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"tokenizer.padding_side = \"right\"\n",
"\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
"model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We check that the tokenizer and the model have the 2 extra tokens we have added."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "tokenizer_vocab: 50259. model_vocab: 50259\n"
          ]
        }
      ],
      "source": [
      "tokenizer_vocab = tokenizer.get_vocab()\n",
"model_vocab = model.config.vocab_size\n",
"print(f\"tokenizer_vocab: {len(tokenizer_vocab)}. model_vocab: {model_vocab}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see that they have 50259 tokens, i.e., the 50257 tokens of GPT2 plus the 2 we added"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a function to generate jokes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "def generate_joke(prompt_text):\n",
"    text = f\"<SJ> {prompt_text}\"\n",
"    tokens = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
"    with torch.no_grad():\n",
"        output = model.generate(**tokens, max_new_tokens=256, eos_token_id=tokenizer.encode(\"<EJ>\")[-1])\n",
"    return tokenizer.decode(output[0], skip_special_tokens=False)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We generate a joke"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "\"<SJ> Why didn't the frog cross the road? Because he was frog-in-the-face. <EJ>\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "generate_joke(\"Why didn't the frog cross the road?\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "If you want to test the model further you can see it in [Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM](https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Fine tuning for text classification with Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We repeat the Pytorch training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Restart the notebook to make sure"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We downloaded the same dataset as when we did the training with the Hugging Face libraries."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a variable with the number of classes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "5"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "num_classes = len(dataset['train'].unique('label'))\n",
"num_classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Before we processed the whole dataset to create a field called `labels`, but now it is not necessary because as we are going to program everything ourselves, we adapt to what the dataset looks like."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokenizer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the tokenizer. We assign the padding token so that it does not give us an error as before."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"openai-community/gpt2\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create a function for tokenizing the dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We tokenize it. We eliminate columns that we don't need, but now we leave the text column"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'label_text'])"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
"        num_rows: 200000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
"        num_rows: 5000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "len subset_train: 200000, len subset_validation: 5000, len subset_test: 5000\n"
          ]
        }
      ],
      "source": [
      "percentage = 1\n",
"subset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
"percentage = 1\n",
"subset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
"subset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
"print(f\"len subset_train: {len(subset_train)}, len subset_validation: {len(subset_validation)}, len subset_test: {len(subset_test)}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We import the weights and assign the padding token"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
"model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Device"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Create the device where everything is going to be executed"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"\n",
"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We pass the model to the device and pass it to FP16 so that it occupies less memory."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n"
          ]
        }
      ],
      "source": [
      "model.half().to(device)\n",
"print()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Pytorch Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Create a pytorch dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import Dataset\n",
"\n",
"class ReviewsDataset(Dataset):\n",
"    def __init__(self, huggingface_dataset):\n",
"        self.dataset = huggingface_dataset\n",
"\n",
"    def __getitem__(self, idx):\n",
"        label = self.dataset[idx]['label']\n",
"        input_ids = torch.tensor(self.dataset[idx]['input_ids'])\n",
"        attention_mask = torch.tensor(self.dataset[idx]['attention_mask'])\n",
"        return input_ids, attention_mask, label\n",
"\n",
"    def __len__(self):\n",
"        return len(self.dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instantiate the datasets"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "train_dataset = ReviewsDataset(subset_train)\n",
"validatation_dataset = ReviewsDataset(subset_validation)\n",
"test_dataset = ReviewsDataset(subset_test)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see a sample"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([768]), torch.Size([768]), 0)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "input_ids, at_mask, label = train_dataset[0]\n",
"input_ids.shape, at_mask.shape, label"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Pytorch Dataloader"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We now create a pytorch dataloader"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import DataLoader\n",
"\n",
"BS = 12\n",
"\n",
"train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
"validation_loader = DataLoader(validatation_dataset, batch_size=BS)\n",
"test_loader = DataLoader(test_dataset, batch_size=BS)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see a sample"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([12, 768]),\n",
" torch.Size([12, 768]),\n",
" tensor([2, 1, 2, 0, 3, 3, 0, 4, 3, 3, 4, 2]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "input_ids, at_mask, labels = next(iter(train_loader))\n",
"input_ids.shape, at_mask.shape, labels"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "To check that everything is OK we pass the sample to the model to see if everything is OK. First we pass the tokens to the device"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "input_ids = input_ids.to(device)\n",
"at_mask = at_mask.to(device)\n",
"labels = labels.to(device)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we pass them to the model"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "odict_keys(['loss', 'logits', 'past_key_values'])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output = model(input_ids=input_ids, attention_mask=at_mask, labels=labels)\n",
"output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see it gives us the loss and the logits"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor(5.9414, device='cuda:0', dtype=torch.float16,\n",
"       grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output['loss']"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([[ 6.1953e+00, -1.2275e+00, -2.4824e+00,  5.8867e+00, -1.4734e+01],\n",
"        [ 5.4062e+00, -8.4570e-01, -2.3203e+00,  5.1055e+00, -1.1555e+01],\n",
"        [ 6.1641e+00, -9.3066e-01, -2.5664e+00,  6.0039e+00, -1.4570e+01],\n",
"        [ 5.2266e+00, -4.2358e-01, -2.0801e+00,  4.7461e+00, -1.1570e+01],\n",
"        [ 3.8184e+00, -2.3460e-03, -1.7666e+00,  3.4160e+00, -7.7969e+00],\n",
"        [ 4.1641e+00, -4.8169e-01, -1.6914e+00,  3.9941e+00, -8.7734e+00],\n",
"        [ 4.6758e+00, -3.0298e-01, -2.1641e+00,  4.1055e+00, -9.3359e+00],\n",
"        [ 4.1953e+00, -3.2471e-01, -2.1875e+00,  3.9375e+00, -8.3438e+00],\n",
"        [-1.1650e+00,  1.3564e+00, -6.2158e-01, -6.8115e-01,  4.8672e+00],\n",
"        [ 4.4961e+00, -8.7891e-02, -2.2793e+00,  4.2812e+00, -9.3359e+00],\n",
"        [ 4.9336e+00, -2.6627e-03, -2.1543e+00,  4.3711e+00, -1.0742e+01],\n",
"        [ 5.9727e+00, -4.3152e-02, -1.4551e+00,  4.3438e+00, -1.2117e+01]],\n",
"       device='cuda:0', dtype=torch.float16, grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output['logits']"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Metric"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We are going to create a function to obtain the metric, which in this case is going to be the accuracy"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def predicted_labels(logits):\n",
"    percent = torch.softmax(logits, dim=1)\n",
"    predictions = torch.argmax(percent, dim=1)\n",
"    return predictions"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def compute_accuracy(logits, labels):\n",
"    predictions = predicted_labels(logits)\n",
"    correct = (predictions == labels).float()\n",
"    return correct.mean()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see how well it calculates"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "0.1666666716337204"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "compute_accuracy(output['logits'], labels).item()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Optimizer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we are going to need an optimizer, we create one"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
"  warnings.warn(\n"
          ]
        }
      ],
      "source": [
      "from transformers import AdamW\n",
"\n",
"LR = 2e-5\n",
"optimizer = AdamW(model.parameters(), lr=LR)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the training loop"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Epoch 1: 100%|██████████| 16667/16667 [44:13<00:00,  6.28it/s, train_loss=nan]\n",
"Epoch 1: 100%|██████████| 417/417 [00:32<00:00, 12.72it/s, valid_loss=nan, accuracy=0]\n",
"Epoch 2: 100%|██████████| 16667/16667 [44:06<00:00,  6.30it/s, train_loss=nan]\n",
"Epoch 2: 100%|██████████| 417/417 [00:32<00:00, 12.77it/s, valid_loss=nan, accuracy=0]\n",
"Epoch 3: 100%|██████████| 16667/16667 [44:03<00:00,  6.30it/s, train_loss=nan]\n",
"Epoch 3: 100%|██████████| 417/417 [00:32<00:00, 12.86it/s, valid_loss=nan, accuracy=0]\n"
          ]
        }
      ],
      "source": [
      "from tqdm import tqdm\n",
"\n",
"EPOCHS = 3\n",
"\n",
"accuracy = 0\n",
"\n",
"for epoch in range(EPOCHS):\n",
"    model.train()\n",
"    train_loss = 0\n",
"    progresbar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}')\n",
"    for input_ids, at_mask, labels in progresbar:\n",
"        input_ids = input_ids.to(device)\n",
"        at_mask = at_mask.to(device)\n",
"        label = labels.to(device)\n",
"\n",
"        output = model(input_ids=input_ids, attention_mask=at_mask, labels=label)\n",
"\n",
"        loss = output['loss']\n",
"        train_loss += loss.item()\n",
"        optimizer.zero_grad()\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"        progresbar.set_postfix({'train_loss': loss.item()})\n",
"    train_loss /= len(train_loader)\n",
"    progresbar.set_postfix({'train_loss': train_loss})\n",
"\n",
"    model.eval()\n",
"    valid_loss = 0\n",
"    progresbar = tqdm(validation_loader, total=len(validation_loader), desc=f'Epoch {epoch + 1}')\n",
"    for input_ids, at_mask, labels in progresbar:\n",
"        input_ids = input_ids.to(device)\n",
"        at_mask = at_mask.to(device)\n",
"        labels = labels.to(device)\n",
"\n",
"        output = model(input_ids=input_ids, attention_mask=at_mask, labels=labels)\n",
"\n",
"        loss = output['loss']\n",
"        valid_loss += loss.item()\n",
"\n",
"        step_accuracy = compute_accuracy(output['logits'], labels)\n",
"        accuracy += step_accuracy\n",
"        progresbar.set_postfix({'valid_loss': loss.item(), 'accuracy': step_accuracy.item()})\n",
"\n",
"    valid_loss /= len(validation_loader)\n",
"    accuracy /= len(validation_loader)\n",
"    progresbar.set_postfix({'valid_loss': valid_loss, 'accuracy': accuracy})"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model use"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's test the model we have trained"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "First we tokenize a text"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1, 768]), torch.Size([1, 768]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "input_tokens = tokenize_function({\"text\": \"I love this product. It is amazing.\"})\n",
"input_tokens['input_ids'].shape, input_tokens['attention_mask'].shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we pass it to the model"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([[nan, nan, nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
"       grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output = model(input_ids=input_tokens['input_ids'].to(device), attention_mask=input_tokens['attention_mask'].to(device))\n",
"output['logits']"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see the predictions of these logits"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "predicted = predicted_labels(output['logits'])\n",
"predicted"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Fine tuning for text generation with Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We repeat the Pytorch training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Restart the notebook to make sure"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We download again the dataset of jokes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['ID', 'Joke'],\n",
"        num_rows: 231657\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
"jokes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Create a subset in case you are short on memory"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Dataset({\n",
"    features: ['ID', 'Joke'],\n",
"    num_rows: 231657\n",
"})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "percent_of_train_dataset = 1    # If you want 50% of the dataset, set this to 0.5\n",
"\n",
"subset_dataset = jokes[\"train\"].select(range(int(len(jokes[\"train\"]) * percent_of_train_dataset)))\n",
"subset_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We divide the dataset into subsets for training, validation and test."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583\n"
          ]
        }
      ],
      "source": [
      "percent_of_train_dataset = 0.90\n",
"\n",
"split_dataset = subset_dataset.train_test_split(train_size=int(subset_dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
"train_dataset = split_dataset[\"train\"]\n",
"validation_test_dataset = split_dataset[\"test\"]\n",
"\n",
"split_dataset = validation_test_dataset.train_test_split(train_size=int(validation_test_dataset.num_rows * 0.5), seed=19, shuffle=False)\n",
"validation_dataset = split_dataset[\"train\"]\n",
"test_dataset = split_dataset[\"test\"]\n",
"\n",
"print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(validation_dataset)}. Size of the test set: {len(test_dataset)}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokenizer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We start the tokenizer and assign the end of string token to the padding token"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoints = \"openai-community/gpt2\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"tokenizer.padding_side = \"right\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We add the special tokens for the beginning and end of the joke."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Added 2 tokens\n"
          ]
        }
      ],
      "source": [
      "new_tokens = ['<SJ>', '<EJ>']   # Start and end of joke tokens\n",
"\n",
"num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
"print(f\"Added {num_added_tokens} tokens\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We add them to the dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "joke_column = \"Joke\"\n",
"\n",
"def format_joke(example):\n",
"    example[joke_column] = '<SJ> ' + example['Joke'] + ' <EJ>'\n",
"    return example\n",
"\n",
"remove_columns = [column for column in train_dataset.column_names if column != joke_column]\n",
"\n",
"train_dataset = train_dataset.map(format_joke, remove_columns=remove_columns)\n",
"validation_dataset = validation_dataset.map(format_joke, remove_columns=remove_columns)\n",
"test_dataset = test_dataset.map(format_joke, remove_columns=remove_columns)\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We tokenize the dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[joke_column], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")\n",
"\n",
"train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instantiate the model, assign the padding token and add the new joke start and joke end tokens."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Embedding(50259, 768)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from transformers import AutoModelForCausalLM\n",
"\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
"model.config.pad_token_id = model.config.eos_token_id\n",
"model.resize_token_embeddings(len(tokenizer))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Device"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the device and pass the model to the device"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"\n",
"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
"model.half().to(device)\n",
"print()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Pytorch Dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Create a pytorch dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import Dataset\n",
"\n",
"class JokesDataset(Dataset):\n",
"    def __init__(self, huggingface_dataset):\n",
"        self.dataset = huggingface_dataset\n",
"\n",
"    def __getitem__(self, idx):\n",
"        input_ids = torch.tensor(self.dataset[idx]['input_ids'])\n",
"        attention_mask = torch.tensor(self.dataset[idx]['attention_mask'])\n",
"        return input_ids, attention_mask\n",
"\n",
"    def __len__(self):\n",
"        return len(self.dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instantiate training, validation and test datasets"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "train_pytorch_dataset = JokesDataset(train_dataset)\n",
"validation_pytorch_dataset = JokesDataset(validation_dataset)\n",
"test_pytorch_dataset = JokesDataset(test_dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Let's see a sample"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([768]), torch.Size([768]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "input_ids, attention_mask = train_pytorch_dataset[0]\n",
"input_ids.shape, attention_mask.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Pytorch Dataloader"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the dataloaders"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import DataLoader\n",
"\n",
"BS = 28\n",
"\n",
"train_loader = DataLoader(train_pytorch_dataset, batch_size=BS, shuffle=True)\n",
"validation_loader = DataLoader(validation_pytorch_dataset, batch_size=BS)\n",
"test_loader = DataLoader(test_pytorch_dataset, batch_size=BS)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We see a sample"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([28, 768]), torch.Size([28, 768]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "input_ids, attention_mask = next(iter(train_loader))\n",
"input_ids.shape, attention_mask.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We pass it to the model"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "odict_keys(['logits', 'past_key_values'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "output = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
"output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "As we can see we have no `loss` value, as we have seen we have to pass the `input_ids` and the `labels`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "odict_keys(['loss', 'logits', 'past_key_values'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "output = model(input_ids.to(device), attention_mask=attention_mask.to(device), labels=input_ids.to(device))\n",
"output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Now we have `loss`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "80.5625"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "output['loss'].item()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Optimizer"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create an optimizer"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
"  warnings.warn(\n"
          ]
        }
      ],
      "source": [
      "from transformers import AdamW\n",
"\n",
"LR = 2e-5\n",
"optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Training"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We create the training loop"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Epoch 1: 100%|██████████| 7447/7447 [51:07<00:00,  2.43it/s, train_loss=nan]\n",
"Epoch 2: 100%|██████████| 7447/7447 [51:06<00:00,  2.43it/s, train_loss=nan]\n",
"Epoch 3: 100%|██████████| 7447/7447 [51:07<00:00,  2.43it/s, train_loss=nan]\n"
          ]
        }
      ],
      "source": [
      "from tqdm import tqdm\n",
"\n",
"EPOCHS = 3\n",
"\n",
"for epoch in range(EPOCHS):\n",
"    model.train()\n",
"    train_loss = 0\n",
"    progresbar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}')\n",
"    for input_ids, at_mask in progresbar:\n",
"        input_ids = input_ids.to(device)\n",
"        at_mask = at_mask.to(device)\n",
"\n",
"        output = model(input_ids=input_ids, attention_mask=at_mask, labels=input_ids)\n",
"\n",
"        loss = output['loss']\n",
"        train_loss += loss.item()\n",
"        optimizer.zero_grad()\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"        progresbar.set_postfix({'train_loss': loss.item()})\n",
"    train_loss /= len(train_loader)\n",
"    progresbar.set_postfix({'train_loss': train_loss})"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Model use"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "We tested the model"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def generate_text(decoded_joke, max_new_tokens=100, stop_token='<EJ>', top_k=0, temperature=1.0):\n",
"    input_tokens = tokenize_function({'Joke': decoded_joke})\n",
"    output = model(input_tokens['input_ids'].to(device), attention_mask=input_tokens['attention_mask'].to(device))\n",
"    nex_token = torch.argmax(output['logits'][:, -1, :], dim=-1).item()\n",
"    nex_token_decoded = tokenizer.decode(nex_token)\n",
"    decoded_joke = decoded_joke + nex_token_decoded\n",
"    for _ in range(max_new_tokens):\n",
"        nex_token = torch.argmax(output['logits'][:, -1, :], dim=-1).item()\n",
"        nex_token_decoded = tokenizer.decode(nex_token)\n",
"        if nex_token_decoded == stop_token:\n",
"            break\n",
"        decoded_joke = decoded_joke + nex_token_decoded\n",
"        input_tokens = tokenize_function({'Joke': decoded_joke})\n",
"        output = model(input_tokens['input_ids'].to(device), attention_mask=input_tokens['attention_mask'].to(device))\n",
"    return decoded_joke"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
            "\"<SJ> Why didn't the frog cross the road!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "generated_text = generate_text(\"<SJ> Why didn't the frog cross the road\")\n",
"generated_text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
