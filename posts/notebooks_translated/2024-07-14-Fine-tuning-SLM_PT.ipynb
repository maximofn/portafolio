{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "# Ajuste fino de SMLs com Hugging Face"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nesta postagem, veremos como ajustar modelos de linguagem pequenos, como ajustar a classificação de texto e a geração de texto. Primeiro, veremos como fazer isso com as bibliotecas da Hugging Face, já que a Hugging Face se tornou um participante muito importante no ecossistema de IA atualmente.\n",
"\n",
"Mas, embora as bibliotecas do Hugging Face sejam muito importantes e úteis, é muito importante saber como o treinamento é realmente feito e o que está acontecendo por trás dele, portanto, vamos repetir o treinamento para classificação e geração de texto, mas com o Pytorch."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..\n",
"\n",
"## Ajuste fino para classificação de texto com o Hugging Face"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Login"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para fazer upload do resultado do treinamento no hub, primeiro precisamos fazer login e, para isso, precisamos de um token.\n",
"\n",
"Para criar um token, acesse a página [setings/tokens](https://huggingface.co/settings/tokens) de sua conta, que terá a seguinte aparência\n",
"\n",
"User-Access-Token-dark](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp)\n",
"\n",
"Clique em `New token` e será exibida uma janela para criar um novo token.\n",
"\n",
"![new-token-dark](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp)\n",
"\n",
"Nomeamos o token e o criamos com a função `write` ou com a função `Fine-grained`, que nos permite selecionar exatamente quais permissões o token terá.\n",
"\n",
"Depois de criado, copiamos e colamos o arquivo abaixo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from huggingface_hub import notebook_login\n",
"notebook_login()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Conjunto de dados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, faremos o download de um conjunto de dados. Neste caso, faremos o download de um conjunto de dados de avaliações da [Amazon](https://huggingface.co/datasets/mteb/amazon_reviews_multi)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada nisso"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text'],\n",
"        num_rows: 200000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text'],\n",
"        num_rows: 5000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que você tem um conjunto de treinamento com 200.000 amostras, um conjunto de validação com 5.000 amostras e um conjunto de teste com 5.000 amostras."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em um exemplo do conjunto de treinamento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'id': 'en_0907914',\n",
" 'text': 'Mixed with fir it’s passable\\n\\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',\n",
" 'label': 3,\n",
" 'label_text': '3'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from random import randint\n",
"\n",
"idx = randint(0, len(dataset['train']) - 1)\n",
"dataset['train'][idx]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que você tem a avaliação no campo `text` e a pontuação dada pelo usuário no campo `label`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como vamos criar um modelo de classificação de texto, precisamos saber quantas classes teremos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "5"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "num_classes = len(dataset['train'].unique('label'))\n",
"num_classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Teremos 5 classes, agora vamos ver o valor mínimo dessas classes para saber se a pontuação começa em 0 ou 1. Para isso, usamos o método `unique`"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'train': [0, 1, 2, 3, 4],\n",
" 'validation': [0, 1, 2, 3, 4],\n",
" 'test': [0, 1, 2, 3, 4]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset.unique('label')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O valor mínimo será 0"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para o treinamento, os rótulos precisam estar em um campo chamado `labels`, enquanto em nosso conjunto de dados eles estão em um campo chamado `label`, portanto, criamos o novo campo `lables` com o mesmo valor de `label`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos uma função que faz o que queremos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
      "def set_labels(example):\n",
"    example['labels'] = example['label']\n",
"    return example"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Aplicamos a função ao conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
      "dataset = dataset.map(set_labels)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vejamos como é o conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'id': 'en_0907914',\n",
" 'text': 'Mixed with fir it’s passable\\n\\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',\n",
" 'label': 3,\n",
" 'label_text': '3',\n",
" 'labels': 3}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset['train'][idx]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokeniser"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como temos revisões de texto no conjunto de dados, precisamos tokenizá-las para colocar os tokens no modelo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"openai-community/gpt2\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora criamos uma função para tokenizar o texto. Vamos fazer com que todas as frases tenham o mesmo comprimento, de modo que o tokenizador truncará quando necessário e adicionará tokens de preenchimento quando necessário. Também pedimos que ele retorne tensores pytorch."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O comprimento de cada frase é de 768 tokens porque estamos usando o modelo GPT2 pequeno, que, como vimos na postagem [GPT2](https://maximofn.com/gpt2/#Arquitectura), tem uma dimensão de incorporação de 768 tokens."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos tentar tokenizar um texto"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
      {
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize_function(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][idx])\n",
"Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2989\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2970\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2971\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2986\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2987\u001b[0m     )\n\u001b[1;32m   2988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2990\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2991\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2992\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2993\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2994\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2995\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2996\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2997\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2998\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2999\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3000\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3001\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3002\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3003\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3004\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3005\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3006\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3008\u001b[0m     )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3053\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3053\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3054\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3055\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   3056\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3057\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3058\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3059\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3060\u001b[0m )\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   3063\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3064\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3081\u001b[0m )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2792\u001b[0m     )\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2796\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2797\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2801\u001b[0m ):\n",
"\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ],
      "source": [
      "tokens = tokenize_function(dataset['train'][idx])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Recebemos um erro porque o tokenizador GPT2 não tem um token para preenchimento e nos pede para atribuir um, sugerindo que façamos `tokenizer.pad_token = tokenizer.eos_token`, e assim o fazemos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Testamos novamente a função de tokenização."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1, 768]), torch.Size([1, 768]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens = tokenize_function(dataset['train'][idx])\n",
"tokens['input_ids'].shape, tokens['attention_mask'].shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora que verificamos que a função tokeniza bem, aplicamos essa função ao conjunto de dados, mas também a aplicamos em lotes para que seja executada mais rapidamente.\n",
"\n",
"Também tiramos proveito disso e eliminamos as colunas que não são necessárias."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora vamos ver como é o conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
"        num_rows: 200000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
"        num_rows: 5000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que temos os campos \"labels\" (rótulos), \"input_ids\" (IDs de entrada) e \"attention_mask\" (máscara de atenção), que é o que nos interessa treinar."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instanciamos um modelo para classificação de sequências e informamos a ele o número de classes que temos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Ele nos informa que os pesos da camada `score` foram inicializados aleatoriamente e que precisamos treiná-los novamente."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "O modelo GPT2 teria a seguinte aparência"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoModelForCausalLM\n",
"\n",
"casual_model = AutoModelForCausalLM.from_pretrained(checkpoint)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Enquanto o modelo GPT2 para gerar texto é o seguinte"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em sua arquitetura"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "GPT2LMHeadModel(\n",
"  (transformer): GPT2Model(\n",
"    (wte): Embedding(50257, 768)\n",
"    (wpe): Embedding(1024, 768)\n",
"    (drop): Dropout(p=0.1, inplace=False)\n",
"    (h): ModuleList(\n",
"      (0-11): 12 x GPT2Block(\n",
"        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (attn): GPT2Attention(\n",
"          (c_attn): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
"          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (mlp): GPT2MLP(\n",
"          (c_fc): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (act): NewGELUActivation()\n",
"          (dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"      )\n",
"    )\n",
"    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"  )\n",
"  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "casual_model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E agora a arquitetura do modelo que usaremos para classificar as avaliações."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "GPT2ForSequenceClassification(\n",
"  (transformer): GPT2Model(\n",
"    (wte): Embedding(50257, 768)\n",
"    (wpe): Embedding(1024, 768)\n",
"    (drop): Dropout(p=0.1, inplace=False)\n",
"    (h): ModuleList(\n",
"      (0-11): 12 x GPT2Block(\n",
"        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (attn): GPT2Attention(\n",
"          (c_attn): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
"          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"        (mlp): GPT2MLP(\n",
"          (c_fc): Conv1D()\n",
"          (c_proj): Conv1D()\n",
"          (act): NewGELUActivation()\n",
"          (dropout): Dropout(p=0.1, inplace=False)\n",
"        )\n",
"      )\n",
"    )\n",
"    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
"  )\n",
"  (score): Linear(in_features=768, out_features=5, bias=False)\n",
")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Há dois aspectos a serem mencionados aqui\n",
"\n",
" + A primeira é que, em ambas, a primeira camada tem dimensões de 50257x768, o que corresponde a 50257 tokens possíveis do vocabulário GPT2 e 768 dimensões da incorporação, portanto, fizemos bem em tokenizar as avaliações com um tamanho de 768 tokens.\n",
" + A segunda é que o modelo `casual` (o modelo de geração de texto) tem, no final, uma camada `Linear` que gera 50257 valores, ou seja, é responsável por prever o próximo token e atribui um valor a cada token possível. Já o modelo de classificação tem uma camada `Linear` que gera apenas 5 valores, um para cada classe, o que nos dará a probabilidade de a avaliação pertencer a cada classe.\n",
"\n",
"É por isso que recebemos a mensagem de que os pesos da camada `score` foram inicializados aleatoriamente, porque a biblioteca de transformadores removeu a camada `Linear` de 768x50257 e adicionou uma camada `Linear` de 768x5, inicializou-a com valores aleatórios e precisamos treiná-la para nosso problema específico."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Excluímos o modelo casual, porque não vamos usá-lo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
      "del casual_model"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinador"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos agora configurar os argumentos de treinamento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import TrainingArguments\n",
"\n",
"metric_name = \"accuracy\"\n",
"model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
"LR = 2e-5\n",
"BS_TRAIN = 28\n",
"BS_EVAL = 40\n",
"EPOCHS = 3\n",
"WEIGHT_DECAY = 0.01\n",
"\n",
"training_args = TrainingArguments(\n",
"    model_name,\n",
"    eval_strategy=\"epoch\",\n",
"    save_strategy=\"epoch\",\n",
"    learning_rate=LR,\n",
"    per_device_train_batch_size=BS_TRAIN,\n",
"    per_device_eval_batch_size=BS_EVAL,\n",
"    num_train_epochs=EPOCHS,\n",
"    weight_decay=WEIGHT_DECAY,\n",
"    lr_scheduler_type=\"cosine\",\n",
"    warmup_ratio = 0.1,\n",
"    fp16=True,\n",
"    load_best_model_at_end=True,\n",
"    metric_for_best_model=metric_name,\n",
"    push_to_hub=True,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Definir uma métrica para o carregador de dados de validação"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
      "import numpy as np\n",
"from evaluate import load\n",
"\n",
"metric = load(\"accuracy\")\n",
"\n",
"def compute_metrics(eval_pred):\n",
"    print(eval_pred)\n",
"    predictions, labels = eval_pred\n",
"    predictions = np.argmax(predictions, axis=1)\n",
"    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora definimos o treinador"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import Trainer\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=dataset['train'],\n",
"    eval_dataset=dataset['validation'],\n",
"    tokenizer=tokenizer,\n",
"    compute_metrics=compute_metrics,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Treinamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51caae4c5a874c81938cee530d7f2017",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "  0%|          | 0/600000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "ename": "ValueError",
          "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label', 'labels']",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1876\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1877\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1878\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1879\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1880\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1881\u001b[0m     )\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m pad_without_fast_tokenizer_warning(\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m    273\u001b[0m         features,\n\u001b[1;32m    274\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[1;32m    275\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m    276\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    277\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_tensors,\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3299\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3297\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3302\u001b[0m     )\n\u001b[1;32m   3304\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
"\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label', 'labels']"
          ]
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Recebemos um erro novamente porque o modelo não foi atribuído a um token de preenchimento, portanto, assim como no tokenizador, nós o atribuímos ao modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
      "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Recriamos os argumentos do instrutor com o novo modelo, que agora tem um token de preenchimento, o instrutor e treinamos novamente."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "nLb8ZZxgmWB9",
        "outputId": "b0a7c6f5-1b10-43a6-cc2d-75d1dbf88714"
      },
      "outputs": [],
      "source": [
      "training_args = TrainingArguments(\n",
"    model_name,\n",
"    eval_strategy=\"epoch\",\n",
"    save_strategy=\"epoch\",\n",
"    learning_rate=LR,\n",
"    per_device_train_batch_size=BS_TRAIN,\n",
"    per_device_eval_batch_size=BS_EVAL,\n",
"    num_train_epochs=EPOCHS,\n",
"    weight_decay=WEIGHT_DECAY,\n",
"    lr_scheduler_type=\"cosine\",\n",
"    warmup_ratio = 0.1,\n",
"    fp16=True,\n",
"    load_best_model_at_end=True,\n",
"    metric_for_best_model=metric_name,\n",
"    push_to_hub=True,\n",
"    logging_dir=\"./runs\",\n",
")\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=dataset['train'],\n",
"    eval_dataset=dataset['validation'],\n",
"    tokenizer=tokenizer,\n",
"    compute_metrics=compute_metrics,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora que vimos que tudo está bem, podemos treinar."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "nLb8ZZxgmWB9",
        "outputId": "b0a7c6f5-1b10-43a6-cc2d-75d1dbf88714"
      },
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='2250' max='21429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [ 2250/21429 45:38 < 6:29:27, 0.82 it/s, Epoch 0.31/3]\n",
"    </div>\n",
"    <table border=\"1\" class=\"dataframe\">\n",
"  <thead>\n",
" <tr style=\"text-align: left;\">\n",
"      <th>Epoch</th>\n",
"      <th>Training Loss</th>\n",
"      <th>Validation Loss</th>\n",
"    </tr>\n",
"  </thead>\n",
"  <tbody>\n",
"  </tbody>\n",
"</table><p>"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='21429' max='21429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [21429/21429 7:19:25, Epoch 3/3]\n",
"    </div>\n",
"    <table border=\"1\" class=\"dataframe\">\n",
"  <thead>\n",
" <tr style=\"text-align: left;\">\n",
"      <th>Epoch</th>\n",
"      <th>Training Loss</th>\n",
"      <th>Validation Loss</th>\n",
"      <th>Accuracy</th>\n",
"    </tr>\n",
"  </thead>\n",
"  <tbody>\n",
"    <tr>\n",
"      <td>1</td>\n",
"      <td>0.807400</td>\n",
"      <td>0.820341</td>\n",
"      <td>0.652000</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>2</td>\n",
"      <td>0.751900</td>\n",
"      <td>0.802189</td>\n",
"      <td>0.654600</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>3</td>\n",
"      <td>0.718100</td>\n",
"      <td>0.810221</td>\n",
"      <td>0.657800</td>\n",
"    </tr>\n",
"  </tbody>\n",
"</table><p>"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "<transformers.trainer_utils.EvalPrediction object at 0x782767ea1450>\n",
"<transformers.trainer_utils.EvalPrediction object at 0x782767eeefe0>\n",
"<transformers.trainer_utils.EvalPrediction object at 0x782767eecfd0>\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "TrainOutput(global_step=21429, training_loss=0.7846888848762739, metrics={'train_runtime': 26367.7801, 'train_samples_per_second': 22.755, 'train_steps_per_second': 0.813, 'total_flos': 2.35173445632e+17, 'train_loss': 0.7846888848762739, 'epoch': 3.0})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Avaliação"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois de treinados, avaliamos o conjunto de dados de teste"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "OsCTTRUcmWB9",
        "outputId": "d9d816a8-1559-4774-c274-60047692e4bc"
      },
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [125/125 01:15]\n",
"    </div>\n",
"    "
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "<transformers.trainer_utils.EvalPrediction object at 0x7826ddfded40>\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "{'eval_loss': 0.7973636984825134,\n",
" 'eval_accuracy': 0.6626,\n",
" 'eval_runtime': 76.3016,\n",
" 'eval_samples_per_second': 65.529,\n",
" 'eval_steps_per_second': 1.638,\n",
" 'epoch': 3.0}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "trainer.evaluate(eval_dataset=dataset['test'])"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Publicar o modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora que temos nosso modelo treinado, podemos compartilhá-lo com o mundo, portanto, primeiro criamos um cartão de modelo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "trainer.create_model_card()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E agora podemos publicá-lo. Como a primeira coisa que fizemos foi fazer login no hub da huggingface, poderemos fazer o upload para o nosso hub sem nenhum problema."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "trainer.push_to_hub()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Uso do modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Limpamos o máximo possível"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"import gc\n",
"\n",
"\n",
"def clear_hardwares():\n",
"    torch.clear_autocast_cache()\n",
"    torch.cuda.ipc_collect()\n",
"    torch.cuda.empty_cache()\n",
"    gc.collect()\n",
"\n",
"\n",
"clear_hardwares()\n",
"clear_hardwares()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como fizemos o upload do modelo em nosso hub, podemos baixá-lo e usá-lo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import pipeline\n",
"\n",
"user = \"maximofn\"\n",
"checkpoints = f\"{user}/{model_name}\"\n",
"task = \"text-classification\"\n",
"classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, se quisermos retornar a probabilidade de todas as classes, basta usar o classificador que acabamos de instanciar, com o parâmetro `top_k=None`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_4', 'score': 0.8253807425498962},\n",
" {'label': 'LABEL_3', 'score': 0.15411493182182312},\n",
" {'label': 'LABEL_2', 'score': 0.013907806016504765},\n",
" {'label': 'LABEL_0', 'score': 0.003939222544431686},\n",
" {'label': 'LABEL_1', 'score': 0.0026572425849735737}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "labels = classifier(\"I love this product\", top_k=None)\n",
"labels"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se quisermos apenas a classe com a maior probabilidade, faremos o mesmo, mas com o parâmetro `top_k=1`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_4', 'score': 0.8253807425498962}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "label = classifier(\"I love this product\", top_k=1)\n",
"label"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "E se quisermos n classes, faremos o mesmo, mas com o parâmetro `top_k=n`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[{'label': 'LABEL_4', 'score': 0.8253807425498962},\n",
" {'label': 'LABEL_3', 'score': 0.15411493182182312}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "two_labels = classifier(\"I love this product\", top_k=2)\n",
"two_labels"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Também podemos testar o modelo com o Automodel e o AutoTokenizer."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
"import torch\n",
"\n",
"model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
"user = \"maximofn\"\n",
"checkpoint = f\"{user}/{model_name}\"\n",
"num_classes = 5\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes).half().eval().to(\"cuda\")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "[0.003963470458984375,\n",
" 0.0026721954345703125,\n",
" 0.01397705078125,\n",
" 0.154541015625,\n",
" 0.82470703125]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "tokens = tokenizer.encode(\"I love this product\", return_tensors=\"pt\").to(model.device)\n",
"with torch.no_grad():\n",
"    output = model(tokens)\n",
"logits = output.logits\n",
"lables = torch.softmax(logits, dim=1).cpu().numpy().tolist()\n",
"lables[0]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se você quiser testar o modelo com mais detalhes, poderá vê-lo em [Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Ajuste fino para geração de texto com o Hugging Face"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para ter certeza de que não tenho problemas de memória VRAM, reiniciei o notebook."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Login"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para fazer upload do resultado do treinamento no hub, primeiro precisamos fazer login e, para isso, precisamos de um token.\n",
"\n",
"Para criar um token, acesse a página [setings/tokens](https://huggingface.co/settings/tokens) de sua conta, que terá a seguinte aparência\n",
"\n",
"User-Access-Token-dark](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp)\n",
"\n",
"Clique em `New token` e será exibida uma janela para criar um novo token.\n",
"\n",
"![new-token-dark](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp)\n",
"\n",
"Nomeamos o token e o criamos com a função `write` ou com a função `Fine-grained`, que nos permite selecionar exatamente quais permissões o token terá.\n",
"\n",
"Depois de criado, copiamos e colamos o arquivo abaixo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from huggingface_hub import notebook_login\n",
"notebook_login()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Conjunto de dados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos usar um conjunto de dados de [piadas em inglês] (https://huggingface.co/datasets/Maximofn/short-jokes-dataset)"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['ID', 'Joke'],\n",
"        num_rows: 231657\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
"jokes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada nisso"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['ID', 'Joke'],\n",
"        num_rows: 231657\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "jokes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que se trata de um único conjunto de treinamento com mais de 200.000 piadas. Portanto, mais adiante, teremos de dividi-lo em treinamento e avaliação."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em um exemplo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'ID': 198387,\n",
" 'Joke': 'My hot dislexic co-worker said she had an important massage to give me in her office... When I got there, she told me it can wait until I put on some clothes.'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from random import randint\n",
"\n",
"idx = randint(0, len(jokes['train']) - 1)\n",
"jokes['train'][idx]"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que ela tem uma identificação da piada que não nos interessa em absoluto e a piada em si"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Caso você tenha pouca memória de GPU, farei um subconjunto do conjunto de dados, escolha a porcentagem de piadas que deseja usar."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Dataset({\n",
"    features: ['ID', 'Joke'],\n",
"    num_rows: 231657\n",
"})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "percent_of_train_dataset = 1    # If you want 50% of the dataset, set this to 0.5\n",
"\n",
"subset_dataset = jokes[\"train\"].select(range(int(len(jokes[\"train\"]) * percent_of_train_dataset)))\n",
"subset_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, dividimos o subconjunto em um conjunto de treinamento e um conjunto de validação."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583\n"
          ]
        }
      ],
      "source": [
      "percent_of_train_dataset = 0.90\n",
"\n",
"split_dataset = subset_dataset.train_test_split(train_size=int(subset_dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
"train_dataset = split_dataset[\"train\"]\n",
"validation_test_dataset = split_dataset[\"test\"]\n",
"\n",
"split_dataset = validation_test_dataset.train_test_split(train_size=int(validation_test_dataset.num_rows * 0.5), seed=19, shuffle=False)\n",
"validation_dataset = split_dataset[\"train\"]\n",
"test_dataset = split_dataset[\"test\"]\n",
"\n",
"print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(validation_dataset)}. Size of the test set: {len(test_dataset)}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokeniser"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instanciar o tokenizador. Instanciar o token de preenchimento do tokenizador para não recebermos um erro como antes."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoints = \"openai-community/gpt2\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"tokenizer.padding_side = \"right\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para maior controle, adicionaremos dois novos tokens para o início e o fim de uma piada."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Added 2 tokens\n"
          ]
        }
      ],
      "source": [
      "new_tokens = ['<SJ>', '<EJ>']   # Start and end of joke tokens\n",
"\n",
"num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
"print(f\"Added {num_added_tokens} tokens\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos uma função para adicionar os novos tokens às sentenças"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
      "joke_column = \"Joke\"\n",
"\n",
"def format_joke(example):\n",
"    example[joke_column] = '<SJ> ' + example['Joke'] + ' <EJ>'\n",
"    return example"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Selecionamos as colunas de que não precisamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "['ID']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "remove_columns = [column for column in train_dataset.column_names if column != joke_column]\n",
"remove_columns"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Formatamos o conjunto de dados e excluímos as colunas de que não precisamos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "train_dataset = train_dataset.map(format_joke, remove_columns=remove_columns)\n",
"validation_dataset = validation_dataset.map(format_joke, remove_columns=remove_columns)\n",
"test_dataset = test_dataset.map(format_joke, remove_columns=remove_columns)\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, criamos uma função para tokenizar as piadas."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[joke_column], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Tokenize o conjunto de dados e exclua a coluna com o texto"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, instanciamos o modelo para geração de texto e atribuímos o token de fim de cadeia ao token de carregamento."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoModelForCausalLM\n",
"\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
"model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos o tamanho do vocabulário do modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "50257"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "vocab_size = model.config.vocab_size\n",
"vocab_size"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Ele tem 50257 tokens, que é o tamanho do vocabulário do GPT2. Mas como dissemos que criaríamos dois novos tokens com o início da piada e o fim da piada, nós os adicionamos ao modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Old vocab size: 50257. New vocab size: 50259. Added 2 tokens\n"
          ]
        }
      ],
      "source": [
      "model.resize_token_embeddings(len(tokenizer))\n",
"\n",
"new_vocab_size = model.config.vocab_size\n",
"print(f\"Old vocab size: {vocab_size}. New vocab size: {new_vocab_size}. Added {new_vocab_size - vocab_size} tokens\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Os dois novos tokens foram adicionados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Definimos os parâmetros de treinamento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import TrainingArguments\n",
"\n",
"metric_name = \"accuracy\"\n",
"model_name = \"GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM\"\n",
"output_dir = f\"./training_results\"\n",
"LR = 2e-5\n",
"BS_TRAIN = 28\n",
"BS_EVAL = 32\n",
"EPOCHS = 3\n",
"WEIGHT_DECAY = 0.01\n",
"WARMUP_STEPS = 100\n",
"\n",
"training_args = TrainingArguments(\n",
"    model_name,\n",
"    eval_strategy=\"epoch\",\n",
"    save_strategy=\"epoch\",\n",
"    learning_rate=LR,\n",
"    per_device_train_batch_size=BS_TRAIN,\n",
"    per_device_eval_batch_size=BS_EVAL,\n",
"    warmup_steps=WARMUP_STEPS,\n",
"    num_train_epochs=EPOCHS,\n",
"    weight_decay=WEIGHT_DECAY,\n",
"    lr_scheduler_type=\"cosine\",\n",
"    warmup_ratio = 0.1,\n",
"    fp16=True,\n",
"    load_best_model_at_end=True,\n",
"    # metric_for_best_model=metric_name,\n",
"    push_to_hub=True,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora não usamos `metric_for_best_model`, depois de definir o treinador, explicamos o motivo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Definimos o treinador"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import Trainer\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=train_dataset,\n",
"    eval_dataset=validation_dataset,\n",
"    tokenizer=tokenizer,\n",
"    # compute_metrics=compute_metrics,\n",
")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nesse caso, não passamos uma função `compute_metrics`; se ela não for passada, a `loss` será usada durante a avaliação para avaliar o modelo. É por isso que, ao definir os argumentos, não definimos `metric_for_best_model`, pois não usaremos uma métrica para avaliar o modelo, mas a `loss`."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Treinamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3497c5690f2946c6936f028f7d1f881d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "  0%|          | 0/625473 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "ename": "ValueError",
          "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
          "output_type": "error",
          "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
"Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
"File \u001b[0;32m~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3282\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3284\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3285\u001b[0m         )\n\u001b[1;32m   3286\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3287\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
"\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
          ]
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, ele nos dá um erro, diz que o modelo não retorna o valor da perda, o que é fundamental para podermos treinar."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, vamos ver como é um exemplo do conjunto de dados."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "{'input_ids': [50257,\n",
"  4162,\n",
"  750,\n",
"  262,\n",
"  18757,\n",
"  6451,\n",
"  2245,\n",
"  2491,\n",
"  30,\n",
"  4362,\n",
"  340,\n",
"  373,\n",
"  734,\n",
"  10032,\n",
"  13,\n",
"  220,\n",
"  50258,\n",
"  50256,\n",
"  50256,\n",
"  ...,\n",
"  50256,\n",
"  50256,\n",
"  50256],\n",
" 'attention_mask': [1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  1,\n",
"  0,\n",
"  0,\n",
"  0,\n",
"  ...,\n",
"  0,\n",
"  0,\n",
"  0]}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "idx = randint(0, len(train_dataset) - 1)\n",
"sample = train_dataset[idx]\n",
"sample"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, temos um dicionário com `input_ids` e `attention_mask` e, se o passarmos para o modelo, obteremos o seguinte"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "None\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"\n",
"output = model(\n",
"    input_ids=torch.Tensor(sample[\"input_ids\"]).long().unsqueeze(0).to(model.device),\n",
"    attention_mask=torch.Tensor(sample[\"attention_mask\"]).long().unsqueeze(0).to(model.device),\n",
")\n",
"print(output.loss)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, ele não retorna o valor da perda porque está aguardando um valor para `labels`, que não foi passado a ele. No exemplo anterior, em que fizemos o ajuste fino para a classificação de texto, dissemos que os rótulos tinham de ser passados para um campo no conjunto de dados chamado `labels`, mas, nesse caso, não temos esse campo no conjunto de dados.\n",
"\n",
"Se agora atribuirmos os `lables` aos `input_ids` e observarmos novamente a perda"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "tensor(102.1873, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"\n",
"output = model(\n",
"    input_ids=torch.Tensor(sample[\"input_ids\"]).long().unsqueeze(0).to(model.device),\n",
"    attention_mask=torch.Tensor(sample[\"attention_mask\"]).long().unsqueeze(0).to(model.device),\n",
"    labels=torch.Tensor(sample[\"input_ids\"]).long().unsqueeze(0).to(model.device)\n",
")\n",
"print(output.loss)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora temos uma \"perda\".\n",
"\n",
"Portanto, temos duas opções: adicionar um campo `labels` ao conjunto de dados, com os valores de `input_ids` ou usar uma função da biblioteca `transformers` chamada `data_collator`; nesse caso, usaremos `DataCollatorForLanguageModeling`. Vamos dar uma olhada nisso"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import DataCollatorForLanguageModeling\n",
"\n",
"my_data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Passamos a amostra `sample` por meio desse `data_collator`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "collated_sample = my_data_collator([sample]).to(model.device)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos como é a saída"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "input_ids (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,\n",
"           340,   373,   734, 10032,    13,   220, 50258, 50256, ..., 50256, 50256]],\n",
"       device='cuda:0')\n",
"attention_mask (torch.Size([1, 768])): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ..., 0, 0]],\n",
"       device='cuda:0')\n",
"labels (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,\n",
"           340,   373,   734, 10032,    13,   220, 50258,  -100,  ...,  -100,  -100]],\n",
"       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
      "for key, value in collated_sample.items():\n",
"    print(f\"{key} ({value.shape}): {value}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como você pode ver, o `data_collator` criou um campo `labels` e atribuiu a ele os valores de `input_ids`. Os tokens que são mascarados receberam um valor de -100. Isso se deve ao fato de que, quando definimos o `data_collator`, passamos a ele o parâmetro `mlm=False`, o que significa que não estamos fazendo `Masked Language Modeling`, mas `Language Modeling`, portanto, ele não mascara nenhum token original."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos ver se agora temos uma `perda` com esse `data_collator`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor(102.7181, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output = model(**collated_sample)\n",
"output.loss"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Portanto, redefinimos o `trainer` com o `data_collator` e treinamos novamente."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import DataCollatorForLanguageModeling\n",
"\n",
"trainer = Trainer(\n",
"    model,\n",
"    training_args,\n",
"    train_dataset=train_dataset,\n",
"    eval_dataset=validation_dataset,\n",
"    tokenizer=tokenizer,\n",
"    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
")"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='22341' max='22341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [22341/22341 2:33:28, Epoch 3/3]\n",
"    </div>\n",
"    <table border=\"1\" class=\"dataframe\">\n",
"  <thead>\n",
" <tr style=\"text-align: left;\">\n",
"      <th>Epoch</th>\n",
"      <th>Training Loss</th>\n",
"      <th>Validation Loss</th>\n",
"    </tr>\n",
"  </thead>\n",
"  <tbody>\n",
"    <tr>\n",
"      <td>1</td>\n",
"      <td>3.386600</td>\n",
"      <td>3.258979</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>2</td>\n",
"      <td>3.259900</td>\n",
"      <td>3.199673</td>\n",
"    </tr>\n",
"    <tr>\n",
"      <td>3</td>\n",
"      <td>3.212600</td>\n",
"      <td>3.192009</td>\n",
"    </tr>\n",
"  </tbody>\n",
"</table><p>"
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "TrainOutput(global_step=22341, training_loss=3.505178199598342, metrics={'train_runtime': 9209.5353, 'train_samples_per_second': 67.916, 'train_steps_per_second': 2.426, 'total_flos': 2.45146666696704e+17, 'train_loss': 3.505178199598342, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "trainer.train()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Avaliação"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Depois de treinado, avaliamos o modelo no conjunto de dados de teste."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/html": [
            "\n",
"    <div>\n",
"      \n",
"      <progress value='362' max='362' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
"      [362/362 01:04]\n",
"    </div>\n",
"    "
            ],
            "text/plain": [
            "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "text/plain": [
            "{'eval_loss': 3.201305866241455,\n",
" 'eval_runtime': 65.0033,\n",
" 'eval_samples_per_second': 178.191,\n",
" 'eval_steps_per_second': 5.569,\n",
" 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "trainer.evaluate(eval_dataset=test_dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Publicar o modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o cartão modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "trainer.create_model_card()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nós o publicamos"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "706edd6325984537ad8202c3de937995",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
            "events.out.tfevents.1720875425.8de3af1b431d.6946.1:   0%|          | 0.00/364 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
{
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
            "CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM/commit/d107b3bb0e02076483238f9975697761015ec390', commit_message='End of training', commit_description='', oid='d107b3bb0e02076483238f9975697761015ec390', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "trainer.push_to_hub()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Uso do modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Limpamos o máximo possível"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"import gc\n",
"\n",
"\n",
"def clear_hardwares():\n",
"    torch.clear_autocast_cache()\n",
"    torch.cuda.ipc_collect()\n",
"    torch.cuda.empty_cache()\n",
"    gc.collect()\n",
"\n",
"\n",
"clear_hardwares()\n",
"clear_hardwares()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Baixamos o modelo e o tokenizador"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
"\n",
"user = \"maximofn\"\n",
"checkpoints = f\"{user}/{model_name}\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"tokenizer.padding_side = \"right\"\n",
"\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
"model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Verificamos se o tokenizador e o modelo têm os dois tokens extras que adicionamos."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "tokenizer_vocab: 50259. model_vocab: 50259\n"
          ]
        }
      ],
      "source": [
      "tokenizer_vocab = tokenizer.get_vocab()\n",
"model_vocab = model.config.vocab_size\n",
"print(f\"tokenizer_vocab: {len(tokenizer_vocab)}. model_vocab: {model_vocab}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos que eles têm 50259 tokens, ou seja, os 50257 tokens do GPT2 mais os 2 que adicionamos."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos uma função para gerar piadas"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
      "def generate_joke(prompt_text):\n",
"    text = f\"<SJ> {prompt_text}\"\n",
"    tokens = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
"    with torch.no_grad():\n",
"        output = model.generate(**tokens, max_new_tokens=256, eos_token_id=tokenizer.encode(\"<EJ>\")[-1])\n",
"    return tokenizer.decode(output[0], skip_special_tokens=False)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Geramos uma piada"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"
          ]
        },
{
          "data": {
            "text/plain": [
            "\"<SJ> Why didn't the frog cross the road? Because he was frog-in-the-face. <EJ>\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "generate_joke(\"Why didn't the frog cross the road?\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Se você quiser testar o modelo com mais detalhes, poderá vê-lo em [Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM](https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Ajuste fino para classificação de texto com o Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Repetimos o treinamento com o Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Reinicie o notebook para ter certeza de que"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Conjunto de dados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Fizemos o download do mesmo conjunto de dados que usamos no treinamento com as bibliotecas Hugging Face."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos uma variável com o número de classes"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "5"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "num_classes = len(dataset['train'].unique('label'))\n",
"num_classes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Antes, processávamos todo o conjunto de dados para criar um campo chamado `labels`, mas agora isso não é necessário porque vamos programar tudo nós mesmos, portanto, nos adaptamos à aparência do conjunto de dados."
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokeniser"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o tokenizador. Atribuímos o token de preenchimento para não recebermos um erro como antes."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoint = \"openai-community/gpt2\"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
"tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos uma função para tokenizar o conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nós a tokenizamos. Removemos as colunas que não são necessárias, mas agora deixamos a coluna de texto."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'label_text'])"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
"        num_rows: 200000\n",
"    })\n",
"    validation: Dataset({\n",
"        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
"        num_rows: 5000\n",
"    })\n",
"    test: Dataset({\n",
"        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
"        num_rows: 5000\n",
"    })\n",
"})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "dataset"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "len subset_train: 200000, len subset_validation: 5000, len subset_test: 5000\n"
          ]
        }
      ],
      "source": [
      "percentage = 1\n",
"subset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
"percentage = 1\n",
"subset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
"subset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
"print(f\"len subset_train: {len(subset_train)}, len subset_validation: {len(subset_validation)}, len subset_test: {len(subset_test)}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Importamos os pesos e atribuímos o token de preenchimento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
      "from transformers import AutoModelForSequenceClassification\n",
"\n",
"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
"model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dispositivo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o dispositivo onde tudo será executado"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
"\n",
"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Passamos o modelo para o dispositivo e o passamos para o FP16 para que ele ocupe menos memória."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n"
          ]
        }
      ],
      "source": [
      "model.half().to(device)\n",
"print()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Conjunto de dados Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criar um conjunto de dados pytorch"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import Dataset\n",
"\n",
"class ReviewsDataset(Dataset):\n",
"    def __init__(self, huggingface_dataset):\n",
"        self.dataset = huggingface_dataset\n",
"\n",
"    def __getitem__(self, idx):\n",
"        label = self.dataset[idx]['label']\n",
"        input_ids = torch.tensor(self.dataset[idx]['input_ids'])\n",
"        attention_mask = torch.tensor(self.dataset[idx]['attention_mask'])\n",
"        return input_ids, attention_mask, label\n",
"\n",
"    def __len__(self):\n",
"        return len(self.dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instanciar os conjuntos de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "train_dataset = ReviewsDataset(subset_train)\n",
"validatation_dataset = ReviewsDataset(subset_validation)\n",
"test_dataset = ReviewsDataset(subset_test)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em um exemplo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([768]), torch.Size([768]), 0)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "input_ids, at_mask, label = train_dataset[0]\n",
"input_ids.shape, at_mask.shape, label"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Carregador de dados Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos agora um carregador de dados pytorch"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import DataLoader\n",
"\n",
"BS = 12\n",
"\n",
"train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
"validation_loader = DataLoader(validatation_dataset, batch_size=BS)\n",
"test_loader = DataLoader(test_dataset, batch_size=BS)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos dar uma olhada em um exemplo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([12, 768]),\n",
" torch.Size([12, 768]),\n",
" tensor([2, 1, 2, 0, 3, 3, 0, 4, 3, 3, 4, 2]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "input_ids, at_mask, labels = next(iter(train_loader))\n",
"input_ids.shape, at_mask.shape, labels"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Para verificar se está tudo certo, passamos a amostra para o modelo para ver se está tudo certo. Primeiro, passamos os tokens para o dispositivo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "input_ids = input_ids.to(device)\n",
"at_mask = at_mask.to(device)\n",
"labels = labels.to(device)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, nós os passamos para o modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "odict_keys(['loss', 'logits', 'past_key_values'])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output = model(input_ids=input_ids, attention_mask=at_mask, labels=labels)\n",
"output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, ele nos fornece a perda e os logits."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor(5.9414, device='cuda:0', dtype=torch.float16,\n",
"       grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output['loss']"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([[ 6.1953e+00, -1.2275e+00, -2.4824e+00,  5.8867e+00, -1.4734e+01],\n",
"        [ 5.4062e+00, -8.4570e-01, -2.3203e+00,  5.1055e+00, -1.1555e+01],\n",
"        [ 6.1641e+00, -9.3066e-01, -2.5664e+00,  6.0039e+00, -1.4570e+01],\n",
"        [ 5.2266e+00, -4.2358e-01, -2.0801e+00,  4.7461e+00, -1.1570e+01],\n",
"        [ 3.8184e+00, -2.3460e-03, -1.7666e+00,  3.4160e+00, -7.7969e+00],\n",
"        [ 4.1641e+00, -4.8169e-01, -1.6914e+00,  3.9941e+00, -8.7734e+00],\n",
"        [ 4.6758e+00, -3.0298e-01, -2.1641e+00,  4.1055e+00, -9.3359e+00],\n",
"        [ 4.1953e+00, -3.2471e-01, -2.1875e+00,  3.9375e+00, -8.3438e+00],\n",
"        [-1.1650e+00,  1.3564e+00, -6.2158e-01, -6.8115e-01,  4.8672e+00],\n",
"        [ 4.4961e+00, -8.7891e-02, -2.2793e+00,  4.2812e+00, -9.3359e+00],\n",
"        [ 4.9336e+00, -2.6627e-03, -2.1543e+00,  4.3711e+00, -1.0742e+01],\n",
"        [ 5.9727e+00, -4.3152e-02, -1.4551e+00,  4.3438e+00, -1.2117e+01]],\n",
"       device='cuda:0', dtype=torch.float16, grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output['logits']"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Métrica"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos criar uma função para obter a métrica, que, neste caso, será a precisão"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def predicted_labels(logits):\n",
"    percent = torch.softmax(logits, dim=1)\n",
"    predictions = torch.argmax(percent, dim=1)\n",
"    return predictions"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def compute_accuracy(logits, labels):\n",
"    predictions = predicted_labels(logits)\n",
"    correct = (predictions == labels).float()\n",
"    return correct.mean()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos ver como ele calcula bem"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "0.1666666716337204"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "compute_accuracy(output['logits'], labels).item()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Otimizador"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como precisaremos de um otimizador, criamos um."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
"  warnings.warn(\n"
          ]
        }
      ],
      "source": [
      "from transformers import AdamW\n",
"\n",
"LR = 2e-5\n",
"optimizer = AdamW(model.parameters(), lr=LR)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o loop de treinamento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Epoch 1: 100%|██████████| 16667/16667 [44:13<00:00,  6.28it/s, train_loss=nan]\n",
"Epoch 1: 100%|██████████| 417/417 [00:32<00:00, 12.72it/s, valid_loss=nan, accuracy=0]\n",
"Epoch 2: 100%|██████████| 16667/16667 [44:06<00:00,  6.30it/s, train_loss=nan]\n",
"Epoch 2: 100%|██████████| 417/417 [00:32<00:00, 12.77it/s, valid_loss=nan, accuracy=0]\n",
"Epoch 3: 100%|██████████| 16667/16667 [44:03<00:00,  6.30it/s, train_loss=nan]\n",
"Epoch 3: 100%|██████████| 417/417 [00:32<00:00, 12.86it/s, valid_loss=nan, accuracy=0]\n"
          ]
        }
      ],
      "source": [
      "from tqdm import tqdm\n",
"\n",
"EPOCHS = 3\n",
"\n",
"accuracy = 0\n",
"\n",
"for epoch in range(EPOCHS):\n",
"    model.train()\n",
"    train_loss = 0\n",
"    progresbar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}')\n",
"    for input_ids, at_mask, labels in progresbar:\n",
"        input_ids = input_ids.to(device)\n",
"        at_mask = at_mask.to(device)\n",
"        label = labels.to(device)\n",
"\n",
"        output = model(input_ids=input_ids, attention_mask=at_mask, labels=label)\n",
"\n",
"        loss = output['loss']\n",
"        train_loss += loss.item()\n",
"        optimizer.zero_grad()\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"        progresbar.set_postfix({'train_loss': loss.item()})\n",
"    train_loss /= len(train_loader)\n",
"    progresbar.set_postfix({'train_loss': train_loss})\n",
"\n",
"    model.eval()\n",
"    valid_loss = 0\n",
"    progresbar = tqdm(validation_loader, total=len(validation_loader), desc=f'Epoch {epoch + 1}')\n",
"    for input_ids, at_mask, labels in progresbar:\n",
"        input_ids = input_ids.to(device)\n",
"        at_mask = at_mask.to(device)\n",
"        labels = labels.to(device)\n",
"\n",
"        output = model(input_ids=input_ids, attention_mask=at_mask, labels=labels)\n",
"\n",
"        loss = output['loss']\n",
"        valid_loss += loss.item()\n",
"\n",
"        step_accuracy = compute_accuracy(output['logits'], labels)\n",
"        accuracy += step_accuracy\n",
"        progresbar.set_postfix({'valid_loss': loss.item(), 'accuracy': step_accuracy.item()})\n",
"\n",
"    valid_loss /= len(validation_loader)\n",
"    accuracy /= len(validation_loader)\n",
"    progresbar.set_postfix({'valid_loss': valid_loss, 'accuracy': accuracy})"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Uso do modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vamos testar o modelo que treinamos"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Primeiro, tokenizamos um texto"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([1, 768]), torch.Size([1, 768]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "input_tokens = tokenize_function({\"text\": \"I love this product. It is amazing.\"})\n",
"input_tokens['input_ids'].shape, input_tokens['attention_mask'].shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora, passamos isso para o modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([[nan, nan, nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
"       grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "output = model(input_ids=input_tokens['input_ids'].to(device), attention_mask=input_tokens['attention_mask'].to(device))\n",
"output['logits']"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos as previsões desses logits"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "predicted = predicted_labels(output['logits'])\n",
"predicted"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "## Ajuste fino para geração de texto com o Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Repetimos o treinamento com o Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Reinicie o notebook para ter certeza de que"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Conjunto de dados"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Baixamos novamente o conjunto de dados de piadas"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "DatasetDict({\n",
"    train: Dataset({\n",
"        features: ['ID', 'Joke'],\n",
"        num_rows: 231657\n",
"    })\n",
"})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from datasets import load_dataset\n",
"\n",
"jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
"jokes"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Crie um subconjunto caso você esteja com pouca memória"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Dataset({\n",
"    features: ['ID', 'Joke'],\n",
"    num_rows: 231657\n",
"})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "percent_of_train_dataset = 1    # If you want 50% of the dataset, set this to 0.5\n",
"\n",
"subset_dataset = jokes[\"train\"].select(range(int(len(jokes[\"train\"]) * percent_of_train_dataset)))\n",
"subset_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Dividimos o conjunto de dados em subconjuntos de treinamento, validação e teste."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583\n"
          ]
        }
      ],
      "source": [
      "percent_of_train_dataset = 0.90\n",
"\n",
"split_dataset = subset_dataset.train_test_split(train_size=int(subset_dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
"train_dataset = split_dataset[\"train\"]\n",
"validation_test_dataset = split_dataset[\"test\"]\n",
"\n",
"split_dataset = validation_test_dataset.train_test_split(train_size=int(validation_test_dataset.num_rows * 0.5), seed=19, shuffle=False)\n",
"validation_dataset = split_dataset[\"train\"]\n",
"test_dataset = split_dataset[\"test\"]\n",
"\n",
"print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(validation_dataset)}. Size of the test set: {len(test_dataset)}\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Tokeniser"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Iniciamos o tokenizador e atribuímos o token do final da string ao token de preenchimento."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
      "from transformers import AutoTokenizer\n",
"\n",
"checkpoints = \"openai-community/gpt2\"\n",
"\n",
"tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
"tokenizer.pad_token = tokenizer.eos_token\n",
"tokenizer.padding_side = \"right\""
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Adicionamos os tokens especiais para o início e o fim de uma piada."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "Added 2 tokens\n"
          ]
        }
      ],
      "source": [
      "new_tokens = ['<SJ>', '<EJ>']   # Start and end of joke tokens\n",
"\n",
"num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
"print(f\"Added {num_added_tokens} tokens\")"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Nós os adicionamos ao conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['Joke'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "joke_column = \"Joke\"\n",
"\n",
"def format_joke(example):\n",
"    example[joke_column] = '<SJ> ' + example['Joke'] + ' <EJ>'\n",
"    return example\n",
"\n",
"remove_columns = [column for column in train_dataset.column_names if column != joke_column]\n",
"\n",
"train_dataset = train_dataset.map(format_joke, remove_columns=remove_columns)\n",
"validation_dataset = validation_dataset.map(format_joke, remove_columns=remove_columns)\n",
"test_dataset = test_dataset.map(format_joke, remove_columns=remove_columns)\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Tokenizamos o conjunto de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 208491\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }),\n",
" Dataset({\n",
"     features: ['input_ids', 'attention_mask'],\n",
"     num_rows: 11583\n",
" }))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "def tokenize_function(examples):\n",
"    return tokenizer(examples[joke_column], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")\n",
"\n",
"train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[joke_column])\n",
"train_dataset, validation_dataset, test_dataset"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instanciar o modelo, atribuir o token de preenchimento e adicionar os novos tokens de início e fim de piada."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "Embedding(50259, 768)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "from transformers import AutoModelForCausalLM\n",
"\n",
"model = AutoModelForCausalLM.from_pretrained(checkpoints)\n",
"model.config.pad_token_id = model.config.eos_token_id\n",
"model.resize_token_embeddings(len(tokenizer))"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Dispositivo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o dispositivo e passamos o modelo para o dispositivo."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
      {
          "name": "stdout",
          "output_type": "stream",
          "text": [
          "\n"
          ]
        }
      ],
      "source": [
      "import torch\n",
"\n",
"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
"model.half().to(device)\n",
"print()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Conjunto de dados Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criar um conjunto de dados pytorch"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import Dataset\n",
"\n",
"class JokesDataset(Dataset):\n",
"    def __init__(self, huggingface_dataset):\n",
"        self.dataset = huggingface_dataset\n",
"\n",
"    def __getitem__(self, idx):\n",
"        input_ids = torch.tensor(self.dataset[idx]['input_ids'])\n",
"        attention_mask = torch.tensor(self.dataset[idx]['attention_mask'])\n",
"        return input_ids, attention_mask\n",
"\n",
"    def __len__(self):\n",
"        return len(self.dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Instanciamos os conjuntos de dados de treinamento, validação e teste."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
      "train_pytorch_dataset = JokesDataset(train_dataset)\n",
"validation_pytorch_dataset = JokesDataset(validation_dataset)\n",
"test_pytorch_dataset = JokesDataset(test_dataset)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Aqui está um exemplo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([768]), torch.Size([768]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "input_ids, attention_mask = train_pytorch_dataset[0]\n",
"input_ids.shape, attention_mask.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Carregador de dados Pytorch"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos os carregadores de dados"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
      "from torch.utils.data import DataLoader\n",
"\n",
"BS = 28\n",
"\n",
"train_loader = DataLoader(train_pytorch_dataset, batch_size=BS, shuffle=True)\n",
"validation_loader = DataLoader(validation_pytorch_dataset, batch_size=BS)\n",
"test_loader = DataLoader(test_pytorch_dataset, batch_size=BS)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Vemos uma amostra"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "(torch.Size([28, 768]), torch.Size([28, 768]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "input_ids, attention_mask = next(iter(train_loader))\n",
"input_ids.shape, attention_mask.shape"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Passamos isso para o modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "odict_keys(['logits', 'past_key_values'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "output = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
"output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Como podemos ver, não temos nenhum valor de `perda`, pois, como vimos, temos que passar os `input_ids` e os `labels`."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "odict_keys(['loss', 'logits', 'past_key_values'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "output = model(input_ids.to(device), attention_mask=attention_mask.to(device), labels=input_ids.to(device))\n",
"output.keys()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Agora temos a \"perda\"."
      ]
    },
{
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "text/plain": [
            "80.5625"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
      "output['loss'].item()"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Otimizador"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos um otimizador"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
"  warnings.warn(\n"
          ]
        }
      ],
      "source": [
      "from transformers import AdamW\n",
"\n",
"LR = 2e-5\n",
"optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Treinamento"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Criamos o loop de treinamento"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          "Epoch 1: 100%|██████████| 7447/7447 [51:07<00:00,  2.43it/s, train_loss=nan]\n",
"Epoch 2: 100%|██████████| 7447/7447 [51:06<00:00,  2.43it/s, train_loss=nan]\n",
"Epoch 3: 100%|██████████| 7447/7447 [51:07<00:00,  2.43it/s, train_loss=nan]\n"
          ]
        }
      ],
      "source": [
      "from tqdm import tqdm\n",
"\n",
"EPOCHS = 3\n",
"\n",
"for epoch in range(EPOCHS):\n",
"    model.train()\n",
"    train_loss = 0\n",
"    progresbar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}')\n",
"    for input_ids, at_mask in progresbar:\n",
"        input_ids = input_ids.to(device)\n",
"        at_mask = at_mask.to(device)\n",
"\n",
"        output = model(input_ids=input_ids, attention_mask=at_mask, labels=input_ids)\n",
"\n",
"        loss = output['loss']\n",
"        train_loss += loss.item()\n",
"        optimizer.zero_grad()\n",
"        loss.backward()\n",
"        optimizer.step()\n",
"        progresbar.set_postfix({'train_loss': loss.item()})\n",
"    train_loss /= len(train_loader)\n",
"    progresbar.set_postfix({'train_loss': train_loss})"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "### Uso do modelo"
      ]
    },
{
      "cell_type": "markdown",
      "metadata": {},
      "source": [
      "Testamos o modelo"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
      "def generate_text(decoded_joke, max_new_tokens=100, stop_token='<EJ>', top_k=0, temperature=1.0):\n",
"    input_tokens = tokenize_function({'Joke': decoded_joke})\n",
"    output = model(input_tokens['input_ids'].to(device), attention_mask=input_tokens['attention_mask'].to(device))\n",
"    nex_token = torch.argmax(output['logits'][:, -1, :], dim=-1).item()\n",
"    nex_token_decoded = tokenizer.decode(nex_token)\n",
"    decoded_joke = decoded_joke + nex_token_decoded\n",
"    for _ in range(max_new_tokens):\n",
"        nex_token = torch.argmax(output['logits'][:, -1, :], dim=-1).item()\n",
"        nex_token_decoded = tokenizer.decode(nex_token)\n",
"        if nex_token_decoded == stop_token:\n",
"            break\n",
"        decoded_joke = decoded_joke + nex_token_decoded\n",
"        input_tokens = tokenize_function({'Joke': decoded_joke})\n",
"        output = model(input_tokens['input_ids'].to(device), attention_mask=input_tokens['attention_mask'].to(device))\n",
"    return decoded_joke"
      ]
    },
{
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
      {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
            "\"<SJ> Why didn't the frog cross the road!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
      "generated_text = generate_text(\"<SJ> Why didn't the frog cross the road\")\n",
"generated_text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
