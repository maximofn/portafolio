{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# LoRA - low rank adaptation of large language models"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The increasing size of language models makes it more and more expensive to train them because more and more VRAM is needed to store all their parameters and the gradients derived from training\n",
                        "\n",
                        "In the paper [LoRA - Low rank adaption of large language models](https://arxiv.org/abs/2106.09685) they propose to freeze the model weights and train two matrices called A and B greatly reducing the number of parameters to be trained.\n",
                        "\n",
                        "![LoRA](https://maximofn.com/wp-content/uploads/2024/07/LoRA_adapat.webp)\n",
                        "\n",
                        "Let's see how this is done"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## LoRA explanation"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Updating of weights in a neural network"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "To understand how LoRA works, we first have to remember what happens when we train a model. Let's go back to the most basic part of deep learning, we have a dense layer of a neural network that is defined as:\n",
                        "\n",
                        "$$\n",
                        "y = Wx + b\n",
                        "$$\n",
                        "\n",
                        "Where $W$ is the weights matrix and $b$ is the bias vector.\n",
                        "\n",
                        "For the sake of simplicity we will assume that there is no bias, so it would look like this\n",
                        "\n",
                        "$$\n",
                        "y = Wx\n",
                        "$$\n",
                        "\n",
                        "Suppose that for an input $x$ we want it to have an output $ŷ$.\n",
                        "\n",
                        " * First what we do is to calculate the output we get with our current value of pesos $W$, i.e. we get the value $y$.\n",
                        " * Next we calculate the error that exists between the value of $y$ that we have obtained and the value that we wanted to obtain $ŷ$. We call this error $loss$, and we calculate it with some mathematical function, now it does not matter which one.\n",
                        " * We compute the gardient (the derivative) of the error $loss$ with respect to the weights matrix $W$, i.e. $$Delta W = \\frac{dloss}{dW}$.\n",
                        " * We update the weights $W$ by subtracting from each of their values the value of the gradient multiplied by a learning factor $alpha$, i.e. $W = W - \\alpha \\Delta W$."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### LoRA"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The authors of LoRA propose that the weights matrix $W$ can be decomposed into\n",
                        "\n",
                        "$$\n",
                        "W \\sim W + \\Delta W\n",
                        "$$\n",
                        "\n",
                        "So, by freezing the $W$ matrix and training only the $\"Delta W$ matrix, it is possible to obtain a model that fits new data without having to retrain the whole model.\n",
                        "\n",
                        "But you may think that $$Delta W$ is a matrix of size equal to $W$ so nothing has been gained, but here the authors rely on `Aghajanyan et al. (2020)`, a paper in which they showed that although the language models are large and their parameters are matrices with very large dimensions, to adapt them to new tasks it is not necessary to change all the values of the matrices, but changing a few values is enough, which in technical terms, is called Low Rank Adaptation. Hence the name LoRA (Low Rank Adaptation)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We have frozen the model and now we want to train the $\\Delta W$ matrix, let's assume that both $W$ and $\\Delta W$ are matrices of size $20 \\times 10$, so we have 200 trainable parameters\n",
                        "\n",
                        "Now suppose that the matrix $\\Delta W$ can be decomposed into the product of two matrices $A$ and $B$, i.e.\n",
                        "\n",
                        "$$\n",
                        "\\Delta W = A \\cdot B\n",
                        "$$\n",
                        "\n",
                        "For this multiplication to occur the sizes of the matrices $A$ and $B$ have to be $20 \\times n$ and $n \\times 10$ respectively. Suppose $n = 5$, so $A$ would be of size $20 \\times 5$, i.e. 100 parameters, and $B$ of size $5 \\times 10$, i.e. 50 parameters, so we would have 100+50=150 trainable parameters. We already have less trainable parameters than before\n",
                        "\n",
                        "Now let's suppose that $W$ is actually a matrix of size $10.000 \\times 10.000$, so we would have 100.000.000 trainable parameters, but if we decompose $\\Delta W$ in $A$ and $B$ with $n = 5$, we would have a matrix of size $10.000 \\times 5$ and another one of size $5 \\times 10.000$, so we would have 50.000 parameters of one and another 50.000 parameters of the other, in total 100.000 trainable parameters, that is to say we have reduced the number of parameters 1000 times.\n",
                        "\n",
                        "You can already see the power of LoRA, when you have very large models, the number of trainable parameters can be greatly reduced.\n",
                        "\n",
                        "If we look again at the image of the LoRA architecture, we will understand it better.\n",
                        "\n",
                        "![LoRA adapt](https://maximofn.com/wp-content/uploads/2024/07/LoRA_adapat.webp)\n",
                        "\n",
                        "But it looks even better, the savings in number of trainable parameters with this image\n",
                        "\n",
                        "![LoRA matmul](https://maximofn.com/wp-content/uploads/2024/07/Lora_matmul.webp)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Implementation of LoRA in transformers"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Since language models are implementations of transformers, let's see how LoRA is implemented in transformers. In the transformer architecture there are linear layers in the $Q$, $K$ and $V$ attention matrices, and in the feedforward layers, so LoRA can be applied to all these linear layers. In the paper they say that for simplicity they apply it only to the linear layers of the $Q$, $K$ and $V$ attention matrices.\n",
                        "\n",
                        "These layers have a size $d_{model} \\times d_{model}$, where $d_{model}$ is the embedding dimension of the model."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Range size r"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In order to have these benefits, the size of the range $r$ have to be smaller than the size of the linear layers. Since we have said that they only implemented it in the linear layers of attention, which have a size $d_{model} \\times d_{model}$, the rank size $r$ has to be smaller than $d_{model}$."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Initialization of matrices A and B"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The matrices $A$ and $B$ are initialized with a random Gaussian distribution for $A$ and zero for $B$, so the product of both matrices will be zero at the beginning, i.e.\n",
                        "\n",
                        "$$\n",
                        "\\Delta W = A \\cdot B = 0\n",
                        "$$"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Influence of LoRA through the parameter $alpha$"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Finally, in the LoRA implementation, a $alpha$ parameter is added to establish the degree of influence of LoRA on training. It is similar to the learning rate in normal fine tuning, but in this case it is used to establish the influence of LoRA on the training. Thus the LoRA formula would look like this\n",
                        "\n",
                        "$$\n",
                        "W = W + \\alpha \\Delta W = W + \\alpha A \\cdot B\n",
                        "$$"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Advantages of LoRA\n",
                        "\n",
                        "Now that we understand how it works, let's take a look at the advantages of this approach\n",
                        "\n",
                        " * Reduction of the number of trainable parameters. As we have seen, the number of trainable parameters is drastically reduced, which makes training much faster and less VRAM is needed, thus saving a lot of costs.\n",
                        " * Adapters in production. We can have in production a single language model and several adapters, each one for a different task, instead of having several models trained for each task, thus saving storage and computational costs. Moreover, this method does not have to add latency in the inference because we can merge the original weight matrix with the adapter, since we have seen that $W \\sim W + \\Delta W = W + A \\cdot B$, so the inference time would be the same as using the original language model.\n",
                        " * Buying and sharing adapters. If we train an adapter, we can share only the adapter. That is, in production, everyone can have the original model and every time we train an adapter we can share only the adapter, so as much smaller arrays would be shared, the size of the files to be shared would be much smaller."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Implementation of LoRA in an LLM"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We are going to repeat the training code of the post [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/), specifically the training for text classification with the Hugging Face libraries, but this time we are going to do it with LoRA. In the previous post we used a batch size of 28 for the training loop and 40 for the evaluation loop, however, as now we are not going to train all the weights of the model, but only the LoRA matrices, we will be able to use a bigger batch size"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Login to the Hub"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We log in to upload the model to the Hub"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from huggingface_hub import notebook_login\n",
                        "notebook_login()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Dataset"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We download the dataset we are going to use, which is a dataset of reviews from [Amazon](https://huggingface.co/datasets/mteb/amazon_reviews_multi)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetDict({\n",
                                          "    train: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 200000\n",
                                          "    })\n",
                                          "    validation: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "    test: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "})"
                                    ]
                              },
                              "execution_count": 1,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from datasets import load_dataset\n",
                        "\n",
                        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
                        "dataset"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a subset in case you want to test the code with a smaller dataset. In my case I will use 100% of the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 2,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "percentage = 1\n",
                        "\n",
                        "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
                        "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
                        "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see a sample"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'id': 'en_0388304',\n",
                                          " 'text': 'The N was missing from on\\n\\nThe N was missing from on',\n",
                                          " 'label': 0,\n",
                                          " 'label_text': '0'}"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from random import randint\n",
                        "\n",
                        "idx = randint(0, len(subset_dataset_train))\n",
                        "subset_dataset_train[idx]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We obtain the number of classes, to obtain the number of classes we use `dataset['train']` and not `subset_dataset_train` because if the subset is too small it is possible that there are no examples with all the possible classes of the original dataset."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "5"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "num_classes = len(dataset['train'].unique('label'))\n",
                        "num_classes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a function to create the `label` field in the dataset. The downloaded dataset has the `labels` field but the `transformers` library needs the field to be called `label` and not `labels`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def set_labels(example):\n",
                        "    example['labels'] = example['label']\n",
                        "    return example"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We apply the function to the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 7,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 7,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
                        "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
                        "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Here is a sample again"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'id': 'en_0388304',\n",
                                          " 'text': 'The N was missing from on\\n\\nThe N was missing from on',\n",
                                          " 'label': 0,\n",
                                          " 'label_text': '0',\n",
                                          " 'labels': 0}"
                                    ]
                              },
                              "execution_count": 8,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train[idx]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Tokenizer"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We implement the tokenizer. To avoid errors, we assign the end of string token to the padding token."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "tokenizer.pad_token = tokenizer.eos_token"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a function for tokenizing the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def tokenize_function(examples):\n",
                        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We apply the function to the dataset and remove the columns that we do not need"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 12,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see again a sample, but in this case we only see the `keys`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "dict_keys(['labels', 'input_ids', 'attention_mask'])"
                                    ]
                              },
                              "execution_count": 13,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train[idx].keys()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We instantiate the model. Also, in order to avoid errors, we assign the end of string token to the padding token."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import AutoModelForSequenceClassification\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
                        "model.config.pad_token_id = model.config.eos_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we have already seen in the post [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/) we get a warning that some layers have not been initialized. This is because in this case, as it is a classification problem and when we have instantiated the model we have told it that we want it to be a classification model with 5 classes, the library has removed the last layer and replaced it with a 5 neuron one at the output. If you do not understand this well go to the post that I quote that is better eplicado"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### LoRA"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Before implementing LoRA, we look at the number of trainable parameters that the model has"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 14,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total trainable parameters before: 124,443,648\n"
                              ]
                        }
                  ],
                  "source": [
                        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                        "print(f\"Total trainable parameters before: {total_params:,}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see that it has 124M trainable parameters. Now let's freeze them"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 15,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total trainable parameters after: 0\n"
                              ]
                        }
                  ],
                  "source": [
                        "for param in model.parameters():\n",
                        "    param.requires_grad = False\n",
                        "\n",
                        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                        "print(f\"Total trainable parameters after: {total_params:,}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "After freezing there are no more trainable parameters"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's see what the model looks like before applying LoRA"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 16,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "GPT2ForSequenceClassification(\n",
                                          "  (transformer): GPT2Model(\n",
                                          "    (wte): Embedding(50257, 768)\n",
                                          "    (wpe): Embedding(1024, 768)\n",
                                          "    (drop): Dropout(p=0.1, inplace=False)\n",
                                          "    (h): ModuleList(\n",
                                          "      (0-11): 12 x GPT2Block(\n",
                                          "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (attn): GPT2Attention(\n",
                                          "          (c_attn): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (mlp): GPT2MLP(\n",
                                          "          (c_fc): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (act): NewGELUActivation()\n",
                                          "          (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "  )\n",
                                          "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 16,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "First we create the LoRA layer.\n",
                        "\n",
                        "It has to inherit from `torch.nn.Module` so that it can act as a layer of a neural network.\n",
                        "\n",
                        "In the `_init_` method we create the `A` and `B` matrices initialized as explained before, the `A` matrix with a random Gaussian distribution and the `B` matrix with zeros. We also create the parameters `rank` and `alpha`.\n",
                        "\n",
                        "In the `forward` method we calculate LoRA as explained above."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 17,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "\n",
                        "class LoRALayer(torch.nn.Module):\n",
                        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
                        "        super().__init__()\n",
                        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
                        "        torch.nn.init.kaiming_uniform_(self.A, a=torch.sqrt(torch.tensor(5.)).item())  # similar to standard weight initialization\n",
                        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
                        "        self.alpha = alpha\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        x = self.alpha * (x @ self.A @ self.B)\n",
                        "        return x"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we create a linear class with LoRA.\n",
                        "\n",
                        "As before, it inherits from `torch.nn.Module` so that it can act as a layer of a neural network.\n",
                        "\n",
                        "In the `_init_` method we create a variable with the original linear layer of the network and we create another variable with the new LoRA layer that we had implemented before\n",
                        "\n",
                        "In the `forward` method we add the outputs of the original linear layer and the LoRA layer."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 18,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "class LoRALinear(torch.nn.Module):\n",
                        "    def __init__(self, linear, rank, alpha):\n",
                        "        super().__init__()\n",
                        "        self.linear = linear\n",
                        "        self.lora = LoRALayer(\n",
                        "            linear.in_features, linear.out_features, rank, alpha\n",
                        "        )\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        return self.linear(x) + self.lora(x)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Finally we create a function that replaces the linear layers by the new linear layer with LoRA that we have created. What it does is that if it finds a linear layer in the model, it replaces it with the linear layer with LoRA, if not, it applies the function within the sublayers of the layer."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 19,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def replace_linear_with_lora(model, rank, alpha):\n",
                        "    for name, module in model.named_children():\n",
                        "        if isinstance(module, torch.nn.Linear):\n",
                        "            # Replace the Linear layer with LinearWithLoRA\n",
                        "            setattr(model, name, LoRALinear(module, rank, alpha))\n",
                        "        else:\n",
                        "            # Recursively apply the same function to child modules\n",
                        "            replace_linear_with_lora(module, rank, alpha)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We apply the function to the model to replace the linear layers of the model by the new linear layer with LoRA"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 20,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "rank = 16\n",
                        "alpha = 16\n",
                        "\n",
                        "replace_linear_with_lora(model, rank=rank, alpha=alpha)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We now see the number of trainable parameters"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 21,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total trainable LoRA parameters: 12,368\n"
                              ]
                        }
                  ],
                  "source": [
                        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We have gone from 124M trainable parameters to 12k trainable parameters, i.e. we have reduced the number of trainable parameters 10,000 times!"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We see the model again"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 22,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "GPT2ForSequenceClassification(\n",
                                          "  (transformer): GPT2Model(\n",
                                          "    (wte): Embedding(50257, 768)\n",
                                          "    (wpe): Embedding(1024, 768)\n",
                                          "    (drop): Dropout(p=0.1, inplace=False)\n",
                                          "    (h): ModuleList(\n",
                                          "      (0-11): 12 x GPT2Block(\n",
                                          "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (attn): GPT2Attention(\n",
                                          "          (c_attn): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (mlp): GPT2MLP(\n",
                                          "          (c_fc): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (act): NewGELUActivation()\n",
                                          "          (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "  )\n",
                                          "  (score): LoRALinear(\n",
                                          "    (linear): Linear(in_features=768, out_features=5, bias=False)\n",
                                          "    (lora): LoRALayer()\n",
                                          "  )\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 22,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's compare them layer by layer\n",
                        "\n",
                        "|Original Model|Model with LoRA|\n",
                        "|-|-|\n",
                        "|GPT2ForSequenceClassification(|GPT2ForSequenceClassification(|\n",
                        "|  (transformer): GPT2Model(|  (transformer): GPT2Model(|\n",
                        "|    (wte): Embedding(50257, 768)|    (wte): Embedding(50257, 768)|\n",
                        "|    (wpe): Embedding(1024, 768)|    (wpe): Embedding(1024, 768)|\n",
                        "|    (drop): Dropout(p=0.1, inplace=False)|    (drop): Dropout(p=0.1, inplace=False)|\n",
                        "|    (h): ModuleList(|    (h): ModuleList(|\n",
                        "|      (0-11): 12 x GPT2Block(|      (0-11): 12 x GPT2Block(|\n",
                        "|        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|\n",
                        "|        (attn): GPT2Attention(|        (attn): GPT2Attention(|\n",
                        "|          (c_attn): Conv1D()|          (c_attn): Conv1D()|\n",
                        "|          (c_proj): Conv1D()|          (c_proj): Conv1D()|\n",
                        "|          (attn_dropout): Dropout(p=0.1, inplace=False)|          (attn_dropout): Dropout(p=0.1, inplace=False)|\n",
                        "|          (resid_dropout): Dropout(p=0.1, inplace=False)|          (resid_dropout): Dropout(p=0.1, inplace=False)|\n",
                        "|        )|        )|\n",
                        "|        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|\n",
                        "|        (mlp): GPT2MLP(|        (mlp): GPT2MLP(|\n",
                        "|          (c_fc): Conv1D()|          (c_fc): Conv1D()|\n",
                        "|          (c_proj): Conv1D()|          (c_proj): Conv1D()|\n",
                        "|          (act): NewGELUActivation()|          (act): NewGELUActivation()|\n",
                        "|          (dropout): Dropout(p=0.1, inplace=False)|          (dropout): Dropout(p=0.1, inplace=False)|\n",
                        "|        )|        )|\n",
                        "|      )|      )|\n",
                        "|    )|    )|\n",
                        "|    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|\n",
                        "|  )|  )|\n",
                        "||  (score): LoRALinear()|\n",
                        "|  (score): Linear(in_features=768, out_features=5, bias=False)|    (linear): Linear(in_features=768, out_features=5, bias=False)|\n",
                        "||    (lora): LoRALayer()|\n",
                        "||  )|\n",
                        "|)|)|\n",
                        "\n",
                        "We see that they are the same except at the end, where in the original model there was a normal linear layer and in the model with LoRA there is a `LoRALinear` layer that inside has the linear layer of the original model and a `LoRALayer` layer."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Training"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Once the model has been instantiated with LoRA, let's train it as usual"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we have said, in the post [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/) we used a batch size of 28 for the training loop and 40 for the evaluation loop, while now that there are fewer trainable parameters we can use a larger batch size.\n",
                        "\n",
                        "Why does this happen? When training a model, the model and its gradients must be saved in the GPU memory, so both with LoRA and without LoRA the model must be saved, but in the case of LoRA only the gradients of 12k parameters are saved, while with LoRA the gradients of 128M parameters are saved, so with LoRA less GPU memory is needed, so a larger batch size can be used."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 23,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import TrainingArguments\n",
                        "\n",
                        "metric_name = \"accuracy\"\n",
                        "model_name = \"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification\"\n",
                        "LR = 2e-5\n",
                        "BS_TRAIN = 400\n",
                        "BS_EVAL = 400\n",
                        "EPOCHS = 3\n",
                        "WEIGHT_DECAY = 0.01\n",
                        "\n",
                        "training_args = TrainingArguments(\n",
                        "    model_name,\n",
                        "    eval_strategy=\"epoch\",\n",
                        "    save_strategy=\"epoch\",\n",
                        "    learning_rate=LR,\n",
                        "    per_device_train_batch_size=BS_TRAIN,\n",
                        "    per_device_eval_batch_size=BS_EVAL,\n",
                        "    num_train_epochs=EPOCHS,\n",
                        "    weight_decay=WEIGHT_DECAY,\n",
                        "    lr_scheduler_type=\"cosine\",\n",
                        "    warmup_ratio = 0.1,\n",
                        "    fp16=True,\n",
                        "    load_best_model_at_end=True,\n",
                        "    metric_for_best_model=metric_name,\n",
                        "    push_to_hub=True,\n",
                        "    logging_dir=\"./runs\",\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 24,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import numpy as np\n",
                        "from evaluate import load\n",
                        "\n",
                        "metric = load(\"accuracy\")\n",
                        "\n",
                        "def compute_metrics(eval_pred):\n",
                        "    print(eval_pred)\n",
                        "    predictions, labels = eval_pred\n",
                        "    predictions = np.argmax(predictions, axis=1)\n",
                        "    return metric.compute(predictions=predictions, references=labels)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 25,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import Trainer\n",
                        "\n",
                        "trainer = Trainer(\n",
                        "    model,\n",
                        "    training_args,\n",
                        "    train_dataset=subset_dataset_train,\n",
                        "    eval_dataset=subset_dataset_validation,\n",
                        "    tokenizer=tokenizer,\n",
                        "    compute_metrics=compute_metrics,\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 27,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/",
                              "height": 256
                        },
                        "id": "AZL4wIFKlC6x",
                        "outputId": "9f6a314c-65f0-46a5-fadf-61f832c1ca73"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [1500/1500 42:41, Epoch 3/3]\n",
                                          "    </div>\n",
                                          "    <table border=\"1\" class=\"dataframe\">\n",
                                          "  <thead>\n",
                                          " <tr style=\"text-align: left;\">\n",
                                          "      <th>Epoch</th>\n",
                                          "      <th>Training Loss</th>\n",
                                          "      <th>Validation Loss</th>\n",
                                          "      <th>Accuracy</th>\n",
                                          "    </tr>\n",
                                          "  </thead>\n",
                                          "  <tbody>\n",
                                          "    <tr>\n",
                                          "      <td>1</td>\n",
                                          "      <td>2.396400</td>\n",
                                          "      <td>1.602937</td>\n",
                                          "      <td>0.269400</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>2</td>\n",
                                          "      <td>1.572700</td>\n",
                                          "      <td>1.531719</td>\n",
                                          "      <td>0.320800</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>3</td>\n",
                                          "      <td>1.534400</td>\n",
                                          "      <td>1.511815</td>\n",
                                          "      <td>0.335800</td>\n",
                                          "    </tr>\n",
                                          "  </tbody>\n",
                                          "</table><p>"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be46440>\n",
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be45c30>\n",
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be8b970>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "TrainOutput(global_step=1500, training_loss=1.8345018310546874, metrics={'train_runtime': 2565.4667, 'train_samples_per_second': 233.876, 'train_steps_per_second': 0.585, 'total_flos': 2.352076406784e+17, 'train_loss': 1.8345018310546874, 'epoch': 3.0})"
                                    ]
                              },
                              "execution_count": 27,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "trainer.train()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Evaluation"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Once trained we evaluate on the test dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 28,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/",
                              "height": 159
                        },
                        "id": "Xcxmbo-3Uz1G",
                        "outputId": "a6ce9a05-22a1-40ce-a32a-b149ed8e6dd4"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [13/13 00:17]\n",
                                          "    </div>\n",
                                          "    "
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be8bbe0>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'eval_loss': 1.5203168392181396,\n",
                                          " 'eval_accuracy': 0.3374,\n",
                                          " 'eval_runtime': 19.3843,\n",
                                          " 'eval_samples_per_second': 257.94,\n",
                                          " 'eval_steps_per_second': 0.671,\n",
                                          " 'epoch': 3.0}"
                                    ]
                              },
                              "execution_count": 28,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "trainer.evaluate(eval_dataset=subset_dataset_test)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Publish the model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now that we have our model trained, we can share it with the world, so first we create a model card."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "trainer.create_model_card()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And now we can publish it. As the first thing we have done is to log in with the huggingface hub, we can upload it to our hub without any problem."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "trainer.push_to_hub()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Model test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We clean as much as possible"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "import gc\n",
                        "\n",
                        "\n",
                        "def clear_hardwares():\n",
                        "    torch.clear_autocast_cache()\n",
                        "    torch.cuda.ipc_collect()\n",
                        "    torch.cuda.empty_cache()\n",
                        "    gc.collect()\n",
                        "\n",
                        "\n",
                        "clear_hardwares()\n",
                        "clear_hardwares()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we have uploaded the model to our hub we can download it and use it."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import pipeline\n",
                        "\n",
                        "user = \"maximofn\"\n",
                        "checkpoints = f\"{user}/{model_name}\"\n",
                        "task = \"text-classification\"\n",
                        "classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now if we want to return the probability of all classes, we simply use the classifier we just instantiated, with the parameter `top_k=None`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 33,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "pqy8oMPuUz1I",
                        "outputId": "78501f30-2983-481a-e908-d5139bbce0e7"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_0', 'score': 0.8419149518013},\n",
                                          " {'label': 'LABEL_1', 'score': 0.09386005252599716},\n",
                                          " {'label': 'LABEL_3', 'score': 0.03624210134148598},\n",
                                          " {'label': 'LABEL_2', 'score': 0.02049318142235279},\n",
                                          " {'label': 'LABEL_4', 'score': 0.0074898069724440575}]"
                                    ]
                              },
                              "execution_count": 33,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "labels = classifier(\"I love this product\", top_k=None)\n",
                        "labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If we only want the class with the highest probability we do the same but with the parameter `top_k=1`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 34,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "qpcw3pN0Uz1J",
                        "outputId": "2667da0e-743c-4300-ac46-b8ff4d43270d"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_0', 'score': 0.8419149518013}]"
                                    ]
                              },
                              "execution_count": 34,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "label = classifier(\"I love this product\", top_k=1)\n",
                        "label"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And if we want n classes we do the same but with the parameter `top_k=n`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 35,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "pRQvB3tTUz1J",
                        "outputId": "0eb134fa-4633-4394-d795-b1563cad1d37"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_0', 'score': 0.8419149518013},\n",
                                          " {'label': 'LABEL_1', 'score': 0.09386005252599716}]"
                                    ]
                              },
                              "execution_count": 35,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "two_labels = classifier(\"I love this product\", top_k=2)\n",
                        "two_labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can also test the model with Automodel and AutoTokenizer."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import torch\n",
                        "\n",
                        "model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
                        "user = \"maximofn\"\n",
                        "checkpoint = f\"{user}/{model_name}\"\n",
                        "num_classes = num_classes\n",
                        "\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes).half().eval().to(\"cuda\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 37,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "9goQMfcLUz1K",
                        "outputId": "7d6dc70b-88a4-4f06-83f4-81457994de3b"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[0.003940582275390625,\n",
                                          " 0.00266265869140625,\n",
                                          " 0.013946533203125,\n",
                                          " 0.1544189453125,\n",
                                          " 0.8251953125]"
                                    ]
                              },
                              "execution_count": 37,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "tokens = tokenizer.encode(\"I love this product\", return_tensors=\"pt\").to(model.device)\n",
                        "with torch.no_grad():\n",
                        "    output = model(tokens)\n",
                        "logits = output.logits\n",
                        "lables = torch.softmax(logits, dim=1).cpu().numpy().tolist()\n",
                        "lables[0]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If you want to test the model further you can see it in [Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Implementation of LoRA in an LLM with PEFT from Hugging Face"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can do the same with the `PEFT` library of Hugging Face. Let's take a look at it"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Login to the Hub"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We log in to upload the model to the Hub"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from huggingface_hub import notebook_login\n",
                        "notebook_login()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Dataset"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We re-download the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetDict({\n",
                                          "    train: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 200000\n",
                                          "    })\n",
                                          "    validation: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "    test: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "})"
                                    ]
                              },
                              "execution_count": 1,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from datasets import load_dataset\n",
                        "\n",
                        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
                        "dataset"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a subset in case you want to test the code with a smaller dataset. In my case I will use 100% of the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 2,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "percentage = 1\n",
                        "\n",
                        "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
                        "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
                        "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We obtain the number of classes, to obtain the number of classes we use `dataset['train']` and not `subset_dataset_train` because if the subset is too small it is possible that there are no examples with all the possible classes of the original dataset."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "5"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "num_classes = len(dataset['train'].unique('label'))\n",
                        "num_classes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a function to create the `label` field in the dataset. The downloaded dataset has the `labels` field but the `transformers` library needs the field to be called `label` and not `labels`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def set_labels(example):\n",
                        "    example['labels'] = example['label']\n",
                        "    return example"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We apply the function to the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
                        "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
                        "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Tokenizer"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We instantiate the tokenizer. To avoid errors, we assign the token of end of string to the token of padding"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "tokenizer.pad_token = tokenizer.eos_token"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a function for tokenizing the dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 7,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def tokenize_function(examples):\n",
                        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We apply the function to the dataset and remove the columns that we do not need"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 8,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We instantiate the model. Also, in order to avoid errors, we assign the end of string token to the padding token."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import AutoModelForSequenceClassification\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
                        "model.config.pad_token_id = model.config.eos_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### LoRA with PEFT"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Before creating the model with LoRA, let's take a look at its layers"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "GPT2ForSequenceClassification(\n",
                                          "  (transformer): GPT2Model(\n",
                                          "    (wte): Embedding(50257, 768)\n",
                                          "    (wpe): Embedding(1024, 768)\n",
                                          "    (drop): Dropout(p=0.1, inplace=False)\n",
                                          "    (h): ModuleList(\n",
                                          "      (0-11): 12 x GPT2Block(\n",
                                          "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (attn): GPT2Attention(\n",
                                          "          (c_attn): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (mlp): GPT2MLP(\n",
                                          "          (c_fc): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (act): NewGELUActivation()\n",
                                          "          (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "  )\n",
                                          "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 10,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we can see there is only one `Linear` layer, which is `score` and that is the one we are going to replace."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can create a LoRA configuration with the PEFT library and then apply LoRA to the mo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 11,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from peft import LoraConfig, TaskType\n",
                        "\n",
                        "peft_config = LoraConfig(\n",
                        "    r=16,\n",
                        "    lora_alpha=32,\n",
                        "    lora_dropout=0.1,\n",
                        "    task_type=TaskType.SEQ_CLS,\n",
                        "    target_modules=[\"score\"],\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "With this configuration we have configured a rank of 16 and an alpha of 32. In addition we have added a dropout to the lora layers of 0.1. We have to indicate the task to the LoRA configuration, in this case it is a sequence classification task. Finally we indicate which layers we want to replace, in this case the `score` layer."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We now apply LoRA to the model"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from peft import get_peft_model\n",
                        "\n",
                        "model = get_peft_model(model, peft_config)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's see how many trainable parameters the model has now"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "trainable params: 12,368 || all params: 124,456,016 || trainable%: 0.0099\n"
                              ]
                        }
                  ],
                  "source": [
                        "model.print_trainable_parameters()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We obtain the same trainable parameters as before"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Training"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Once the model has been instantiated with LoRA, let's train it as usual"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 17,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import TrainingArguments\n",
                        "\n",
                        "metric_name = \"accuracy\"\n",
                        "model_name = \"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification\"\n",
                        "LR = 2e-5\n",
                        "BS_TRAIN = 400\n",
                        "BS_EVAL = 400\n",
                        "EPOCHS = 3\n",
                        "WEIGHT_DECAY = 0.01\n",
                        "\n",
                        "training_args = TrainingArguments(\n",
                        "    model_name,\n",
                        "    eval_strategy=\"epoch\",\n",
                        "    save_strategy=\"epoch\",\n",
                        "    learning_rate=LR,\n",
                        "    per_device_train_batch_size=BS_TRAIN,\n",
                        "    per_device_eval_batch_size=BS_EVAL,\n",
                        "    num_train_epochs=EPOCHS,\n",
                        "    weight_decay=WEIGHT_DECAY,\n",
                        "    lr_scheduler_type=\"cosine\",\n",
                        "    warmup_ratio = 0.1,\n",
                        "    fp16=True,\n",
                        "    load_best_model_at_end=True,\n",
                        "    metric_for_best_model=metric_name,\n",
                        "    push_to_hub=True,\n",
                        "    logging_dir=\"./runs\",\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 18,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import numpy as np\n",
                        "from evaluate import load\n",
                        "\n",
                        "metric = load(\"accuracy\")\n",
                        "\n",
                        "def compute_metrics(eval_pred):\n",
                        "    print(eval_pred)\n",
                        "    predictions, labels = eval_pred\n",
                        "    predictions = np.argmax(predictions, axis=1)\n",
                        "    return metric.compute(predictions=predictions, references=labels)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 20,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import Trainer\n",
                        "\n",
                        "trainer = Trainer(\n",
                        "    model,\n",
                        "    training_args,\n",
                        "    train_dataset=subset_dataset_train,\n",
                        "    eval_dataset=subset_dataset_validation,\n",
                        "    tokenizer=tokenizer,\n",
                        "    compute_metrics=compute_metrics,\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='811' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [ 811/1500 22:43 < 19:20, 0.59 it/s, Epoch 1.62/3]\n",
                                          "    </div>\n",
                                          "    <table border=\"1\" class=\"dataframe\">\n",
                                          "  <thead>\n",
                                          " <tr style=\"text-align: left;\">\n",
                                          "      <th>Epoch</th>\n",
                                          "      <th>Training Loss</th>\n",
                                          "      <th>Validation Loss</th>\n",
                                          "      <th>Accuracy</th>\n",
                                          "    </tr>\n",
                                          "  </thead>\n",
                                          "  <tbody>\n",
                                          "    <tr>\n",
                                          "      <td>1</td>\n",
                                          "      <td>2.275100</td>\n",
                                          "      <td>1.512476</td>\n",
                                          "      <td>0.318200</td>\n",
                                          "    </tr>\n",
                                          "  </tbody>\n",
                                          "</table><p>"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f774a50bbe0>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [1500/1500 42:28, Epoch 3/3]\n",
                                          "    </div>\n",
                                          "    <table border=\"1\" class=\"dataframe\">\n",
                                          "  <thead>\n",
                                          " <tr style=\"text-align: left;\">\n",
                                          "      <th>Epoch</th>\n",
                                          "      <th>Training Loss</th>\n",
                                          "      <th>Validation Loss</th>\n",
                                          "      <th>Accuracy</th>\n",
                                          "    </tr>\n",
                                          "  </thead>\n",
                                          "  <tbody>\n",
                                          "    <tr>\n",
                                          "      <td>1</td>\n",
                                          "      <td>2.275100</td>\n",
                                          "      <td>1.512476</td>\n",
                                          "      <td>0.318200</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>2</td>\n",
                                          "      <td>1.515900</td>\n",
                                          "      <td>1.417553</td>\n",
                                          "      <td>0.373800</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>3</td>\n",
                                          "      <td>1.463500</td>\n",
                                          "      <td>1.405058</td>\n",
                                          "      <td>0.381400</td>\n",
                                          "    </tr>\n",
                                          "  </tbody>\n",
                                          "</table><p>"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f77486a7c40>\n",
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f7749eb5690>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "TrainOutput(global_step=1500, training_loss=1.751504597981771, metrics={'train_runtime': 2551.7753, 'train_samples_per_second': 235.13, 'train_steps_per_second': 0.588, 'total_flos': 2.352524525568e+17, 'train_loss': 1.751504597981771, 'epoch': 3.0})"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "trainer.train()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Evaluation"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Once trained we evaluate on the test dataset"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [13/13 00:17]\n",
                                          "    </div>\n",
                                          "    "
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f77a1d1f7c0>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'eval_loss': 1.4127237796783447,\n",
                                          " 'eval_accuracy': 0.3862,\n",
                                          " 'eval_runtime': 19.3275,\n",
                                          " 'eval_samples_per_second': 258.699,\n",
                                          " 'eval_steps_per_second': 0.673,\n",
                                          " 'epoch': 3.0}"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "trainer.evaluate(eval_dataset=subset_dataset_test)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Publish the model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We create a model card"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "trainer.create_model_card()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We publish it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "application/vnd.google.colaboratory.intrinsic+json": {
                                          "type": "string"
                                    },
                                    "text/plain": [
                                          "CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification/commit/839066c2bde02689a6b3f5624ac25f89c4de217d', commit_message='End of training', commit_description='', oid='839066c2bde02689a6b3f5624ac25f89c4de217d', pr_url=None, pr_revision=None, pr_num=None)"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "trainer.push_to_hub()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## PEFT-trained model test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We clean as much as possible"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "import gc\n",
                        "\n",
                        "\n",
                        "def clear_hardwares():\n",
                        "    torch.clear_autocast_cache()\n",
                        "    torch.cuda.ipc_collect()\n",
                        "    torch.cuda.empty_cache()\n",
                        "    gc.collect()\n",
                        "\n",
                        "\n",
                        "clear_hardwares()\n",
                        "clear_hardwares()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As we have uploaded the model to our hub we can download it and use it."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import pipeline\n",
                        "\n",
                        "user = \"maximofn\"\n",
                        "checkpoints = f\"{user}/{model_name}\"\n",
                        "task = \"text-classification\"\n",
                        "classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now if we want to return the probability of all classes, we simply use the classifier we just instantiated, with the parameter `top_k=None`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_1', 'score': 0.9979197382926941},\n",
                                          " {'label': 'LABEL_0', 'score': 0.002080311067402363}]"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "labels = classifier(\"I love this product\", top_k=None)\n",
                        "labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If we only want the class with the highest probability we do the same but with the parameter `top_k=1`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_1', 'score': 0.9979197382926941}]"
                                    ]
                              },
                              "execution_count": 4,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "label = classifier(\"I love this product\", top_k=1)\n",
                        "label"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And if we want n classes we do the same but with the parameter `top_k=n`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_1', 'score': 0.9979197382926941},\n",
                                          " {'label': 'LABEL_0', 'score': 0.002080311067402363}]"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "two_labels = classifier(\"I love this product\", top_k=2)\n",
                        "two_labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "If you want to test the model further you can see it in [Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification)"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "base",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.9"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
