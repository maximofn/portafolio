{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# LoRA - adaptação de baixa classificação de grandes modelos de linguagem"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação.."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "O tamanho cada vez maior dos modelos de linguagem torna cada vez mais caro treiná-los, pois é necessário cada vez mais VRAM para armazenar todos os seus parâmetros e gradientes derivados do treinamento.\n",
                        "\n",
                        "No artigo [LoRA - Low rank adaption of large language models](https://arxiv.org/abs/2106.09685), eles propõem congelar os pesos do modelo e treinar duas matrizes chamadas A e B, o que reduz bastante o número de parâmetros a serem treinados.\n",
                        "\n",
                        "![LoRA](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp)\n",
                        "\n",
                        "Vejamos como isso é feito"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Explicação do LoRA"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Atualização de pesos em uma rede neural"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para entender como o LoRA funciona, primeiro precisamos lembrar o que acontece quando treinamos um modelo. Vamos voltar à parte mais básica da aprendizagem profunda: temos uma camada densa de uma rede neural que é definida como:\n",
                        "\n",
                        "$$\n",
                        "y = Wx + b\n",
                        "$$\n",
                        "\n",
                        "Onde $W$ é a matriz de pesos e $b$ é o vetor de polarização.\n",
                        "\n",
                        "Para simplificar, vamos supor que não haja viés, de modo que ficaria assim\n",
                        "\n",
                        "$$\n",
                        "y = Wx\n",
                        "$$\n",
                        "\n",
                        "Suponha que, para uma entrada $x$, queremos que ela tenha uma saída $ŷ$.\n",
                        "\n",
                        " * Primeiro, calculamos o resultado que obtemos com nosso valor atual de pesos $W$, ou seja, obtemos o valor $y$.\n",
                        " * Em seguida, calculamos o erro que existe entre o valor de $y$ que obtivemos e o valor que queríamos obter $ŷ$. Chamamos esse erro de $loss$ e o calculamos com alguma função matemática, não importa qual.\n",
                        " * Calculamos a derivada do erro $loss$ com relação à matriz de peso $W$, ou seja, $$Delta W = \\frac{dloss}{dW}$.\n",
                        " * Atualizamos os pesos $W$ subtraindo de cada um de seus valores o valor do gradiente multiplicado por um fator de aprendizado $alpha$, ou seja, $W = W - \\alpha \\Delta W$."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### LoRA"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "O que os autores do LoRA propõem é que a matriz de peso $W$ possa ser decomposta em\n",
                        "\n",
                        "$$\n",
                        "W \\sim W + \\Delta W\n",
                        "$$\n",
                        "\n",
                        "Portanto, ao congelar a matriz $W$ e treinar somente a matriz $\"Delta W$, é possível obter um modelo que se ajusta aos novos dados sem precisar treinar novamente o modelo inteiro.\n",
                        "\n",
                        "Mas você pode pensar que $$Delta W$ é uma matriz de tamanho igual a $W$ e, portanto, nada foi ganho, mas aqui os autores se baseiam em `Aghajanyan et al. (2020)`, um artigo no qual eles mostraram que, embora os modelos de linguagem sejam grandes e seus parâmetros sejam matrizes com dimensões muito grandes, para adaptá-los a novas tarefas não é necessário alterar todos os valores das matrizes, mas alterar alguns valores é suficiente, o que, em termos técnicos, é chamado de Low Rank Adaptation (LoRA). Daí o nome LoRA (Low Rank Adaptation)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Congelamos o modelo e agora queremos treinar a matriz $\\Delta W$. Vamos supor que tanto $W$ quanto $\\Delta W$ sejam matrizes de tamanho $20 \\times 10$, portanto, temos 200 parâmetros treináveis.\n",
                        "\n",
                        "Agora, vamos supor que a matriz $\\Delta W$ possa ser decomposta no produto de duas matrizes $A$ e $B$, ou seja\n",
                        "\n",
                        "$$\n",
                        "\\Delta W = A \\cdot B\n",
                        "$$\n",
                        "\n",
                        "Para que essa multiplicação ocorra, os tamanhos das matrizes $A$ e $B$ devem ser $20 \\times n$ e $n \\times 10$, respectivamente. Suponha que $n = 5$, então $A$ teria o tamanho de $20 \\times 5$, ou seja, 100 parâmetros, e $B$ o tamanho de $5 \\times 10$, ou seja, 50 parâmetros, de modo que teríamos 100+50=150 parâmetros treináveis. Já temos menos parâmetros treináveis do que antes\n",
                        "\n",
                        "Agora vamos supor que $W$ seja, na verdade, uma matriz de tamanho $10.000 \\times 10.000$, de modo que teríamos 100.000.000 parâmetros treináveis, mas se decompusermos $\\Delta W$ em $A$ e $B$ com $n = 5$, teríamos uma matriz de tamanho $10.000 \\times 5$ e outra de tamanho $5 \\times 10.000$, de modo que teríamos 50.000 parâmetros de uma e outros 50.000 parâmetros da outra, em um total de 100.000 parâmetros treináveis, ou seja, reduzimos o número de parâmetros 1.000 vezes.\n",
                        "\n",
                        "Você já pode ver o poder do LoRA: quando você tem modelos muito grandes, o número de parâmetros treináveis pode ser bastante reduzido.\n",
                        "\n",
                        "Se olharmos novamente para a imagem da arquitetura do LoRA, entenderemos melhor.\n",
                        "\n",
                        "![LoRA adapt](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp)\n",
                        "\n",
                        "Mas a economia no número de parâmetros treináveis com essa imagem parece ainda melhor.\n",
                        "\n",
                        "![LoRA matmul](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Lora_matmul.webp)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Implementação de LoRA em transformadores"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como os modelos de linguagem são implementações de transformadores, vamos ver como o LoRA é implementado nos transformadores. Na arquitetura do transformador, há camadas lineares nas matrizes de atenção $Q$, $K$ e $V$ e nas camadas de feedforward, de modo que o LoRA pode ser aplicado a todas essas camadas lineares. No artigo, eles afirmam que, para simplificar, aplicam o LoRA somente às camadas lineares das matrizes de atenção $Q$, $K$ e $V$.\n",
                        "\n",
                        "Essas camadas têm um tamanho de $d_{model} \\times d_{model}$, em que $d_{model}$ é a dimensão de incorporação do modelo."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Tamanho do intervalo r"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para obter esses benefícios, o tamanho do intervalo $r$ deve ser menor que o tamanho das camadas lineares. Como dissemos que eles só o implementaram nas camadas lineares de atenção, que têm um tamanho $d_{model} \\times d_{model}$, o tamanho do intervalo $r$ deve ser menor que $d_{model}$."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Inicialização das matrizes A e B"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "As matrizes $A$ e $B$ são inicializadas com uma distribuição gaussiana aleatória para $A$ e zero para $B$, de modo que o produto de ambas as matrizes será zero no início, ou seja\n",
                        "\n",
                        "$$\n",
                        "\\Delta W = A \\cdot B = 0\n",
                        "$$"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Influência do LoRA por meio do parâmetro $alpha$"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Por fim, na implementação do LoRA, um parâmetro $alpha$ é adicionado para estabelecer o grau de influência do LoRA no treinamento. Ele é semelhante à taxa de aprendizado no ajuste fino normal, mas, nesse caso, é usado para estabelecer a influência do LoRA no treinamento. Assim, a fórmula do LoRA seria a seguinte\n",
                        "\n",
                        "$$\n",
                        "W = W + \\alpha \\Delta W = W + \\alpha A \\cdot B\n",
                        "$$"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Vantagens da LoRA\n",
                        "\n",
                        "Agora que entendemos como ele funciona, vamos examinar as vantagens desse método.\n",
                        "\n",
                        " * Redução do número de parâmetros treináveis. Como vimos, o número de parâmetros treináveis é drasticamente reduzido, o que torna o treinamento muito mais rápido e menos VRAM é necessária, economizando assim muitos custos.\n",
                        " * Adaptadores em produção. Podemos ter um único modelo de linguagem e vários adaptadores em produção, cada um para uma tarefa diferente, em vez de ter vários modelos treinados para cada tarefa, economizando, assim, custos computacionais e de armazenamento. Além disso, esse método não precisa adicionar latência na inferência porque a matriz de pesos original pode ser mesclada com o adaptador, já que vimos que $W \\sim W + \\Delta W = W + A \\cdot B$, de modo que o tempo de inferência seria o mesmo que usar o modelo de linguagem original.\n",
                        " * Comprar e compartilhar adaptadores. Se treinarmos um adaptador, poderemos compartilhar somente o adaptador. Ou seja, na produção, todos podem ter o modelo original e, cada vez que treinarmos um adaptador, poderemos compartilhar apenas o adaptador, de modo que, como matrizes muito menores seriam compartilhadas, o tamanho dos arquivos a serem compartilhados seria muito menor."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Implementação de LoRA em um LLM"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos repetir o código de treinamento da postagem [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/), especificamente o treinamento para classificação de texto com as bibliotecas Hugging Face, mas, desta vez, faremos isso com LoRA. Na publicação anterior, usamos um tamanho de lote de 28 para o loop de treinamento e 40 para o loop de avaliação; no entanto, como agora não vamos treinar todos os pesos do modelo, mas apenas as matrizes LoRA, poderemos usar um tamanho de lote maior."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Faça login no hub"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Fazemos login para carregar o modelo no Hub"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from huggingface_hub import notebook_login\n",
                        "notebook_login()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Conjunto de dados"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Baixamos o conjunto de dados que usaremos, que é um conjunto de dados de avaliações da [Amazon](https://huggingface.co/datasets/mteb/amazon_reviews_multi)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetDict({\n",
                                          "    train: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 200000\n",
                                          "    })\n",
                                          "    validation: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "    test: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "})"
                                    ]
                              },
                              "execution_count": 1,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from datasets import load_dataset\n",
                        "\n",
                        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
                        "dataset"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos um subconjunto para o caso de você querer testar o código com um conjunto de dados menor. No meu caso, usarei 100% do conjunto de dados."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 2,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "percentage = 1\n",
                        "\n",
                        "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
                        "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
                        "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos uma amostra"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'id': 'en_0388304',\n",
                                          " 'text': 'The N was missing from on\\n\\nThe N was missing from on',\n",
                                          " 'label': 0,\n",
                                          " 'label_text': '0'}"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from random import randint\n",
                        "\n",
                        "idx = randint(0, len(subset_dataset_train))\n",
                        "subset_dataset_train[idx]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para obter o número de classes, usamos `dataset['train']` e não `subset_dataset_train` porque, se o subconjunto for muito pequeno, talvez não haja exemplos com todas as classes possíveis do conjunto de dados original."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "5"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "num_classes = len(dataset['train'].unique('label'))\n",
                        "num_classes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos uma função para criar o campo `label` no conjunto de dados. O conjunto de dados baixado tem o campo `labels`, mas a biblioteca `transformers` precisa que o campo seja chamado `label` e não `labels`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def set_labels(example):\n",
                        "    example['labels'] = example['label']\n",
                        "    return example"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aplicamos a função ao conjunto de dados"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 7,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 7,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
                        "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
                        "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aqui está um exemplo novamente"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "{'id': 'en_0388304',\n",
                                          " 'text': 'The N was missing from on\\n\\nThe N was missing from on',\n",
                                          " 'label': 0,\n",
                                          " 'label_text': '0',\n",
                                          " 'labels': 0}"
                                    ]
                              },
                              "execution_count": 8,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train[idx]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Tokeniser"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Implementamos o tokenizador. Para evitar erros, atribuímos o token de fim de cadeia ao token de preenchimento."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "tokenizer.pad_token = tokenizer.eos_token"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos uma função para tokenizar o conjunto de dados"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def tokenize_function(examples):\n",
                        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aplicamos a função ao conjunto de dados e removemos as colunas de que não precisamos."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 12,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos uma amostra novamente, mas, nesse caso, vemos apenas as `keys`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "dict_keys(['labels', 'input_ids', 'attention_mask'])"
                                    ]
                              },
                              "execution_count": 13,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train[idx].keys()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Modelo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Instanciamos o modelo. Além disso, para evitar erros, atribuímos o token do final da string ao token de preenchimento."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import AutoModelForSequenceClassification\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
                        "model.config.pad_token_id = model.config.eos_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como vimos na postagem [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/), recebemos um aviso de que algumas camadas não foram inicializadas. Isso ocorre porque, nesse caso, como se trata de um problema de classificação e, quando instanciamos o modelo, dissemos a ele que queríamos que fosse um modelo de classificação com 5 classes, a biblioteca eliminou a última camada e a substituiu por uma de 5 neurônios na saída. Se você não entender isso, vá para a postagem que citei, que está mais bem explicada."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### LoRA"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Antes de implementar o LoRA, verificamos o número de parâmetros treináveis que o modelo tem."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 14,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total trainable parameters before: 124,443,648\n"
                              ]
                        }
                  ],
                  "source": [
                        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                        "print(f\"Total trainable parameters before: {total_params:,}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos que você tem 124 milhões de parâmetros treináveis. Agora vamos congelá-los"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 15,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total trainable parameters after: 0\n"
                              ]
                        }
                  ],
                  "source": [
                        "for param in model.parameters():\n",
                        "    param.requires_grad = False\n",
                        "\n",
                        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                        "print(f\"Total trainable parameters after: {total_params:,}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Após o congelamento, não há mais parâmetros treináveis"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos ver como é o modelo antes de aplicar o LoRA."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 16,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "GPT2ForSequenceClassification(\n",
                                          "  (transformer): GPT2Model(\n",
                                          "    (wte): Embedding(50257, 768)\n",
                                          "    (wpe): Embedding(1024, 768)\n",
                                          "    (drop): Dropout(p=0.1, inplace=False)\n",
                                          "    (h): ModuleList(\n",
                                          "      (0-11): 12 x GPT2Block(\n",
                                          "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (attn): GPT2Attention(\n",
                                          "          (c_attn): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (mlp): GPT2MLP(\n",
                                          "          (c_fc): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (act): NewGELUActivation()\n",
                                          "          (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "  )\n",
                                          "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 16,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Primeiro, criamos a camada LoRA.\n",
                        "\n",
                        "Ele precisa herdar do `torch.nn.Module` para que possa atuar como uma camada de uma rede neural.\n",
                        "\n",
                        "No método `_init_`, criamos os vetores `A` e `B` inicializados conforme explicado acima, o vetor `A` com uma distribuição gaussiana aleatória e o vetor `B` com zeros. Também criamos os parâmetros `rank` e `alpha`.\n",
                        "\n",
                        "No método \"forward\", calculamos o LoRA conforme explicado acima."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 17,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "\n",
                        "class LoRALayer(torch.nn.Module):\n",
                        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
                        "        super().__init__()\n",
                        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
                        "        torch.nn.init.kaiming_uniform_(self.A, a=torch.sqrt(torch.tensor(5.)).item())  # similar to standard weight initialization\n",
                        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
                        "        self.alpha = alpha\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        x = self.alpha * (x @ self.A @ self.B)\n",
                        "        return x"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, criamos uma classe linear com LoRA.\n",
                        "\n",
                        "Como antes, ele herda do `torch.nn.Module` para que possa atuar como uma camada de uma rede neural.\n",
                        "\n",
                        "No método `_init_`, criamos uma variável com a camada linear original da rede e criamos outra variável com a nova camada LoRA que implementamos anteriormente.\n",
                        "\n",
                        "No método `forward`, adicionamos as saídas da camada linear original e da camada LoRA."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 18,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "class LoRALinear(torch.nn.Module):\n",
                        "    def __init__(self, linear, rank, alpha):\n",
                        "        super().__init__()\n",
                        "        self.linear = linear\n",
                        "        self.lora = LoRALayer(\n",
                        "            linear.in_features, linear.out_features, rank, alpha\n",
                        "        )\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        return self.linear(x) + self.lora(x)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Por fim, criamos uma função que substitui as camadas lineares pela nova camada linear com LoRA que criamos. Se encontrar uma camada linear no modelo, ela a substituirá pela camada linear com LoRA; caso contrário, aplicará a função nas subcamadas da camada."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 19,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def replace_linear_with_lora(model, rank, alpha):\n",
                        "    for name, module in model.named_children():\n",
                        "        if isinstance(module, torch.nn.Linear):\n",
                        "            # Replace the Linear layer with LinearWithLoRA\n",
                        "            setattr(model, name, LoRALinear(module, rank, alpha))\n",
                        "        else:\n",
                        "            # Recursively apply the same function to child modules\n",
                        "            replace_linear_with_lora(module, rank, alpha)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aplicamos a função ao modelo para substituir as camadas lineares do modelo pela nova camada linear com LoRA."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 20,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "rank = 16\n",
                        "alpha = 16\n",
                        "\n",
                        "replace_linear_with_lora(model, rank=rank, alpha=alpha)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora vemos o número de parâmetros treináveis"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 21,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Total trainable LoRA parameters: 12,368\n"
                              ]
                        }
                  ],
                  "source": [
                        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Passamos de 124 milhões de parâmetros treináveis para 12 mil parâmetros treináveis, ou seja, reduzimos o número de parâmetros treináveis em 10.000 vezes!"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Analisamos o modelo novamente"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 22,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "GPT2ForSequenceClassification(\n",
                                          "  (transformer): GPT2Model(\n",
                                          "    (wte): Embedding(50257, 768)\n",
                                          "    (wpe): Embedding(1024, 768)\n",
                                          "    (drop): Dropout(p=0.1, inplace=False)\n",
                                          "    (h): ModuleList(\n",
                                          "      (0-11): 12 x GPT2Block(\n",
                                          "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (attn): GPT2Attention(\n",
                                          "          (c_attn): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (mlp): GPT2MLP(\n",
                                          "          (c_fc): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (act): NewGELUActivation()\n",
                                          "          (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "  )\n",
                                          "  (score): LoRALinear(\n",
                                          "    (linear): Linear(in_features=768, out_features=5, bias=False)\n",
                                          "    (lora): LoRALayer()\n",
                                          "  )\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 22,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos compará-los camada por camada\n",
                        "\n",
                        "|Modelo original|Modelo com LoRA|\n",
                        "|-|-|\n",
                        "|GPT2ForSequenceClassification(|GPT2ForSequenceClassification(|\n",
                        "|  (transformer): GPT2Model(|  (transformer): GPT2Model(|\n",
                        "|    (wte): Embedding(50257, 768)|    (wte): Embedding(50257, 768)|\n",
                        "|    (wpe): Embedding(1024, 768)|    (wpe): Embedding(1024, 768)|\n",
                        "|    (drop): Dropout(p=0.1, inplace=False)|    (drop): Dropout(p=0.1, inplace=False)|\n",
                        "|    (h): ModuleList(|    (h): ModuleList(|\n",
                        "|      (0-11): 12 x GPT2Block(|      (0-11): 12 x GPT2Block(|\n",
                        "|        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|\n",
                        "|        (attn): GPT2Attention(|        (attn): GPT2Attention(|\n",
                        "|          (c_attn): Conv1D()|          (c_attn): Conv1D()|\n",
                        "|          (c_proj): Conv1D()|          (c_proj): Conv1D()|\n",
                        "|          (attn_dropout): Dropout(p=0.1, inplace=False)|          (attn_dropout): Dropout(p=0.1, inplace=False)|\n",
                        "|          (resid_dropout): Dropout(p=0.1, inplace=False)|          (resid_dropout): Dropout(p=0.1, inplace=False)|\n",
                        "|        )|        )|\n",
                        "|        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|\n",
                        "|        (mlp): GPT2MLP(|        (mlp): GPT2MLP(|\n",
                        "|          (c_fc): Conv1D()|          (c_fc): Conv1D()|\n",
                        "|          (c_proj): Conv1D()|          (c_proj): Conv1D()|\n",
                        "|          (act): NewGELUActivation()|          (act): NewGELUActivation()|\n",
                        "|          (dropout): Dropout(p=0.1, inplace=False)|          (dropout): Dropout(p=0.1, inplace=False)|\n",
                        "|        )|        )|\n",
                        "|      )|      )|\n",
                        "|    )|    )|\n",
                        "|    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|\n",
                        "|  )|  )|\n",
                        "||  (score): LoRALinear()|\n",
                        "|  (score): Linear(in_features=768, out_features=5, bias=False)|    (linear): Linear(in_features=768, out_features=5, bias=False)|\n",
                        "||    (lora): LoRALayer()|\n",
                        "||  )|\n",
                        "|)|)|\n",
                        "\n",
                        "Podemos ver que eles são iguais, exceto no final, onde no modelo original havia uma camada linear normal e no modelo com LoRA há uma camada `LoRALinear` que tem a camada linear do modelo original e uma camada `LoRALayer` dentro."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Treinamento"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Depois que o modelo tiver sido instanciado com o LoRA, vamos treiná-lo como de costume."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como dissemos, na postagem [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/), usamos um tamanho de lote de 28 para o loop de treinamento e 40 para o loop de avaliação, mas agora que há menos parâmetros treináveis, podemos usar um tamanho de lote maior.\n",
                        "\n",
                        "Por que isso acontece? Quando você treina um modelo, precisa salvar o modelo e seus gradientes na memória da GPU; portanto, tanto com o LoRA quanto sem o LoRA, você precisa salvar o modelo de qualquer maneira, mas no caso do LoRA, você salva apenas os gradientes de 12 mil parâmetros, enquanto com o LoRA você salva os gradientes de 128 milhões de parâmetros; portanto, com o LoRA, você precisa de menos memória da GPU, o que permite usar um tamanho de lote maior."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 23,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import TrainingArguments\n",
                        "\n",
                        "metric_name = \"accuracy\"\n",
                        "model_name = \"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification\"\n",
                        "LR = 2e-5\n",
                        "BS_TRAIN = 400\n",
                        "BS_EVAL = 400\n",
                        "EPOCHS = 3\n",
                        "WEIGHT_DECAY = 0.01\n",
                        "\n",
                        "training_args = TrainingArguments(\n",
                        "    model_name,\n",
                        "    eval_strategy=\"epoch\",\n",
                        "    save_strategy=\"epoch\",\n",
                        "    learning_rate=LR,\n",
                        "    per_device_train_batch_size=BS_TRAIN,\n",
                        "    per_device_eval_batch_size=BS_EVAL,\n",
                        "    num_train_epochs=EPOCHS,\n",
                        "    weight_decay=WEIGHT_DECAY,\n",
                        "    lr_scheduler_type=\"cosine\",\n",
                        "    warmup_ratio = 0.1,\n",
                        "    fp16=True,\n",
                        "    load_best_model_at_end=True,\n",
                        "    metric_for_best_model=metric_name,\n",
                        "    push_to_hub=True,\n",
                        "    logging_dir=\"./runs\",\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 24,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import numpy as np\n",
                        "from evaluate import load\n",
                        "\n",
                        "metric = load(\"accuracy\")\n",
                        "\n",
                        "def compute_metrics(eval_pred):\n",
                        "    print(eval_pred)\n",
                        "    predictions, labels = eval_pred\n",
                        "    predictions = np.argmax(predictions, axis=1)\n",
                        "    return metric.compute(predictions=predictions, references=labels)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 25,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import Trainer\n",
                        "\n",
                        "trainer = Trainer(\n",
                        "    model,\n",
                        "    training_args,\n",
                        "    train_dataset=subset_dataset_train,\n",
                        "    eval_dataset=subset_dataset_validation,\n",
                        "    tokenizer=tokenizer,\n",
                        "    compute_metrics=compute_metrics,\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 27,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/",
                              "height": 256
                        },
                        "id": "AZL4wIFKlC6x",
                        "outputId": "9f6a314c-65f0-46a5-fadf-61f832c1ca73"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [1500/1500 42:41, Epoch 3/3]\n",
                                          "    </div>\n",
                                          "    <table border=\"1\" class=\"dataframe\">\n",
                                          "  <thead>\n",
                                          " <tr style=\"text-align: left;\">\n",
                                          "      <th>Epoch</th>\n",
                                          "      <th>Training Loss</th>\n",
                                          "      <th>Validation Loss</th>\n",
                                          "      <th>Accuracy</th>\n",
                                          "    </tr>\n",
                                          "  </thead>\n",
                                          "  <tbody>\n",
                                          "    <tr>\n",
                                          "      <td>1</td>\n",
                                          "      <td>2.396400</td>\n",
                                          "      <td>1.602937</td>\n",
                                          "      <td>0.269400</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>2</td>\n",
                                          "      <td>1.572700</td>\n",
                                          "      <td>1.531719</td>\n",
                                          "      <td>0.320800</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>3</td>\n",
                                          "      <td>1.534400</td>\n",
                                          "      <td>1.511815</td>\n",
                                          "      <td>0.335800</td>\n",
                                          "    </tr>\n",
                                          "  </tbody>\n",
                                          "</table><p>"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be46440>\n",
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be45c30>\n",
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be8b970>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "TrainOutput(global_step=1500, training_loss=1.8345018310546874, metrics={'train_runtime': 2565.4667, 'train_samples_per_second': 233.876, 'train_steps_per_second': 0.585, 'total_flos': 2.352076406784e+17, 'train_loss': 1.8345018310546874, 'epoch': 3.0})"
                                    ]
                              },
                              "execution_count": 27,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "trainer.train()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Avaliação"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Depois de treinados, avaliamos o conjunto de dados de teste"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 28,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/",
                              "height": 159
                        },
                        "id": "Xcxmbo-3Uz1G",
                        "outputId": "a6ce9a05-22a1-40ce-a32a-b149ed8e6dd4"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [13/13 00:17]\n",
                                          "    </div>\n",
                                          "    "
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be8bbe0>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'eval_loss': 1.5203168392181396,\n",
                                          " 'eval_accuracy': 0.3374,\n",
                                          " 'eval_runtime': 19.3843,\n",
                                          " 'eval_samples_per_second': 257.94,\n",
                                          " 'eval_steps_per_second': 0.671,\n",
                                          " 'epoch': 3.0}"
                                    ]
                              },
                              "execution_count": 28,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "trainer.evaluate(eval_dataset=subset_dataset_test)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Publicar o modelo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora que temos nosso modelo treinado, podemos compartilhá-lo com o mundo, portanto, primeiro criamos um cartão de modelo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "trainer.create_model_card()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "E agora podemos publicá-lo. Como a primeira coisa que fizemos foi fazer login no hub da huggingface, poderemos fazer o upload para o nosso hub sem nenhum problema."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "trainer.push_to_hub()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Teste de modelo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Limpamos o máximo possível"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "import gc\n",
                        "\n",
                        "\n",
                        "def clear_hardwares():\n",
                        "    torch.clear_autocast_cache()\n",
                        "    torch.cuda.ipc_collect()\n",
                        "    torch.cuda.empty_cache()\n",
                        "    gc.collect()\n",
                        "\n",
                        "\n",
                        "clear_hardwares()\n",
                        "clear_hardwares()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como fizemos o upload do modelo em nosso hub, podemos baixá-lo e usá-lo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import pipeline\n",
                        "\n",
                        "user = \"maximofn\"\n",
                        "checkpoints = f\"{user}/{model_name}\"\n",
                        "task = \"text-classification\"\n",
                        "classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, se quisermos retornar a probabilidade de todas as classes, basta usar o classificador que acabamos de instanciar, com o parâmetro `top_k=None`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 33,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "pqy8oMPuUz1I",
                        "outputId": "78501f30-2983-481a-e908-d5139bbce0e7"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_0', 'score': 0.8419149518013},\n",
                                          " {'label': 'LABEL_1', 'score': 0.09386005252599716},\n",
                                          " {'label': 'LABEL_3', 'score': 0.03624210134148598},\n",
                                          " {'label': 'LABEL_2', 'score': 0.02049318142235279},\n",
                                          " {'label': 'LABEL_4', 'score': 0.0074898069724440575}]"
                                    ]
                              },
                              "execution_count": 33,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "labels = classifier(\"I love this product\", top_k=None)\n",
                        "labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se quisermos apenas a classe com a maior probabilidade, faremos o mesmo, mas com o parâmetro `top_k=1`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 34,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "qpcw3pN0Uz1J",
                        "outputId": "2667da0e-743c-4300-ac46-b8ff4d43270d"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_0', 'score': 0.8419149518013}]"
                                    ]
                              },
                              "execution_count": 34,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "label = classifier(\"I love this product\", top_k=1)\n",
                        "label"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "E se quisermos n classes, faremos o mesmo, mas com o parâmetro `top_k=n`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 35,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "pRQvB3tTUz1J",
                        "outputId": "0eb134fa-4633-4394-d795-b1563cad1d37"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_0', 'score': 0.8419149518013},\n",
                                          " {'label': 'LABEL_1', 'score': 0.09386005252599716}]"
                                    ]
                              },
                              "execution_count": 35,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "two_labels = classifier(\"I love this product\", top_k=2)\n",
                        "two_labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Também podemos testar o modelo com o Automodel e o AutoTokenizer."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "import torch\n",
                        "\n",
                        "model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
                        "user = \"maximofn\"\n",
                        "checkpoint = f\"{user}/{model_name}\"\n",
                        "num_classes = num_classes\n",
                        "\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes).half().eval().to(\"cuda\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 37,
                  "metadata": {
                        "colab": {
                              "base_uri": "https://localhost:8080/"
                        },
                        "id": "9goQMfcLUz1K",
                        "outputId": "7d6dc70b-88a4-4f06-83f4-81457994de3b"
                  },
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[0.003940582275390625,\n",
                                          " 0.00266265869140625,\n",
                                          " 0.013946533203125,\n",
                                          " 0.1544189453125,\n",
                                          " 0.8251953125]"
                                    ]
                              },
                              "execution_count": 37,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "tokens = tokenizer.encode(\"I love this product\", return_tensors=\"pt\").to(model.device)\n",
                        "with torch.no_grad():\n",
                        "    output = model(tokens)\n",
                        "logits = output.logits\n",
                        "lables = torch.softmax(logits, dim=1).cpu().numpy().tolist()\n",
                        "lables[0]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se você quiser testar o modelo com mais detalhes, poderá vê-lo em [Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Implementação de LoRA em um LLM com PEFT da Hugging Face"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Podemos fazer o mesmo com a biblioteca `PEFT` da Hugging Face. Vamos dar uma olhada nela"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Faça login no hub"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Fazemos login para carregar o modelo no Hub"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from huggingface_hub import notebook_login\n",
                        "notebook_login()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Conjunto de dados"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Baixamos novamente o conjunto de dados"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "DatasetDict({\n",
                                          "    train: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 200000\n",
                                          "    })\n",
                                          "    validation: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "    test: Dataset({\n",
                                          "        features: ['id', 'text', 'label', 'label_text'],\n",
                                          "        num_rows: 5000\n",
                                          "    })\n",
                                          "})"
                                    ]
                              },
                              "execution_count": 1,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "from datasets import load_dataset\n",
                        "\n",
                        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
                        "dataset"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos um subconjunto para o caso de você querer testar o código com um conjunto de dados menor. No meu caso, usarei 100% do conjunto de dados."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 2,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "percentage = 1\n",
                        "\n",
                        "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
                        "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
                        "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para obter o número de classes, usamos `dataset['train']` e não `subset_dataset_train` porque, se o subconjunto for muito pequeno, talvez não haja exemplos com todas as classes possíveis do conjunto de dados original."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "5"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "num_classes = len(dataset['train'].unique('label'))\n",
                        "num_classes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos uma função para criar o campo `label` no conjunto de dados. O conjunto de dados baixado tem o campo `labels`, mas a biblioteca `transformers` precisa que o campo seja chamado `label` e não `labels`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def set_labels(example):\n",
                        "    example['labels'] = example['label']\n",
                        "    return example"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aplicamos a função ao conjunto de dados"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
                        "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
                        "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Tokeniser"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Instanciamos o tokenizador. Para evitar erros, atribuímos o token de fim de cadeia ao token de preenchimento."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer\n",
                        "\n",
                        "checkpoint = \"openai-community/gpt2\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
                        "tokenizer.pad_token = tokenizer.eos_token"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos uma função para tokenizar o conjunto de dados"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 7,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def tokenize_function(examples):\n",
                        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Aplicamos a função ao conjunto de dados e removemos as colunas de que não precisamos."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "(Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 200000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }),\n",
                                          " Dataset({\n",
                                          "     features: ['labels', 'input_ids', 'attention_mask'],\n",
                                          "     num_rows: 5000\n",
                                          " }))"
                                    ]
                              },
                              "execution_count": 8,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
                        "\n",
                        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Modelo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Instanciamos o modelo. Além disso, para evitar erros, atribuímos o token do final da string ao token de preenchimento."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import AutoModelForSequenceClassification\n",
                        "\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
                        "model.config.pad_token_id = model.config.eos_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### LoRA com PEFT"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Antes de criar o modelo com o LoRA, vamos dar uma olhada em suas camadas"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "GPT2ForSequenceClassification(\n",
                                          "  (transformer): GPT2Model(\n",
                                          "    (wte): Embedding(50257, 768)\n",
                                          "    (wpe): Embedding(1024, 768)\n",
                                          "    (drop): Dropout(p=0.1, inplace=False)\n",
                                          "    (h): ModuleList(\n",
                                          "      (0-11): 12 x GPT2Block(\n",
                                          "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (attn): GPT2Attention(\n",
                                          "          (c_attn): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "        (mlp): GPT2MLP(\n",
                                          "          (c_fc): Conv1D()\n",
                                          "          (c_proj): Conv1D()\n",
                                          "          (act): NewGELUActivation()\n",
                                          "          (dropout): Dropout(p=0.1, inplace=False)\n",
                                          "        )\n",
                                          "      )\n",
                                          "    )\n",
                                          "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
                                          "  )\n",
                                          "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
                                          ")"
                                    ]
                              },
                              "execution_count": 10,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "model"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como podemos ver, há apenas uma camada `Linear`, que é `score`, e é essa que vamos substituir."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Podemos criar uma configuração do LoRA com a biblioteca PEFT e, em seguida, aplicar o LoRA ao mapa."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 11,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from peft import LoraConfig, TaskType\n",
                        "\n",
                        "peft_config = LoraConfig(\n",
                        "    r=16,\n",
                        "    lora_alpha=32,\n",
                        "    lora_dropout=0.1,\n",
                        "    task_type=TaskType.SEQ_CLS,\n",
                        "    target_modules=[\"score\"],\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Com essa configuração, definimos uma classificação de 16 e um alfa de 32. Além disso, adicionamos um dropout de 0,1 às camadas lora. Temos de indicar a tarefa para a configuração do LoRA; nesse caso, é uma tarefa de classificação de sequência. Por fim, indicamos quais camadas queremos substituir, neste caso, a camada `score`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, aplicamos o LoRA ao modelo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from peft import get_peft_model\n",
                        "\n",
                        "model = get_peft_model(model, peft_config)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos ver quantos parâmetros treináveis o modelo tem agora."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 13,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "trainable params: 12,368 || all params: 124,456,016 || trainable%: 0.0099\n"
                              ]
                        }
                  ],
                  "source": [
                        "model.print_trainable_parameters()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Obtemos os mesmos parâmetros treináveis de antes"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Treinamento"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Depois que o modelo tiver sido instanciado com o LoRA, vamos treiná-lo como de costume."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 17,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import TrainingArguments\n",
                        "\n",
                        "metric_name = \"accuracy\"\n",
                        "model_name = \"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification\"\n",
                        "LR = 2e-5\n",
                        "BS_TRAIN = 400\n",
                        "BS_EVAL = 400\n",
                        "EPOCHS = 3\n",
                        "WEIGHT_DECAY = 0.01\n",
                        "\n",
                        "training_args = TrainingArguments(\n",
                        "    model_name,\n",
                        "    eval_strategy=\"epoch\",\n",
                        "    save_strategy=\"epoch\",\n",
                        "    learning_rate=LR,\n",
                        "    per_device_train_batch_size=BS_TRAIN,\n",
                        "    per_device_eval_batch_size=BS_EVAL,\n",
                        "    num_train_epochs=EPOCHS,\n",
                        "    weight_decay=WEIGHT_DECAY,\n",
                        "    lr_scheduler_type=\"cosine\",\n",
                        "    warmup_ratio = 0.1,\n",
                        "    fp16=True,\n",
                        "    load_best_model_at_end=True,\n",
                        "    metric_for_best_model=metric_name,\n",
                        "    push_to_hub=True,\n",
                        "    logging_dir=\"./runs\",\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 18,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import numpy as np\n",
                        "from evaluate import load\n",
                        "\n",
                        "metric = load(\"accuracy\")\n",
                        "\n",
                        "def compute_metrics(eval_pred):\n",
                        "    print(eval_pred)\n",
                        "    predictions, labels = eval_pred\n",
                        "    predictions = np.argmax(predictions, axis=1)\n",
                        "    return metric.compute(predictions=predictions, references=labels)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 20,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import Trainer\n",
                        "\n",
                        "trainer = Trainer(\n",
                        "    model,\n",
                        "    training_args,\n",
                        "    train_dataset=subset_dataset_train,\n",
                        "    eval_dataset=subset_dataset_validation,\n",
                        "    tokenizer=tokenizer,\n",
                        "    compute_metrics=compute_metrics,\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='811' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [ 811/1500 22:43 < 19:20, 0.59 it/s, Epoch 1.62/3]\n",
                                          "    </div>\n",
                                          "    <table border=\"1\" class=\"dataframe\">\n",
                                          "  <thead>\n",
                                          " <tr style=\"text-align: left;\">\n",
                                          "      <th>Epoch</th>\n",
                                          "      <th>Training Loss</th>\n",
                                          "      <th>Validation Loss</th>\n",
                                          "      <th>Accuracy</th>\n",
                                          "    </tr>\n",
                                          "  </thead>\n",
                                          "  <tbody>\n",
                                          "    <tr>\n",
                                          "      <td>1</td>\n",
                                          "      <td>2.275100</td>\n",
                                          "      <td>1.512476</td>\n",
                                          "      <td>0.318200</td>\n",
                                          "    </tr>\n",
                                          "  </tbody>\n",
                                          "</table><p>"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f774a50bbe0>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [1500/1500 42:28, Epoch 3/3]\n",
                                          "    </div>\n",
                                          "    <table border=\"1\" class=\"dataframe\">\n",
                                          "  <thead>\n",
                                          " <tr style=\"text-align: left;\">\n",
                                          "      <th>Epoch</th>\n",
                                          "      <th>Training Loss</th>\n",
                                          "      <th>Validation Loss</th>\n",
                                          "      <th>Accuracy</th>\n",
                                          "    </tr>\n",
                                          "  </thead>\n",
                                          "  <tbody>\n",
                                          "    <tr>\n",
                                          "      <td>1</td>\n",
                                          "      <td>2.275100</td>\n",
                                          "      <td>1.512476</td>\n",
                                          "      <td>0.318200</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>2</td>\n",
                                          "      <td>1.515900</td>\n",
                                          "      <td>1.417553</td>\n",
                                          "      <td>0.373800</td>\n",
                                          "    </tr>\n",
                                          "    <tr>\n",
                                          "      <td>3</td>\n",
                                          "      <td>1.463500</td>\n",
                                          "      <td>1.405058</td>\n",
                                          "      <td>0.381400</td>\n",
                                          "    </tr>\n",
                                          "  </tbody>\n",
                                          "</table><p>"
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f77486a7c40>\n",
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f7749eb5690>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "TrainOutput(global_step=1500, training_loss=1.751504597981771, metrics={'train_runtime': 2551.7753, 'train_samples_per_second': 235.13, 'train_steps_per_second': 0.588, 'total_flos': 2.352524525568e+17, 'train_loss': 1.751504597981771, 'epoch': 3.0})"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "trainer.train()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Avaliação"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Depois de treinados, avaliamos o conjunto de dados de teste"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/html": [
                                          "\n",
                                          "    <div>\n",
                                          "      \n",
                                          "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                                          "      [13/13 00:17]\n",
                                          "    </div>\n",
                                          "    "
                                    ],
                                    "text/plain": [
                                          "<IPython.core.display.HTML object>"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "<transformers.trainer_utils.EvalPrediction object at 0x7f77a1d1f7c0>\n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "{'eval_loss': 1.4127237796783447,\n",
                                          " 'eval_accuracy': 0.3862,\n",
                                          " 'eval_runtime': 19.3275,\n",
                                          " 'eval_samples_per_second': 258.699,\n",
                                          " 'eval_steps_per_second': 0.673,\n",
                                          " 'epoch': 3.0}"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "trainer.evaluate(eval_dataset=subset_dataset_test)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Publicar o modelo"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Criamos um cartão modelo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "trainer.create_model_card()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Nós o publicamos"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "application/vnd.google.colaboratory.intrinsic+json": {
                                          "type": "string"
                                    },
                                    "text/plain": [
                                          "CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification/commit/839066c2bde02689a6b3f5624ac25f89c4de217d', commit_message='End of training', commit_description='', oid='839066c2bde02689a6b3f5624ac25f89c4de217d', pr_url=None, pr_revision=None, pr_num=None)"
                                    ]
                              },
                              "metadata": {},
                              "output_type": "display_data"
                        }
                  ],
                  "source": [
                        "trainer.push_to_hub()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Teste do modelo treinado com PEFT"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Limpamos o máximo possível"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 100,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torch\n",
                        "import gc\n",
                        "\n",
                        "\n",
                        "def clear_hardwares():\n",
                        "    torch.clear_autocast_cache()\n",
                        "    torch.cuda.ipc_collect()\n",
                        "    torch.cuda.empty_cache()\n",
                        "    gc.collect()\n",
                        "\n",
                        "\n",
                        "clear_hardwares()\n",
                        "clear_hardwares()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como fizemos o upload do modelo em nosso hub, podemos baixá-lo e usá-lo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
                                    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                              ]
                        }
                  ],
                  "source": [
                        "from transformers import pipeline\n",
                        "\n",
                        "user = \"maximofn\"\n",
                        "checkpoints = f\"{user}/{model_name}\"\n",
                        "task = \"text-classification\"\n",
                        "classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, se quisermos retornar a probabilidade de todas as classes, basta usar o classificador que acabamos de instanciar, com o parâmetro `top_k=None`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 3,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_1', 'score': 0.9979197382926941},\n",
                                          " {'label': 'LABEL_0', 'score': 0.002080311067402363}]"
                                    ]
                              },
                              "execution_count": 3,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "labels = classifier(\"I love this product\", top_k=None)\n",
                        "labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se quisermos apenas a classe com a maior probabilidade, faremos o mesmo, mas com o parâmetro `top_k=1`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 4,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_1', 'score': 0.9979197382926941}]"
                                    ]
                              },
                              "execution_count": 4,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "label = classifier(\"I love this product\", top_k=1)\n",
                        "label"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "E se quisermos n classes, faremos o mesmo, mas com o parâmetro `top_k=n`."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 5,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[{'label': 'LABEL_1', 'score': 0.9979197382926941},\n",
                                          " {'label': 'LABEL_0', 'score': 0.002080311067402363}]"
                                    ]
                              },
                              "execution_count": 5,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "two_labels = classifier(\"I love this product\", top_k=2)\n",
                        "two_labels"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Se você quiser testar o modelo com mais detalhes, poderá vê-lo em [Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification)"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "base",
                  "language": "python",
                  "name": "python3"
            },
            "maximofn": {
      "date": "2024-07-20",
      "description_es": "¡Prepárate para llevar la adaptación de tus modelos al siguiente nivel con LoRA! 🚀 Esta técnica de adaptación de baja rango es como una capa de superhéroe para tus redes neuronales - les ayuda a aprender nuevos trucos sin olvidar los antiguos 🤯. Y lo mejor de todo? Puedes implementarla en solo unas pocas líneas de código PyTorch 💻. ¡Y si eres como yo, un pobre de GPU que lucha con recursos limitados 💸, LoRA es como un regalo del cielo: te permite adaptar tus modelos sin necesidad de entrenarlos desde cero ni gastar una fortuna en hardware 🙏. ¡Revisa el post para obtener una guía paso a paso y un ejemplo práctico!",
      "description_en": "Get ready to take your model adaptation to the next level with LoRA! 🚀 This low-rank adaptation technique is like a superhero cape for your neural networks - it helps them learn new tricks without forgetting old ones 🤯. And the best thing about it? You can implement it in just a few lines of PyTorch code 💻 And if you're like me, a poor GPU guy struggling with limited resources 💸, LoRA is like a godsend: it lets you adapt your models without training them from scratch or spending a fortune on hardware 🙏 Check out the post for a step-by-step guide and a practical example!",
      "description_pt": "Prepare-se para levar a adaptação de modelos para o próximo nível com o LoRA! 🚀 Essa técnica de adaptação de baixa classificação é como uma capa de super-herói para suas redes neurais - ela as ajuda a aprender novos truques sem esquecer os antigos 🤯. E o melhor de tudo? Você pode implementá-la em apenas algumas linhas de código PyTorch 💻 E se você for como eu, um pobre cara da GPU lutando com recursos limitados 💸, o LoRA é como uma dádiva de Deus: ele permite que você adapte seus modelos sem treiná-los do zero ou gastar uma fortuna em hardware 🙏 Confira a postagem para obter um guia passo a passo e um exemplo prático!",
      "end_url": "lora",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_thumbnail_PT.webp",
      "keywords_en": "lora, low rank adaptation, neural networks, pytorch, gpu, hardware",
      "keywords_es": "lora, adaptación de baja clasificación, redes neuronales, pytorch, gpu, hardware",
      "keywords_pt": "lora, adaptação de baixa classificação, redes neurais, pytorch, gpu, hardware",
      "title_en": "LoRA – low rank adaptation of large language models",
      "title_es": "LoRA – low rank adaptation of large language models",
      "title_pt": "LoRA – low rank adaptation of large language models"
    },
    "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.9"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
