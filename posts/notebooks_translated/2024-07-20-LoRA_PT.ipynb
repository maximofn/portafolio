{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA - adapta\u00e7\u00e3o de baixa classifica\u00e7\u00e3o de grandes modelos lingu\u00edsticos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O aumento do tamanho dos modelos de linguagem faz com que sejam cada vez mais caros de treinar, pois \u00e9 necess\u00e1rio cada vez mais VRAM para armazenar todos os seus par\u00e2metros e os gradientes derivados do treinamento.",
        "\n",
        "No artigo [LoRA - Low rank adaption of large language models](https://arxiv.org/abs/2106.09685) prop\u00f5em congelar os pesos do modelo e treinar duas matrizes chamadas A e B, reduzindo significativamente o n\u00famero de par\u00e2metros que precisam ser treinados.",
        "\n",
        "![LoRA](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp)",
        "\n",
        "Vamos a ver como se faz isso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explica\u00e7\u00e3o de LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Atualiza\u00e7\u00e3o de pesos em uma rede neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entender como funciona LoRA, primeiro temos que lembrar o que ocorre quando treinamos um modelo. Voltemos \u00e0 parte mais b\u00e1sica do deep learning, temos uma camada densa de uma rede neural que \u00e9 definida como:",
        "\n",
        "$$",
        "y = Wx + b",
        "$$",
        "\n",
        "Onde $W$ \u00e9 a matriz de pesos e $b$ \u00e9 o vetor de vieses.",
        "\n",
        "Para simplificar, vamos a supor que n\u00e3o h\u00e1 vi\u00e9s, portanto ficaria assim",
        "\n",
        "$$",
        "y = Wx",
        "$$",
        "\n",
        "Suponhamos que para uma entrada $x$ queremos que tenha uma sa\u00edda $\u0177$",
        "\n",
        "* Primeiro, o que fazemos \u00e9 calcular a sa\u00edda que obtemos com nosso valor atual de pesos $W$, ou seja, obtemos o valor $y$",
        "* Em seguida, calculamos o erro que existe entre o valor de $y$ que obtivemos e o valor que quer\u00edamos obter $\u0177$. A esse erro chamamos de $loss$, e o calculamos com alguma fun\u00e7\u00e3o matem\u00e1tica, agora n\u00e3o importa qual.",
        "* Calculamos o gradiente (a derivada) do erro $loss$ em rela\u00e7\u00e3o \u00e0 matriz de pesos $W$, ou seja, $\\Delta W = \\frac{dloss}{dW}$",
        "* Atualizamos os pesos $W$ subtraindo de cada um dos seus valores o valor do gradiente multiplicado por um fator de aprendizado $\\alpha$, ou seja, $W = W - \\alpha \\Delta W$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os autores de LoRA prop\u00f5em que a matriz de pesos $W$ pode ser decomposta em",
        "\n",
        "$$",
        "W \\sim W + \\Delta W",
        "$$",
        "\n",
        "Assim, ao congelar a matriz $W$ e treinar apenas a matriz $\\Delta W$, pode-se obter um modelo que se adeque a novos dados sem ter que retreinar todo o modelo.",
        "\n",
        "Mas voc\u00ea pode pensar que $\\Delta W$ \u00e9 uma matriz do mesmo tamanho de $W$, portanto nada foi ganho, mas aqui os autores se baseiam em `Aghajanyan et al. (2020)`, um artigo no qual eles demonstraram que, embora os modelos de linguagem sejam grandes e seus par\u00e2metros sejam matrizes com dimens\u00f5es muito grandes, para adapt\u00e1-los a novas tarefas n\u00e3o \u00e9 necess\u00e1rio alterar todos os valores das matrizes, mas sim alterar alguns poucos valores, o que tecnicamente \u00e9 chamado de adapta\u00e7\u00e3o de baixa classifica\u00e7\u00e3o. Da\u00ed o nome LoRA (Low Rank Adaptation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congelamos o modelo e agora queremos treinar a matriz $\\Delta W$. Suponhamos que tanto $W$ quanto $\\Delta W$ s\u00e3o matrizes de tamanho $20 \\times 10$, portanto temos 200 par\u00e2metros trein\u00e1veis.",
        "\n",
        "Agora suponhamos que a matriz $\\Delta W$ pode ser decomposta no produto de duas matrizes $A$ e $B$, ou seja",
        "\n",
        "$$",
        "\\Delta W = A \\cdot B",
        "$$",
        "\n",
        "Para que esta multiplica\u00e7\u00e3o ocorra, os tamanhos das matrizes $A$ e $B$ t\u00eam que ser $20 \\times n$ e $n \\times 10$, respectivamente. Suponhamos que $n = 5$, portanto $A$ seria de tamanho $20 \\times 5$, ou seja, 100 par\u00e2metros, e $B$ de tamanho $5 \\times 10$, ou seja, 50 par\u00e2metros, por isso ter\u00edamos 100+50=150 par\u00e2metros trein\u00e1veis. J\u00e1 temos menos par\u00e2metros trein\u00e1veis do que antes.",
        "\n",
        "Agora suponhamos que $W$ na verdade \u00e9 uma matriz de tamanho $10.000 \\times 10.000$, portanto ter\u00edamos 100.000.000 par\u00e2metros trein\u00e1veis, mas se decompor $\\Delta W$ em $A$ e $B$ com $n = 5$, ter\u00edamos uma matriz de tamanho $10.000 \\times 5$ e outra de tamanho $5 \\times 10.000$, portanto ter\u00edamos 50.000 par\u00e2metros de uma e outros 50.000 par\u00e2metros de outra, no total 100.000 par\u00e2metros trein\u00e1veis, ou seja, reduzimos o n\u00famero de par\u00e2metros 1000 vezes",
        "\n",
        "J\u00e1 pode ver o poder da LoRA, quando se t\u00eam modelos muito grandes, o n\u00famero de par\u00e2metros trein\u00e1veis pode ser reduzido drasticamente.",
        "\n",
        "Se voltarmos a ver a imagem da arquitetura de LoRA, a entenderemos melhor.",
        "\n",
        "![LoRA adapt](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp)",
        "\n",
        "Mas parece ainda melhor, a economia no n\u00famero de par\u00e2metros trein\u00e1veis com esta imagem",
        "\n",
        "![LoRA matmul](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Lora_matmul.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementa\u00e7\u00e3o de LoRA em transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como os modelos de linguagem s\u00e3o implementa\u00e7\u00f5es de transformers, vamos ver como se implementa LoRA em transformers. Na arquitetura transformer h\u00e1 camadas lineares nas matrizes de aten\u00e7\u00e3o $Q$, $K$ e $V$, e nas camadas feedforward, por isso pode-se aplicar LoRA a todas essas camadas lineares. No paper, eles falam que, por simplicidade, o aplicam apenas \u00e0s camadas lineares das matrizes de aten\u00e7\u00e3o $Q$, $K$ e $V$.",
        "\n",
        "Estas camadas t\u00eam um tamanho $d_{model} \\times d_{model}$, onde $d_{model}$ \u00e9 a dimens\u00e3o de embedding do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tamanho do intervalo r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder ter esses benef\u00edcios, o tamanho do intervalo $r$ deve ser menor que o tamanho das camadas lineares. Como dissemos que s\u00f3 o implementavam nas camadas lineares de aten\u00e7\u00e3o, que t\u00eam um tamanho $d_{model} \\times d_{model}$, o tamanho do intervalo $r$ deve ser menor que $d_{model}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializa\u00e7\u00e3o das matrizes A e B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As matrizes $A$ e $B$ s\u00e3o inicializadas com uma distribui\u00e7\u00e3o gaussiana aleat\u00f3ria para $A$ e zero para $B$, assim o produto de ambas as matrizes ser\u00e1 zero no in\u00edcio, ou seja",
        "\n",
        "$$",
        "\\Delta W = A \\cdot B = 0",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Influ\u00eancia de LoRA por meio do par\u00e2metro $\\alpha$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, na implementa\u00e7\u00e3o de LoRA, adiciona-se um par\u00e2metro $\\alpha$ para estabelecer o grau de influ\u00eancia de LoRA no treinamento. \u00c9 similar \u00e0 taxa de aprendizado (learning rate) no fine tuning normal, mas neste caso \u00e9 usado para estabelecer a influ\u00eancia de LoRA no treinamento. Dessa forma, a f\u00f3rmula de LoRA ficaria assim",
        "\n",
        "$$",
        "W = W + \\alpha \\Delta W = W + \\alpha A \\cdot B",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vantagens do LoRA",
        "\n",
        "Agora que entendemos como isso funciona, vamos ver as vantagens desse m\u00e9todo.",
        "\n",
        "* Redu\u00e7\u00e3o do n\u00famero de par\u00e2metros trein\u00e1veis. Como vimos, o n\u00famero de par\u00e2metros trein\u00e1veis \u00e9 reduzido drasticamente, o que torna o treinamento muito mais r\u00e1pido e requer menos VRAM, economizando muitos custos.",
        "* Adaptadores em produ\u00e7\u00e3o. Podemos ter em produ\u00e7\u00e3o um \u00fanico modelo de linguagem e v\u00e1rios adaptadores, cada um para uma tarefa diferente, em vez de ter v\u00e1rios modelos treinados para cada tarefa, o que economiza custos de armazenamento e computa\u00e7\u00e3o. Al\u00e9m disso, este m\u00e9todo n\u00e3o precisa adicionar lat\u00eancia na infer\u00eancia porque a matriz de pesos original pode ser fusionada com o adaptador, j\u00e1 que vimos que $W \\sim W + \\Delta W = W + A \\cdot B$, portanto, o tempo de infer\u00eancia seria o mesmo que usar o modelo de linguagem original.",
        "* Compartilhar adaptadores. Se treinarmos um adaptador, podemos compartilhar apenas o adaptador. Isso significa que, em produ\u00e7\u00e3o, todos podem ter o modelo original e cada vez que treinamos um adaptador, compartilhamos apenas o adaptador, portanto, como seriam compartilhadas matrizes muito menores, o tamanho dos arquivos compartilhados seria muito menor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementa\u00e7\u00e3o de LoRA em um LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a repetir o c\u00f3digo de treinamento do post [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/), em espec\u00edfico o treinamento para classifica\u00e7\u00e3o de texto com as bibliotecas da Hugging Face, mas desta vez vamos fazer isso com LoRA. No post anterior, usamos um batch size de 28 para o loop de treinamento e de 40 para o de avalia\u00e7\u00e3o, no entanto, como agora n\u00e3o vamos treinar todos os pesos do modelo, mas apenas as matrizes de LoRA, poderemos usar um batch size maior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Login no Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N\u00f3s nos logamos para fazer o upload do modelo para o Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baixamos o conjunto de dados que vamos utilizar, que \u00e9 um conjunto de dados de avalia\u00e7\u00f5es do [Amazon](https://huggingface.co/datasets/mteb/amazon_reviews_multi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'text', 'label', 'label_text'],\n",
              "        num_rows: 200000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'text', 'label', 'label_text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'text', 'label', 'label_text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos um subconjunto se voc\u00ea quiser testar o c\u00f3digo com um conjunto de dados menor. No meu caso, usarei 100% do conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text'],\n",
              "     num_rows: 200000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text'],\n",
              "     num_rows: 5000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text'],\n",
              "     num_rows: 5000\n",
              " }))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "percentage = 1\n",
        "\n",
        "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
        "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
        "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
        "\n",
        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos uma amostra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'en_0388304',\n",
              " 'text': 'The N was missing from on\\n\\nThe N was missing from on',\n",
              " 'label': 0,\n",
              " 'label_text': '0'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from random import randint\n",
        "\n",
        "idx = randint(0, len(subset_dataset_train))\n",
        "subset_dataset_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos o n\u00famero de classes, para obter o n\u00famero de classes usamos `dataset['train']` e n\u00e3o `subset_dataset_train` porque se o subconjunto for muito pequeno \u00e9 poss\u00edvel que n\u00e3o haja exemplos com todas as poss\u00edveis classes do conjunto de dados original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(dataset['train'].unique('label'))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma fun\u00e7\u00e3o para criar o campo `label` no dataset. O dataset baixado tem o campo `labels`, mas a biblioteca `transformers` precisa que o campo seja chamado de `label` e n\u00e3o `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_labels(example):\n",
        "    example['labels'] = example['label']\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicamos a fun\u00e7\u00e3o ao conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
              "     num_rows: 200000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
              "     num_rows: 5000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
              "     num_rows: 5000\n",
              " }))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
        "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
        "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
        "\n",
        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voltamos a ver um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'en_0388304',\n",
              " 'text': 'The N was missing from on\\n\\nThe N was missing from on',\n",
              " 'label': 0,\n",
              " 'label_text': '0',\n",
              " 'labels': 0}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementamos o tokenizador. Para que n\u00e3o d\u00ea erro, atribu\u00edmos o token de end of string ao token de padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma fun\u00e7\u00e3o para tokenizar o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicamos a fun\u00e7\u00e3o ao conjunto de dados e, de passagem, eliminamos as colunas que n\u00e3o precisamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['labels', 'input_ids', 'attention_mask'],\n",
              "     num_rows: 200000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['labels', 'input_ids', 'attention_mask'],\n",
              "     num_rows: 5000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['labels', 'input_ids', 'attention_mask'],\n",
              "     num_rows: 5000\n",
              " }))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
        "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
        "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
        "\n",
        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voltamos a ver uma amostra, mas neste caso s\u00f3 vemos as `keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['labels', 'input_ids', 'attention_mask'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset_train[idx].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos o modelo. Tamb\u00e9m, para que n\u00e3o nos d\u00ea erro, atribu\u00edmos o token de end of string ao token de padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como j\u00e1 vimos no post [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/) obtemos um warning que diz que algumas camadas n\u00e3o foram inicializadas. Isso acontece porque, neste caso, como \u00e9 um problema de classifica\u00e7\u00e3o e quando instanciamos o modelo dissemos que queremos que seja um modelo de classifica\u00e7\u00e3o com 5 classes, a biblioteca removeu a \u00faltima camada e a substituiu por uma com 5 neur\u00f4nios na sa\u00edda. Se n\u00e3o entende bem isso, veja o post que cito, que est\u00e1 melhor explicado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de implementar LoRA, vemos o n\u00famero de par\u00e2metros trein\u00e1veis que tem o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters before: 124,443,648\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que tem 124M de par\u00e2metros trein\u00e1veis. Agora vamos congel\u00e1-los."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters after: 0\n"
          ]
        }
      ],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ap\u00f3s o congelamento, n\u00e3o h\u00e1 mais par\u00e2metros trein\u00e1veis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver como \u00e9 o modelo antes de aplicar LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro criamos a camada LoRA layer.",
        "\n",
        "Tem que herdar de `torch.nn.Module` para poder atuar como uma camada de uma rede neural",
        "\n",
        "No m\u00e9todo `_init_` criamos as matrizes `A` e `B` inicializadas como explicado anteriormente, a matriz `A` com uma distribui\u00e7\u00e3o gaussiana aleat\u00f3ria e a matriz `B` com zeros. Tamb\u00e9m criamos os par\u00e2metros `rank` e `alpha`.",
        "\n",
        "No m\u00e9todo `forward` calculamos LoRA conforme explicado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "        torch.nn.init.kaiming_uniform_(self.A, a=torch.sqrt(torch.tensor(5.)).item())  # similar to standard weight initialization\n",
        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos uma classe linear com LoRA.",
        "\n",
        "Assim como antes, herde de `torch.nn.Module` para que possa atuar como uma camada de uma rede neural.",
        "\n",
        "No m\u00e9todo `init` criamos uma vari\u00e1vel com a camada linear original da rede e criamos outra vari\u00e1vel com a nova camada LoRA que hav\u00edamos implementado anteriormente.",
        "\n",
        "No m\u00e9todo `forward` somamos as sa\u00eddas da camada linear original e da camada LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoRALinear(torch.nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, criamos uma fun\u00e7\u00e3o que substitua as camadas lineares pela nova camada linear com LoRA que criamos. O que ela faz \u00e9 que se encontrar uma camada linear no modelo, a substitui pela camada linear com LoRA; caso contr\u00e1rio, aplica a fun\u00e7\u00e3o dentro das subcamadas da camada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Replace the Linear layer with LinearWithLoRA\n",
        "            setattr(model, name, LoRALinear(module, rank, alpha))\n",
        "        else:\n",
        "            # Recursively apply the same function to child modules\n",
        "            replace_linear_with_lora(module, rank, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicamos a fun\u00e7\u00e3o ao modelo para substituir as camadas lineares do modelo pela nova camada linear com LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "rank = 16\n",
        "alpha = 16\n",
        "\n",
        "replace_linear_with_lora(model, rank=rank, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos agora o n\u00famero de par\u00e2metros trein\u00e1veis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable LoRA parameters: 12,368\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passamos de 124M de par\u00e2metros trein\u00e1veis para 12k par\u00e2metros trein\u00e1veis, ou seja, reduzimos o n\u00famero de par\u00e2metros trein\u00e1veis 10.000 vezes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voltamos a ver o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): LoRALinear(\n",
              "    (linear): Linear(in_features=768, out_features=5, bias=False)\n",
              "    (lora): LoRALayer()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a compar\u00e1-los camada por camada",
        "\n",
        "|Modelo original|Modelo com LoRA|",
        "|-|-|",
        "|GPT2ForSequenceClassification(|GPT2ForSequenceClassification(|",
        "|  (transformer): GPT2Model(|  (transformer): GPT2Model(|",
        "|    (wte): Embedding(50257, 768)|    (wte): Embedding(50257, 768)|",
        "|    (wpe): Embedding(1024, 768)|    (wpe): Embedding(1024, 768)|",
        "|    (drop): Dropout(p=0.1, inplace=False)|    (drop): Dropout(p=0.1, inplace=False)|",
        "|    (h): ModuleList(|    (h): ModuleList(|",
        "|      (0-11): 12 x GPT2Block(|      (0-11): 12 x GPT2Block(|",
        "|        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|",
        "|        (attn): GPT2Attention(|        (attn): GPT2Attention(|",
        "|          (c_attn): Conv1D()|          (c_attn): Conv1D()|",
        "|          (c_proj): Conv1D()|          (c_proj): Conv1D()|",
        "|          (attn_dropout): Dropout(p=0.1, inplace=False)|          (attn_dropout): Dropout(p=0.1, inplace=False)|",
        "|          (resid_dropout): Dropout(p=0.1, inplace=False)|          (resid_dropout): Dropout(p=0.1, inplace=False)|",
        "|        )|        )|",
        "|        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|",
        "|        (mlp): GPT2MLP(|        (mlp): GPT2MLP(|",
        "|          (c_fc): Conv1D()|          (c_fc): Conv1D()|",
        "|          (c_proj): Conv1D()|          (c_proj): Conv1D()|",
        "|          (act): NewGELUActivation()|          (act): NewGELUActivation()|",
        "|          (dropout): Dropout(p=0.1, inplace=False)|          (dropout): Dropout(p=0.1, inplace=False)|",
        "|        )|        )|",
        "|      )|      )|",
        "|    )|    )|",
        "|    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)|",
        "|  )|  )|",
        "||  (pontua\u00e7\u00e3o): LoRALinear()|",
        "|  (score): Linear(in_features=768, out_features=5, bias=False)|    (linear): Linear(in_features=768, out_features=5, bias=False)|",
        "||    (lora): LoRAL\u5c42()| \n\nNota: Parece que hubo un error en la traducci\u00f3n del t\u00e9rmino \"LoRALayer\" al portugu\u00e9s. La traducci\u00f3n correcta ser\u00eda:\n\n||    (lora): LoRALayer()|",
        "||  )|",
        "|)|)|",
        "\n",
        "Vemos que s\u00e3o iguais, exceto no final, onde no modelo original havia uma camada linear normal e no modelo com LoRA h\u00e1 uma camada `LoRALinear` que dentro possui a camada linear do modelo original e uma camada `LoRALayer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez instanciado o modelo com LoRA, vamos trein\u00e1-lo como sempre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissem, no post [Fine tuning SLMs](https://maximofn.com/fine-tuning-sml/) usamos um batch size de 28 para o loop de treinamento e de 40 para o de avalia\u00e7\u00e3o, enquanto agora que h\u00e1 menos par\u00e2metros trein\u00e1veis podemos usar um batch size maior.",
        "\n",
        "Por que isso acontece? Quando se treina um modelo, \u00e9 necess\u00e1rio armazenar na mem\u00f3ria da GPU o modelo e os gradientes desse modelo, ent\u00e3o tanto com LoRA quanto sem LoRA, o modelo precisa ser armazenado igualmente, mas no caso do LoRA, apenas os gradientes de 12k par\u00e2metros s\u00e3o armazenados, enquanto que sem LoRA, os gradientes de 128M de par\u00e2metros s\u00e3o armazenados, por isso com LoRA \u00e9 necess\u00e1rio menos mem\u00f3ria da GPU, permitindo o uso de um batch size maior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "metric_name = \"accuracy\"\n",
        "model_name = \"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification\"\n",
        "LR = 2e-5\n",
        "BS_TRAIN = 400\n",
        "BS_EVAL = 400\n",
        "EPOCHS = 3\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    model_name,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BS_TRAIN,\n",
        "    per_device_eval_batch_size=BS_EVAL,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio = 0.1,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    push_to_hub=True,\n",
        "    logging_dir=\"./runs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from evaluate import load\n",
        "\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    print(eval_pred)\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=subset_dataset_train,\n",
        "    eval_dataset=subset_dataset_validation,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "AZL4wIFKlC6x",
        "outputId": "9f6a314c-65f0-46a5-fadf-61f832c1ca73"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 42:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.396400</td>\n",
              "      <td>1.602937</td>\n",
              "      <td>0.269400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.572700</td>\n",
              "      <td>1.531719</td>\n",
              "      <td>0.320800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.534400</td>\n",
              "      <td>1.511815</td>\n",
              "      <td>0.335800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be46440>\n",
            "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be45c30>\n",
            "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be8b970>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=1.8345018310546874, metrics={'train_runtime': 2565.4667, 'train_samples_per_second': 233.876, 'train_steps_per_second': 0.585, 'total_flos': 2.352076406784e+17, 'train_loss': 1.8345018310546874, 'epoch': 3.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Avalia\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez treinado, avaliamos sobre o dataset de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "Xcxmbo-3Uz1G",
        "outputId": "a6ce9a05-22a1-40ce-a32a-b149ed8e6dd4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:17]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<transformers.trainer_utils.EvalPrediction object at 0x7cd07be8bbe0>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.5203168392181396,\n",
              " 'eval_accuracy': 0.3374,\n",
              " 'eval_runtime': 19.3843,\n",
              " 'eval_samples_per_second': 257.94,\n",
              " 'eval_steps_per_second': 0.671,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate(eval_dataset=subset_dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publicar o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 temos nosso modelo treinado, j\u00e1 podemos compartilh\u00e1-lo com o mundo, ent\u00e3o primeiro criamos uma **model card**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.create_model_card()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E j\u00e1 podemos public\u00e1-lo. Como a primeira coisa que fizemos foi fazer login no hub do Hugging Face, podemos envi\u00e1-lo para o nosso hub sem nenhum problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Teste do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Limpez tudo o poss\u00edvel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "\n",
        "def clear_hardwares():\n",
        "    torch.clear_autocast_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "clear_hardwares()\n",
        "clear_hardwares()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como subimos o modelo ao nosso hub, podemos baix\u00e1-lo e us\u00e1-lo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "user = \"maximofn\"\n",
        "checkpoints = f\"{user}/{model_name}\"\n",
        "task = \"text-classification\"\n",
        "classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, se quisermos que nos retorne a probabilidade de todas as classes, simplesmente usamos o classificador que acabamos de instanciar, com o par\u00e2metro `top_k=None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqy8oMPuUz1I",
        "outputId": "78501f30-2983-481a-e908-d5139bbce0e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 0.8419149518013},\n",
              " {'label': 'LABEL_1', 'score': 0.09386005252599716},\n",
              " {'label': 'LABEL_3', 'score': 0.03624210134148598},\n",
              " {'label': 'LABEL_2', 'score': 0.02049318142235279},\n",
              " {'label': 'LABEL_4', 'score': 0.0074898069724440575}]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = classifier(\"I love this product\", top_k=None)\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se quisermos apenas a classe com a maior probabilidade, fazemos o mesmo mas com o par\u00e2metro `top_k=1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpcw3pN0Uz1J",
        "outputId": "2667da0e-743c-4300-ac46-b8ff4d43270d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 0.8419149518013}]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label = classifier(\"I love this product\", top_k=1)\n",
        "label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E se quisermos n classes, fazemos o mesmo mas com o par\u00e2metro `top_k=n`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRQvB3tTUz1J",
        "outputId": "0eb134fa-4633-4394-d795-b1563cad1d37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 0.8419149518013},\n",
              " {'label': 'LABEL_1', 'score': 0.09386005252599716}]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "two_labels = classifier(\"I love this product\", top_k=2)\n",
        "two_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb\u00e9m podemos testar o modelo com Automodel e AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"GPT2-small-finetuned-amazon-reviews-en-classification\"\n",
        "user = \"maximofn\"\n",
        "checkpoint = f\"{user}/{model_name}\"\n",
        "num_classes = num_classes\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes).half().eval().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9goQMfcLUz1K",
        "outputId": "7d6dc70b-88a4-4f06-83f4-81457994de3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.003940582275390625,\n",
              " 0.00266265869140625,\n",
              " 0.013946533203125,\n",
              " 0.1544189453125,\n",
              " 0.8251953125]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer.encode(\"I love this product\", return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    output = model(tokens)\n",
        "logits = output.logits\n",
        "lables = torch.softmax(logits, dim=1).cpu().numpy().tolist()\n",
        "lables[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se voc\u00ea quiser testar mais o modelo, voc\u00ea pode v\u00ea-lo em [Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementa\u00e7\u00e3o de LoRA em um LLM com PEFT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos fazer o mesmo com a biblioteca `PEFT` do Hugging Face. Vamos v\u00ea-lo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Login no Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N\u00f3s fazemos login para fazer o upload do modelo para o Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voltamos a baixar o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'text', 'label', 'label_text'],\n",
              "        num_rows: 200000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'text', 'label', 'label_text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'text', 'label', 'label_text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos um subconjunto caso voc\u00ea queira testar o c\u00f3digo com um conjunto de dados menor. No meu caso, usarei 100% do conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text'],\n",
              "     num_rows: 200000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text'],\n",
              "     num_rows: 5000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text'],\n",
              "     num_rows: 5000\n",
              " }))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "percentage = 1\n",
        "\n",
        "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
        "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
        "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
        "\n",
        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos o n\u00famero de classes, para obter o n\u00famero de classes usamos `dataset['train']` e n\u00e3o `subset_dataset_train` porque se o subconjunto for muito pequeno \u00e9 poss\u00edvel que n\u00e3o haja exemplos com todas as poss\u00edveis classes do conjunto de dados original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(dataset['train'].unique('label'))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma fun\u00e7\u00e3o para criar o campo `label` no dataset. O dataset baixado tem o campo `labels`, mas a biblioteca `transformers` precisa que o campo seja chamado de `label` e n\u00e3o `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_labels(example):\n",
        "    example['labels'] = example['label']\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicamos a fun\u00e7\u00e3o ao conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
              "     num_rows: 200000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
              "     num_rows: 5000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
              "     num_rows: 5000\n",
              " }))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
        "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
        "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
        "\n",
        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos o tokenizador. Para que n\u00e3o nos d\u00ea erro, atribu\u00edmos o token de end of string ao token de padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma fun\u00e7\u00e3o para tokenizar o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicamos a fun\u00e7\u00e3o ao conjunto de dados e, de passagem, eliminamos as colunas que n\u00e3o precisamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['labels', 'input_ids', 'attention_mask'],\n",
              "     num_rows: 200000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['labels', 'input_ids', 'attention_mask'],\n",
              "     num_rows: 5000\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['labels', 'input_ids', 'attention_mask'],\n",
              "     num_rows: 5000\n",
              " }))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
        "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
        "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
        "\n",
        "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos o modelo. Tamb\u00e9m, para que n\u00e3o d\u00ea erro, atribu\u00edmos o token de end of string ao token de padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA com PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de criar o modelo com LoRA, vamos a ver suas camadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, h\u00e1 apenas uma camada `Linear`, que \u00e9 `score` e que \u00e9 a que vamos substituir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos criar uma configura\u00e7\u00e3o de LoRA com a biblioteca PEFT e depois aplicar LoRA ao mo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    target_modules=[\"score\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com esta configura\u00e7\u00e3o, definimos um rank de 16 e um alpha de 32. Al\u00e9m disso, adicionamos um dropout \u00e0s camadas de LoRA de 0.1. Precisamos indicar a tarefa para a configura\u00e7\u00e3o de LoRA, neste caso \u00e9 uma tarefa de sequence classification. Por fim, indicamos quais camadas queremos substituir, neste caso a camada `score`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora aplicamos LoRA ao modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver quantos par\u00e2metros trein\u00e1veis o modelo tem agora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 12,368 || all params: 124,456,016 || trainable%: 0.0099\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos os mesmos par\u00e2metros trein\u00e1veis que antes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez instanciado o modelo com LoRA, vamos trein\u00e1-lo como sempre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "metric_name = \"accuracy\"\n",
        "model_name = \"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification\"\n",
        "LR = 2e-5\n",
        "BS_TRAIN = 400\n",
        "BS_EVAL = 400\n",
        "EPOCHS = 3\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    model_name,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BS_TRAIN,\n",
        "    per_device_eval_batch_size=BS_EVAL,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio = 0.1,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    push_to_hub=True,\n",
        "    logging_dir=\"./runs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from evaluate import load\n",
        "\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    print(eval_pred)\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=subset_dataset_train,\n",
        "    eval_dataset=subset_dataset_validation,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='811' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 811/1500 22:43 < 19:20, 0.59 it/s, Epoch 1.62/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.275100</td>\n",
              "      <td>1.512476</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<transformers.trainer_utils.EvalPrediction object at 0x7f774a50bbe0>\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 42:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.275100</td>\n",
              "      <td>1.512476</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.515900</td>\n",
              "      <td>1.417553</td>\n",
              "      <td>0.373800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.463500</td>\n",
              "      <td>1.405058</td>\n",
              "      <td>0.381400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<transformers.trainer_utils.EvalPrediction object at 0x7f77486a7c40>\n",
            "<transformers.trainer_utils.EvalPrediction object at 0x7f7749eb5690>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=1.751504597981771, metrics={'train_runtime': 2551.7753, 'train_samples_per_second': 235.13, 'train_steps_per_second': 0.588, 'total_flos': 2.352524525568e+17, 'train_loss': 1.751504597981771, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Avalia\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez treinado, avaliamos sobre o dataset de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:17]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<transformers.trainer_utils.EvalPrediction object at 0x7f77a1d1f7c0>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.4127237796783447,\n",
              " 'eval_accuracy': 0.3862,\n",
              " 'eval_runtime': 19.3275,\n",
              " 'eval_samples_per_second': 258.699,\n",
              " 'eval_steps_per_second': 0.673,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.evaluate(eval_dataset=subset_dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publicar o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos uma model card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.create_model_card()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O publicamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification/commit/839066c2bde02689a6b3f5624ac25f89c4de217d', commit_message='End of training', commit_description='', oid='839066c2bde02689a6b3f5624ac25f89c4de217d', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Teste do modelo treinado com PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Limpamos tudo o poss\u00edvel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "\n",
        "def clear_hardwares():\n",
        "    torch.clear_autocast_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "clear_hardwares()\n",
        "clear_hardwares()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como subimos o modelo ao nosso hub, podemos baix\u00e1-lo e us\u00e1-lo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "user = \"maximofn\"\n",
        "checkpoints = f\"{user}/{model_name}\"\n",
        "task = \"text-classification\"\n",
        "classifier = pipeline(task, model=checkpoints, tokenizer=checkpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, se quisermos que nos retorne a probabilidade de todas as classes, simplesmente usamos o classificador que acabamos de instanciar, com o par\u00e2metro `top_k=None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.9979197382926941},\n",
              " {'label': 'LABEL_0', 'score': 0.002080311067402363}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = classifier(\"I love this product\", top_k=None)\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se quisermos apenas a classe com a maior probabilidade fazemos o mesmo mas com o par\u00e2metro `top_k=1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.9979197382926941}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label = classifier(\"I love this product\", top_k=1)\n",
        "label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E se quisermos n classes, fazemos o mesmo mas com o par\u00e2metro `top_k=n`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.9979197382926941},\n",
              " {'label': 'LABEL_0', 'score': 0.002080311067402363}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "two_labels = classifier(\"I love this product\", top_k=2)\n",
        "two_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se voc\u00ea quiser testar mais o modelo, pode v\u00ea-lo em [Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification](https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "maximofn": {
      "date": "2024-07-20",
      "description_en": "Get ready to take your model adaptation to the next level with LoRA! \ud83d\ude80 This low-rank adaptation technique is like a superhero cape for your neural networks - it helps them learn new tricks without forgetting old ones \ud83e\udd2f. And the best thing about it? You can implement it in just a few lines of PyTorch code \ud83d\udcbb And if you're like me, a poor GPU guy struggling with limited resources \ud83d\udcb8, LoRA is like a godsend: it lets you adapt your models without training them from scratch or spending a fortune on hardware \ud83d\ude4f Check out the post for a step-by-step guide and a practical example!",
      "description_es": "\u00a1Prep\u00e1rate para llevar la adaptaci\u00f3n de tus modelos al siguiente nivel con LoRA! \ud83d\ude80 Esta t\u00e9cnica de adaptaci\u00f3n de baja rango es como una capa de superh\u00e9roe para tus redes neuronales - les ayuda a aprender nuevos trucos sin olvidar los antiguos \ud83e\udd2f. Y lo mejor de todo? Puedes implementarla en solo unas pocas l\u00edneas de c\u00f3digo PyTorch \ud83d\udcbb. \u00a1Y si eres como yo, un pobre de GPU que lucha con recursos limitados \ud83d\udcb8, LoRA es como un regalo del cielo: te permite adaptar tus modelos sin necesidad de entrenarlos desde cero ni gastar una fortuna en hardware \ud83d\ude4f. \u00a1Revisa el post para obtener una gu\u00eda paso a paso y un ejemplo pr\u00e1ctico!",
      "description_pt": "Prepare-se para levar a adapta\u00e7\u00e3o de modelos para o pr\u00f3ximo n\u00edvel com o LoRA! \ud83d\ude80 Essa t\u00e9cnica de adapta\u00e7\u00e3o de baixa classifica\u00e7\u00e3o \u00e9 como uma capa de super-her\u00f3i para suas redes neurais - ela as ajuda a aprender novos truques sem esquecer os antigos \ud83e\udd2f. E o melhor de tudo? Voc\u00ea pode implement\u00e1-la em apenas algumas linhas de c\u00f3digo PyTorch \ud83d\udcbb E se voc\u00ea for como eu, um pobre cara da GPU lutando com recursos limitados \ud83d\udcb8, o LoRA \u00e9 como uma d\u00e1diva de Deus: ele permite que voc\u00ea adapte seus modelos sem trein\u00e1-los do zero ou gastar uma fortuna em hardware \ud83d\ude4f Confira a postagem para obter um guia passo a passo e um exemplo pr\u00e1tico!",
      "end_url": "lora",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_thumbnail_ES.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_thumbnail_ES.webp",
      "keywords_en": "lora, low rank adaptation, neural networks, pytorch, gpu, hardware",
      "keywords_es": "lora, adaptaci\u00f3n de baja clasificaci\u00f3n, redes neuronales, pytorch, gpu, hardware",
      "keywords_pt": "lora, adapta\u00e7\u00e3o de baixa classifica\u00e7\u00e3o, redes neurais, pytorch, gpu, hardware",
      "title_en": "LoRA \u2013 low rank adaptation of large language models",
      "title_es": "LoRA \u2013 low rank adaptation of large language models",
      "title_pt": "LoRA \u2013 low rank adaptation of large language models"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}