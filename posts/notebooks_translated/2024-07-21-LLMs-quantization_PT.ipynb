{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantiza\u00e7\u00e3o de LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os modelos de linguagem est\u00e3o cada vez maiores, o que os torna cada vez mais custosos e caros de executar.",
        "\n",
        "![LLMs-size-evolution](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LLMs-size-evolution.webp)",
        "\n",
        "![Llama-size-evolution](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Llama-size-evolution.webp)",
        "\n",
        "Por exemplo, o modelo Llama 3 400B, se seus par\u00e2metros estiverem armazenados no formato FP32, cada par\u00e2metro ocupa, portanto, 4 bytes, o que significa que apenas para armazenar o modelo s\u00e3o necess\u00e1rios 400*(10e9)*4 bytes = 1,6 TB de mem\u00f3ria VRAM. Isso equivale a 20 GPUs com 80GB de mem\u00f3ria VRAM cada, as quais, al\u00e9m disso, n\u00e3o s\u00e3o baratas.",
        "\n",
        "Mas se deixarmos de lado modelos gigantes e nos focarmos em modelos de tamanhos mais comuns, por exemplo, 70B de par\u00e2metros, apenas armazenar o modelo representa 70*(10e9)*4 bytes = 280 GB de mem\u00f3ria VRAM, o que equivale a 4 GPUs de 80GB de mem\u00f3ria VRAM cada uma.",
        "\n",
        "Isso acontece porque armazenamos os pesos no formato FP32, ou seja, cada par\u00e2metro ocupa 4 bytes. Mas o que acontece se conseguirmos que cada par\u00e2metro ocupe menos bytes? Isso \u00e9 chamado de quantiza\u00e7\u00e3o.",
        "\n",
        "Por exemplo, se conseguirmos que um modelo de 70B de par\u00e2metros ocupe meio byte por par\u00e2metro, ent\u00e3o precisar\u00edamos apenas de 70*(10e9)*0.5 bytes = 35 GB de mem\u00f3ria VRAM, o que equivale a 2 GPUs de 24GB de mem\u00f3ria VRAM cada uma, as quais j\u00e1 podem ser consideradas GPUs de usu\u00e1rios normais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portanto, precisamos maneiras de poder reduzir o tamanho desses modelos. Existem tr\u00eas formas de fazer isso: a destila\u00e7\u00e3o, a poda e a quantiza\u00e7\u00e3o.",
        "\n",
        "A destila\u00e7\u00e3o consiste em treinar um modelo menor a partir das sa\u00eddas do maior. Isso significa que uma entrada \u00e9 fornecida tanto ao modelo pequeno quanto ao grande, considerando-se que a sa\u00edda correta \u00e9 a do modelo grande, de modo que o treinamento do modelo pequeno \u00e9 realizado de acordo com a sa\u00edda do modelo grande. Mas isso requer ter armazenado o modelo grande, o que n\u00e3o \u00e9 o que queremos ou podemos fazer.",
        "\n",
        "A poda consiste em eliminar par\u00e2metros do modelo, tornando-o cada vez menor. Este m\u00e9todo se baseia na ideia de que os modelos de linguagem atuais est\u00e3o sobredimensionados e apenas alguns poucos par\u00e2metros s\u00e3o os que realmente contribuem com informa\u00e7\u00f5es. Por isso, se conseguirmos eliminar os par\u00e2metros que n\u00e3o fornecem informa\u00e7\u00f5es, obteremos um modelo menor. Mas isso n\u00e3o \u00e9 simples atualmente, porque n\u00e3o temos uma maneira de saber bem quais par\u00e2metros s\u00e3o importantes e quais n\u00e3o s\u00e3o.",
        "\n",
        "Por outro lado, a quantiza\u00e7\u00e3o consiste em reduzir o tamanho de cada um dos par\u00e2metros do modelo. E \u00e9 isso que vamos explicar neste post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Formato dos par\u00e2metros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os par\u00e2metros dos pesos podem ser armazenados em v\u00e1rios tipos de formatos",
        "\n",
        "![numbers-representation](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/numbers-representation.webp)",
        "\n",
        "Originalmente, era usado o FP32 para armazenar os par\u00e2metros, mas devido ao fato de come\u00e7armos a ficar sem mem\u00f3ria para armazenar os modelos, passamos a usar o FP16, o que n\u00e3o dava resultados ruins.",
        "\n",
        "No entanto, o problema do FP16 \u00e9 que ele n\u00e3o alcan\u00e7a valores t\u00e3o altos quanto o FP32, o que pode levar ao caso de overflow de valores, ou seja, ao realizar c\u00e1lculos internos na rede, o resultado pode ser t\u00e3o alto que n\u00e3o possa ser representado em FP16, gerando erros. Isso ocorre porque o modelo foi treinado em FP32, o que permite que todos os poss\u00edveis c\u00e1lculos internos sejam realizados, mas ao passar para FP16 posteriormente para realizar infer\u00eancias, alguns c\u00e1lculos internos podem causar overflow.",
        "\n",
        "Devido a esses erros de overflow, foram criados o TF32 e o BF16, os quais t\u00eam a mesma quantidade de bits de expoente, o que permite que eles alcancem valores t\u00e3o altos quanto o FP32, mas com a vantagem de ocupar menos mem\u00f3ria por terem menos bits. No entanto, ambos, por terem menos bits de mantissa, n\u00e3o podem representar n\u00fameros com tanta precis\u00e3o quanto o FP32, o que pode resultar em erros de arredondamento, mas pelo menos n\u00e3o obteremos um erro ao executar a rede. O TF32 tem no total 19 bits, enquanto o BF16 tem 16 bits. Geralmente, usa-se mais o BF16 porque se economiza mais mem\u00f3ria.",
        "\n",
        "Hist\u00f3ricamente existiram os formatos INT8 e UINT8, que podem representar n\u00fameros de -128 a 127 e de 0 a 255, respectivamente. Embora sejam formatos bons porque permitem economizar mem\u00f3ria, j\u00e1 que cada par\u00e2metro ocupa 1 byte em compara\u00e7\u00e3o aos 4 bytes do FP32, o problema que t\u00eam \u00e9 que s\u00f3 podem representar um intervalo pequeno de n\u00fameros e, al\u00e9m disso, apenas inteiros, pelo que podem ocorrer os dois problemas vistos anteriormente: desbordamento e falta de precis\u00e3o.",
        "\n",
        "Para resolver o problema de que os formatos INT8 e UINT8 representam apenas n\u00fameros inteiros, foram criados os formatos FP8 e FP4, mas ainda n\u00e3o est\u00e3o muito consolidados, nem possuem um formato muito padronizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embora tenhamos meios de armazenar os par\u00e2metros dos modelos em formatos menores, e embora consigamos resolver os problemas de overflow e arredondamento, temos outro problema, que \u00e9 o fato de que nem todas as GPUs s\u00e3o capazes de representar todos os formatos. Isso ocorre porque esses problemas de mem\u00f3ria s\u00e3o relativamente novos, de modo que as GPUs mais antigas n\u00e3o foram projetadas para resolver esses problemas e, portanto, n\u00e3o s\u00e3o capazes de representar todos os formatos.",
        "\n",
        "![GPUs-data-formating](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPUs-data-formating.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como \u00faltimo detalhe, como curiosidade, durante o treinamento dos modelos \u00e9 utilizada a chamada precis\u00e3o mista. Os pesos do modelo s\u00e3o armazenados no formato FP32, no entanto, o `forward pass` e o `backward pass` s\u00e3o realizados em FP16 para ser mais r\u00e1pido. Os gradientes resultantes do `backward pass` s\u00e3o armazenados em FP16 e usados para modificar os valores FP32 dos pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tipos de quantiza\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantiza\u00e7\u00e3o de ponto zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este \u00e9 o tipo de quantiza\u00e7\u00e3o mais simples. Consiste em reduzir o intervalo de valores de maneira linear, o menor valor de FP32 corresponde ao menor valor do novo formato, o zero de FP32 corresponde ao zero do novo formato e o maior valor de FP32 corresponde ao maior valor do novo formato.",
        "\n",
        "Por exemplo, se quisermos passar os n\u00fameros representados de -1 at\u00e9 1 no formato UINT8, como os limites do UINT8 s\u00e3o -127 e 127, se quisermos representar o valor 0.3, o que fazemos \u00e9 multiplicar 0.3 por 127, o que d\u00e1 38.1 e arredond\u00e1-lo para 38, que \u00e9 o valor que seria armazenado no UINT8.",
        "\n",
        "Se quisermos fazer o passo contr\u00e1rio, para converter 38 para o formato entre -1 e 1, o que fazemos \u00e9 dividir 38 por 127, que d\u00e1 0.2992, que \u00e9 aproximadamente 0.3, e podemos ver que temos um erro de 0.008",
        "\n",
        "![quantization-zero-point](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-zero-point.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantiza\u00e7\u00e3o Afim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste tipo de quantiza\u00e7\u00e3o, se tiver um array de valores em um formato e quiser passar para outro, primeiro divide-se o array inteiro pelo valor m\u00e1ximo do array e depois multiplica-se o array inteiro pelo valor m\u00e1ximo do novo formato.",
        "\n",
        "![quantization-affine](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-affine.webp)",
        "\n",
        "Por exemplo, na imagem acima temos o array",
        "\n",
        "```\n",
        "[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]",
        "```\n",
        "\n",
        "Como seu valor m\u00e1ximo \u00e9 `5.4`, dividimos o array por esse valor e obtemos",
        "\n",
        "```\n",
        "[0.2222222222, -0.09259259259, -0.7962962963, 0.2222222222, -0.5740740741, 0.1481481481, 0.4444444444, 1]",
        "```\n",
        "\n",
        "Se agora multiplicarmos todos os valores por `127`, que \u00e9 o valor m\u00e1ximo de UINT8, obtemos",
        "\n",
        "```\n",
        "[28,22222222, -11,75925926, -101,1296296, 28,22222222, -72,90740741, 18,81481481, 56,44444444, 127]",
        "```\n",
        "\n",
        "Que, arredondando, seria",
        "\n",
        "```\n",
        "[28, -12, -101, 28, -73, 19, 56, 127]",
        "```\n",
        "\n",
        "Se quisermos realizar o passo inverso, ter\u00edamos que dividir o array resultante por `127`, que daria",
        "\n",
        "```\n",
        "[0,2204724409, -0,09448818898, -0,7952755906, 0,2204724409, -0,5748031496, 0,1496062992, 0,4409448819, 1]",
        "```\n",
        "\n",
        "E multiplicar novamente por `5.4`, com o que obter\u00edamos",
        "\n",
        "```\n",
        "[1,190551181, -0.5102362205, -4.294488189, 1.190551181, -3.103937008, 0.8078740157, 2.381102362, 5.4]",
        "```\n",
        "\n",
        "Se compararmos com o array original, vemos que temos um erro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Momentos de quantiza\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantiza\u00e7\u00e3o p\u00f3s-treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o nome sugere, a quantiza\u00e7\u00e3o ocorre ap\u00f3s o treinamento. O modelo \u00e9 treinado em FP32 e depois \u00e9 quantizado para outro formato. Este m\u00e9todo \u00e9 o mais simples, mas pode levar a erros de precis\u00e3o na quantiza\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantiza\u00e7\u00e3o durante o treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Durante o treinamento, \u00e9 realizado o `forward pass` no modelo original e em um modelo quantizado, e s\u00e3o observados os poss\u00edveis erros decorrentes da quantiza\u00e7\u00e3o para poder mitig\u00e1-los. Este processo torna o treinamento mais custoso, pois voc\u00ea precisa armazenar na mem\u00f3ria o modelo original e o quantizado, e mais lento, pois voc\u00ea precisa realizar o `forward pass` em dois modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M\u00e9todos de quantiza\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A seguir mostro os links para os posts onde explico cada um dos m\u00e9todos, para que este post n\u00e3o fique muito longo",
        "\n",
        "* [LLM.int8()](/llm-int8)",
        "* [GPTQ](/gptq)",
        "* [QLoRA](/qlora)",
        "* AWQ",
        "* QuIP",
        "* GGUF",
        "* HQQ",
        "* AQLM",
        "* FBGEMM FP8"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "maximofn": {
      "date": "2024-07-21",
      "description_en": "Imagine having a giant language model that can answer any question, from the capital of France to the perfect brownie recipe! \ud83c\udf5e\ufe0f\ud83c\uddeb\ud83c\uddf7 But what happens when that model has to fit on a mobile device? \ud83d\udcf1 That's where quantization comes in! \ud83c\udf89 This technique allows us to reduce the size of models without sacrificing their accuracy, which means we can enjoy artificial intelligence on our mobile devices without the need for a supercomputer. \ud83d\udcbb It's like compressing an elephant into a shoebox, but without crushing the elephant! \ud83d\udc18\ud83d\ude02",
      "description_es": "\u00a1Imagina que tienes un modelo de lenguaje gigante que puede responder a cualquier pregunta, desde la capital de Francia hasta la receta perfecta para hacer brownies! \ud83c\udf5e\ufe0f\ud83c\uddeb\ud83c\uddf7 Pero, \u00bfqu\u00e9 pasa cuando ese modelo tiene que caber en un dispositivo m\u00f3vil? \ud83d\udcf1 \u00a1Eso es donde entra en juego la cuantizaci\u00f3n! \ud83c\udf89 Esta t\u00e9cnica nos permite reducir el tama\u00f1o de los modelos sin sacrificar su precisi\u00f3n, lo que significa que podemos disfrutar de inteligencia artificial en nuestros dispositivos m\u00f3viles sin necesidad de un supercomputador. \ud83d\udcbb \u00a1Es como comprimir un elefante en una caja de zapatos, pero sin aplastar al elefante! \ud83d\udc18\ud83d\ude02",
      "description_pt": "Imagine que voc\u00ea tem um modelo de linguagem gigante que pode responder a qualquer pergunta, desde a capital da Fran\u00e7a at\u00e9 a receita perfeita de brownies! \ud83c\udf5e\ufe0f\ud83c\uddeb\ud83c\uddf7 Mas o que acontece quando esse modelo precisa caber em um dispositivo m\u00f3vel? \ud83d\udcf1 \u00c9 a\u00ed que entra a quantiza\u00e7\u00e3o! \ud83c\udf89 Essa t\u00e9cnica nos permite reduzir o tamanho dos modelos sem sacrificar sua precis\u00e3o, o que significa que podemos desfrutar da intelig\u00eancia artificial em nossos dispositivos m\u00f3veis sem a necessidade de um supercomputador. \ud83d\udcbb \u00c9 como espremer um elefante em uma caixa de sapatos, mas sem esmagar o elefante! \ud83d\udc18\ud83d\ude02",
      "end_url": "llms-quantization",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-thumbnail.webp",
      "keywords_en": "quantization, LLMs, GPT, AI, machine learning, deep learning, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, post-training, during-training, zero-point, affine, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8",
      "keywords_es": "cuantizaci\u00f3n, LLMs, GPT, IA, aprendizaje autom\u00e1tico, aprendizaje profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, post-entrenamiento, durante-entrenamiento, punto-cero, afin, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8",
      "keywords_pt": "quantiza\u00e7\u00e3o, LLMs, GPT, IA, aprendizado de m\u00e1quina, aprendizado profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, p\u00f3s-treinamento, durante-treinamento, ponto-zero, afim, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8",
      "title_en": "LLMs quantization",
      "title_es": "LLMs quantization",
      "title_pt": "LLMs quantization"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}