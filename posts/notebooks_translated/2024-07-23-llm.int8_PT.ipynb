{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# llm.int8() - Multiplica\u00e7\u00e3o de Matrizes de 8 bits para Transformers em Escala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No post [LLMs quantization](https://maximofn.com/llms-quantization/) explicamos a import\u00e2ncia da quantiza\u00e7\u00e3o dos LLMs para economizar mem\u00f3ria. Al\u00e9m disso, explicamos que existe uma maneira de quantiza\u00e7\u00e3o que \u00e9 a [cuantiza\u00e7\u00e3o de ponto zero](https://maximofn.com/llms-quantization/#Cuantizaci%C3%B3n-de-punto-cero) que consiste em transformar os valores dos par\u00e2metros dos pesos linearmente, mas isso tem o problema da degrada\u00e7\u00e3o dos modelos de linguagem a partir do momento em que eles ultrapassam 2,7B de par\u00e2metros.",
        "\n",
        "![llm.int8()-degrada\u00e7\u00e3o](https://images.maximofn.com/llm.int8-degradation.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantiza\u00e7\u00e3o vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como a quantiza\u00e7\u00e3o de todos os par\u00e2metros dos modelos produz erro nos grandes modelos de linguagem, o que prop\u00f5em no paper [llm.int8()](https://arxiv.org/abs/2208.07339) \u00e9 realizar a quantiza\u00e7\u00e3o vetorial, ou seja, separar as matrizes de pesos em vetores, de maneira que alguns desses vetores podem ser quantizados em 8 bits, enquanto outros n\u00e3o. Portanto, os que podem ser quantizados em 8 bits s\u00e3o quantizados e as multiplica\u00e7\u00f5es matriciais s\u00e3o realizadas no formato INT8, enquanto os vetores que n\u00e3o podem ser quantizados permanecem no formato FP16 e as multiplica\u00e7\u00f5es s\u00e3o realizadas no formato FP16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos v\u00ea-lo com um exemplo",
        "\n",
        "Suponhamos que temos a matriz",
        "\n",
        "![llm.int8()-A](https://images.maximofn.com/llm.int8-A.webp)",
        "\n",
        "e queremos multiplic\u00e1-la pela matriz",
        "\n",
        "![llm.int8()-B](https://images.maximofn.com/llm.int8-B.webp)",
        "\n",
        "Estabelecemos um valor limiar e todas as colunas da primeira matriz que tenham um valor maior que esse limiar s\u00e3o deixadas no formato FP16. As linhas equivalentes \u00e0s linhas da primeira matriz, na segunda matriz tamb\u00e9m s\u00e3o deixadas no formato FP16.",
        "\n",
        "Como as colunas segunda e quarta da primeira matriz (colunas amarelas) t\u00eam valores maiores que um certo limiar, ent\u00e3o as linhas segunda e quarta da segunda matriz (linhas amarelas) s\u00e3o mantidas no formato FP16.",
        "\n",
        "Em caso de ter valores limiares na segunda matriz, far-se-ia o mesmo. Por exemplo, se uma linha da segunda matriz tivesse um valor maior que um limiar, ela seria deixada no formato FP16, e essa coluna na primeira matriz seria deixada no formato FP16.",
        "\n",
        "O restante das linhas e colunas que n\u00e3o s\u00e3o deixadas no formato FP16 \u00e9 quantizado em 8 bits e as multiplica\u00e7\u00f5es s\u00e3o realizadas no formato INT8",
        "\n",
        "Ent\u00e3o, separamos a primeira matriz em duas submatrizes",
        "\n",
        "![llm.int8()-A_separated](https://images.maximofn.com/llm.int8-A_separated_.webp)",
        "\n",
        "E a segunda matriz nas duas matrizes",
        "\n",
        "![llm.int8()-B_separated](https://images.maximofn.com/llm.int8-B_separated_.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multiplicamos as matrizes em INT8 de um lado",
        "\n",
        "![llm.int8()-AxB-int8](https://images.maximofn.com/llm.int8-AxB-int8_.webp)",
        "\n",
        "E as que est\u00e3o em formato FP16 por outro lado",
        "\n",
        "![llm.int8()-AxB-fp16](https://images.maximofn.com/llm.int8-AxB-fp16_.webp)",
        "\n",
        "Como se pode ver, multiplicar as matrizes no formato INT8 nos d\u00e1 como resultado uma matriz de tamanho 3x2, e multiplicar as matrizes no formato FP16 nos d\u00e1 como resultado outra matriz de tamanho 3x2, portanto, se as somarmos",
        "\n",
        "![llm.int8()-fp16+int8](https://images.maximofn.com/llm.int8-fp16int8_.webp)",
        "\n",
        "Curiosamente, nos d\u00e1 o mesmo resultado que se tiv\u00e9ssemos multiplicado as matrizes originais",
        "\n",
        "![llm.int8()-AxB](https://images.maximofn.com/llm.int8-AxB_.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder ver por que ocorre isso, se desenvolvermos o produto vetorial das duas matrizes originais",
        "\n",
        "![llm.int8()-AxB-explained](https://images.maximofn.com/llm.int8-AxB-explained.webp)",
        "\n",
        "Vemos que a separa\u00e7\u00e3o que fizemos n\u00e3o d\u00e1 problemas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portanto, podemos concluir que podemos separar linhas e colunas das matrizes para realizar as multiplica\u00e7\u00f5es matriciais. Esta separa\u00e7\u00e3o ser\u00e1 feita quando algum elemento da linha ou coluna seja maior que um valor limite, de maneira que as linhas ou colunas que n\u00e3o tenham um valor maior que esse limite ser\u00e3o codificadas em INT8 ocupando apenas um byte e as linhas ou colunas que tenham algum elemento maior que esse limite ser\u00e3o convertidas para FP16 ocupando 2 bytes. Dessa forma, n\u00e3o teremos problemas de arredondamento, pois os c\u00e1lculos que realizarmos em INT8 ser\u00e3o feitos com valores que garantam que as multiplica\u00e7\u00f5es n\u00e3o ultrapassem o intervalo dos 8 bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Valor limiar \u03b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissem, vamos a separar em linhas e colunas que tenham algum elemento maior que um valor limiar, mas \u00bfqual valor limiar devemos escolher? Os autores do paper realizaram experimentos com v\u00e1rios valores e determinaram que esse valor limiar deveria ser \u03b1=6. Acima desse valor come\u00e7aram a obter degrada\u00e7\u00f5es nos modelos de linguagem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uso de llm.int8()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver como quantizar um modelo com llm.int8() com a biblioteca transformers. Para isso, \u00e9 necess\u00e1rio ter o `bitsandbytes` instalado.",
        "\n",
        "```bash\n",
        "pip install bitsandbytes",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Carregamos um modelo com 1B de par\u00e2metros duas vezes, uma de maneira normal e a segunda quantizando-o com llm.int8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos quanto mem\u00f3ria ocupa cada um dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhT4atcp9f3O",
        "outputId": "c8b8aa56-efb9-4084-b603-bf9b15e687a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4.098002195358276, 1.1466586589813232)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_memory_footprint()/(1024**3), model_8bit.get_memory_footprint()/(1024**3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se pode ver, o modelo quantizado ocupa muito menos mem\u00f3ria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos agora fazer um teste de gera\u00e7\u00e3o de texto com os dois modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aIZHa6p9f3O",
        "outputId": "3fc4007f-b96d-4734-f0d2-7d46638845e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    1, 15043,   590,  1024,   338,  5918,  4200,   322,   306,   626,\n",
              "           263,  6189, 29257, 10863,   261]], device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tokens = tokenizer(\"Hello my name is Maximo and I am a Machine Learning Engineer\", return_tensors=\"pt\").to(device)\n",
        "input_tokens.input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos a sa\u00edda com o modelo normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHGSazU0BvpN",
        "outputId": "0b57850a-9864-4c7f-884b-4610412b6abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor's degree in Computer Science from [University Name] and a Master's degree in Computer Science from [University Name]. I\n",
            "1.7616662979125977\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "max_new_tokens = 50\n",
        "outputs = model.generate(\n",
        "    input_ids=input_tokens.input_ids,\n",
        "    attention_mask=input_tokens.attention_mask,\n",
        "    max_length=input_tokens.input_ids.shape[1] + max_new_tokens,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(time.time() - t0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E agora com o modelo quantizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTc6pG-NBvpO",
        "outputId": "79f205f8-44c3-463c-956b-3945cb1edc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor's degree in Computer Science from [University Name] and a Master's degree in Computer Science from [University Name]. I\n",
            "9.100712776184082\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "max_new_tokens = 50\n",
        "outputs = model_8bit.generate(\n",
        "    input_ids=input_tokens.input_ids,\n",
        "    attention_mask=input_tokens.attention_mask,\n",
        "    max_length=input_tokens.input_ids.shape[1] + max_new_tokens,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(time.time() - t0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos duas coisas: por um lado, que na sa\u00edda obtemos o mesmo texto; portanto, com um modelo muito menor podemos obter a mesma sa\u00edda. No entanto, o modelo quantizado leva muito mais tempo para ser executado, ent\u00e3o se for necess\u00e1rio usar esse modelo em tempo real n\u00e3o seria recomend\u00e1vel.",
        "\n",
        "Isso \u00e9 contradit\u00f3rio, porque poder\u00edamos pensar que um modelo menor teria que ser executado mais rapidamente, mas \u00e9 preciso considerar que na realidade os dois modelos, o normal e o quantizado, realizam as mesmas opera\u00e7\u00f5es, apenas um realiza todas as opera\u00e7\u00f5es em FP32 e o outro as faz em INT8 e FP16, no entanto, o modelo quantizado precisa encontrar linhas e colunas com valores maiores que o valor de limiar, separ\u00e1-las, realizar as opera\u00e7\u00f5es em INT8 e FP16 e depois juntar os resultados novamente, por isso o modelo quantizado leva mais tempo para ser executado."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "maximofn": {
      "date": "2024-07-23",
      "description_en": "Get ready to save space and speed up your models! \ud83d\udca5 In this post, I'm going to explore the llm.int8() method, a quantization technique that allows you to reduce the size of your machine learning models without sacrificing too much accuracy. \ud83d\udcca That means you'll be able to train and deploy larger and more complex models in less space and with lower resource consumption! \ud83d\udcbb Let's see how to use llm.int8() with transformers to quantize a model and make it more efficient, without losing the essence of its artificial intelligence. \ud83e\udd16",
      "description_es": "\u00a1Prep\u00e1rate para ahorrar espacio y acelerar tus modelos! \ud83d\udca5 En este post, voy a explorar el m\u00e9todo llm.int8(), una t\u00e9cnica de cuantizaci\u00f3n que te permite reducir el tama\u00f1o de tus modelos de aprendizaje autom\u00e1tico sin sacrificar demasiada precisi\u00f3n. \ud83d\udcca \u00a1Eso significa que podr\u00e1s entrenar y desplegar modelos m\u00e1s grandes y complejos en menos espacio y con menor consumo de recursos! \ud83d\udcbb Vamos a ver c\u00f3mo utilizar llm.int8() con transformers para cuantizar un modelo y hacer que sea m\u00e1s eficiente, sin perder la esencia de su inteligencia artificial. \ud83e\udd16",
      "description_pt": "Prepare-se para economizar espa\u00e7o e acelerar seus modelos! \ud83d\udca5 Nesta postagem, vou explorar o m\u00e9todo llm.int8(), uma t\u00e9cnica de quantiza\u00e7\u00e3o que permite reduzir o tamanho dos seus modelos de aprendizado de m\u00e1quina sem sacrificar muito a precis\u00e3o. \ud83d\udcca Isso significa que voc\u00ea poder\u00e1 treinar e implantar modelos maiores e mais complexos em menos espa\u00e7o e com menos consumo de recursos! \ud83d\udcbb Vamos ver como usar llm.int8() com transformadores para quantizar um modelo e torn\u00e1-lo mais eficiente, sem perder a ess\u00eancia de sua intelig\u00eancia artificial. \ud83e\udd16",
      "end_url": "llm-int8",
      "image": "https://images.maximofn.com/llm.int8()-thumbnail.webp",
      "image_hover_path": "https://images.maximofn.com/llm.int8()-thumbnail.webp",
      "keywords_en": "llm.int8(), transformers, quantization, machine learning, artificial intelligence, INT8, FP16",
      "keywords_es": "llm.int8(), transformers, cuantizaci\u00f3n, aprendizaje autom\u00e1tico, inteligencia artificial, INT8, FP16",
      "keywords_pt": "llm.int8(), transformers, quantiza\u00e7\u00e3o, aprendizado de m\u00e1quina, intelig\u00eancia artificial, INT8, FP16",
      "title_en": "llm.int8() \u2013 8-bit Matrix Multiplication for Transformers at Scale",
      "title_es": "llm.int8() \u2013 8-bit Matrix Multiplication for Transformers at Scale",
      "title_pt": "llm.int8() \u2013 8-bit Matrix Multiplication for Transformers at Scale"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}