{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DoLa: Decodifica\u00e7\u00e3o por Camadas Contrastantes Melhora a Factualidade em Modelos de Linguagem Grandes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A medida que os LLMs aumentam de tamanho e surgem novas capacidades, temos um problema: as alucina\u00e7\u00f5es. Os autores do artigo [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883) prop\u00f5em um m\u00e9todo para evitar esse problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prop\u00f5em uma abordagem de decodifica\u00e7\u00e3o contrastiva, onde a probabilidade de sa\u00edda da pr\u00f3xima palavra \u00e9 obtida a partir da diferen\u00e7a nos logits entre uma camada superior e uma inferior. Ao enfatizar o conhecimento das camadas superiores e diminuir a import\u00e2ncia das inferiores, podemos fazer com que os LM sejam mais f\u00e1ticos e, portanto, reduzir as alucina\u00e7\u00f5es.",
        "\n",
        "Na figura a seguir, essa ideia \u00e9 ilustrada. Enquanto `Seattle` mant\u00e9m uma alta probabilidade em todas as camadas, a probabilidade da resposta correta `Olympia` aumenta ap\u00f3s as camadas superiores injetarem mais conhecimento factual. Contrastar as diferen\u00e7as entre as diferentes camadas pode revelar a resposta correta neste caso.",
        "\n",
        "![DoLa-figure1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M\u00e9todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Um LLM consiste em uma camada de embedding, v\u00e1rios transformers sequenciais e, em seguida, uma camada de sa\u00edda. O que prop\u00f5em \u00e9 medir a sa\u00edda de cada transformer por meio da diverg\u00eancia de Jensen-Shannon (JSD).",
        "\n",
        "Na figura seguinte, pode-se ver essa medida na sa\u00edda de cada transformer para uma frase de entrada no LLM. Cada coluna corresponde a um token da frase.",
        "\n",
        "![DoLa-figure2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure2.webp)",
        "\n",
        "Podem observar dois padr\u00f5es",
        "\n",
        "* O primeiro ocorre quando s\u00e3o previstas entidades de nome ou datas importantes, como `Wole Soyinka` e `1986`, que requerem conhecimento f\u00e1tico. Pode-se ver que a JSD calculada continua extremamente alta nas camadas superiores. Este padr\u00e3o indica que o modelo ainda est\u00e1 mudando suas previs\u00f5es nas \u00faltimas camadas, e potencialmente injetando mais conhecimento f\u00e1tico nas previs\u00f5es",
        " \n",
        "* O segundo ocorre quando se preveem palavras funcionais, como `was`, `the`, `to`, `in`, e os tokens copiados da pergunta de entrada, como `first Nigerian`, `Nobel Prize`. Quando esses tokens \"f\u00e1ceis\" s\u00e3o previstos, podemos observar que a JSD torna-se muito pequena a partir das camadas intermedi\u00e1rias. Essa descoberta indica que o modelo j\u00e1 decidiu qual token gerar nas camadas intermedi\u00e1rias e mant\u00e9m as distribui\u00e7\u00f5es de sa\u00edda quase inalteradas nas camadas superiores. Essa descoberta tamb\u00e9m \u00e9 consistente com as suposi\u00e7\u00f5es em LLMs de sa\u00edda precoce `Schuster et al., 2022`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando a previs\u00e3o da pr\u00f3xima palavra requer conhecimento f\u00e1tico, o LLM parece alterar as previs\u00f5es nas camadas superiores. Contrastar as camadas antes e depois de uma mudan\u00e7a s\u00fabita pode, portanto, amplificar o conhecimento que emerge das camadas superiores e fazer com que o modelo se baseie mais em seu conhecimento interno f\u00e1tico. Al\u00e9m disso, essa evolu\u00e7\u00e3o da informa\u00e7\u00e3o parece variar de um token para outro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seu m\u00e9todo requer selecionar com precis\u00e3o a camada prematura que cont\u00e9m informa\u00e7\u00f5es plaus\u00edveis mas menos factuais, que pode n\u00e3o estar sempre na mesma camada inicial. Portanto, eles prop\u00f5em encontrar essa camada prematura atrav\u00e9s de uma sele\u00e7\u00e3o din\u00e2mica da camada prematura, como visto na imagem a seguir.",
        "\n",
        "![DoLa-figure3](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure3.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sele\u00e7\u00e3o din\u00e2mica da camada prematura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para selecionar a camada prematura, eles calculam a diverg\u00eancia de Jensen-Shannon (JSD) entre as camadas intermedi\u00e1rias e a final. A camada prematura \u00e9 selecionada como a camada com o JSD mais alto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No entanto, como esse processo pode ser um pouco lento, o que fazem \u00e9 agrupar v\u00e1rias camadas para fazer menos c\u00e1lculos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contraste das previs\u00f5es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos a \u00faltima camada (camada madura) e a camada prematura, podemos contrastar as previs\u00f5es de ambas camadas. Para isso, calculam a probabilidade logar\u00edtmica do pr\u00f3ximo token na camada madura e na camada prematura. Em seguida, subtraem a probabilidade logar\u00edtmica da camada prematura da da camada madura, assim dando mais import\u00e2ncia ao conhecimento da camada madura."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Penaliza\u00e7\u00e3o por repeti\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A motiva\u00e7\u00e3o do DoLa \u00e9 diminuir a import\u00e2ncia do conhecimento lingu\u00edstico das camadas inferiores e amplificar o conhecimento f\u00e1tico do mundo real. No entanto, isso pode levar o modelo a gerar par\u00e1grafos gramaticalmente incorretos.",
        "\n",
        "Empiricamente, eles n\u00e3o observaram esse problema, mas encontraram que a distribui\u00e7\u00e3o DoLa resultante \u00e0s vezes tem uma maior tend\u00eancia a repetir frases geradas anteriormente, especialmente durante a gera\u00e7\u00e3o de longas sequ\u00eancias de racioc\u00ednio na cadeia de pensamento.",
        "\n",
        "Ent\u00e3o, eles incluem uma penaliza\u00e7\u00e3o por repeti\u00e7\u00e3o introduzida em `Keskar et al. (2019)` com `\u03b8 = 1.2` durante a decodifica\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementa\u00e7\u00e3o com transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver como implementar DoLa com a biblioteca `transformers` do Hugging Face. Para obter mais informa\u00e7\u00f5es sobre como aplicar DoLa com a biblioteca `transformers`, voc\u00ea pode consultar o seguinte [enlace](https://huggingface.co/docs/transformers/main/en/generation_strategies#dola-decoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro nos logamos no Hub, pois vamos usar o Llama 3 8B, para poder us\u00e1-lo \u00e9 necess\u00e1rio solicitar permiss\u00e3o \u00e0 Meta, ent\u00e3o para poder baix\u00e1-lo \u00e9 preciso estar logado para que saibam quem est\u00e1 fazendo o download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora instanciamos o tokenizador e o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "import torch\n",
        "\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoints, torch_dtype=compute_dtype, device_map=\"auto\")\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Atribu\u00edmos um valor fixo de semente para a reprodutibilidade do exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos os tokens de entrada para o LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'What does Darth Vader say to Luke in \"The Empire Strikes Back\"?'\n",
        "text = f\"Answer with a short answer.\\n\\nQuestion: {question}\\n\\nAnswer: \"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos agora a entrada vanilla, ou seja, sem aplicar DoLa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"No, I am your father.\" (Note: This is a famous misquote. The actual quote is \"No, I am your father\" is not in the movie. The correct quote is \"No, I am your father.\" is not\n"
          ]
        }
      ],
      "source": [
        "generate_kwargs={\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"top_p\": None,\n",
        "    \"temperature\": None\n",
        "}\n",
        "\n",
        "vanilla_output = model.generate(**inputs, **generate_kwargs)\n",
        "\n",
        "print(tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que sabe que h\u00e1 um erro famoso, mas n\u00e3o consegue dizer a frase correta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora aplicando DoLa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"No, I am your father.\" (Note: This is one of the most famous lines in movie history, and it's often misquoted as \"Luke, I am your father.\")\n"
          ]
        }
      ],
      "source": [
        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers='high', repetition_penalty=1.2)\n",
        "\n",
        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora ele consegue dizer a frase correta e o [erro famoso](https://www.bbc.co.uk/bitesize/articles/zc38kty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos fazer outro teste com outro exemplo, reinicio o notebook e uso outro modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "import torch\n",
        "\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "checkpoints = \"huggyllama/llama-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoints, torch_dtype=compute_dtype, device_map=\"auto\")\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Atribu\u00edmos um valor fixo de semente para a reprodu\u00e7\u00e3o do exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Claro, por favor, proporciona el texto markdown que deseas que traduzca al portug\u00e9s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"On what date was the Declaration of Independence officially signed?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos a sa\u00edda vanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Declaration of Independence was signed on July 4, 1776.\n",
            "What was the date of the signing of the Declaration of Independence?\n",
            "The Declaration of Independence was signed on July 4,\n"
          ]
        }
      ],
      "source": [
        "generate_kwargs={\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"top_p\": None,\n",
        "    \"temperature\": None\n",
        "}\n",
        "\n",
        "vanilla_output = model.generate(**inputs, **generate_kwargs)\n",
        "\n",
        "print(tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, gera a sa\u00edda incorretamente, pois embora seja celebrado em 4 de julho, na verdade foi assinada no [2 de agosto](https://www.nps.gov/inde/learn/historyculture/resources-declarationofindependence.htm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a provar agora com DoLa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "July 4, 1776. This is the most well-known date in U.S. history. The day has been celebrated with parades, barbeques, fireworks and festivals for hundreds of years.\n"
          ]
        }
      ],
      "source": [
        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers='high', repetition_penalty=1.2)\n",
        "\n",
        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Siga sem gerar uma sa\u00edda correta, ent\u00e3o vamos indicar que apenas contraste a camada final com as camadas 28 e 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "It was officially signed on 2 August 1776, when 56 members of the Second Continental Congress put their John Hancocks to the Declaration. The 2-page document had been written in 17\n"
          ]
        }
      ],
      "source": [
        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers=[28,30], repetition_penalty=1.2)\n",
        "\n",
        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora sim, consigo gerar a resposta correta."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "maximofn": {
      "date": "2024-08-01",
      "description_en": "Have you ever talked to an LLM and they answered you something that sounds like they've been drinking machine coffee all night long \ud83d\ude02 That's what we call a hallucination in the LLM world! But don't worry, because it's not that your language model is crazy (although it can sometimes seem that way \ud83e\udd2a). The truth is that LLMs can be a bit... creative when it comes to generating text. But thanks to DoLa, a method that uses contrast layers to improve the feasibility of LLMs, we can keep our language models from turning into science fiction writers \ud83d\ude02. In this post, I'll explain how DoLa works and show you a code example so you can better understand how to make your LLMs more reliable and less prone to making up stories. Let's save our LLMs from insanity and make them more useful! \ud83d\ude80",
      "description_es": "\u00bfAlguna vez has hablado con un LLM y te ha respondido algo que suena como si hubiera estado bebiendo caf\u00e9 de m\u00e1quina durante toda la noche? \ud83d\ude02 \u00a1Eso es lo que llamamos una alucinaci\u00f3n en el mundo de los LLMs! Pero no te preocupes, porque no es que tu modelo de lenguaje est\u00e9 loco (aunque a veces puede parecerlo \ud83e\udd2a). La verdad es que los LLMs pueden ser un poco... creativos cuando se trata de generar texto. Pero gracias a DoLa, un m\u00e9todo que utiliza capas de contraste para mejorar la factibilidad de los LLMs, podemos evitar que nuestros modelos de lenguaje se conviertan en escritores de ciencia ficci\u00f3n \ud83d\ude02. En este post, te explicar\u00e9 c\u00f3mo funciona DoLa y te mostrar\u00e9 un ejemplo de c\u00f3digo para que puedas entender mejor c\u00f3mo hacer que tus LLMs sean m\u00e1s fiables y menos propensos a inventar historias. \u00a1Vamos a salvar a nuestros LLMs de la locura y hacer que sean m\u00e1s \u00fatiles! \ud83d\ude80",
      "description_pt": "Voc\u00ea j\u00e1 conversou com um LLM e ele lhe respondeu algo que parece ter bebido caf\u00e9 de m\u00e1quina a noite toda? \ud83d\ude02 Isso \u00e9 o que chamamos de alucina\u00e7\u00e3o no mundo dos LLMs! Mas n\u00e3o se preocupe, pois n\u00e3o \u00e9 que seu modelo de linguagem esteja louco (embora \u00e0s vezes possa parecer isso \ud83e\udd2a). A verdade \u00e9 que os LLMs podem ser um pouco... criativos quando se trata de gerar texto. Mas gra\u00e7as ao DoLa, um m\u00e9todo que usa camadas de contraste para melhorar a viabilidade dos LLMs, podemos evitar que nossos modelos de linguagem se transformem em escritores de fic\u00e7\u00e3o cient\u00edfica \ud83d\ude02. Nesta publica\u00e7\u00e3o, explicarei como o DoLa funciona e mostrarei um exemplo de c\u00f3digo para que voc\u00ea possa entender melhor como tornar seus LLMs mais confi\u00e1veis e menos propensos a inventar hist\u00f3rias. Vamos salvar nossos LLMs da loucura e torn\u00e1-los mais \u00fateis! \ud83d\ude80",
      "end_url": "dola",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-thumbnail.webp",
      "keywords_en": "dola, decoding by contrasting layers, factuality, large language models, transformers, hugging face, nlp, natural language processing, machine learning, artificial intelligence",
      "keywords_es": "dola, decodificaci\u00f3n por capas contrastantes, factibilidad, grandes modelos de lenguaje, transformers, hugging face, nlp, procesamiento de lenguaje natural, aprendizaje autom\u00e1tico, inteligencia artificial",
      "keywords_pt": "dola, decodifica\u00e7\u00e3o por camadas contrastantes, factibilidade, grandes modelos de linguagem, transformers, hugging face, nlp, processamento de linguagem natural, aprendizado de m\u00e1quina, intelig\u00eancia artificial",
      "title_en": "DoLa \u2013 Decoding by Contrasting Layers Improves Factuality in Large Language Models",
      "title_es": "DoLa \u2013 Decoding by Contrasting Layers Improves Factuality in Large Language Models",
      "title_pt": "DoLa \u2013 Decoding by Contrasting Layers Improves Factuality in Large Language Models"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}