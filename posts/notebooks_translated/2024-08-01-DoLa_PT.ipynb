{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# DoLa: a decodificação por camadas contrastantes melhora a factualidade em grandes modelos de linguagem"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Entretanto, à medida que os LLMs aumentam de tamanho e surgem novos recursos, temos um problema, que é o aliasing. Os autores do artigo [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883) propõem um método para evitar esse problema."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Eles propõem uma abordagem de decodificação contrastiva, em que a probabilidade de saída da próxima palavra é obtida a partir da diferença de logits entre uma camada superior e uma inferior. Ao enfatizar o conhecimento nas camadas superiores e reduzir a ênfase no conhecimento das camadas inferiores, podemos tornar as LMs mais factuais e, assim, reduzir as alucinações.\n",
                        "\n",
                        "A figura abaixo mostra essa ideia. Embora `Seattle` mantenha uma alta probabilidade em todas as camadas, a probabilidade da resposta correta `Olympia` aumenta depois que as camadas superiores injetam mais conhecimento factual. O contraste das diferenças entre as diferentes camadas pode revelar a resposta correta nesse caso.\n",
                        "\n",
                        "![DoLa-figure1](https://maximofn.com/wp-content/uploads/2024/07/DoLa-figure1.webp)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Método"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Um LLM consiste em uma camada de incorporação, vários transformadores sequenciais e, em seguida, uma camada de saída. O que eles propõem é medir a saída de cada transformador usando a divergência de Jensen-Shannon (JSD).\n",
                        "\n",
                        "A figura a seguir mostra essa medida na saída de cada transformador para uma frase de entrada do LLM. Cada coluna corresponde a um token da frase\n",
                        "\n",
                        "![DoLa-figure2](https://maximofn.com/wp-content/uploads/2024/07/DoLa-figure2.webp)\n",
                        "\n",
                        "Dois padrões podem ser observados\n",
                        "\n",
                        " * O primeiro ocorre ao prever entidades nomeadas ou datas importantes, como `Wole Soyinka` e `1986`, que exigem conhecimento factual. É possível observar que o JSD calculado permanece extremamente alto nas camadas superiores. Esse padrão indica que o modelo continua alterando suas previsões nas camadas posteriores e, possivelmente, injetando mais conhecimento factual nas previsões.\n",
                        " \n",
                        " * A segunda ocorre ao prever palavras funcionais, como `was`, `the`, `to`, `in`, e tokens copiados da pergunta de entrada, como `first Nigerian`, `Nobel Prize`. Quando esses tokens \"fáceis\" são previstos, podemos observar que o JSD se torna muito pequeno a partir das camadas intermediárias. Essa constatação indica que o modelo já decidiu qual token gerar nas camadas intermediárias e mantém as distribuições de saída praticamente inalteradas nas camadas superiores. Essa descoberta também é consistente com as suposições nos LLMs de saída inicial `Schuster et al., 2022`."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Quando a previsão da próxima palavra requer conhecimento factual, o LLM parece alterar as previsões nas camadas superiores. O contraste das camadas antes e depois de uma mudança repentina pode, portanto, ampliar o conhecimento que emerge das camadas superiores e fazer com que o modelo confie mais em seu conhecimento factual interno. Além disso, essa evolução das informações parece variar de token para token."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Seu método requer a seleção precisa da camada prematura que contém informações plausíveis, mas menos factuais, que nem sempre podem estar na mesma camada inicial. Portanto, eles propõem encontrar essa camada prematura selecionando dinamicamente a camada prematura, conforme visto na imagem a seguir.\n",
                        "\n",
                        "![DoLa-figure3](https://maximofn.com/wp-content/uploads/2024/07/DoLa-figure3.webp)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Seleção dinâmica da camada prematura"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Para selecionar a camada prematura, eles calculam a divergência de Jensen-Shannon (JSD) entre as camadas intermediárias e a camada final. A camada prematura é selecionada como a camada com a maior JSD."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Entretanto, como esse processo pode ser um pouco lento, o que eles fazem é agrupar várias camadas para fazer menos cálculos."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Contraste de previsões"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora que temos a última camada (camada madura) e a camada prematura, podemos contrastar as previsões de ambas as camadas. Para isso, eles calculam a probabilidade de log do próximo token na camada madura e na camada prematura. Em seguida, eles subtraem a probabilidade de log da camada prematura daquela da camada madura, dando assim mais peso ao conhecimento da camada madura."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Penalidade de repetição"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "A motivação da DoLa é reduzir a ênfase no conhecimento linguístico das camadas inferiores e ampliar o conhecimento factual do mundo real. Entretanto, isso pode fazer com que o modelo gere parágrafos gramaticalmente incorretos.\n",
                        "\n",
                        "Empiricamente, eles não observaram esse problema, mas descobriram que a distribuição DoLa resultante às vezes tem uma tendência maior de repetir frases geradas anteriormente, especialmente durante a geração de longas sequências de raciocínio na cadeia de pensamento.\n",
                        "\n",
                        "Portanto, eles incluem uma penalidade de repetição introduzida em `Keskar et al. (2019)` com `θ = 1,2` durante a decodificação."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Implementação com transformadores"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos ver como implementar a DoLa com a biblioteca `transformers` da Hugging Face. Para obter mais informações sobre como implementar a DoLa com a biblioteca `transformers`, você pode consultar o seguinte [link](https://huggingface.co/docs/transformers/main/en/generation_strategies#dola-decoding)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Primeiro, fazemos o login no Hub, porque vamos usar o Llama 3 8B e, para usá-lo, precisamos pedir permissão ao Meta, portanto, para fazer o download, precisamos estar conectados para que ele saiba quem está fazendo o download."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from huggingface_hub import notebook_login\n",
                        "notebook_login()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, instanciamos o tokenizador e o modelo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
                        "import torch\n",
                        "\n",
                        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
                        "\n",
                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "tokenizer.pad_token = tokenizer.eos_token\n",
                        "model = AutoModelForCausalLM.from_pretrained(checkpoints, torch_dtype=compute_dtype, device_map=\"auto\")\n",
                        "model.config.pad_token_id = model.config.eos_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Atribuímos um valor de semente fixo para a reprodutibilidade do exemplo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "set_seed(42)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Geramos os tokens de entrada do LLM."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "question = 'What does Darth Vader say to Luke in \"The Empire Strikes Back\"?'\n",
                        "text = f\"Answer with a short answer.\\n\\nQuestion: {question}\\n\\nAnswer: \"\n",
                        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, geramos a entrada vanilla, ou seja, sem aplicar o DoLa"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    " \"No, I am your father.\" (Note: This is a famous misquote. The actual quote is \"No, I am your father\" is not in the movie. The correct quote is \"No, I am your father.\" is not\n"
                              ]
                        }
                  ],
                  "source": [
                        "generate_kwargs={\n",
                        "    \"do_sample\": False,\n",
                        "    \"max_new_tokens\": 50,\n",
                        "    \"top_p\": None,\n",
                        "    \"temperature\": None\n",
                        "}\n",
                        "\n",
                        "vanilla_output = model.generate(**inputs, **generate_kwargs)\n",
                        "\n",
                        "print(tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vemos que ele sabe que há um erro famoso, mas não diz a frase verdadeira"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora aplicando o DoLa"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    " \"No, I am your father.\" (Note: This is one of the most famous lines in movie history, and it's often misquoted as \"Luke, I am your father.\")\n"
                              ]
                        }
                  ],
                  "source": [
                        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers='high', repetition_penalty=1.2)\n",
                        "\n",
                        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora ele consegue dar a frase correta e o [famoso erro](https://www.bbc.co.uk/bitesize/articles/zc38kty)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos fazer outro teste com outro exemplo, reiniciar o notebook e usar outro modelo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
                        "import torch\n",
                        "\n",
                        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
                        "\n",
                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                        "checkpoints = \"huggyllama/llama-7b\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
                        "tokenizer.pad_token = tokenizer.eos_token\n",
                        "model = AutoModelForCausalLM.from_pretrained(checkpoints, torch_dtype=compute_dtype, device_map=\"auto\")\n",
                        "model.config.pad_token_id = model.config.eos_token_id"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Atribuímos um valor de semente fixo para a reprodutibilidade do exemplo."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "set_seed(42)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Estou escrevendo uma nova pergunta"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "text = \"On what date was the Declaration of Independence officially signed?\"\n",
                        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Geramos a saída vanilla"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "\n",
                                    "The Declaration of Independence was signed on July 4, 1776.\n",
                                    "What was the date of the signing of the Declaration of Independence?\n",
                                    "The Declaration of Independence was signed on July 4,\n"
                              ]
                        }
                  ],
                  "source": [
                        "generate_kwargs={\n",
                        "    \"do_sample\": False,\n",
                        "    \"max_new_tokens\": 50,\n",
                        "    \"top_p\": None,\n",
                        "    \"temperature\": None\n",
                        "}\n",
                        "\n",
                        "vanilla_output = model.generate(**inputs, **generate_kwargs)\n",
                        "\n",
                        "print(tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Como podemos ver, ela gera a partida errada, pois, embora seja comemorada em 4 de julho, na verdade foi assinada em [2 de julho](https://education.nationalgeographic.org/resource/signing-declaration-independence/)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Vamos experimentar o DoLa agora"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "\n",
                                    "July 4, 1776. This is the most well-known date in U.S. history. The day has been celebrated with parades, barbeques, fireworks and festivals for hundreds of years.\n"
                              ]
                        }
                  ],
                  "source": [
                        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers='high', repetition_penalty=1.2)\n",
                        "\n",
                        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Ele ainda não gera a saída correta, portanto, vamos instruí-lo a contrastar apenas a camada final com as camadas 28 e 30."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "\n",
                                    "It was officially signed on 2 August 1776, when 56 members of the Second Continental Congress put their John Hancocks to the Declaration. The 2-page document had been written in 17\n"
                              ]
                        }
                  ],
                  "source": [
                        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers=[28,30], repetition_penalty=1.2)\n",
                        "\n",
                        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Agora, ele consegue gerar a resposta correta"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "nlp_",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.9"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
