{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG: Fundamentos e RAG ing\u00eanuo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste post, vamos ver em que consiste a t\u00e9cnica de `RAG` (`Retrieval Augmented Generation`) e como ela pode ser implementada em um modelo de linguagem. Al\u00e9m disso, faremos isso com a arquitetura de RAG mais b\u00e1sica, chamada `naive RAG`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para que saia\u514d\u8d39, em vez de usar uma conta da OpenAI (como voc\u00ea ver\u00e1 na maioria dos tutoriais), vamos usar o `API inference` do Hugging Face, que tem um free tier de 1000 requisi\u00e7\u00f5es por dia, o que \u00e9 mais do que suficiente para fazer este post.\n\n(Note: There was a mistake in the translation. \"\u514d\u8d39\" is Chinese for \"free\". The correct Portuguese word should be used instead.)\n\nHere's the corrected version:\n\nPara que saia gr\u00e1tis, em vez de usar uma conta da OpenAI (como voc\u00ea ver\u00e1 na maioria dos tutoriais), vamos usar o `API inference` do Hugging Face, que tem um free tier de 1000 requisi\u00e7\u00f5es por dia, o que \u00e9 mais do que suficiente para fazer este post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configura\u00e7\u00e3o da `API Inference` do Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder usar a `API Inference` da HuggingFace, o primeiro que voc\u00ea precisa \u00e9 ter uma conta na HuggingFace. Uma vez que voc\u00ea tenha, \u00e9 necess\u00e1rio ir para [Access tokens](https://huggingface.co/settings/keys) nas configura\u00e7\u00f5es do seu perfil e gerar um novo token.",
        "\n",
        "Temos que dar um nome. No meu caso, vou cham\u00e1-lo de `rag-fundamentos` e ativar a permiss\u00e3o `Make calls to serverless Inference API`. Isso criar\u00e1 um token que teremos que copiar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para gerenciar o token, vamos a criar um arquivo no mesmo caminho em que estamos trabalhando chamado \".env\" e vamos colocar o token que copiamos no arquivo da seguinte maneira:",
        "\n",
        "``` bash\n",
        "RAG_FUNDAMENTOS_T\u00c9CNICAS_AVAN\u00c7ADAS_TOKEN=\"hf_....\"",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para poder obter o token, precisamos ter instalado `dotenv`, que instalamos atrav\u00e9s de",
        "\n",
        "```bash\n",
        "pip install python-dotenv",
        "```\n",
        "\n",
        "E executamos o seguinte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN = os.getenv(\"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos um token, criamos um cliente. Para isso, precisamos ter a biblioteca `huggingface_hub` instalada. A instalamos atrav\u00e9s do conda ou pip.",
        "\n",
        "``` bash\n",
        "conda install -c conda-forge huggingface_hub",
        "```\n",
        "\n",
        "o",
        "\n",
        "```bash\n",
        "pip install --upgrade huggingface_hub",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora temos que escolher qual modelo vamos usar. Voc\u00ea pode ver os modelos dispon\u00edveis na p\u00e1gina de [Supported models](https://huggingface.co/docs/api-inference/supported-models) da documenta\u00e7\u00e3o da `API Inference` do Hugging Face.",
        "\n",
        "Como na hora de escrever o post, o melhor dispon\u00edvel \u00e9 `Qwen2.5-72B-Instruct`, vamos usar esse modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = \"Qwen/Qwen2.5-72B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora podemos criar o cliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<InferenceClient(model='Qwen/Qwen2.5-72B-Instruct', timeout=None)>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(api_key=RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN, model=MODEL)\n",
        "client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fazemos um teste para ver se funciona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u00a1Hola! Estoy bien, gracias por preguntar. \u00bfC\u00f3mo est\u00e1s t\u00fa? \u00bfEn qu\u00e9 puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "message = [\n",
        "\t{ \"role\": \"user\", \"content\": \"Hola, qu\u00e9 tal?\" }\n",
        "]\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "\tmessages=message, \n",
        "\ttemperature=0.5,\n",
        "\tmax_tokens=1024,\n",
        "\ttop_p=0.7,\n",
        "\tstream=False\n",
        ")\n",
        "\n",
        "response = stream.choices[0].message.content\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O que \u00e9 `RAG`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`RAG` s\u00e3o as siglas de `Retrieval Augmented Generation`, \u00e9 uma t\u00e9cnica criada para obter informa\u00e7\u00f5es de documentos. Embora os LLMs possam ser muito poderosos e ter muito conhecimento, nunca ser\u00e3o capazes de responder sobre documentos privados, como relat\u00f3rios da sua empresa, documenta\u00e7\u00e3o interna, etc. Por isso foi criado o `RAG`, para poder usar esses LLMs nessa documenta\u00e7\u00e3o privada.",
        "\n",
        "![O que \u00e9 RAG?](https://images.maximofn.com/RAG.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A ideia consiste em um usu\u00e1rio fazer uma pergunta sobre essa documenta\u00e7\u00e3o privada, o sistema \u00e9 capaz de obter a parte da documenta\u00e7\u00e3o onde est\u00e1 a resposta para essa pergunta, passa-se ao LLM a pergunta e a parte da documenta\u00e7\u00e3o e o LLM gera a resposta para o usu\u00e1rio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Como a informa\u00e7\u00e3o \u00e9 armazenada?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u00c9 sabido, e se voc\u00ea n\u00e3o sabia, vou te contar agora, que os LLMs t\u00eam um limite de informa\u00e7\u00e3o que podem receber, a isso se chama janela de contexto. Isso \u00e9 devido \u00e0s arquiteturas internas dos LLMs que, no momento, n\u00e3o v\u00eam ao caso. Mas o importante \u00e9 que n\u00e3o se pode passar um documento e uma pergunta assim, porque \u00e9 prov\u00e1vel que o LLM n\u00e3o seja capaz de processar toda essa informa\u00e7\u00e3o.",
        "\n",
        "Nos casos em que se passa mais informa\u00e7\u00e3o do que o contexto da sua janela permite, geralmente acontece que o LLM n\u00e3o presta aten\u00e7\u00e3o ao final da entrada. Imagine que voc\u00ea pergunte ao LLM algo sobre seu documento, e essa informa\u00e7\u00e3o esteja no final do documento e o LLM n\u00e3o a leia.",
        "\n",
        "Por isso, o que se faz \u00e9 dividir a documenta\u00e7\u00e3o em blocos chamados `chunk`s. Dessa forma, a documenta\u00e7\u00e3o \u00e9 armazenada em um monte de `chunk`s, que s\u00e3o peda\u00e7os dessa documenta\u00e7\u00e3o. Assim, quando o usu\u00e1rio faz uma pergunta, o `chunk` no qual est\u00e1 a resposta para essa pergunta \u00e9 passado ao LLM.",
        "\n",
        "Al\u00e9m de dividir a documenta\u00e7\u00e3o em `chunk`s, esses s\u00e3o convertidos em embeddings, que s\u00e3o representa\u00e7\u00f5es num\u00e9ricas dos `chunk`s. Isso \u00e9 feito porque os LLMs na verdade n\u00e3o entendem texto, mas sim n\u00fameros, e os `chunk`s s\u00e3o convertidos em n\u00fameros para que o LLM possa entend\u00ea-los. Se quiser entender mais sobre os embeddings, voc\u00ea pode ler meu post sobre [transformers](https://www.maximofn.com/transformers) no qual explico como funcionam os transformers, que \u00e9 a arquitetura por tr\u00e1s dos LLMs. Voc\u00ea tamb\u00e9m pode ler meu post sobre [ChromaDB](https://www.maximofn.com/chromadb) onde explico como os embeddings s\u00e3o armazenados em um banco de dados vetorial. E seria interessante voc\u00ea ler meu post sobre a biblioteca [HuggingFace Tokenizers](https://www.maximofn.com/hugging-face-tokenizers) na qual se explica como o texto \u00e9 tokenizado, que \u00e9 o passo anterior \u00e0 gera\u00e7\u00e3o dos embeddings.",
        "\n",
        "![RAG - embeddings](https://images.maximofn.com/RAG-embeddings.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Como obter o `chunk` correto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dissemos que a documenta\u00e7\u00e3o \u00e9 dividida em `chunk`s e o `chunk` no qual est\u00e1 a resposta \u00e0 pergunta do usu\u00e1rio \u00e9 passado para o LLM. Mas, como se sabe em qual `chunk` est\u00e1 a resposta? Para isso, convertemos a pergunta do usu\u00e1rio em um embedding e calculamos a similaridade entre o embedding da pergunta e os embeddings dos `chunk`s. Dessa forma, o `chunk` com maior similaridade \u00e9 o que \u00e9 passado para o LLM.",
        "\n",
        "![](https://images.maximofn.com/rag-chunk_retreival.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vamos revisar o que \u00e9 `RAG`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De um lado temos o `retrieval`, que \u00e9 obter o `chunk` correto da documenta\u00e7\u00e3o, do outro lado temos o `augmented`, que \u00e9 passar para o LLM a pergunta do usu\u00e1rio e o `chunk`, e por \u00faltimo temos o `generation`, que \u00e9 obter a resposta gerada pelo LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base de dados vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vimos que a documenta\u00e7\u00e3o se divide em `chunk`s e \u00e9 armazenada em um banco de dados vetorial, portanto precisamos usar um. Para este post, vou usar [ChromaDB](https://www.trychroma.com/), que \u00e9 um banco de dados vetorial bastante usado e, al\u00e9m disso, tenho um [post](https://www.maximofn.com/chromadb) no qual explico como ele funciona."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ent\u00e3o primeiro precisamos instalar a biblioteca do ChromaDB, para isso a instalamos com Conda ou com pip",
        "\n",
        "```bash\n",
        "conda install conda-forge::chromadb",
        "```\n",
        "\n",
        "o",
        "\n",
        "```bash\n",
        "pip install chromadb",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fun\u00e7\u00e3o de embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissemos, tudo vai se basear em embeddings. Portanto, o primeiro passo \u00e9 criar uma fun\u00e7\u00e3o para obter embeddings de um texto. Vamos usar o modelo `sentence-transformers/all-MiniLM-L6-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "      \n",
        "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
        "    api_key=RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN,\n",
        "    model_name=EMBEDDING_MODEL\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testamos a fun\u00e7\u00e3o de embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = huggingface_ef([\"Hello, how are you?\",])\n",
        "embedding[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos um embedding de dimens\u00e3o 384. Embora a miss\u00e3o deste post n\u00e3o seja explicar os embeddings, em resumo, nossa fun\u00e7\u00e3o de embedding categorizou a frase `Hello, how are you?` em um espa\u00e7o de 384 dimens\u00f5es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cliente ChromaDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos nossa fun\u00e7\u00e3o de embedding, podemos criar um cliente de ChromaDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro criamos uma pasta onde ser\u00e1 salva a base de dados vetorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "      \n",
        "chroma_path = Path(\"chromadb_persisten_storage\")\n",
        "chroma_path.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos o cliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chromadb import PersistentClient\n",
        "\n",
        "chroma_client = PersistentClient(path = str(chroma_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cole\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando temos o cliente do ChromaDB, a pr\u00f3xima coisa que precisamos fazer \u00e9 criar uma cole\u00e7\u00e3o. Uma cole\u00e7\u00e3o \u00e9 um conjunto de vetores, no nosso caso os `chunks` da documenta\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O criamos indicando a fun\u00e7\u00e3o de embedding que vamos usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "collection_name = \"document_qa_collection\"\n",
        "collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=huggingface_ef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregamento de documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que criamos a base de dados vetorial, temos que dividir a documenta\u00e7\u00e3o em `chunks` e salv\u00e1-los na base de dados vetorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fun\u00e7\u00e3o de carregamento de documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro criamos uma fun\u00e7\u00e3o para carregar todos os documentos `.txt` de um diret\u00f3rio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_one_document_from_directory(directory, file):\n",
        "    with open(os.path.join(directory, file), \"r\") as f:\n",
        "        return {\"id\": file, \"text\": f.read()}\n",
        "\n",
        "def load_documents_from_directory(directory):\n",
        "    documents = []\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith(\".txt\"):\n",
        "            documents.append(load_one_document_from_directory(directory, file))\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fun\u00e7\u00e3o para dividir a documenta\u00e7\u00e3o em `chunk`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que temos os documentos, os dividimos em `chunks`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - chunk_overlap\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fun\u00e7\u00e3o para gerar embeddings de um `chunk`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos os `chunk`s, geramos os `embedding`s de cada um deles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mais tarde veremos por que, mas para gerar os embeddings vamos fazer isso localmente e n\u00e3o atrav\u00e9s da API do Hugging Face. Para isso, precisamos ter instalado [PyTorch](https://pytorch.org) e `sentence-transformers`, para isso fazemos",
        "\n",
        "``` bash\n",
        "pip install -U sentence-transformers",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL).to(device)\n",
        "\n",
        "def get_embeddings(text):\n",
        "    try:\n",
        "        embedding = embedding_model.encode(text, device=device)\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos testar agora essa fun\u00e7\u00e3o de embeddings localmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Hello, how are you?\"\n",
        "embedding = get_embeddings(text)\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que obtemos um embedding da mesma dimens\u00e3o que quando o faz\u00edamos com a API do Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O modelo `sentence-transformers/all-MiniLM-L6-v2` tem apenas 22M de par\u00e2metros, portanto voc\u00ea ser\u00e1 capaz de execut\u00e1-lo em qualquer GPU. Mesmo se n\u00e3o tiver GPU, voc\u00ea ainda poder\u00e1 execut\u00e1-lo em uma CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O LLM que vamos a usar para gerar as respostas, que \u00e9 o `Qwen2.5-72B-Instruct`, como seu nome indica, \u00e9 um modelo de 72B de par\u00e2metros, por isso este modelo n\u00e3o pode ser executado em qualquer GPU e em uma CPU seria impens\u00e1vel devido \u00e0 lentid\u00e3o. Por isso, este LLM sim o usaremos atrav\u00e9s da API, mas na hora de gerar os `embedding`s podemos fazer localmente sem problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentos com os quais vamos testar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para fazer todas essas verifica\u00e7\u00f5es, baixei o conjunto de dados [aws-case-studies-and-blogs](https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs) e o coloquei na pasta `rag-txt_dataset`. Com os seguintes comandos, explico como baix\u00e1-lo e descompact\u00e1-lo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos a pasta onde vamos baixar os documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir rag_txt_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baixamos o `.zip` com os documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o ./rag_txt_dataset/archive.zip https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Descompactamos o .zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  rag_txt_dataset/archive.zip\n",
            "  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  \n",
            "  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/6sense Case Study.txt  \n",
            "  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/AEON Case Study.txt  \n",
            "  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  \n",
            "  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  \n",
            "  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  \n",
            "  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt  \n",
            "  ...\n",
            "  inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt  \n",
            "  inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/iptiQ Case Study.txt  \n",
            "  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/myposter Case Study.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip rag_txt_dataset/archive.zip -d rag_txt_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apagamos o .zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm rag_txt_dataset/archive.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver o que ficou."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'23andMe Case Study _ Life Sciences _ AWS.txt'\n",
            "'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt'\n",
            "'54gene _ Case Study _ AWS.txt'\n",
            "'6sense Case Study.txt'\n",
            "'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt'\n",
            "'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt'\n",
            "'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt'\n",
            "'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt'\n",
            "'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt'\n",
            "'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt'\n",
            "'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt'\n",
            "'Actuate AI Case study.txt'\n",
            "'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt'\n",
            "'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt'\n",
            "'AEON Case Study.txt'\n",
            "'ALTBalaji _ Amazon Web Services.txt'\n",
            "'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt'\n",
            "'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt'\n",
            "'Anghami Case Study.txt'\n",
            "'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt'\n",
            "...\n",
            "'What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt'\n",
            "'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt'\n",
            " Windsor.txt\n",
            "'Wireless Car Case Study _ AWS IoT Core _ AWS.txt'\n",
            "'Yamato Logistics (HK) case study.txt'\n",
            "'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt'\n",
            "'Zoox Case Study _ Automotive _ AWS.txt'\n"
          ]
        }
      ],
      "source": [
        "!ls rag_txt_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A criar os `chunk`s!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Listamos os documentos com a fun\u00e7\u00e3o que hav\u00edamos criado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = \"rag_txt_dataset\"\n",
        "documents = load_documents_from_directory(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verificamos que fizemos corretamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt\n",
            "Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt\n",
            "Windsor.txt\n",
            "Bank of Montreal Case Study _ AWS.txt\n",
            "The Mill Adventure Case Study.txt\n",
            "Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt\n",
            "Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt\n",
            "THREAD _ Life Sciences _ AWS.txt\n",
            "Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt\n",
            "Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt\n"
          ]
        }
      ],
      "source": [
        "for document in documents[0:10]:\n",
        "    print(document[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos os `chunk`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunked_documents = []\n",
        "for document in documents:\n",
        "    chunks = split_text(document[\"text\"])\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunked_documents.append({\"id\": f\"{document['id']}_{i}\", \"text\": chunk})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3611"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunked_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, h\u00e1 3611 `chunk`s. Como o limite di\u00e1rio da API do Hugging Face s\u00e3o 1000 chamadas na conta gratuita, se quisermos criar embeddings de todos os `chunk`s, acabar\u00edamos com as chamadas dispon\u00edveis e al\u00e9m disso n\u00e3o poder\u00edamos criar embeddings de todos os `chunk`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reiteramos, este modelo de embeddings \u00e9 muito pequeno, com apenas 22M de par\u00e2metros, portanto pode ser executado em quase qualquer computador, mais r\u00e1pido ou mais devagar, mas pode ser executado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como s\u00f3 vamos a criar os embeddings dos `chunk`s uma vez, mesmo que n\u00e3o tenhamos um computador muito potente e leve muito tempo, apenas ser\u00e1 executado uma vez. Depois, quando quisermos fazer perguntas sobre a documenta\u00e7\u00e3o, geraremos os embeddings do prompt com a API da Hugging Face e usaremos o LLM com a API. Portanto, s\u00f3 teremos que passar pelo processo de gera\u00e7\u00e3o dos embeddings dos `chunk`s uma vez."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos os embeddings dos `chunk`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u00daltima biblioteca que precisaremos instalar. Como o processo de gera\u00e7\u00e3o dos embeddings dos `chunk`s ser\u00e1 lento, vamos instalar `tqdm` para mostrar uma barra de progresso. Instalamos com Conda ou pip, conforme sua prefer\u00eancia.",
        "\n",
        "```bash\n",
        "conda install conda-forge::tqdm",
        "```\n",
        "\n",
        "o",
        "\n",
        "```bash\n",
        "pip install tqdm",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos os embeddings dos `chunk`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3611/3611 [00:16<00:00, 220.75it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "progress_bar = tqdm.tqdm(chunked_documents)\n",
        "\n",
        "for chunk in progress_bar:\n",
        "    embedding = get_embeddings(chunk[\"text\"])\n",
        "    if embedding is not None:\n",
        "        chunk[\"embedding\"] = embedding\n",
        "    else:\n",
        "        print(f\"Error with document {chunk['id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,\n",
            "\n",
            "text: Reducing Virtual Machines from 40 to 12\n",
            "The founders of BNS had been contemplating a migration from the company\u2019s on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.\n",
            "Fran\u00e7ais\n",
            "Configures security according to cloud best practices\n",
            "Clive Pereira, R&D director at BNS Group, explains, \u201cThe database that records Praisal\u2019s SMS traffic resides in Praisal\u2019s AWS environment. Praisal can now run complete analytics across its data and gain insights into what\u2019s happening with its SMS traffic, which is a real game-changer for the organization.\u201d\u00a0 \n",
            "Espa\u00f1ol\n",
            " AWS ISV Accelerate Program\n",
            " Receiving Strategic, Foundational Support from ISV Specialists\n",
            " Learn More\n",
            "The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.\u201d \n",
            "\u65e5\u672c\u8a9e\n",
            "  Contact Sales \n",
            "BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,\n",
            "\n",
            "embedding shape: (384,)\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "\n",
        "idx = randint(0, len(chunked_documents))\n",
        "print(f\"Chunk id: {chunked_documents[idx]['id']},\\n\\ntext: {chunked_documents[idx]['text']},\\n\\nembedding shape: {chunked_documents[idx]['embedding'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carregar os `chunk`s na base de dados vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que temos todos os chunks gerados, os carregamos na base de dados vetorial. Voltamos a usar `tqdm` para mostrar uma barra de progresso, pois isso tamb\u00e9m ser\u00e1 lento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3611/3611 [00:59<00:00, 60.77it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "progress_bar = tqdm.tqdm(chunked_documents)\n",
        "\n",
        "for chunk in progress_bar:\n",
        "    collection.upsert(\n",
        "        ids=[chunk[\"id\"]],\n",
        "        documents=chunk[\"text\"],\n",
        "        embeddings=chunk[\"embedding\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos a base de dados vetorial, podemos fazer perguntas \u00e0 documenta\u00e7\u00e3o. Para isso, precisamos de uma fun\u00e7\u00e3o que nos retorne o `chunk` correto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obter o `chunk` correto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora precisamos de uma fun\u00e7\u00e3o que nos retorne o `chunk` correto, vamos cri\u00e1-la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_documents(query, k=5):\n",
        "    results = collection.query(query_texts=query, n_results=k)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, criamos uma `query`.",
        "\n",
        "Para gerar a query, selecionei aleatoriamente o documento `Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt`, passei-o para um LLM e pedi que gerasse uma pergunta sobre o documento. A pergunta que foi gerada \u00e9",
        "\n",
        "```\n",
        "Como a Neeva utilizou o Karpenter e as Inst\u00e2ncias Spot do Amazon EC2 para melhorar sua gest\u00e3o de infraestrutura e otimiza\u00e7\u00e3o de custos?",
        "```\n",
        "\n",
        "Ent\u00e3o obtemos os `chunks` mais relevantes diante dessa pergunta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?\"\n",
        "top_chunks = get_top_k_documents(query=query, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver quais `chunk`s nos foram devolvidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937\n",
            "Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982\n",
            "Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777\n",
            "Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486\n",
            "Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(top_chunks[\"ids\"][0])):\n",
        "    print(f\"Rank {i+1}: {top_chunks['ids'][0][i]}, distance: {top_chunks['distances'][0][i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como eu havia dito, o documento que escolhi aleatoriamente era `Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt` e, como se pode ver, os `chunk`s que nos foram retornados s\u00e3o desse documento. Isso significa que, dos mais de 3000 `chunk`s que havia no banco de dados, ele foi capaz de me retornar os `chunk`s mais relevantes para essa pergunta, parece que isso funciona!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gerar a resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como j\u00e1 temos os `chunk`s mais relevantes, passamo-los ao LLM, juntamente com a pergunta, para que ele gere uma resposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(query, relevant_chunks, temperature=0.5, max_tokens=1024, top_p=0.7, stream=False):\n",
        "    context = \"\\n\\n\".join([chunk for chunk in relevant_chunks])\n",
        "    prompt = f\"You are an assistant for question-answering. You have to answer the following question:\\n\\n{query}\\n\\nAnswer the question with the following information:\\n\\n{context}\"\n",
        "    message = [\n",
        "        { \"role\": \"user\", \"content\": prompt }\n",
        "    ]\n",
        "    stream = client.chat.completions.create(\n",
        "        messages=message, \n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        stream=stream,\n",
        "    )\n",
        "    response = stream.choices[0].message.content\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testamos a fun\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here\u2019s how:\n",
            "\n",
            "### Early Collaboration with Karpenter\n",
            "In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.\n",
            "\n",
            "### Combining Spot Instances and On-Demand Instances\n",
            "Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.\n",
            "\n",
            "### Flexibility and Instance Diversification\n",
            "According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.\n",
            "\n",
            "### Improved Scalability and Agility\n",
            "By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:\n",
            "- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.\n",
            "- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.\n",
            "\n",
            "### Enhanced Development Cycles\n",
            "The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.\n",
            "\n",
            "### Cost Savings and Budget Control\n",
            "Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.\n",
            "\n",
            "### Future Plans\n",
            "Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, \"The bulk of our compute is or will be managed using Karpenter going forward.\"\n",
            "\n",
            "### Conclusion\n",
            "By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.\n"
          ]
        }
      ],
      "source": [
        "response = generate_response(query, top_chunks[\"documents\"][0])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando pedi ao LLM para gerar uma pergunta sobre o documento, tamb\u00e9m pedi que gerasse a resposta correta. Esta \u00e9 a resposta que o LLM me deu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` text\n",
        "A Neeva utilizou o Karpenter e as Inst\u00e2ncias Spot do Amazon EC2 para melhorar sua gest\u00e3o de infraestrutura e otimiza\u00e7\u00e3o de custos de v\u00e1rias maneiras:",
        "\n",
        "Gest\u00e3o Simplificada de Inst\u00e2ncias:",
        "\n",
        "Karpenter: Ao adotar o Karpenter, a Neeva simplificou o processo de provisionamento e gerenciamento de recursos computacionais para seus clusters do Amazon EKS. O Karpenter provisiona e desprovisiona inst\u00e2ncias automaticamente com base na carga de trabalho, eliminando a necessidade de configura\u00e7\u00f5es manuais e reduzindo a complexidade de compreender diferentes inst\u00e2ncias computacionais.",
        "Inst\u00e2ncias Spot: A Neeva utilizou Inst\u00e2ncias Spot do Amazon EC2, que s\u00e3o capacidade n\u00e3o utilizada do EC2 dispon\u00edvel com um desconto significativo (at\u00e9 90% de economia de custos). Isso permitiu \u00e0 empresa controlar os custos enquanto atendia aos seus requisitos de desempenho.",
        "Escalabilidade Aumentada:",
        "\n",
        "Karpenter: A capacidade do Karpenter de escalar recursos dinamicamente permitiu que a Neeva iniciasse novas inst\u00e2ncias rapidamente, permitindo que a empresa iterasse com maior velocidade e executasse mais experimentos em menos tempo.",
        "Inst\u00e2ncias Spot: O uso de Inst\u00e2ncias Spot proporcionou flexibilidade e diversifica\u00e7\u00e3o de inst\u00e2ncias, facilitando o escalonamento dos recursos de computa\u00e7\u00e3o da Neeva de forma eficiente.",
        "Produtividade Melhorada:",
        "\n",
        "Karpenter: Ao democratizar as altera\u00e7\u00f5es de infraestrutura, o Karpenter permitiu que qualquer engenheiro modificasse as configura\u00e7\u00f5es do Kubernetes, reduzindo a depend\u00eancia de expertise especializada. Isso economizou at\u00e9 100 horas por semana de tempo de espera em administra\u00e7\u00e3o de sistemas para a equipe da Neeva.",
        "Inst\u00e2ncias Spot: A capacidade de provisionar e desprovisionar rapidamente Inst\u00e2ncias Spot reduziu os atrasos no pipeline de desenvolvimento, garantindo que os trabalhos n\u00e3o ficassem travados devido \u00e0 falta de recursos dispon\u00edveis.",
        "Efici\u00eancia Custo:",
        "\n",
        "Karpenter: As melhores pr\u00e1ticas do Karpenter para inst\u00e2ncias Spot, incluindo flexibilidade e diversifica\u00e7\u00e3o de inst\u00e2ncias, ajudaram a Neeva a usar essas inst\u00e2ncias de forma mais eficaz, permanecendo dentro do or\u00e7amento.",
        "Inst\u00e2ncias Spot: As economias de custos com o uso de Inst\u00e2ncias Spot permitiram que a Neeva executasse trabalhos em larga escala, como indexa\u00e7\u00e3o, por quase o mesmo custo, mas em um tempo muito menor. Por exemplo, a Neeva reduziu seus trabalhos de indexa\u00e7\u00e3o de 18 horas para apenas 3 horas.",
        "Melhor Utiliza\u00e7\u00e3o de Recursos:",
        "\n",
        "Karpenter: O Karpenter proporcionou uma melhor visibilidade sobre o uso de recursos computacionais, permitindo que a Neeva rastreasse e otimizasse seu consumo de recursos mais de perto.",
        "Inst\u00e2ncias Spot: A combina\u00e7\u00e3o do Karpenter e das Inst\u00e2ncias Spot permitiu que a Neeva executasse modelos de linguagem grandes de forma mais eficiente, melhorando a experi\u00eancia de busca para seus usu\u00e1rios.",
        "Em resumo, a ado\u00e7\u00e3o do Karpenter e das Inst\u00e2ncias Spot do Amazon EC2 pela Neeva melhorou significativamente sua gest\u00e3o de infraestrutura, otimiza\u00e7\u00e3o de custos e efici\u00eancia geral no desenvolvimento, permitindo que a empresa oferecesse melhores experi\u00eancias de pesquisa sem an\u00fancios aos seus usu\u00e1rios.",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E esta tem sido a resposta gerada pelo nosso `RAG`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` text\n",
        "Neeva, um mecanismo de busca nativo em nuvem e sem an\u00fancios fundado em 2019, aproveitou o Karpenter e as Inst\u00e2ncias Spot do Amazon EC2 para melhorar significativamente sua gest\u00e3o de infraestrutura e otimiza\u00e7\u00e3o de custos. Eis como:",
        "\n",
        "### Colabora\u00e7\u00e3o Inicial com o Karpenter",
        "No final de 2021, a Neeva come\u00e7ou a trabalhar em estreita colabora\u00e7\u00e3o com a equipe do Karpenter, experimentando e contribuindo com corre\u00e7\u00f5es para uma vers\u00e3o inicial do Karpenter. Essa colabora\u00e7\u00e3o permitiu que a Neeva integrasse o Karpenter ao seu painel do Kubernetes, possibilitando \u00e0 empresa coletar m\u00e9tricas valiosas sobre uso e desempenho.",
        "\n",
        "### Combinando Inst\u00e2ncias Spot e Inst\u00e2ncias On-Demand",
        "A Neeva executa seus jobs em larga escala, o que pode levar a custos significativos. Para gerenciar esses custos de forma eficaz, a empresa adotou uma combina\u00e7\u00e3o de Amazon EC2 Spot Instances e On-Demand Instances. As Spot Instances permitem que a Neeva lance lances sobre capacidade EC2 n\u00e3o utilizada, frequentemente por uma fra\u00e7\u00e3o do pre\u00e7o On-Demand, enquanto as On-Demand Instances fornecem a necess\u00e1ria confiabilidade para pipelines cr\u00edticos.",
        "\n",
        "### Flexibilidade e Diversifica\u00e7\u00e3o de Inst\u00e2ncias",
        "De acordo com Mohit Agarwal, l\u00edder de engenharia de infraestrutura da Neeva, a ado\u00e7\u00e3o de melhores pr\u00e1ticas para Inst\u00e2ncias Spot pelo Karpenter, incluindo flexibilidade e diversifica\u00e7\u00e3o de inst\u00e2ncias, foi crucial. Essa abordagem garante que a Neeva possa ajustar dinamicamente seus recursos de computa\u00e7\u00e3o para atender \u00e0s cargas de trabalho vari\u00e1veis, minimizando custos.",
        "\n",
        "### Melhor Escalabilidade e Agilidade",
        "Ao usar o Karpenter para provisionar recursos de infraestrutura para seus clusters do Amazon EKS, a Neeva alcan\u00e7ou v\u00e1rios benef\u00edcios importantes:",
        "- **Escalabilidade**: O Neeva pode escalar seus recursos de computa\u00e7\u00e3o para cima ou para baixo conforme necess\u00e1rio, garantindo que sempre tenha a capacidade necess\u00e1ria para lidar com suas cargas de trabalho.",
        "- **Agilidade**: A empresa pode iterar rapidamente e democratizar as mudan\u00e7as de infraestrutura, reduzindo o tempo gasto com a administra\u00e7\u00e3o do sistema em at\u00e9 100 horas por semana.",
        "\n",
        "### Ciclos de Desenvolvimento Aperfei\u00e7oados",
        "A integra\u00e7\u00e3o do Karpenter e das Inst\u00e2ncias Spot tamb\u00e9m acelerou os ciclos de desenvolvimento da Neeva. A empresa agora pode lan\u00e7ar novos recursos e melhorias mais rapidamente, o que \u00e9 essencial para manter uma vantagem competitiva no mercado de motores de busca.",
        "\n",
        "### Economia de Custos e Controle Or\u00e7ament\u00e1rio",
        "Usando Inst\u00e2ncias Spot, a Neeva tem conseguido permanecer dentro do seu or\u00e7amento enquanto atende aos seus requisitos de desempenho. Essa otimiza\u00e7\u00e3o de custos \u00e9 crucial para uma empresa que prioriza experi\u00eancias centradas no usu\u00e1rio e n\u00e3o possui incentivos conflitantes provenientes da publicidade.",
        "\n",
        "### Planos Futuros",
        "A Neeva est\u00e1 comprometida em continuar sua inova\u00e7\u00e3o e expans\u00e3o. A empresa planeja lan\u00e7ar em novas regi\u00f5es e melhorar ainda mais seu mecanismo de busca, mantendo a efici\u00eancia de custos. Como Mohit Agarwal observa, \"A maior parte do nosso compute ser\u00e1 gerenciada usando Karpenter a partir de agora.\"",
        "\n",
        "### Conclus\u00e3o",
        "Ao aproveitar o Karpenter e as Inst\u00e2ncias Spot do Amazon EC2, a Neeva n\u00e3o apenas otimizou seus custos de infraestrutura, mas tamb\u00e9m melhorou sua escalabilidade, agilidade e velocidade de desenvolvimento. Esta abordagem estrat\u00e9gica posicionou a Neeva para fornecer experi\u00eancias de pesquisa de alta qualidade e sem an\u00fancios aos seus usu\u00e1rios, mantendo um forte foco no controle de custos e inova\u00e7\u00e3o.",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portanto, podemos concluir que o `RAG` funcionou corretamente!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limites de naive RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissemos, hoje explicamos `naive RAG`, que \u00e9 a arquitetura mais simples do RAG, mas tem suas limita\u00e7\u00f5es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Arquitetura Naive RAG](https://images.maximofn.com/naive_RAG_architecture.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As limita\u00e7\u00f5es desta arquitetura s\u00e3o:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limites na busca de informa\u00e7\u00f5es (retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Conhecimento limitado do contexto e da documenta\u00e7\u00e3o: Quando o sistema de RAG ing\u00eanuo busca os chunks, ele procura aqueles que t\u00eam um significado sem\u00e2ntico similar ao prompt, mas n\u00e3o \u00e9 capaz de saber quais s\u00e3o os mais relevantes para a pergunta do usu\u00e1rio, ou quais s\u00e3o os que possuem informa\u00e7\u00f5es mais atualizadas, ou se sua informa\u00e7\u00e3o \u00e9 mais correta do que a de outros chunks. Por exemplo, se um usu\u00e1rio perguntar sobre os problemas dos ado\u00e7antes no sistema digestivo, o RAG ing\u00eanuo pode retornar documentos sobre ado\u00e7antes ou sobre o sistema digestivo, mas n\u00e3o \u00e9 capaz de saber que os documentos sobre o sistema digestivo s\u00e3o os mais relevantes para a pergunta do usu\u00e1rio. Outro exemplo \u00e9 se o usu\u00e1rio perguntar sobre os \u00faltimos avan\u00e7os na IA, mas o RAG ing\u00eanuo n\u00e3o \u00e9 capaz de saber quais s\u00e3o os \u00faltimos papers da base de dados.",
        "\n",
        "* N\u00e3o h\u00e1 uma sincroniza\u00e7\u00e3o entre o retrieval e o gerador. Como vimos, s\u00e3o dois sistemas independentes; de um lado, o retrieval busca os documentos mais semelhantes \u00e0 pergunta do usu\u00e1rio, e esses documentos s\u00e3o passados ao gerador, que gera uma resposta.",
        "\n",
        "* Escalabilidade ineficiente para grandes bancos de dados. Como a recupera\u00e7\u00e3o busca os documentos com maior similaridade sem\u00e2ntica em toda a base de dados, quando esta fica muito grande, podemos ter tempos de pesquisa muito longos.",
        "\n",
        "* Pouca adapta\u00e7\u00e3o \u00e0 pergunta do usu\u00e1rio. Se o usu\u00e1rio fizer uma pergunta que envolva v\u00e1rios documentos, ou seja, n\u00e3o h\u00e1 nenhum documento que contenha toda a informa\u00e7\u00e3o da pergunta do usu\u00e1rio, o sistema recuperar\u00e1 todos esses documentos e os passar\u00e1 para o gerador, que pode us\u00e1-los ou n\u00e3o. Ou, em um caso pior, pode deixar de lado algum documento relevante para gerar a resposta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limites na gera\u00e7\u00e3o de respostas (generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* O modelo poderia alucinar respostas mesmo ao fornecer informa\u00e7\u00f5es relevantes.",
        "\n",
        "* O modelo pode estar limitado por quest\u00f5es relacionadas a \u00f3dio, discrimina\u00e7\u00e3o, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ultrapassar esses limites, geralmente s\u00e3o utilizadas t\u00e9cnicas como o",
        "\n",
        "* Pr\u00e9-recupera\u00e7\u00e3o: Que inclui t\u00e9cnicas para melhorar a indexa\u00e7\u00e3o, tornando a busca de informa\u00e7\u00f5es mais eficiente. Ou t\u00e9cnicas como a melhoria da pergunta do usu\u00e1rio para que o retrieval possa encontrar os documentos mais relevantes.",
        "\n",
        "* P\u00f3s-recupera\u00e7\u00e3o: Aqui s\u00e3o utilizadas t\u00e9cnicas como o re-ranqueamento dos documentos, que \u00e9 uma t\u00e9cnica usada para melhorar a busca por informa\u00e7\u00f5es relevantes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "maximofn": {
      "date": "2024-10-23",
      "description_en": "Forget about Ctrl+F! \ud83e\udd2f With RAG, your documents will answer your questions directly. \ud83d\ude0e Step-by-step tutorial with Hugging Face and ChromaDB. Unleash the power of AI (and show off to your friends)! \ud83d\udcaa",
      "description_es": "\u00a1Olv\u00eddate de Ctrl+F! \ud83e\udd2f Con RAG, tus documentos responder\u00e1n a tus preguntas directamente. \ud83d\ude0e Tutorial paso a paso con Hugging Face y ChromaDB. \u00a1Libera el poder de la IA (y presume con tus amigos)! \ud83d\udcaa",
      "description_pt": "Esque\u00e7a o Ctrl+F! \ud83e\udd2f Com RAG, seus documentos responder\u00e3o \u00e0s suas perguntas diretamente. \ud83d\ude0e Tutorial passo a passo com Hugging Face e ChromaDB. Liberte o poder da IA (e impressione seus amigos)! \ud83d\udcaa",
      "end_url": "rag-fundamentals",
      "image": "https://images.maximofn.com/rag-fundamentals.webp",
      "image_hover_path": "https://images.maximofn.com/rag-fundamentals.webp",
      "keywords_en": "rag, retriever, reader, hugging face, transformers, chromadb, vector database, question-answering, qa, nlp, natural language processing, machine learning, artificial intelligence, ai",
      "keywords_es": "rag, retriever, reader, hugging face, transformers, chromadb, base de datos vectorial, question-answering, qa, nlp, procesamiento de lenguaje natural, machine learning, inteligencia artificial, ia",
      "keywords_pt": "rag, retriever, reader, hugging face, transformers, chromadb, banco de dados vetorial, question-answering, qa, nlp, processamento de linguagem natural, machine learning, intelig\u00eancia artificial, ia",
      "title_en": "RAG fundamentals",
      "title_es": "Fundamentos de RAG",
      "title_pt": "Fundamentos de RAG"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}