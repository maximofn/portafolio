{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG: Fundamentos e RAG ingênuo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste post vamos a ver em que consiste a técnica de `RAG` (`Retrieval Augmented Generation`) e como se pode implementar em um modelo de linguagem. Além disso, o faremos com a arquitetura de RAG mais básica, chamada `naive RAG`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para que saia grátis, em vez de usar uma conta da OpenAI (como você verá na maioria dos tutoriais) vamos usar o `API inference` do Hugging Face, que tem um free tier de 1000 requests por dia, que para fazer este post é mais do que suficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração da `API Inference` do Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder usar a `API Inference` da HuggingFace, o primeiro que você precisa é ter uma conta na HuggingFace. Uma vez que você tenha, você deve ir até [Access tokens](https://huggingface.co/settings/keys) nas configurações do seu perfil e gerar um novo token.\n",
        "\n",
        "Tem que dar um nome. No meu caso, vou chamá-lo de `rag-fundamentals` e ativar a permissão `Make calls to serverless Inference API`. Isso criará um token que precisamos copiar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para gerenciar o token, vamos a criar um arquivo no mesmo caminho em que estamos trabalhando chamado \".env\" e vamos colocar o token que copiamos no arquivo da seguinte maneira:\n",
        "\n",
        "``` bash\n",
        "RAG_FUNDAMENTOS_TÉCNICAS_AVANÇADAS_TOKEN=\"hf_....\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para obter o token, precisamos ter o `dotenv` instalado, que podemos instalar através de\n",
        "\n",
        "```bash\n",
        "pip install python-dotenv\n",
        "```\n",
        "\n",
        "E executamos o seguinte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN = os.getenv(\"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos um token, criamos um cliente. Para isso, precisamos ter a biblioteca `huggingface_hub` instalada. A instalamos através do conda ou pip.\n",
        "\n",
        "``` bash\n",
        "conda install -c conda-forge huggingface_hub\n",
        "```\n",
        "\n",
        "o\n",
        "\n",
        "```bash\n",
        "pip install --upgrade huggingface_hub\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora temos que escolher qual modelo vamos usar. Você pode ver os modelos disponíveis na página de [Supported models](https://huggingface.co/docs/api-inference/supported-models) da documentação da `API Inference` do Hugging Face.\n",
        "\n",
        "Como na hora de escrever o post, o melhor disponível é `Qwen2.5-72B-Instruct`, vamos usar esse modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = \"Qwen/Qwen2.5-72B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora podemos criar o cliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<InferenceClient(model='Qwen/Qwen2.5-72B-Instruct', timeout=None)>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(api_key=RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN, model=MODEL)\n",
        "client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fazemos um teste para ver se funciona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "message = [\n",
        "\t{ \"role\": \"user\", \"content\": \"Hola, qué tal?\" }\n",
        "]\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "\tmessages=message, \n",
        "\ttemperature=0.5,\n",
        "\tmax_tokens=1024,\n",
        "\ttop_p=0.7,\n",
        "\tstream=False\n",
        ")\n",
        "\n",
        "response = stream.choices[0].message.content\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O que é `RAG`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`RAG` são as siglas de `Retrieval Augmented Generation`, é uma técnica criada para obter informações de documentos. Embora os LLMs possam ser muito poderosos e ter muito conhecimento, nunca serão capazes de responder sobre documentos privados, como relatórios da sua empresa, documentação interna, etc. Por isso foi criado o `RAG`, para poder usar esses LLMs nessa documentação privada.\n",
        "\n",
        "![O que é RAG?](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A ideia consiste em um usuário fazer uma pergunta sobre essa documentação privada, o sistema é capaz de obter a parte da documentação onde está a resposta para essa pergunta, passa-se ao LLM a pergunta e a parte da documentação e o LLM gera a resposta para o usuário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Como a informação é armazenada?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "É sabido, e se você não sabia, eu te conto agora, que os LLMs têm um limite de informação que pode ser passada para eles, a isso se chama janela de contexto. Isso é devido às arquiteturas internas dos LLMs que, no momento, não vêm ao caso. Mas o importante é que não se pode passar um documento e uma pergunta assim, porque é provável que o LLM não seja capaz de processar toda essa informação.\n",
        "\n",
        "Nos casos em que se passa mais informações do que o contexto da janela permite, geralmente acontece que o LLM não presta atenção ao final da entrada. Imagine que você pergunte ao LLM algo sobre seu documento, essa informação esteja no final do documento e o LLM não a leia.\n",
        "\n",
        "Por isso, o que se faz é dividir a documentação em blocos chamados `chunk`s. De modo que a documentação é armazenada em um monte de `chunk`s, que são pedaços dessa documentação. Assim, quando o usuário faz uma pergunta, o `chunk` no qual está a resposta para essa pergunta é passado ao LLM.\n",
        "\n",
        "Além de dividir a documentação em `chunk`s, esses são convertidos em embeddings, que são representações numéricas dos `chunk`s. Isso é feito porque os LLMs na verdade não entendem texto, mas sim números, e os `chunk`s são convertidos em números para que o LLM possa entender. Se quiser entender mais sobre os embeddings, você pode ler meu post sobre [transformers](https://www.maximofn.com/transformers) no qual explico como funcionam os transformers, que é a arquitetura por trás dos LLMs. Você também pode ler meu post sobre [ChromaDB](https://www.maximofn.com/chromadb) onde explico como os embeddings são armazenados em um banco de dados vetorial. E seria interessante você ler meu post sobre a biblioteca [HuggingFace Tokenizers](https://www.maximofn.com/hugging-face-tokenizers) na qual se explica como o texto é tokenizado, que é o passo anterior à geração dos embeddings.\n",
        "\n",
        "![RAG - embeddings](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Como obter o `chunk` correto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dissemos que a documentação é dividida em `chunk`s e que o `chunk` contendo a resposta à pergunta do usuário é passado para o LLM. Mas, como saber em qual `chunk` está a resposta? Para isso, a pergunta do usuário é convertida em um embedding, e calcula-se a similaridade entre o embedding da pergunta e os embeddings dos `chunk`s. Dessa forma, o `chunk` com maior similaridade é o que é passado para o LLM.\n",
        "\n",
        "![](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vamos revisar o que é `RAG`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De um lado temos o `retrieval`, que é obter o `chunk` correto da documentação, do outro lado temos o `augmented`, que é passar ao LLM a pergunta do usuário e o `chunk` e, por último, temos o `generation`, que é obter a resposta gerada pelo LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base de dados vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vimos que a documentação é dividida em `chunk`s e armazenada em um banco de dados vetorial, por isso precisamos usar um. Para este post, vou usar [ChromaDB](https://www.trychroma.com/), que é um banco de dados vetorial bastante usado e, além disso, tenho um [post](https://www.maximofn.com/chromadb) no qual explico como ele funciona."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Então primeiro precisamos instalar a biblioteca do ChromaDB, para isso a instalamos com Conda ou com pip\n",
        "\n",
        "```bash\n",
        "conda install conda-forge::chromadb\n",
        "```\n",
        "\n",
        "o\n",
        "\n",
        "```bash\n",
        "pip install chromadb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função de embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissemos, tudo vai se basear em embeddings. Portanto, o primeiro passo é criar uma função para obter embeddings de um texto. Vamos usar o modelo `sentence-transformers/all-MiniLM-L6-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "      \n",
        "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
        "    api_key=RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN,\n",
        "    model_name=EMBEDDING_MODEL\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testamos a função de embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = huggingface_ef([\"Hello, how are you?\",])\n",
        "embedding[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtemos um embedding de dimensão 384. Embora a missão deste post não seja explicar os embeddings, em resumo, nossa função de embedding categorizou a frase `Hello, how are you?` em um espaço de 384 dimensões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cliente ChromaDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos nossa função de embedding, podemos criar um cliente de ChromaDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro criamos uma pasta onde será salva a base de dados vetorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "      \n",
        "chroma_path = Path(\"chromadb_persisten_storage\")\n",
        "chroma_path.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos o cliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chromadb import PersistentClient\n",
        "\n",
        "chroma_client = PersistentClient(path = str(chroma_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Coleção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando temos o cliente do ChromaDB, o próximo passo é criar uma coleção. Uma coleção é um conjunto de vetores, no nosso caso os `chunks` da documentação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O criamos indicando a função de embedding que vamos usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "collection_name = \"document_qa_collection\"\n",
        "collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=huggingface_ef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregamento de documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que criamos a base de dados vetorial, temos que dividir a documentação em `chunks` e salvá-los na base de dados vetorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função de carregamento de documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro criamos uma função para carregar todos os documentos `.txt` de um diretório"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_one_document_from_directory(directory, file):\n",
        "    with open(os.path.join(directory, file), \"r\") as f:\n",
        "        return {\"id\": file, \"text\": f.read()}\n",
        "\n",
        "def load_documents_from_directory(directory):\n",
        "    documents = []\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith(\".txt\"):\n",
        "            documents.append(load_one_document_from_directory(directory, file))\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função para dividir a documentação em `chunk`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que temos os documentos, os dividimos em `chunk`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - chunk_overlap\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função para gerar embeddings de um `chunk`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos os `chunk`s, geramos os `embedding`s de cada um deles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver por quê, mas para gerar os embeddings vamos fazer isso de maneira local e não através da API do Hugging Face. Para isso, precisamos ter instalado [PyTorch](https://pytorch.org) e `sentence-transformers`, para isso fazemos\n",
        "\n",
        "``` bash\n",
        "pip install -U sentence-transformers\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL).to(device)\n",
        "\n",
        "def get_embeddings(text):\n",
        "    try:\n",
        "        embedding = embedding_model.encode(text, device=device)\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos testar agora essa função de embeddings localmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Hello, how are you?\"\n",
        "embedding = get_embeddings(text)\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que obtemos um embedding da mesma dimensão que quando o fazíamos com a API do Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O modelo `sentence-transformers/all-MiniLM-L6-v2` tem apenas 22M de parâmetros, portanto você será capaz de executá-lo em qualquer GPU. Mesmo se não tiver uma GPU, você poderá executá-lo em uma CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O LLM que vamos a usar para gerar as respostas, que é o `Qwen2.5-72B-Instruct`, como seu nome indica, é um modelo de 72B de parâmetros, por isso este modelo não pode ser executado em qualquer GPU e em uma CPU seria impensável devido à lentidão. Por isso, este LLM será usado através da API, mas na hora de gerar os `embedding`s podemos fazer isso localmente sem problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentos com os quais vamos testar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para fazer todas essas verificações, baixei o conjunto de dados [aws-case-studies-and-blogs](https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs) e o coloquei na pasta `rag-txt_dataset`. Com os seguintes comandos, explico como baixá-lo e descompactá-lo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos a pasta onde vamos baixar os documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir rag_txt_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baixamos o `.zip` com os documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o ./rag_txt_dataset/archive.zip https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Descompactamos o .zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  rag_txt_dataset/archive.zip\n",
            "  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  \n",
            "  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/6sense Case Study.txt  \n",
            "  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/AEON Case Study.txt  \n",
            "  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  \n",
            "  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  \n",
            "  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  \n",
            "  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  \n",
            "  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt  \n",
            "  ...\n",
            "  inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt  \n",
            "  inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/iptiQ Case Study.txt  \n",
            "  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  \n",
            "  inflating: rag_txt_dataset/myposter Case Study.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip rag_txt_dataset/archive.zip -d rag_txt_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apagamos o .zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm rag_txt_dataset/archive.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos ver o que ficou."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'23andMe Case Study _ Life Sciences _ AWS.txt'\n",
            "'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt'\n",
            "'54gene _ Case Study _ AWS.txt'\n",
            "'6sense Case Study.txt'\n",
            "'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt'\n",
            "'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt'\n",
            "'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt'\n",
            "'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt'\n",
            "'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt'\n",
            "'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt'\n",
            "'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt'\n",
            "'Actuate AI Case study.txt'\n",
            "'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt'\n",
            "'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt'\n",
            "'AEON Case Study.txt'\n",
            "'ALTBalaji _ Amazon Web Services.txt'\n",
            "'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt'\n",
            "'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt'\n",
            "'Anghami Case Study.txt'\n",
            "'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt'\n",
            "...\n",
            "'What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt'\n",
            "'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt'\n",
            " Windsor.txt\n",
            "'Wireless Car Case Study _ AWS IoT Core _ AWS.txt'\n",
            "'Yamato Logistics (HK) case study.txt'\n",
            "'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt'\n",
            "'Zoox Case Study _ Automotive _ AWS.txt'\n"
          ]
        }
      ],
      "source": [
        "!ls rag_txt_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A criar os `chunk`s!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Listamos os documentos com a função que havíamos criado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = \"rag_txt_dataset\"\n",
        "documents = load_documents_from_directory(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verificamos que fizemos tudo certo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt\n",
            "Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt\n",
            "Windsor.txt\n",
            "Bank of Montreal Case Study _ AWS.txt\n",
            "The Mill Adventure Case Study.txt\n",
            "Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt\n",
            "Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt\n",
            "THREAD _ Life Sciences _ AWS.txt\n",
            "Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt\n",
            "Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt\n"
          ]
        }
      ],
      "source": [
        "for document in documents[0:10]:\n",
        "    print(document[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos os `chunk`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunked_documents = []\n",
        "for document in documents:\n",
        "    chunks = split_text(document[\"text\"])\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunked_documents.append({\"id\": f\"{document['id']}_{i}\", \"text\": chunk})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3611"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunked_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, há 3611 `chunk`s. Como o limite diário da API do Hugging Face são 1000 chamadas na conta gratuita, se quisermos criar embeddings de todos os `chunk`s, acabaríamos com as chamadas disponíveis e ainda não poderíamos criar embeddings de todos os `chunk`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reiteramos, este modelo de embeddings é muito pequeno, apenas 22M de parâmetros, portanto pode ser executado em quase qualquer computador, mais rápido ou mais devagar, mas pode ser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como só vamos a criar os embeddings dos `chunk`s uma vez, mesmo que não tenhamos um computador muito potente e leve muito tempo, apenas será executado uma vez. Depois, quando quisermos fazer perguntas sobre a documentação, aí sim geraremos os embeddings do prompt com a API do Hugging Face e usaremos o LLM com a API. Portanto, só teremos que passar pelo processo de gerar os embeddings dos `chunk`s uma vez."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos os embeddings dos `chunk`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Última biblioteca que precisaremos instalar. Como o processo de geração dos embeddings dos `chunk`s será lento, vamos instalar o `tqdm` para mostrar uma barra de progresso. Instalamos com conda ou pip, conforme sua preferência.\n",
        "\n",
        "```bash\n",
        "conda install conda-forge::tqdm\n",
        "```\n",
        "\n",
        "o\n",
        "\n",
        "```bash\n",
        "pip install tqdm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geramos os embeddings dos `chunk`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3611/3611 [00:16<00:00, 220.75it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "progress_bar = tqdm.tqdm(chunked_documents)\n",
        "\n",
        "for chunk in progress_bar:\n",
        "    embedding = get_embeddings(chunk[\"text\"])\n",
        "    if embedding is not None:\n",
        "        chunk[\"embedding\"] = embedding\n",
        "    else:\n",
        "        print(f\"Error with document {chunk['id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,\n",
            "\n",
            "text: Reducing Virtual Machines from 40 to 12\n",
            "The founders of BNS had been contemplating a migration from the company’s on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.\n",
            "Français\n",
            "Configures security according to cloud best practices\n",
            "Clive Pereira, R&D director at BNS Group, explains, “The database that records Praisal’s SMS traffic resides in Praisal’s AWS environment. Praisal can now run complete analytics across its data and gain insights into what’s happening with its SMS traffic, which is a real game-changer for the organization.”  \n",
            "Español\n",
            " AWS ISV Accelerate Program\n",
            " Receiving Strategic, Foundational Support from ISV Specialists\n",
            " Learn More\n",
            "The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.” \n",
            "日本語\n",
            "  Contact Sales \n",
            "BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,\n",
            "\n",
            "embedding shape: (384,)\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "\n",
        "idx = randint(0, len(chunked_documents))\n",
        "print(f\"Chunk id: {chunked_documents[idx]['id']},\\n\\ntext: {chunked_documents[idx]['text']},\\n\\nembedding shape: {chunked_documents[idx]['embedding'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carregar os `chunk`s na base de dados vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma vez que temos todos os chunks gerados, os carregamos na base de dados vetorial. Voltamos a usar `tqdm` para mostrar uma barra de progresso, pois isso também será lento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3611/3611 [00:59<00:00, 60.77it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "progress_bar = tqdm.tqdm(chunked_documents)\n",
        "\n",
        "for chunk in progress_bar:\n",
        "    collection.upsert(\n",
        "        ids=[chunk[\"id\"]],\n",
        "        documents=chunk[\"text\"],\n",
        "        embeddings=chunk[\"embedding\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que temos a base de dados vetorial, podemos fazer perguntas à documentação. Para isso, precisamos de uma função que nos retorne o `chunk` correto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obter o `chunk` correto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora precisamos de uma função que nos retorne o `chunk` correto, vamos criá-la."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_documents(query, k=5):\n",
        "    results = collection.query(query_texts=query, n_results=k)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por último, criamos uma `query`.\n",
        "\n",
        "Para gerar a query, escolhi aleatoriamente o documento `Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt`, passei-o para um LLM e pedi que gerasse uma pergunta sobre o documento. A pergunta que foi gerada é\n",
        "\n",
        "```\n",
        "Como a Neeva utilizou o Karpenter e as Instâncias Spot do Amazon EC2 para melhorar a gestão da sua infraestrutura e otimização de custos?\n",
        "```\n",
        "\n",
        "Então obtemos os `chunks` mais relevantes diante dessa pergunta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?\"\n",
        "top_chunks = get_top_k_documents(query=query, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver quais `chunk`s nos foram retornados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937\n",
            "Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982\n",
            "Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777\n",
            "Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486\n",
            "Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(top_chunks[\"ids\"][0])):\n",
        "    print(f\"Rank {i+1}: {top_chunks['ids'][0][i]}, distance: {top_chunks['distances'][0][i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como eu havia dito, o documento que havia escolhido aleatoriamente era `Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt` e, como se pode ver, os `chunk`s que nos foram devolvidos são desse documento. Ou seja, de mais de 3000 `chunk`s que havia no banco de dados, ele foi capaz de me devolver os `chunk`s mais relevantes para aquela pergunta, parece que isso funciona!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gerar a resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como já temos os `chunk`s mais relevantes, passamo-los ao LLM, juntamente com a pergunta, para que ele gere uma resposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(query, relevant_chunks, temperature=0.5, max_tokens=1024, top_p=0.7, stream=False):\n",
        "    context = \"\\n\\n\".join([chunk for chunk in relevant_chunks])\n",
        "    prompt = f\"You are an assistant for question-answering. You have to answer the following question:\\n\\n{query}\\n\\nAnswer the question with the following information:\\n\\n{context}\"\n",
        "    message = [\n",
        "        { \"role\": \"user\", \"content\": prompt }\n",
        "    ]\n",
        "    stream = client.chat.completions.create(\n",
        "        messages=message, \n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        stream=stream,\n",
        "    )\n",
        "    response = stream.choices[0].message.content\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testamos a função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here’s how:\n",
            "\n",
            "### Early Collaboration with Karpenter\n",
            "In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.\n",
            "\n",
            "### Combining Spot Instances and On-Demand Instances\n",
            "Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.\n",
            "\n",
            "### Flexibility and Instance Diversification\n",
            "According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.\n",
            "\n",
            "### Improved Scalability and Agility\n",
            "By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:\n",
            "- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.\n",
            "- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.\n",
            "\n",
            "### Enhanced Development Cycles\n",
            "The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.\n",
            "\n",
            "### Cost Savings and Budget Control\n",
            "Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.\n",
            "\n",
            "### Future Plans\n",
            "Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, \"The bulk of our compute is or will be managed using Karpenter going forward.\"\n",
            "\n",
            "### Conclusion\n",
            "By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.\n"
          ]
        }
      ],
      "source": [
        "response = generate_response(query, top_chunks[\"documents\"][0])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando pedi ao LLM para gerar uma pergunta sobre o documento, também pedi que me fornecesse a resposta correta. Esta é a resposta que o LLM me deu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` text\n",
        "A Neeva utilizou o Karpenter e as Instâncias Spot do Amazon EC2 para melhorar sua gestão de infraestrutura e otimização de custos de várias maneiras:\n",
        "\n",
        "Gestão Simplificada de Instâncias:\n",
        "\n",
        "Karpenter: Ao adotar o Karpenter, a Neeva simplificou o processo de provisionamento e gerenciamento de recursos computacionais para seus clusters do Amazon EKS. O Karpenter provisiona e desprovisiona instâncias automaticamente com base na carga de trabalho, eliminando a necessidade de configurações manuais e reduzindo a complexidade de compreensão das diferentes instâncias computacionais.\n",
        "Instâncias Spot: A Neeva utilizou Instâncias Spot do Amazon EC2, que são capacidade ociosa do EC2 disponível com um desconto significativo (até 90% de economia de custos). Isso permitiu à empresa controlar os custos enquanto atendia aos seus requisitos de desempenho.\n",
        "Escalabilidade Aumentada:\n",
        "\n",
        "Karpenter: A capacidade do Karpenter de escalar recursos dinamicamente permitiu que a Neeva iniciasse novas instâncias rapidamente, permitindo que a empresa iterasse com maior velocidade e realizasse mais experimentos em menos tempo.\n",
        "Instâncias Spot: O uso de Instâncias Spot proporcionou flexibilidade e diversificação de instâncias, facilitando o escalonamento dos recursos de computação da Neeva de forma eficiente.\n",
        "Produtividade Melhorada:\n",
        "\n",
        "Karpenter: Ao democratizar as alterações de infraestrutura, o Karpenter permitiu que qualquer engenheiro modificasse as configurações do Kubernetes, reduzindo a dependência de expertise especializada. Isso economizou até 100 horas por semana de tempo de espera em administração de sistemas para a equipe da Neeva.\n",
        "Instâncias Spot: A capacidade de provisionar e desprovisionar rapidamente Instâncias Spot reduziu os atrasos no pipeline de desenvolvimento, garantindo que os trabalhos não ficassem travados devido à falta de recursos disponíveis.\n",
        "Eficiência de Custo:\n",
        "\n",
        "Karpenter: As melhores práticas do Karpenter para instâncias Spot, incluindo flexibilidade e diversificação de instâncias, ajudaram a Neeva a usar essas instâncias de forma mais eficaz, permanecendo dentro do orçamento.\n",
        "Instâncias Spot: A economia de custos com o uso de Instâncias Spot permitiu que a Neeva executasse trabalhos em larga escala, como indexação, quase pelo mesmo custo, mas em uma fração do tempo. Por exemplo, a Neeva reduziu seus trabalhos de indexação de 18 horas para apenas 3 horas.\n",
        "Melhor Utilização de Recursos:\n",
        "\n",
        "Karpenter: O Karpenter proporcionou melhor visibilidade sobre o uso de recursos de computação, permitindo que a Neeva rastreasse e otimizasse seu consumo de recursos mais de perto.\n",
        "Instâncias Spot: A combinação do Karpenter e das Instâncias Spot permitiu que a Neeva executasse modelos de linguagem grandes de forma mais eficiente, melhorando a experiência de pesquisa para seus usuários.\n",
        "Em resumo, a adoção do Karpenter e das Instâncias Spot do Amazon EC2 pela Neeva melhorou significativamente sua gestão de infraestrutura, otimização de custos e eficiência geral no desenvolvimento, permitindo que a empresa oferecesse melhores experiências de pesquisa sem anúncios aos seus usuários.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E esta tem sido a resposta gerada pelo nosso `RAG`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` text\n",
        "Neeva, um mecanismo de busca nativo em nuvem e sem anúncios fundado em 2019, aproveitou o Karpenter e as Instâncias Spot do Amazon EC2 para melhorar significativamente sua gestão de infraestrutura e otimização de custos. Eis como:\n",
        "\n",
        "### Colaboração Inicial com o Karpenter\n",
        "No final de 2021, a Neeva começou a trabalhar em estreita colaboração com a equipe do Karpenter, experimentando e contribuindo com correções para uma versão inicial do Karpenter. Essa colaboração permitiu que a Neeva integrasse o Karpenter ao seu painel do Kubernetes, possibilitando à empresa coletar métricas valiosas sobre uso e desempenho.\n",
        "\n",
        "### Combinando Instâncias Spot e Instâncias On-Demand\n",
        "A Neeva executa seus trabalhos em larga escala, o que pode levar a custos significativos. Para gerenciar esses custos de forma eficaz, a empresa adotou uma combinação de Amazon EC2 Spot Instances e On-Demand Instances. As Spot Instances permitem que a Neeva lance lances sobre capacidade EC2 não utilizada, frequentemente por uma fração do preço On-Demand, enquanto as On-Demand Instances fornecem a necessária confiabilidade para pipelines críticos.\n",
        "\n",
        "### Flexibilidade e Diversificação de Instâncias\n",
        "De acordo com Mohit Agarwal, engenheiro de infraestrutura sênior da Neeva, a adoção de melhores práticas para Instâncias Spot pelo Karpenter, incluindo flexibilidade e diversificação de instâncias, foi crucial. Esta abordagem garante que a Neeva possa ajustar dinamicamente seus recursos de computação para atender às cargas de trabalho variáveis, minimizando custos.\n",
        "\n",
        "### Melhor Escalabilidade e Agilidade\n",
        "Ao usar o Karpenter para provisionar recursos de infraestrutura para seus clusters do Amazon EKS, a Neeva alcançou vários benefícios importantes:\n",
        "- **Escalabilidade**: O Neeva pode escalar seus recursos de computação para cima ou para baixo conforme necessário, garantindo que sempre tenha a capacidade necessária para lidar com suas cargas de trabalho.\n",
        "- **Agilidade**: A empresa pode iterar rapidamente e democratizar as mudanças de infraestrutura, reduzindo o tempo gasto com a administração do sistema em até 100 horas por semana.\n",
        "\n",
        "### Ciclos de Desenvolvimento Aprimorados\n",
        "A integração do Karpenter e das Instâncias Spot também acelerou os ciclos de desenvolvimento da Neeva. A empresa agora pode lançar novos recursos e melhorias mais rapidamente, o que é essencial para manter uma vantagem competitiva no mercado de motores de busca.\n",
        "\n",
        "### Economia de Custos e Controle Orçamentário\n",
        "Usando Instâncias Spot, a Neeva tem sido capaz de permanecer dentro do seu orçamento enquanto atende aos seus requisitos de desempenho. Esta otimização de custos é crítica para uma empresa que prioriza experiências centradas no usuário e não possui incentivos concorrentes de publicidade.\n",
        "\n",
        "### Planos Futuros\n",
        "A Neeva está comprometida em continuar sua inovação e expansão. A empresa planeja lançar em novas regiões e melhorar ainda mais seu mecanismo de busca, tudo isso mantendo a eficiência de custos. Como Mohit Agarwal observa, \"A maior parte do nosso computação é ou será gerenciada usando Karpenter a partir de agora.\"\n",
        "\n",
        "### Conclusão\n",
        "Ao aproveitar o Karpenter e as Instâncias Spot do Amazon EC2, a Neeva não apenas otimizou seus custos de infraestrutura, mas também melhorou sua escalabilidade, agilidade e velocidade de desenvolvimento. Essa abordagem estratégica posicionou a Neeva para oferecer experiências de pesquisa de alta qualidade e sem anúncios aos seus usuários, mantendo um forte foco no controle de custos e inovação.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos concluir então que o `RAG` funcionou corretamente!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limites de naive RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dissemos, hoje explicamos `naive RAG`, que é a arquitetura mais simples do RAG, mas tem suas limitações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Arquitetura RAG ingênua](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naive_RAG_architecture.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As limitações desta arquitetura são:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limites na busca de informações (retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Conhecimento limitado do contexto e da documentação: Quando o sistema de RAG ingênuo busca os chunks, ele procura aqueles que têm um significado semântico similar ao prompt, mas não é capaz de saber quais são os mais relevantes para a pergunta do usuário, ou quais são os que possuem informações mais atualizadas, ou se sua informação é mais correta do que a de outros chunks. Por exemplo, se um usuário perguntar sobre os problemas dos adoçantes no sistema digestivo, o RAG ingênuo pode retornar documentos sobre adoçantes ou sobre o sistema digestivo, mas não é capaz de saber que os documentos sobre o sistema digestivo são os mais relevantes para a pergunta do usuário. Outro exemplo é se o usuário perguntar sobre os últimos avanços em IA, mas o RAG ingênuo não é capaz de saber quais são os últimos papers da base de dados.\n",
        "\n",
        "* Não há uma sincronização entre o retrieval e o gerador. Como vimos, são dois sistemas independentes; de um lado, o retrieval busca os documentos mais semelhantes à pergunta do usuário, e esses documentos são passados ao gerador, que gera uma resposta.\n",
        "\n",
        "* Escalabilidade ineficiente para grandes bancos de dados. Como a recuperação busca os documentos com maior similaridade semântica em todo o banco de dados, quando este fica muito grande, podemos ter tempos de busca muito longos.\n",
        "\n",
        "* Pouca adaptação à pergunta do usuário. Se o usuário fizer uma pergunta que envolve vários documentos, ou seja, não há nenhum documento que contenha toda a informação da pergunta do usuário, o sistema recuperará todos esses documentos e os passará ao gerador, que pode usá-los ou não. Ou, em um caso pior, pode deixar de incluir algum documento relevante para gerar a resposta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limites na geração de respostas (generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* O modelo poderia alucinar respostas mesmo ao fornecer informações relevantes.\n",
        "\n",
        "* O modelo pode estar limitado por questões relacionadas a ódio, discriminação, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para superar estes limites, costuma-se utilizar técnicas como o\n",
        "\n",
        "* Pré-recuperação: Que inclui técnicas para melhorar a indexação, tornando a busca de informações mais eficiente. Ou técnicas como a melhoria da pergunta do usuário para que o retrieval possa encontrar os documentos mais relevantes.\n",
        "\n",
        "* Pós-recuperação: Aqui são usadas técnicas como o re-rank dos documentos, que é uma técnica utilizada para melhorar a busca por informações relevantes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "maximofn": {
      "date": "2024-10-23",
      "description_en": "Forget about Ctrl+F! 🤯 With RAG, your documents will answer your questions directly. 😎 Step-by-step tutorial with Hugging Face and ChromaDB. Unleash the power of AI (and show off to your friends)! 💪",
      "description_es": "¡Olvídate de Ctrl+F! 🤯 Con RAG, tus documentos responderán a tus preguntas directamente. 😎 Tutorial paso a paso con Hugging Face y ChromaDB. ¡Libera el poder de la IA (y presume con tus amigos)! 💪",
      "description_pt": "Esqueça o Ctrl+F! 🤯 Com RAG, seus documentos responderão às suas perguntas diretamente. 😎 Tutorial passo a passo com Hugging Face e ChromaDB. Liberte o poder da IA (e impressione seus amigos)! 💪",
      "end_url": "rag-fundamentals",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-fundamentals.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-fundamentals.webp",
      "keywords_en": "rag, retriever, reader, hugging face, transformers, chromadb, vector database, question-answering, qa, nlp, natural language processing, machine learning, artificial intelligence, ai",
      "keywords_es": "rag, retriever, reader, hugging face, transformers, chromadb, base de datos vectorial, question-answering, qa, nlp, procesamiento de lenguaje natural, machine learning, inteligencia artificial, ia",
      "keywords_pt": "rag, retriever, reader, hugging face, transformers, chromadb, banco de dados vetorial, question-answering, qa, nlp, processamento de linguagem natural, machine learning, inteligência artificial, ia",
      "title_en": "RAG fundamentals",
      "title_es": "Fundamentos de RAG",
      "title_pt": "Fundamentos de RAG"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
