{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Open Source Integrations in Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use of language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage of Ollama Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we have already seen in the post on [Ollama](https://www.maximofn.com/ollama), it is a framework built on `llama.cpp` that allows us to use language models easily and also supports quantized models.",
        "\n",
        "So let's see how to use `Qwen2.5 7B` with Ollama in Langchain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before anything else, we need to install the Llama module from Langchain",
        "\n",
        "``` bash\n",
        "pip install -U langchain-ollama",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing to do is download the `qwen2.5:7b` model from Ollama. We use the 7B version so that anyone with a GPU can use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25lpulling manifest \u2819 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u2839 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u2838 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u2838 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u2834 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u2826 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u2827 \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f 9.7 MB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f  12 MB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f  17 MB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   0% \u2595                \u258f  21 MB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   1% \u2595                \u258f  31 MB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   1% \u2595                \u258f  42 MB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   1% \u2595                \u258f  51 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   1% \u2595                \u258f  57 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   1% \u2595                \u258f  65 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   1% \u2595                \u258f  70 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   2% \u2595                \u258f  79 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   2% \u2595                \u258f  88 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   2% \u2595                \u258f  93 MB/4.7 GB   47 MB/s   1m37s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   2% \u2595                \u258f 103 MB/4.7 GB   47 MB/s   1m36s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   2% \u2595                \u258f 112 MB/4.7 GB   47 MB/s   1m36s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   3% \u2595                \u258f 117 MB/4.7 GB   47 MB/s   1m36s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   3% \u2595                \u258f 126 MB/4.7 GB   63 MB/s   1m11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   3% \u2595                \u258f 135 MB/4.7 GB   63 MB/s   1m11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   3% \u2595                \u258f 141 MB/4.7 GB   63 MB/s   1m11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 2bada8a74506...   3% \u2595                \u258f 150 MB/4.7 GB   63 MB/s   1m11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "...\n",
            "pulling eb4402837c78... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.5 KB                         \n",
            "pulling 832dd9e00a68... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \n",
            "pulling 2f15b3218f05... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  487 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "removing any unused layers \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "!ollama pull qwen2.5:7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chat models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first option we have for using language models with Ollama in Langchain is to use the `ChatOllama` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the `ChatOllama` object with the model `qwen2.5:7b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "chat_model = ChatOllama(\n",
        "    model = \"qwen2.5:7b\",\n",
        "    temperature = 0.8,\n",
        "    num_predict = 256,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Offline Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we perform inference with the model in an offline manner, that is, without using streaming. This method waits to have the entire response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLMs es el acr\u00f3nimo de Large Language Models (Modelos de Lenguaje Grandes). Estos son sistemas de inteligencia artificial basados en modelos de aprendizaje autom\u00e1tico profundo que est\u00e1n dise\u00f1ados para comprender y generar texto humano. Algunas caracter\u00edsticas clave de los LLMs incluyen:\n",
            "\n",
            "1. **Tama\u00f1o**: Generalmente se refiere a modelos con billones de par\u00e1metros, lo cual les permite aprender de grandes cantidades de datos.\n",
            "\n",
            "2. **Capacidad de Generaci\u00f3n de Texto**: Pueden generar texto coherente y relevante basado en entradas iniciales o prompts proporcionados por el usuario.\n",
            "\n",
            "3. **Entendimiento del Lenguaje Natural**: Poseen un entendimiento profundo de la gram\u00e1tica, sem\u00e1ntica y contexto del lenguaje humano.\n",
            "\n",
            "4. **Aplicaciones Vers\u00e1tiles**: Se utilizan en una amplia gama de tareas como asistentes virtuales, traducci\u00f3n autom\u00e1tica, resumen de texto, escritura creativa, etc.\n",
            "\n",
            "5. **Entrenamiento Supervisado**: A menudo se entrenan utilizando conjuntos de datos extensos y variados para mejorar su capacidad de comprensi\u00f3n e\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    (\"system\", \"Eres un asistente de IA que responde preguntas sobre IA.\"),\n",
        "    (\"human\", \"\u00bfQu\u00e9 son los LLMs?\"),\n",
        "]\n",
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Streaming Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to do streaming, we can use the `stream` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLMs es el acr\u00f3nimo de Large Language Models, lo cual se refiere a modelos de inteligencia artificial de lenguaje de gran escala. Estos son sistemas entrenados con t\u00e9cnicas de aprendizaje supervisado y no supervisado que pueden generar texto humano\u4f3c\u7684\uff0c\u51c6\u786e\u56de\u7b54\u5982\u4e0b\uff1a\n",
            "\n",
            "LLMs es el acr\u00f3nimo de Large Language Models, lo cual se refiere a modelos de inteligencia artificial de lenguaje de gran escala. Estos son sistemas que se entrenan con grandes conjuntos de datos de texto para aprender patrones y relaciones ling\u00fc\u00edsticas. Algunas caracter\u00edsticas clave de los LLMs incluyen:\n",
            "\n",
            "1. **Tama\u00f1o**: Generalmente se basan en arquitecturas de redes neuronales profunda como Transformers, lo que les permite manejar cantidades extremadamente grandes de par\u00e1metros.\n",
            "2. **Entrenamiento**: Se entrena con datos de texto muy variados para capturar una amplia gama de conocimientos y habilidades ling\u00fc\u00edsticas.\n",
            "3. **Generaci\u00f3n de texto**: Pueden generar textos coherentes y relevantes bas\u00e1ndose en el contexto proporcionado, lo que les permite realizar tareas como la escritura creativa,"
          ]
        }
      ],
      "source": [
        "for chunk in chat_model.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to use embeddings, we can use the `OllamaEmbeddings` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the `OllamaEmbeddings` object with the model `qwen2.5:7b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "embeddings_model = OllamaEmbeddings(\n",
        "    model = \"qwen2.5:7b\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Embed a single text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3584, list, [0.00045780753, -0.010200562, 0.0059901197])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text = \"\u00bfQu\u00e9 son los LLMs?\"\n",
        "embedding = embeddings_model.embed_query(input_text)\n",
        "len(embedding), type(embedding), embedding[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Embed of multiple texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3584,\n",
              " list,\n",
              " [0.00045780753, -0.010200562, 0.0059901197],\n",
              " 3584,\n",
              " list,\n",
              " [0.0007678218, -0.01124029, 0.008565228])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_texts = [\"\u00bfQu\u00e9 son los LLMs?\", \"\u00bfQu\u00e9 son los embeddings?\"]\n",
        "embeddings = embeddings_model.embed_documents(input_texts)\n",
        "len(embeddings[0]), type(embeddings[0]), embeddings[0][0:3], len(embeddings[1]), type(embeddings[1]), embeddings[1][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we only want to use a language model, we can use the `OllamaLLM` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the `OllamaLLM` object with the model `qwen2.5:7b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "llm_model = OllamaLLM(\n",
        "    model = \"qwen2.5:7b\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Offline Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we perform inference with the model in an offline manner, that is, without using streaming. This method waits to have the entire response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los LLMs es una abreviatura com\u00fan en espa\u00f1ol que se refiere a \"Lenguajes de Marcado de Libre Mando\". Sin embargo, en el contexto del procesamiento del lenguaje natural y la inteligencia artificial, es probable que est\u00e9s buscando informaci\u00f3n sobre otro t\u00e9rmino.\n",
            "\n",
            "En ingl\u00e9s, el t\u00e9rmino LLMs (Large Language Models) se refiere a modelos de lenguaje de gran tama\u00f1o. Estos son sistemas de IA que est\u00e1n entrenados en una amplia gama de datos de texto para comprender y generar texto humano-like. Algunas caracter\u00edsticas clave de los LLMs incluyen:\n",
            "\n",
            "1. Tama\u00f1o: Generalmente se refiere a modelos con millones o incluso billones de par\u00e1metros.\n",
            "2. Generaci\u00f3n de texto: Pueden generar texto en respuesta a una entrada dada, lo que les hace \u00fatiles para tareas como escritura autom\u00e1tica, conversaciones de chat, etc.\n",
            "3. Entrenamiento en datos variados: Son entrenados con grandes conjuntos de datos de internet o corpora literales y acad\u00e9micos.\n",
            "4. Aplicaciones: Se utilizan en una variedad de aplicaciones, incluyendo asistentes virtuales, traducci\u00f3n autom\u00e1tica, an\u00e1lisis de sentimientos, resumen de texto, etc.\n",
            "\n",
            "Un ejemplo famoso de LLMs es la familia de modelos GPT (Generative Pre-trained Transformer) desarrollados por OpenAI. Otros ejemplos incluyen los modelos de PaLM y Qwen de Anthropic, entre otros.\n",
            "\n",
            "\u00bfTe gustar\u00eda saber m\u00e1s sobre alg\u00fan aspecto en particular de estos modelos?\n"
          ]
        }
      ],
      "source": [
        "message = \"\u00bfQu\u00e9 son los LLMs?\"\n",
        "response = llm_model.invoke(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Streaming Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to do streaming, we can use the `stream` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los LLMs es una abreviatura que se refiere a los Modelos de Lenguaje de Grande Escala (Large Language Models en ingl\u00e9s). Estos son modelos de inteligencia artificial desarrollados para entender y generar texto humano. Algunas caracter\u00edsticas clave de los LLMs incluyen:\n",
            "\n",
            "1. **Capacidad de Generaci\u00f3n de Texto**: Pueden generar texto coherente, original e informativo en varios estilos y formatos.\n",
            "\n",
            "2. **Entendimiento del Contexto**: Tienen una comprensi\u00f3n profunda del contexto y la gram\u00e1tica para producir respuestas apropiadas a las entradas proporcionadas.\n",
            "\n",
            "3. **Multiplicidad de Usos**: Se pueden aplicar a diversas tareas, como escritura creativa, asistencia en atenci\u00f3n al cliente, creaci\u00f3n de contenido, traducci\u00f3n autom\u00e1tica, etc.\n",
            "\n",
            "4. **Aprendizaje Supervisado**: Son entrenados con grandes cantidades de texto para aprender patrones y conocimientos del lenguaje humano.\n",
            "\n",
            "5. **Limitaciones Eticas y Jur\u00eddicas**: Existen preocupaciones sobre el uso \u00e9tico y legal de estos modelos, incluyendo el potencial de generar contenido inapropiado o enga\u00f1oso.\n",
            "\n",
            "Algunos ejemplos populares de LLMs incluyen los creados por empresas como Anthropic (Clara), Google (Switch Transformer), Microsoft (Cobalt) y, aunque ya no actualmente soportado, OpenAI (GPT-3)."
          ]
        }
      ],
      "source": [
        "for chunk in llm_model.stream(message):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage of HuggingFace models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although Ollama is very easy to use, it doesn't have as many models as there are on the HuggingFace Hub. Additionally, it might not have the latest great model you want to use, so we're going to see how to use HuggingFace models in Langchain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before anything, we need to install the HuggingFace module of Langchain",
        "\n",
        "``` bash\n",
        "pip install -U langchain-huggingface",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Login to the HuggingFace Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to log in to the HuggingFace Hub. For this, we need to create an access token. Once created, we save it in a `.env` file with the name `LANGCHAIN_TOKEN` as follows:",
        "\n",
        "``` bash\n",
        "echo \"LANGCHAIN_TOKEN=hf_...\" > .env",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To be able to read it, we install the `python-dotenv` module.",
        "\n",
        "``` bash\n",
        "pip install python-dotenv",
        "```\n",
        "\n",
        "Now let's log in to the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "LANGCHAIN_TOKEN = os.getenv(\"LANGCHAIN_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(LANGCHAIN_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chat models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first option we have for using language models with HuggingFace in Langchain is to use the `HuggingFaceEndpoint` class. This option calls the HuggingFace API to perform inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the `chat_model` object with the model `Qwen2.5-72B-Instruct`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "chat = ChatHuggingFace(llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Offline Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we perform inference with the model in an offline manner, that is, without using streaming. This method waits to have the entire response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los LLMs, o Large Language Models (Modelos de Lenguaje Grande en espa\u00f1ol), son sistemas de inteligencia artificial dise\u00f1ados para procesar y generar texto basado en lenguaje natural. Estos modelos son capaces de comprender, analizar y producir texto humano de manera sofisticada, lo que los hace \u00fatiles en una amplia variedad de aplicaciones, como la traducci\u00f3n de idiomas, el an\u00e1lisis de sentimientos, la generaci\u00f3n de respuestas a preguntas, la escritura creativa, la asistencia en tareas, entre otras.\n",
            "\n",
            "### Caracter\u00edsticas principales de los LLMs:\n",
            "\n",
            "1. **Escalabilidad**: Los LLMs suelen tener millones o incluso billones de par\u00e1metros, lo que les permite capturar la complejidad del lenguaje humano con un alto nivel de detalle.\n",
            "\n",
            "2. **Entrenamiento supervisado y no supervisado**: Pueden ser entrenados con grandes cantidades de datos de texto, tanto de forma supervisada (con etiquetas) como no supervisada (sin etiquetas).\n",
            "\n",
            "3. **Transfer learning**: Una vez entrenados, estos modelos pueden ser finetuneados (ajustados) para tareas espec\u00edficas con un conjunto de datos m\u00e1s peque\u00f1o, lo que los hace muy vers\u00e1tiles.\n",
            "\n",
            "4. **Generaci\u00f3n de texto**: Son capaces de generar texto coherente y contextualmente relevante, lo que los hace ideales para tareas como la escritura creativa, la redacci\u00f3n de informes, y la generaci\u00f3n de respuestas en chatbots.\n",
            "\n",
            "5. **Comprensi\u00f3n del contexto**: Pueden entender el contexto y las relaciones entre palabras y frases, lo que les permite manejar di\u00e1logos y discursos complejos.\n",
            "\n",
            "6. **Interacci\u00f3n humana**: Son dise\u00f1ados para interactuar de manera fluida y natural con seres humanos, lo que los hace \u00fatiles en aplicaciones conversacionales.\n",
            "\n",
            "### Ejemplos de LLMs:\n",
            "\n",
            "- **GPT-3 (Generative Pre-trained Transformer 3)**: Desarrollado por OpenAI, es uno de losmodelos m\u00e1s conocidos y potentes.\n",
            "- **BERT (Bidirectional Encoder Representations from Transformers)**: Dise\u00f1ado por Google, es conocido por su capacidad de comprensi\u00f3n del contexto.\n",
            "- **T5 (Text-to-Text Transfer Transformer)**: Tambi\u00e9n de Google, se destaca por su flexibilidad y versatilidad.\n",
            "\n",
            "Los LLMs representan un avance significativo en el campo de la inteligencia artificial, especialmente en el procesamiento del lenguaje natural, y contin\u00faan evoluinto r\u00e1pidamente.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    (\"system\", \"Eres un asistente de IA que responde preguntas sobre IA.\"),\n",
        "    (\"human\", \"\u00bfQu\u00e9 son los LLMs?\"),\n",
        "]\n",
        "response = chat.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Streaming Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to do streaming, we can use the `stream` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los LLMs, o Large Language Models (Modelos de Lenguaje Grande en espa\u00f1ol), son sistemas de inteligencia artificial dise\u00f1ados para procesar y generar texto basado en lenguaje natural. Estos modelos son capaces de comprender, analizar y producir texto humano de manera sofisticada, lo que los hace \u00fatiles en una amplia variedad de aplicaciones, como la traducci\u00f3n de idiomas, el an\u00e1lisis de sentimientos, la generaci\u00f3n de respuestas a preguntas, la escritura creativa, la asistencia en tareas, entre otras.\n",
            "\n",
            "### Caracter\u00edsticas principales de los LLMs:\n",
            "\n",
            "1. **Escalabilidad**: Los LLMs suelen tener millones o incluso billones de par\u00e1metros, lo que les permite capturar la complejidad del lenguaje humano con un alto nivel de detalle.\n",
            "\n",
            "2. **Entrenamiento supervisado y no supervisado**: Pueden ser entrenados con grandes cantidades de datos de texto, tanto de forma supervisada (con etiquetas) como no supervisada (sin etiquetas).\n",
            "\n",
            "3. **Transfer learning**: Una vez entrenados, estos modelos pueden ser finetuneados (ajustados) para tareas espec\u00edficas con un conjunto de datos m\u00e1s peque\u00f1o, lo que los hace muy vers\u00e1tiles.\n",
            "\n",
            "4. **Generaci\u00f3n de texto**: Son capaces de generar texto coherente y contextualmente relevante, lo que los hace ideales para tareas como la escritura creativa, la redacci\u00f3n de informes, y la generaci\u00f3n de respuestas en chatbots.\n",
            "\n",
            "5. **Comprensi\u00f3n del contexto**: Pueden entender el contexto y las relaciones entre palabras y frases, lo que les permite manejar di\u00e1logos y discursos complejos.\n",
            "\n",
            "6. **Interacci\u00f3n humana**: Son dise\u00f1ados para interactuar de manera fluida y natural con seres humanos, lo que los hace \u00fatiles en aplicaciones conversacionales.\n",
            "\n",
            "### Ejemplos de LLMs:\n",
            "\n",
            "- **GPT-3 (Generative Pre-trained Transformer 3)**: Desarrollado por OpenAI, es uno de losmodelos m\u00e1s conocidos y potentes.\n",
            "- **BERT (Bidirectional Encoder Representations from Transformers)**: Dise\u00f1ado por Google, es conocido por su capacidad de comprensi\u00f3n del contexto.\n",
            "- **T5 (Text-to-Text Transfer Transformer)**: Tambi\u00e9n de Google, se destaca por su flexibilidad y versatilidad.\n",
            "\n",
            "Los LLMs representan un avance significativo en el campo de la inteligencia artificial, especialmente en el procesamiento del lenguaje natural, y contin\u00faan evoluinto r\u00e1pidamente."
          ]
        }
      ],
      "source": [
        "for chunk in chat.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Embeddings locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to create embeddings locally, we can use the `HuggingFaceEmbeddings` class. For this, we need to have the `sentence-transformers` library installed.",
        "\n",
        "``` bash\n",
        "pip install -U sentence-transformers",
        "```\n",
        "\n",
        "Then we can create embeddings locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768,\n",
              " list,\n",
              " [-0.03638569265604019, -0.003062659176066518, 0.005454241763800383])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_kwargs = {\"device\": device}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "input_text = \"\u00bfQu\u00e9 son los LLMs?\"\n",
        "embedding = embeddings_model.embed_query(input_text)\n",
        "len(embedding), type(embedding), embedding[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to create embeddings for multiple texts, we can use the `embed_documents` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768,\n",
              " list,\n",
              " [-0.0363856740295887, -0.0030626414809376, 0.005454237572848797],\n",
              " 768,\n",
              " list,\n",
              " [-0.014533628709614277, 0.01950662210583687, -0.01753164641559124])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_texts = [\"\u00bfQu\u00e9 son los LLMs?\", \"\u00bfQu\u00e9 son los embeddings?\"]\n",
        "embeddings = embeddings_model.embed_documents(input_texts)\n",
        "len(embeddings[0]), type(embeddings[0]), embeddings[0][0:3], len(embeddings[1]), type(embeddings[1]), embeddings[1][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Embeddings in HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to use the HuggingFace API to create embeddings, we can use the `HuggingFaceEmbeddings` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768,\n",
              " list,\n",
              " [-0.03638569638133049, -0.0030626363586634398, 0.005454216152429581])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "embeddings_model = HuggingFaceEndpointEmbeddings(\n",
        "    model=model,\n",
        "    task=\"feature-extraction\",\n",
        ")\n",
        "\n",
        "input_text = \"\u00bfQu\u00e9 son los LLMs?\"\n",
        "embedding = embeddings_model.embed_query(input_text)\n",
        "len(embedding), type(embedding), embedding[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to create embeddings for multiple texts, you can use the `embed_documents` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768,\n",
              " list,\n",
              " [-0.03638570010662079, -0.003062596544623375, 0.005454217083752155],\n",
              " 768,\n",
              " list,\n",
              " [-0.014533626846969128, 0.019506637006998062, -0.01753171905875206])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_texts = [\"\u00bfQu\u00e9 son los LLMs?\", \"\u00bfQu\u00e9 son los embeddings?\"]\n",
        "embeddings = embeddings_model.embed_documents(input_texts)\n",
        "len(embeddings[0]), type(embeddings[0]), embeddings[0][0:3], len(embeddings[1]), type(embeddings[1]), embeddings[1][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can create a Transformers pipeline and use it in Langchain, for which we need to install the `transformers` module.",
        "\n",
        "``` bash\n",
        "pip install -U transformers",
        "```\n",
        "\n",
        "To ensure that everyone can try the example, we will use the model `Qwen/Qwen2.5-3B-Instruct`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have two ways, creating a pipeline using Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62bbb34650bf4147bc0b1af637591c93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "langchain_pipeline = HuggingFacePipeline.from_model_id(\n",
        "    model_id=model,\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or we can create a pipeline using Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15b70ea3a43943848a6f3c19f0c848e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForCausalLM.from_pretrained(model)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512\n",
        ")\n",
        "transformers_pipeline = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Offline Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u00bfQu\u00e9 son los LLMs? - El Blog de Ciberseguridad\n",
            "Publicado por: CiberSeguridad 18/03/2022\n",
            "En el mundo de la tecnolog\u00eda y la inteligencia artificial, las nuevas tecnolog\u00edas que surgen a menudo se presentan como un paso hacia adelante en la mejora de nuestras vidas. Sin embargo, a veces estas innovaciones pueden resultar en desaf\u00edos significativos, especialmente cuando se trata de la seguridad cibern\u00e9tica.\n",
            "En este art\u00edculo, exploraremos los LLMs, o modelos ling\u00fc\u00edsticos grandes (Large Language Models), una nueva clase de inteligencia artificial que est\u00e1 revolucionando el campo de la comunicaci\u00f3n y la creaci\u00f3n de contenido. Aunque estos modelos son muy prometedores, tambi\u00e9n tienen implicaciones significativas para la seguridad cibern\u00e9tica. En esta entrada, te explicamos qu\u00e9 son los LLMs y c\u00f3mo afectan a la seguridad digital.\n",
            "\u00bfQu\u00e9 son los LLMs?\n",
            "Los modelos ling\u00fc\u00edsticos grandes son algoritmos de aprendizaje autom\u00e1tico que han sido entrenados con grandes conjuntos de texto, generalmente en formato de texto natural. Estos modelos pueden procesar y generar texto de manera similar a una persona, lo que los hace extremadamente \u00fatiles para tareas como la traducci\u00f3n, la autocompletado de texto y la generaci\u00f3n de contenido creativo.\n",
            "Los LLMs se basan en t\u00e9cnicas de aprendizaje profundo, que son una forma avanzada de algoritmo de aprendizaje autom\u00e1tico. Este tipo de algoritmo utiliza redes neuronales para aprender patrones y relaciones en datos grandes, permitiendo a los modelos predecir resultados o tomar decisiones basadas en esa informaci\u00f3n.\n",
            "Estos modelos son capaces de aprender y generar texto en un amplio rango de temas, desde descripciones de libros hasta di\u00e1logos interactivos, lo que los hace excelentes para tareas de generaci\u00f3n de contenido. Tambi\u00e9n pueden ser utilizados para tareas m\u00e1s espec\u00edficas, como la traducci\u00f3n de idiomas o la resoluci\u00f3n de problemas de l\u00f3gica.\n",
            "\u00bfC\u00f3mo afectan a la seguridad cibern\u00e9tica?\n",
            "Los LLMs representan una gran oportunidad para mejorar la eficiencia y la precisi\u00f3n de muchas tareas de texto, pero tambi\u00e9n traen consigo desaf\u00edos significativos en t\u00e9rminos de seguridad. Algunas de las preocupaciones m\u00e1s comunes incluyen:\n",
            "Privacidad: Los LLM\n"
          ]
        }
      ],
      "source": [
        "langchain_response = langchain_pipeline.invoke(\"\u00bfQu\u00e9 son los LLMs?\")\n",
        "print(langchain_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_318295/4120882068.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  transformers_response = transformers_pipeline(\"\u00bfQu\u00e9 son los LLMs?\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u00bfQu\u00e9 son los LLMs? \u00bfC\u00f3mo funcionan y qu\u00e9 pueden hacer?\n",
            "\n",
            "Los LLMs (Large Language Models) son modelos de inteligencia artificial que han sido entrenados con enormes cantidades de texto. Estos modelos pueden procesar y generar texto en una variedad de lenguajes, desde el ingl\u00e9s hasta idiomas menos comunes. Los LLMs est\u00e1n dise\u00f1ados para entender y generar texto con un nivel de precisi\u00f3n similar al humano.\n",
            "\n",
            "Funcionamiento:\n",
            "1. Entrenamiento: Los LLMs se entrenan utilizando grandes conjuntos de datos de texto, como libros electr\u00f3nicos, art\u00edculos de noticias, etc.\n",
            "2. An\u00e1lisis: Despu\u00e9s del entrenamiento, los LLMs analizan patrones en el texto y aprenden a asociar palabras y frases con otros textos similares.\n",
            "3. Generaci\u00f3n de texto: Cuando se le pide al modelo que genere texto, analiza los patrones que ha aprendido y genera texto basado en esos patrones.\n",
            "\n",
            "Pueden hacer:\n",
            "1. Traducci\u00f3n: Convertir texto de un idioma a otro.\n",
            "2. Resumen de texto: Summarizar largos documentos en versiones m\u00e1s cortas.\n",
            "3. Creaci\u00f3n de contenido: Generar art\u00edculos, correos electr\u00f3nicos, historias, etc.\n",
            "4. Resoluci\u00f3n de problemas: Ayudar a resolver problemas o proporcionar soluciones basadas en el texto.\n",
            "5. Chatbots: Usar para crear asistentes virtuales que puedan responder preguntas y ayudar con tareas.\n",
            "6. Crea contenido: Escribe contenido creativo como poes\u00eda, relatos, etc.\n",
            "7. Ajuste de lenguaje: Corrige errores gramaticales y mejora la escritura.\n",
            "\n",
            "Es importante tener en cuenta que aunque estos modelos son muy poderosos, todav\u00eda tienen limitaciones y pueden generar contenido incorrecto o inapropiado. Adem\u00e1s, la privacidad y la \u00e9tica son aspectos importantes a considerar al usar estos modelos. \n",
            "\n",
            "En resumen, los LLMs son herramientas poderosas que pueden ser utilizadas en una variedad de aplicaciones, pero tambi\u00e9n requieren cuidado y consideraci\u00f3n al utilizarlos. Es recomendable siempre verificar el contenido generado por estos modelos y estar atentos a sus posibles limitaciones. \n",
            "\n",
            "Adem\u00e1s, es importante destacar que los LLMs no son sustitutos perfectos para el pensamiento cr\u00edtico humano. Aunque pueden generar texto de\n"
          ]
        }
      ],
      "source": [
        "transformers_response = transformers_pipeline(\"\u00bfQu\u00e9 son los LLMs?\")\n",
        "print(transformers_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Streaming Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Cursos de Inteligencia Artificial en l\u00ednea\n",
            "Los LLMs, o Lenguajes de Lenguaje de Modelos, son modelos de aprendizaje autom\u00e1tico que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generaci\u00f3n de texto, el chatbot, el an\u00e1lisis de sentimientos, etc.\n",
            "En resumen, los LLMs son modelos de aprendizaje autom\u00e1tico que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generaci\u00f3n de texto, el chatbot, el an\u00e1lisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creaci\u00f3n de contenido hasta el asesoramiento.\n",
            "Los LLMs son modelos de aprendizaje autom\u00e1tico que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generaci\u00f3n de texto, el chatbot, el an\u00e1lisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creaci\u00f3n de contenido hasta el asesoramiento. Los LLMs est\u00e1n basados en redes neuronales y se entrenan con grandes conjuntos de datos de texto para aprender a generar texto similar al de un humano.\n",
            "Los LLMs est\u00e1n disponibles en diferentes plataformas, incluyendo Google's PaLM, Anthropic's Claude, Microsoft's Bing, e incluso tu propio tel\u00e9fono inteligente puede tener uno. Los LLMs pueden ser utilizados para una variedad de tareas, incluyendo la generaci\u00f3n de texto, el chatbot, el an\u00e1lisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creaci\u00f3n de contenido hasta el asesoramiento. Los LLMs est\u00e1n basados en redes neuronales y se entrenan con grandes conjuntos de datos de texto para aprender a generar texto similar al de un humano. Los LLMs est\u00e1n disponibles en diferentes plataformas, incluyendo Google's PaLM, Anthropic's Claude, Microsoft's Bing, e incluso tu propio tel\u00e9fono inteligente puede tener uno"
          ]
        }
      ],
      "source": [
        "for chunk in langchain_pipeline.stream(\"\u00bfQu\u00e9 son los LLMs?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | El Blog de la Inteligencia Artificial\n",
            "Home \u00bb Noticias \u00bb \u00bfQu\u00e9 son los LLMs?\n",
            "LLM es la sigla en ingl\u00e9s para Large Language Models, que traducido al castellano ser\u00eda \"Grandes Modelos de Lenguaje\". Son modelos de inteligencia artificial (IA) que permiten a las m\u00e1quinas entender y generar texto.\n",
            "Los LLMs se basan en el aprendizaje profundo, una t\u00e9cnica de IA que utiliza redes neuronales para modelar relaciones entre variables. Estos modelos son capaces de aprender patrones en grandes conjuntos de datos, lo que les permite predecir o generar texto similar al que se les ha proporcionado.\n",
            "Las principales caracter\u00edsticas de los LLMs incluyen:\n",
            "Capacidad de procesamiento de lenguaje natural: Los LLMs pueden entender y generar texto, lo que les permite interactuar con los usuarios de manera fluida y natural.\n",
            "Capacidad de aprendizaje continuo: Los LLMs pueden aprender y mejorar con cada interacci\u00f3n, lo que les permite ofrecer respuestas m\u00e1s precisas y relevantes a medida que ganan experiencia.\n",
            "Capacidad de generaci\u00f3n de texto: Los LLMs pueden generar texto similar al que se les ha proporcionado, lo que les permite crear contenido de calidad sin necesidad de intervenci\u00f3n humana manual.\n",
            "Ventajas de los LLMs:\n",
            "Mayor eficiencia: Los LLMs pueden procesar grandes cantidades de informaci\u00f3n de manera r\u00e1pida y precisa, lo que les permite ahorrar tiempo y recursos en comparaci\u00f3n con los m\u00e9todos humanos.\n",
            "Mejora de la precisi\u00f3n: Los LLMs pueden aprender y ajustarse a los patrones del lenguaje, lo que les permite ofrecer respuestas m\u00e1s precisas y relevantes a los usuarios.\n",
            "Aumento de la productividad: Los LLMs pueden automatizar tareas repetitivas y mon\u00f3tonas, liberando tiempo y recursos para que los humanos puedan centrarse en tareas m\u00e1s estrat\u00e9gicas y creativas.\n",
            "Entendimiento del lenguaje humano: Los LLMs pueden entender y generar texto en varios idiomas, lo que les permite interactuar con un amplio espectro de usuarios.\n",
            "Evaluaci\u00f3n de la inteligencia artificial\n",
            "Los LLMs est\u00e1n siendo utilizados en una variedad de aplicaciones, desde asistentes virtuales hasta an\u00e1lisis de texto y traducci\u00f3n autom\u00e1tica. Tambi\u00e9n est\u00e1n siendo evaluados por su capacidad para generar contenido de"
          ]
        }
      ],
      "source": [
        "for chunk in transformers_pipeline.stream(\"\u00bfQu\u00e9 son los LLMs?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use of Vector Databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage of ChromaDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To be able to use ChromaDB in Langchain, we first have to install `langchain-chroma`.",
        "\n",
        "``` bash\n",
        "pip install -qU chromadb langchain-chroma",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_kwargs = {\"device\": device}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"chroma_db\",\n",
        "    embedding_function=embedding_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1', '2', '3']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(page_content=\"This is a Mojo docs\", metadata={\"source\": \"Mojo source\"})\n",
        "document_2 = Document(page_content=\"This is Rust docs\", metadata={\"source\": \"Rust source\"})\n",
        "document_3 = Document(page_content=\"i will be deleted :(\")\n",
        "\n",
        "documents = [document_1, document_2, document_3]\n",
        "ids = [\"1\", \"2\", \"3\"]\n",
        "vector_store.add_documents(documents=documents, ids=ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Update documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "updated_document = Document(\n",
        "    page_content=\"This is Python docs\",\n",
        "    metadata={\"source\": \"Python source\"}\n",
        ")\n",
        "\n",
        "vector_store.update_documents(ids=[\"1\"],documents=[updated_document])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.delete(ids=[\"3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Document Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* This is Python docs [{'source': 'Python source'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search(query=\"python\",k=1)\n",
        "for doc in results:\n",
        "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filtered Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* This is Python docs [{'source': 'Python source'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search(query=\"python\",k=1,filter={\"source\": \"Python source\"})\n",
        "for doc in results:\n",
        "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Search with punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* [SIM=0.809454] This is Python docs [{'source': 'Python source'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search_with_score(query=\"python\",k=1)\n",
        "for doc, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Usage as Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'Python source'}, page_content='This is Python docs')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 1, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n",
        ")\n",
        "retriever.invoke(\"python\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cosine Similarity Measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method calculates the cosine similarity between two vectors, so instead of passing it two strings, you have to pass it two vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So first we create an embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_kwargs = {\"device\": device}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "document1 = \"Python\"\n",
        "document2 = \"python\"\n",
        "\n",
        "embedding1 = embeddings_model.embed_query(document1)\n",
        "embedding2 = embeddings_model.embed_query(document2)\n",
        "\n",
        "embedding1_numpy = np.array(embedding1)\n",
        "embedding2_numpy = np.array(embedding2)\n",
        "\n",
        "embedding1_torch = torch.tensor(embedding1_numpy).unsqueeze(0)\n",
        "embedding2_torch = torch.tensor(embedding2_numpy).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we can calculate the cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_chroma.vectorstores import cosine_similarity\n",
        "\n",
        "similarity = cosine_similarity(embedding1_torch, embedding2_torch)\n",
        "similarity\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "maximofn": {
      "date": "2024-12-16",
      "description_en": "Learn how to use Langchain with the most popular open-source integrations. In this post, we will explore how to integrate Langchain with ChromaDB, Ollama and HuggingFace.",
      "description_es": "Aprende a usar Langchain con las integraciones de c\u00f3digo abierto m\u00e1s populares. En este post, exploraremos c\u00f3mo integrar Langchain con ChromaDB, Ollama y HuggingFace.",
      "description_pt": "Aprenda a usar Langchain com as integra\u00e7\u00f5es de c\u00f3digo aberto mais populares. Neste post, exploraremos como integrar Langchain com ChromaDB, Ollama e HuggingFace.",
      "end_url": "langchain-opensource-integrations",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/langchain_opensource_integrations_shot.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/langchain_opensource_integrations_shot.webp",
      "keywords_en": "langchain, open source, integrations, chromadb, ollama, huggingface",
      "keywords_es": "langchain, open source, integrations, chromadb, ollama, huggingface",
      "keywords_pt": "langchain, open source, integrations, chromadb, ollama, huggingface",
      "title_en": "Langchain with open source integrations",
      "title_es": "Langchain con integraciones de c\u00f3digo abierto",
      "title_pt": "Langchain com integra\u00e7\u00f5es de c\u00f3digo aberto"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}