{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy backend on HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this post, we will see how to deploy a backend on HuggingFace. We will cover two methods: the common way by creating an application with Gradio, and a different option using FastAPI, Langchain, and Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For both cases, it will be necessary to have an account on HuggingFace, as we are going to deploy the backend in a HuggingFace space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy backend with Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all, we create a new space on Hugging Face.\n",
        "\n",
        "* We put a name, a description, and choose the license.\n",
        "* We chose Gradio as the type of SDK. When choosing Gradio, templates will appear, so we selected the chatbot template.\n",
        "* We select the HW on which we are going to deploy the backend, I will choose the free CPU, but you choose what you consider best.\n",
        "* And finally, we need to choose whether we want to create the space as public or private.\n",
        "\n",
        "![backend gradio - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When creating the space, we can clone it or we can view the files on the Hugging Face page itself. We can see that 3 files have been created: `app.py`, `requirements.txt`, and `README.md`. So let's take a look at what to put in each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we have the code for the application. Since we chose the chatbot template, we already have a lot done, but we will need to change 2 things: first, the language model and the system prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a language model, I see ``HuggingFaceH4/zephyr-7b-beta``, but we are going to use ``Qwen/Qwen2.5-72B-Instruct``, which is a very capable model.\n",
        "\n",
        "So, look for the text ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` and replace it with ``client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")``, or wait until I put all the code later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also change the system prompt, which by default is ``You are a friendly Chatbot.``, but since the model is trained mostly in English, it is likely that if you speak to it in another language it will respond in English, so we will change it to ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
        "\n",
        "So, look for the text ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` and replace it with ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, or wait as I am going to put all the code now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\"\"\"\n",
        "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\n",
        "client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "\n",
        "\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
        "        if val[1]:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    for message in client.chat_completion(\n",
        "        messages,\n",
        "        max_tokens=max_tokens,\n",
        "        stream=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    ):\n",
        "        token = message.choices[0].delta.content\n",
        "\n",
        "        response += token\n",
        "        yield response\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the file where the dependencies will be written, but for this case it's going to be very simple:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the file where we will put the information about the space. In HuggingFace spaces, at the beginning of the readmes, a code is placed so that HuggingFace knows how to display the thumbnail of the space, which file to use to run the code, SDK version, etc.\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2\n",
        "emoji: ðŸ’¬\n",
        "colorFrom: yellow\n",
        "colorTo: purple\n",
        "sdk: gradio\n",
        "sdk_version: 5.0.1\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Gradio SmolLM2 chat\n",
        "---\n",
        "\n",
        "An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have cloned the space, we need to make a commit and a push. If we have modified the files in HuggingFace, saving them is enough.\n",
        "\n",
        "So when the changes are in HuggingFace, we will have to wait a few seconds for the space to build and then we can use it.\n",
        "\n",
        "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alright, we've created a chatbot, but that wasn't the intention; we came here to build a backend! Stop, stop, look at what it says below the chatbot\n",
        "\n",
        "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see a text ``Use via API``, where if we click it, a menu with an API opens for us to use the chatbot.\n",
        "\n",
        "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that it provides documentation on how to use the API, both with Python, JavaScript, and bash."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the example Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space âœ”\n",
            "Â¡Hola MÃ¡ximo! Mucho gusto, estoy bien, gracias por preguntar. Â¿CÃ³mo estÃ¡s tÃº? Â¿En quÃ© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, Â¿cÃ³mo estÃ¡s? Me llamo MÃ¡ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are making calls to the `InferenceClient` API from HuggingFace, so we might wonder, why did we create a backend if we can call the HuggingFace API directly? You will see this in the following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es MÃ¡ximo. Â¿Es correcto?\n"
          ]
        }
      ],
      "source": [
        "result = client.predict(\n",
        "\t\tmessage=\"Â¿CÃ³mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Gradio chat template handles the history for us, so that each time we create a new `client`, a new conversation thread is created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try to create a new client and see if a new conversation thread is created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space âœ”\n",
            "Hola Luis, estoy muy bien, gracias por preguntar. Â¿CÃ³mo estÃ¡s tÃº? Es un gusto conocerte. Â¿En quÃ© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "new_client = Client(\"Maximofn/SmolLM2\")\n",
        "result = new_client.predict(\n",
        "\t\tmessage=\"Hola, Â¿cÃ³mo estÃ¡s? Me llamo Luis\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we ask him again what my name is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Te llamas Luis. Â¿Hay algo mÃ¡s en lo que pueda ayudarte?\n"
          ]
        }
      ],
      "source": [
        "result = new_client.predict(\n",
        "\t\tmessage=\"Â¿CÃ³mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, we have two clients, each with their own conversation thread."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy backend with FastAPI, Langchain and Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to do the same, create a chatbot backend, with the same model, but in this case using FastAPI, Langchain and Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to create a new space, but in this case we will do it differently\n",
        "\n",
        "* We put a name, a description, and choose the license.\n",
        "* We chose Docker as the type of SDK. When choosing Docker, templates will appear, so we selected a blank template.\n",
        "* We select the HW on which we will deploy the backend, I will choose the free CPU, but you choose what you consider best.\n",
        "* And lastly, we need to choose whether we want to create the space as public or private.\n",
        "\n",
        "![backend docker - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, when creating the space, we see that we only have one file, the `README.md`. So we are going to have to create all the code ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create the application code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with the necessary libraries\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "```\n",
        "\n",
        "We load `fastapi` to create the API routes, `pydantic` to create the query templates, `huggingface_hub` to create a language model, `langchain` to indicate whether messages are from the chatbot or the user, and `langgraph` to create the chatbot.\n",
        "\n",
        "We also load `os` and `dotenv` to be able to load the environment variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the HuggingFace token\n",
        "\n",
        "``` python\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the language model\n",
        "\n",
        "``` python\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create a function to call the model\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```\n",
        "\n",
        "We convert the messages from LangChain format to HuggingFace format, so we can use the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a template for the queries\n",
        "\n",
        "``` python\n",
        "class QueryRequest(BaseModel):\n",
        "query: str\n",
        "thread_id: str = \"default\"\n",
        "```\n",
        "\n",
        "The queries will have a `query`, the user's message, and a `thread_id`, which is the identifier of the conversation thread and we will explain later what we use it for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a LangGraph graph\n",
        "\n",
        "``` python\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "With this, we create a LangGraph graph, which is a data structure that allows us to create a chatbot and manages the chatbot's state for us, including, among other things, the message history. This way, we don't have to do it ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the FastAPI application\n",
        "\n",
        "``` python\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the API endpoints\n",
        "\n",
        "``` python\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "\"Welcome endpoint\"\n",
        "return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "\"\"\"\n",
        "Endpoint to generate text using the language model\n",
        "    \n",
        "Args:\n",
        "request: QueryRequest\n",
        "query: str\n",
        "thread_id: str = \"default\"\n",
        "\n",
        "Devuelve:\n",
        "dict: A dictionary containing the generated text and the thread ID\n",
        "\"\"\"\n",
        "try:\n",
        "# Configure the thread ID\n",
        "config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "# Create the input message\n",
        "input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "# Invoke the graph\n",
        "output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "# Get the model response\n",
        "response = output[\"messages\"][-1].content\n",
        "        \n",
        "return {\n",
        "\"generated_text\": \"response\",\n",
        "\"thread_id\": request.thread_id\n",
        "It seems like you've provided an incomplete or incorrect Markdown text to translate. Could you please provide the correct Markdown text that needs translation?\n",
        "except Exception as e:\n",
        "raise HTTPException(status_code=500, detail=f\"Error generating text: {str(e)}\")\n",
        "```\n",
        "\n",
        "We have created the endpoint `/` that will return a text when we access the API, and the endpoint `/generate` which we will use to generate the text.\n",
        "\n",
        "If we look at the `generate` function, we have the variable `config`, which is a dictionary that contains the `thread_id`. This `thread_id` allows us to maintain a message history for each user, so different users can use the same endpoint and have their own message history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we have the code for the application to run.\n",
        "\n",
        "``` python\n",
        "if __name__ == \"__main__\":\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's write all the code together\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "\"\"\"\n",
        "Llamar al modelo con los mensajes dados\n",
        "\n",
        "Args:\n",
        "state: MessagesState\n",
        "\n",
        "Devuelve:\n",
        "dict: A dictionary containing the generated text and the thread ID\n",
        "\"\"\"\n",
        "# Convert LangChain messages to HuggingFace format\n",
        "hf_messages = []\n",
        "for msg in state[\"messages\"]:\n",
        "if isinstance(msg, HumanMessage):\n",
        "hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "elif isinstance(msg, AIMessage):\n",
        "hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "# Call the API\n",
        "response = model.chat_completion(\n",
        "messages=hf_messages,\n",
        "temperature=0.5,\n",
        "max_tokens=64,\n",
        "top_p=0.7\n",
        ")\n",
        "    \n",
        "# Convert the response to LangChain format\n",
        "ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# Define the data model for the request\n",
        "class QueryRequest(BaseModel):\n",
        "query: str\n",
        "thread_id: str = \"default\"\n",
        "\n",
        "# Create the FastAPI application\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "\"Welcome endpoint\"\n",
        "return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "\"\"\"\n",
        "Endpoint to generate text using the language model\n",
        "    \n",
        "Args:\n",
        "request: QueryRequest\n",
        "query: str\n",
        "thread_id: str = \"default\"\n",
        "\n",
        "Returns:\n",
        "dict: A dictionary containing the generated text and the thread ID\n",
        "\"\"\"\n",
        "try:\n",
        "# Configure the thread ID\n",
        "config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "# Create the input message\n",
        "input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "# Invoke the graph\n",
        "output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "# Get the model response\n",
        "response = output[\"messages\"][-1].content\n",
        "        \n",
        "return {\n",
        "\"generated_text\": \"response,\"\n",
        "\"thread_id\": request.thread_id\n",
        "}\n",
        "except Exception as e:\n",
        "raise HTTPException(status_code=500, detail=f\"Error generating text: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we see how to create the Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we indicate which image we are going to start from\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the working directory\n",
        "\n",
        "``` dockerfile\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We copy the file with the dependencies and install\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We copy the rest of the code\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user . /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We expose port 7860\n",
        "\n",
        "``` dockerfile\n",
        "EXPOSE 7860\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the environment variables\n",
        "\n",
        "``` dockerfile\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Secret exists!\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, we indicate the command to run the application\n",
        "\n",
        "``` dockerfile\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we put it all together\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Secret exists!\"\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the file with the dependencies\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "requests\n",
        "pydantic>=2.0.0\n",
        "langchain\n",
        "langchain-huggingface\n",
        "langchain-core\n",
        "langgraph > 0.2.27\n",
        "python-dotenv.2.11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the README.md file with information about the space and instructions for HuggingFace.\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2 Backend\n",
        "emoji: ðŸ“Š\n",
        "colorFrom: yellow\n",
        "colorTo: red\n",
        "sdk: docker\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Backend of SmolLM2 chat\n",
        "app_port: 7860\n",
        "---\n",
        "\n",
        "# SmolLM2 Backend\n",
        "\n",
        "This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "### In HuggingFace Spaces\n",
        "\n",
        "This project is designed to run in HuggingFace Spaces. To configure it:\n",
        "\n",
        "1. Create a new Space in HuggingFace with SDK Docker\n",
        "2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:\n",
        "   - Go to the \"Settings\" tab of your Space\n",
        "   - Scroll down to the \"Repository secrets\" section\n",
        "   - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value\n",
        "   - Save the changes\n",
        "\n",
        "### Local development\n",
        "\n",
        "For local development:\n",
        "\n",
        "1. Clone this repository\n",
        "2. Create a `.env` file in the project root with your HuggingFace token:\n",
        "   ``\n",
        "   HUGGINGFACE_TOKEN=your_token_here\n",
        "   ``\n",
        "3. Install the dependencies:\n",
        "   ``\n",
        "   pip install -r requirements.txt\n",
        "   ``\n",
        "\n",
        "## Local execution\n",
        "\n",
        "``bash\n",
        "uvicorn app:app --reload\n",
        "``\n",
        "\n",
        "The API will be available at `http://localhost:8000`.\n",
        "\n",
        "## Endpoints\n",
        "\n",
        "### GET `/`\n",
        "\n",
        "Welcome endpoint that returns a greeting message.\n",
        "\n",
        "### POST `/generate`\n",
        "\n",
        "Endpoint to generate text using the language model.\n",
        "\n",
        "**Request parameters:**\n",
        "``json\n",
        "{\n",
        "  \"query\": \"Your question here\",\n",
        "  \"thread_id\": \"optional_thread_identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "**Response:**\n",
        "``json\n",
        "{\n",
        "  \"generated_text\": \"Generated text by the model\",\n",
        "  \"thread_id\": \"thread identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "## Docker\n",
        "\n",
        "To run the application in a Docker container:\n",
        "\n",
        "``bash\n",
        "# Build the image\n",
        "docker build -t smollm2-backend .\n",
        "\n",
        "# Run the container\n",
        "docker run -p 8000:8000 --env-file .env smollm2-backend\n",
        "``\n",
        "\n",
        "## API documentation\n",
        "\n",
        "The interactive API documentation is available at:\n",
        "- Swagger UI: `http://localhost:8000/docs`\n",
        "- ReDoc: `http://localhost:8000/redoc`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace Token\n",
        "\n",
        "If you've noticed in the code and the Dockerfile, we used a HuggingFace token, so we will have to create one. In our HuggingFace account, we create a [new token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), give it a name, and grant it the following permissions:\n",
        "\n",
        "* Read access to contents of all repos under your personal namespace\n",
        "* Read access to contents of all repos under your personal namespace\n",
        "* Make calls to inference providers\n",
        "* Make calls to Inference Endpoints\n",
        "\n",
        "![backend docker - token](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add the token to the space secrets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the token, we need to add it to the space. At the top of the app, we will see a button called `Settings`, we press it and we will be able to see the space configuration section.\n",
        "\n",
        "If we scroll down, we can see a section where we can add `Variables` and `Secrets`. In this case, since we are adding a token, we will add it to the `Secrets`.\n",
        "\n",
        "We set the name to `HUGGINGFACE_TOKEN` and the value of the token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have cloned the space, we need to make a commit and a push. If we have modified the files in HuggingFace, saving them is enough.\n",
        "\n",
        "So when the changes are in HuggingFace, we will have to wait a few seconds for the space to be built and then we can use it.\n",
        "\n",
        "In this case, we have only built a backend, so what we will see when entering the space is what we defined in the endpoint `/`\n",
        "\n",
        "![backend docker - space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to know the URL of the backend to be able to make API calls. To do this, we have to click on the three dots in the top right corner to see the options.\n",
        "\n",
        "![backend docker - options](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the drop-down menu, we click on `Embed this Space`, which will open a window indicating how to embed the space with an iframe and also providing the URL of the space.\n",
        "\n",
        "![backend docker - embed](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we now go to that URL, we will see the same as in space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FastAPI, besides being an extremely fast API, has another great advantage: it generates documentation automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we add `/docs` to the URL we saw earlier, we will be able to see the API documentation with `Swagger UI`.\n",
        "\n",
        "![backend docker - swagger doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also add `/redoc` to the URL to view the documentation with `ReDoc`.\n",
        "\n",
        "![backend docker - redoc doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The good thing about `Swagger UI` documentation is that it allows us to test the API directly from the browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add `/docs` to the URL we obtained, open the dropdown for the `/generate` endpoint, and click on `Try it out`. We modify the value of the `query` and the `thread_id`, and then press `Execute`.\n",
        "\n",
        "In the first case I will put\n",
        "\n",
        "* **query**: Hello, how are you? I'm Maximo\n",
        "* **thread_id**: user1\n",
        "\n",
        "![backend docker - test API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We received the following response: `Hello Maximo! I'm doing very well, thank you for asking. How are you? What can I help you with today?`\n",
        "\n",
        "![backend docker -response 1 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now try the same question but with a different `thread_id`, in this case `user2`.\n",
        "\n",
        "![backend docker - query 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And it responds like this `Hello Luis! I'm doing very well, thank you for asking. How are you? What can I help you with today?`\n",
        "\n",
        "![backend docker - response 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we ask for our name with both users and get this\n",
        "\n",
        "* For the user **user1**: `Your name is Maximus. Is there anything else I can help you with?`\n",
        "* For the user **user2**: `You are called Luis. Is there anything else I can help you with today, Luis?`\n",
        "\n",
        "![backend docker - response 2 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp)\n",
        "\n",
        "![backend docker - response 2 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy backend with Gradio and model running on the server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two backends we have created are actually not running a model, but rather making calls to HuggingFace Inference Endpoints. However, you might want everything to run on the server, including the model. It could be that you have fine-tuned an LLM for your use case, so you can no longer make calls to Inference Endpoints.\n",
        "\n",
        "So let's see how to modify the code of the two backends to run a model on the server and not make calls to Inference Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When creating the space on HuggingFace, we do the same as before: create a new space, give it a name and a description, select Gradio as the SDK, choose the hardware on which we will deploy itâ€”I select the most basic and free hardwareâ€”and choose whether to make it private or public."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to make changes in `app.py` and `requirements.txt` so that instead of making calls to Inference Endpoints, the model runs locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The changes we have to make are"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library and import `torch`\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of creating a model using `InferenceClient`, we create it with `AutoModelForCausalLM` and `AutoTokenizer`.\n",
        "\n",
        "``` python\n",
        "# Load the model and the tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "model_name,\n",
        "torch_dtype=torch.float16,\n",
        "device_map=\"auto\"\n",
        ")\n",
        "```\n",
        "\n",
        "I use `HuggingFaceTB/SmolLM2-1.7B-Instruct` because it is a fairly capable model with only 1.7B parameters. Since I chose the most basic hardware, I can't use very large models. If you want to use a larger model, you have two options: use the free hardware and accept that inference will be slower, or use more powerful hardware, but at a cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modify the `respond` function to build the prompt with the necessary structure for the `transformers` library, tokenize the prompt, perform inference, and detokenize the response.\n",
        "\n",
        "``` python\n",
        "def respond(\n",
        "message,\n",
        "history: list[tuple[str, str]],\n",
        "It seems like you've mentioned a \"system_message,\" but there's no specific content to translate. If you have a markdown text that needs translation, please provide it and I'll translate it for you.\n",
        "max_tokens,\n",
        "temperature,\n",
        "top_p,\n",
        "):\n",
        "# Build the prompt with the correct format\n",
        "prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "for val in history:\n",
        "if val[0]:\n",
        "prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "if val[1]:\n",
        "prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "# Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "# Generate the response\n",
        "outputs = model.generate(\n",
        "**inputs,**\n",
        "max_new_tokens=max_tokens,\n",
        "temperature=temperature,\n",
        "top_p=top_p,\n",
        "do_sample=True,\n",
        "pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "    \n",
        "# Decode the response\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "# Extract only the assistant's response part\n",
        "response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "yield response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I leave all the code\n",
        "\n",
        "``` python\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\n",
        "\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    # Construir el prompt con el formato correcto\n",
        "    prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "        if val[1]:\n",
        "            prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "    prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "    # Tokenizar el prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generar la respuesta\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decodificar la respuesta\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extraer solo la parte de la respuesta del asistente\n",
        "    response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "    yield response\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(\n",
        "            value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", \n",
        "            label=\"System message\"\n",
        "        ),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this file, we need to add the new libraries we are going to use, in this case `transformers`, `accelerate` and `torch`. The entire file would be:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "gradio>=4.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.25.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### API Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We deploy the space and test the API directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2-localmodel.hf.space âœ”\n",
            "Hola MÃ¡ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en dÃ­a. Â¿CÃ³mo puedo servirte?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2_localModel\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, Â¿cÃ³mo estÃ¡s? Me llamo MÃ¡ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'm surprised how quickly the model responds even on a server without a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy backend with FastAPI, Langchain and Docker and model running on the server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we do the same as before, but with FastAPI, LangChain and Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When creating the space on HuggingFace, we do the same as before: create a new space, give it a name and a description, select Docker as the SDK, choose the hardware on which we are going to deploy itâ€”I select the most basic and free hardwareâ€”and decide whether to make it private or public."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We no longer import `InferenceClient` and now import `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library and import `torch`.\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We instantiate the model and the tokenizer with `AutoModelForCausalLM` and `AutoTokenizer`.\n",
        "\n",
        "``` python\n",
        "# Initialize the model and tokenizer\n",
        "print(\"Loading model and tokenizer...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "\n",
        "try:\n",
        "# Load the model in BF16 format for better performance and lower memory usage\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "if device == \"cuda\":\n",
        "print(\"Using GPU for the model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "model_name,\n",
        "torch_dtype=torch.bfloat16,\n",
        "device_map=\"auto\",\n",
        "low_cpu_mem_usage=True\n",
        ")\n",
        "else:\n",
        "print(\"Using CPU for the model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "model_name,\n",
        "device_map={\"\": device},\n",
        "torch_dtype=torch.float32\n",
        ")\n",
        "\n",
        "print(f\"Model successfully loaded on: {device}\")\n",
        "except Exception as e:\n",
        "print(f\"Error loading the model: {str(e)}\")\n",
        "raise\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We redefine the `call_model` function to perform inference with the local model.\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to chat format\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Prepare the input using the chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=512,  # Increase the number of tokens for longer responses\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and clean the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response (after the last user message)\n",
        "    response = response.split(\"Assistant:\")[-1].strip()\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to remove `langchain-huggingface` and add `transformers`, `accelerate` and `torch` in the `requirements.txt` file. The file would look like:\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "requests\n",
        "pydantic>=2.0.0\n",
        "langchain>=0.1.0\n",
        "langchain-core>=0.1.10\n",
        "langgraph>=0.2.27\n",
        "python-dotenv>=1.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.26.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We no longer need to have `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` because since the model will be on the server and we won't be making calls to Inference Endpoints, we don't need the token. The file would look like:\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We deploy the space and test the API. In this case, I will test it directly from Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta: system\n",
            "You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\n",
            "user\n",
            "Hola, Â¿cÃ³mo estÃ¡s?\n",
            "assistant\n",
            "Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.\n",
            "Thread ID: user1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://maximofn-smollm2-backend-localmodel.hf.space/generate\"\n",
        "data = {\n",
        "    \"query\": \"Hola, Â¿cÃ³mo estÃ¡s?\",\n",
        "    \"thread_id\": \"user1\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"Respuesta:\", result[\"generated_text\"])\n",
        "    print(\"Thread ID:\", result[\"thread_id\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This takes a bit longer than the previous one. In reality, it takes the normal time for a model running on a server without a GPU. The odd thing is when we deploy it on Gradio. I don't know what HuggingFace does behind the scenes, or maybe it's just a coincidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have seen how to create a backend with an LLM, both by making calls to the HuggingFace Inference Endpoint and by making calls to a model running locally. We have seen how to do this with Gradio or with FastAPI, Langchain, and Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here you have the knowledge to deploy your own models, even if they are not LLMs, they could be multimodal models. From here you can do whatever you want."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "maximofn": {
      "date": "2025-03-02",
      "description_en": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "description_es": "Â¿Quieres desplegar un backend con tu propio LLM? En este post te explico cÃ³mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.",
      "description_pt": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "end_url": "deploy-backend-with-llm-in-huggingface",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "keywords_en": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_es": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_pt": "hugging face, fastapi, langchain, docker, backend, llm",
      "title_en": "Deploy backend with LLM in HuggingFace",
      "title_es": "Desplegar backend con LLM en HuggingFace",
      "title_pt": "Desplegar backend com LLM no HuggingFace"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
