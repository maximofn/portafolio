{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deployar backend no HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste post, vamos ver como deployar um backend no HuggingFace. Vamos ver como fazer isso de duas maneiras, atrav√©s da forma comum, criando uma aplica√ß√£o com Gradio, e atrav√©s de uma op√ß√£o diferente usando FastAPI, Langchain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ambos casos ser√° necess√°rio ter uma conta no HuggingFace, j√° que vamos implantar o backend em um espa√ßo do HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend com Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar espa√ßo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro de tudo, criamos um novo espa√ßo na Hugging Face.\n",
        "\n",
        "* Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.\n",
        "* Escolhemos o Gradio como o tipo de SDK. Ao escolher o Gradio, ser√£o exibidas algumas templates, ent√£o escolhemos a template do chatbot.\n",
        "* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.\n",
        "* E por √∫ltimo, temos que escolher se queremos criar o espa√ßo p√∫blico ou privado.\n",
        "\n",
        "![backend gradio - criar espa√ßo](https://images.maximofn.com/backend-gradio-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ao criar o space, podemos clon√°-lo ou podemos ver os arquivos na pr√≥pria p√°gina do HuggingFace. Podemos ver que foram criados 3 arquivos, `app.py`, `requirements.txt` e `README.md`. Ent√£o, vamos ver o que colocar em cada um."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui est√° o c√≥digo do aplicativo. Como escolhemos o template de chatbot, j√° temos muito feito, mas vamos ter que mudar 2 coisas: primeiro, o modelo de linguagem e o system prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como modelo de linguagem, vejo ``HuggingFaceH4/zephyr-7b-beta``, mas vamos utilizar ``Qwen/Qwen2.5-72B-Instruct``, que √© um modelo muito capaz.\n",
        "\n",
        "Ent√£o, procure pelo texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` e substitua-o por ``client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")``, ou espere que colocarei todo o c√≥digo mais tarde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb√©m vamos alterar o system prompt, que por padr√£o √© ``You are a friendly Chatbot.``, mas como o modelo foi treinado principalmente em ingl√™s, √© prov√°vel que se voc√™ falar com ele em outro idioma, ele responda em ingl√™s. Ent√£o, vamos mud√°-lo para ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
        "\n",
        "Ent√£o, procure pelo texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` e substitua-o por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, ou espere at√© eu colocar todo o c√≥digo agora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "import gradio as gr\n",
        "do huggingface_hub import InferenceClient\n",
        "\n",
        "\"\"\"\"\"\"\n",
        "Para mais informa√ß√µes sobre o suporte da API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\"\"\"\n",
        "client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "\n",
        "\n",
        "def responder(\n",
        "mensagem,\n",
        "hist√≥ria: list[tuple[str, str]],\n",
        "Mensagem do sistema,\n",
        "max_tokens,\n",
        "temperatura,\n",
        "top_p,\n",
        "):\n",
        "mensagens = [{\"papel\": \"sistema\", \"conte√∫do\": system_message}]\n",
        "\n",
        "for val in history:\n",
        "if val[0]:\n",
        "messages.append({\"role\": \"user\", \"content\": val[0]})\n",
        "if val[1]:\n",
        "messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
        "\n",
        "messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "response = \"\"\n",
        "\n",
        "para mensagem em client.chat_completion(\n",
        "mensagens,\n",
        "max_tokens=max_tokens,\n",
        "stream=True,\n",
        "temperature=temperature,\n",
        "top_p=top_p,\n",
        "):\n",
        "token = message.choices[0].delta.content\n",
        "\n",
        "response += token\n",
        "yield response\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\n",
        "Para informa√ß√µes sobre como personalizar a ChatInterface, consulte a documenta√ß√£o do gradio: https://www.gradio.app/docs/gradio/chatinterface\n",
        "\"\"\"\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "responda,\n",
        "```markdown\n",
        "additional_inputs=[\n",
        "```\n",
        "gr.Textbox(value=\"Voc√™ √© um chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™.\", label=\"Mensagem do sistema\"),\n",
        "gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"M√°ximo de novos tokens\"),\n",
        "gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo=\"Temperatura\"),\n",
        "gr.Slider(\n",
        "m√≠nimo=0.1,\n",
        "m√°ximo=1.0,\n",
        "value=0.95,\n",
        "passo=0.05,\n",
        "label=\"Top-p (amostragem do n√∫cleo)\"\n",
        "),\n",
        "],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este √© o arquivo onde ser√£o escritas as depend√™ncias, mas para este caso vai ser muito simples:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LEIA-ME.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este √© o arquivo no qual vamos colocar as informa√ß√µes do espa√ßo. Nos spaces da HuggingFace, no in√≠cio dos readmes, coloca-se um c√≥digo para que a HuggingFace saiba como exibir a miniatura do espa√ßo, qual arquivo deve ser usado para executar o c√≥digo, vers√£o do sdk, etc.\n",
        "\n",
        "``` md\n",
        "---\n",
        "t√≠tulo: SmolLM2\n",
        "emoji: üí¨\n",
        "colorFrom: amarelo\n",
        "colorTo: roxo\n",
        "sdk: gradio\n",
        "sdk_version: 5.0.1\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "licen√ßa: apache-2.0\n",
        "short_description: Bate-papo com o Gradio SmolLM2\n",
        "---\n",
        "\n",
        "Um exemplo de chatbot usando [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index) e a [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implanta√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.\n",
        "\n",
        "Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.\n",
        "\n",
        "![backend gradio - chatbot](https://images.maximofn.com/backend-gradio-chatbot.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Muito bem, fizemos um chatbot, mas n√£o era essa a inten√ß√£o, aqui t√≠nhamos vindo fazer um backend! P√°ra, p√°ra, olha o que diz abaixo do chatbot\n",
        "\n",
        "![backend gradio - Use via API](https://images.maximofn.com/backend-gradio-chatbot-edited.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver um texto ``Use via API``, onde se clicarmos, se abrir√° um menu com uma API para poder usar o chatbot.\n",
        "\n",
        "![backend gradio - API](https://images.maximofn.com/backend%20gradio%20-%20API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que nos d√° uma documenta√ß√£o de como usar a API, tanto com Python, com JavaScript, quanto com bash."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos o c√≥digo de exemplo de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
            "¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estamos fazendo chamadas √† API do `InferenceClient` da HuggingFace, ent√£o poder√≠amos pensar, Para que fizemos um backend, se podemos chamar diretamente a API da HuggingFace? Bem, voc√™ vai ver isso abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es M√°ximo. ¬øEs correcto?\n"
          ]
        }
      ],
      "source": [
        "result = client.predict(\n",
        "\t\tmessage=\"¬øC√≥mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O modelo de bate-papo do Gradio gerencia o hist√≥rico para n√≥s, de forma que cada vez que criamos um novo `cliente`, uma nova thread de conversa √© criada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a tentar criar um novo cliente e ver se uma nova thread de conversa √© criada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
            "Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "new_client = Client(\"Maximofn/SmolLM2\")\n",
        "result = new_client.predict(\n",
        "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo Luis\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos perguntar novamente como me chamo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?\n"
          ]
        }
      ],
      "source": [
        "result = new_client.predict(\n",
        "\t\tmessage=\"¬øC√≥mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, temos dois clientes, cada um com seu pr√≥prio fio de conversa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy do backend com FastAPI, Langchain e Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos a fazer o mesmo, criar um backend de um chatbot, com o mesmo modelo, mas nesse caso usando FastAPI, Langchain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar espa√ßo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos que criar um novo espa√ßo, mas nesse caso faremos de outra maneira\n",
        "\n",
        "* Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.\n",
        "* Escolhemos Docker como o tipo de SDK. Ao escolher Docker, aparecer√£o modelos, ent√£o escolhemos um modelo em branco.\n",
        "* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.\n",
        "* E por fim, √© preciso escolher se queremos criar o espa√ßo p√∫blico ou privado.\n",
        "\n",
        "![backend docker - criar espa√ßo](https://images.maximofn.com/backend-docker-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, ao criar o space, vemos que temos apenas um arquivo, o ``README.md``. Ent√£o vamos ter que criar todo o c√≥digo n√≥s mesmos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a criar o c√≥digo do aplicativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Come√ßamos com as bibliotecas necess√°rias\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "do huggingface_hub import InferenceClient\n",
        "\n",
        "```markdown\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "```\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "```\n",
        "\n",
        "Carregamos `fastapi` para poder criar as rotas da API, `pydantic` para criar o template das queries, `huggingface_hub` para poder criar um modelo de linguagem, `langchain` para indicar ao modelo se as mensagens s√£o do chatbot ou do usu√°rio e `langgraph` para criar o chatbot.\n",
        "\n",
        "Al√©m disso, carregamos `os` e `dotenv` para poder carregar as vari√°veis de ambiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Carregamos o token do HuggingFace\n",
        "\n",
        "``` python\n",
        "# Token da HuggingFace\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o modelo de linguagem\n",
        "\n",
        "``` python\n",
        "# Inicializar o modelo da HuggingFace\n",
        "model = InferenceClient(\n",
        "model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos agora uma fun√ß√£o para chamar o modelo\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```\n",
        "\n",
        "Convertemos as mensagens do formato LangChain para o formato HuggingFace, assim podemos usar o modelo de linguagem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos uma template para as queries\n",
        "\n",
        "``` python\n",
        "class QueryRequest(BaseModel):\n",
        "query: str\n",
        "thread_id: str = \"padr√£o\"\n",
        "```\n",
        "\n",
        "As consultas ter√£o um `query`, a mensagem do usu√°rio, e um `thread_id`, que √© o identificador do fio da conversa√ß√£o e mais adiante explicaremos para que o utilizamos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos um grafo de LangGraph\n",
        "\n",
        "``` python\n",
        "# Definir o gr√°fico\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Defina o n√≥do na gr√°fico\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"modelo\", call_model)\n",
        "\n",
        "# Adicionar mem√≥ria\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "Com isso, criamos um grafo de LangGraph, que √© uma estrutura de dados que nos permite criar um chatbot e gerenciar o estado do chatbot para n√≥s, ou seja, entre outras coisas, o hist√≥rico de mensagens. Dessa forma, n√£o precisamos fazer isso n√≥s mesmos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos a aplica√ß√£o de FastAPI\n",
        "\n",
        "``` python\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API para gerar texto usando LangChain e LangGraph\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos os endpoints da API\n",
        "\n",
        "``` python\n",
        "# Ponto de entrada Bem-vindo\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "\"\"\"Ponto de entrada Welcome\"\"\"\n",
        "return {\"detail\": \"Bem-vindo ao tutorial de FastAPI, Langchain, Docker\"}\n",
        "\n",
        "# Gerar ponto final\n",
        "@app.post(\"/generate\")\n",
        "async def gerar(request: QueryRequest):\n",
        "\"\"\"\"\"\"\n",
        "Ponto final para gerar texto usando o modelo de linguagem\n",
        "    \n",
        "Argumentos:\n",
        "solicita√ß√£o: QueryRequest\n",
        "query: str\n",
        "thread_id: str = \"padr√£o\"\n",
        "\n",
        "Retorna:\n",
        "um dicion√°rio contendo o texto gerado e o ID do thread\n",
        "\"\"\"\"\"\"\n",
        "tente:\n",
        "# Configurar o ID da thread\n",
        "config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "# Crie a mensagem de entrada\n",
        "input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "# Invocar o gr√°fico\n",
        "output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "# Obter a resposta do modelo\n",
        "resposta = output[\"messages\"][-1].conte√∫do\n",
        "        \n",
        "return {\n",
        "\"generated_text\": resposta,\n",
        "\"thread_id\": request.thread_id\n",
        "}\n",
        "except Exception as e:\n",
        "raise HTTPException(status_code=500, detail=f\"Erro ao gerar texto: {str(e)}\")\n",
        "```\n",
        "\n",
        "Criamos o endpoint `/` que nos retornar√° um texto quando acessarmos a API, e o endpoint `/generate` que √© o que usaremos para gerar o texto.\n",
        "\n",
        "Se n√≥s olharmos para a fun√ß√£o `generate`, temos a vari√°vel `config`, que √© um dicion√°rio que cont√©m o `thread_id`. Este `thread_id` √© o que nos permite ter um hist√≥rico de mensagens de cada usu√°rio, desta forma, diferentes usu√°rios podem usar o mesmo endpoint e ter seu pr√≥prio hist√≥rico de mensagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por √∫ltimo, temos o c√≥digo para que se possa executar a aplica√ß√£o\n",
        "\n",
        "``` python\n",
        "if __name__ == \"__main__\":\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos escrever todo o c√≥digo juntos\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# Define the data model for the request\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    thread_id: str = \"default\"\n",
        "\n",
        "# Create the FastAPI application\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "    \"\"\"Welcome endpoint\"\"\"\n",
        "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    Endpoint to generate text using the language model\n",
        "    \n",
        "    Args:\n",
        "        request: QueryRequest\n",
        "        query: str\n",
        "        thread_id: str = \"default\"\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configure the thread ID\n",
        "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "        # Create the input message\n",
        "        input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "        # Invoke the graph\n",
        "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "        # Get the model response\n",
        "        response = output[\"messages\"][-1].content\n",
        "        \n",
        "        return {\n",
        "            \"generated_text\": response,\n",
        "            \"thread_id\": request.thread_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vemos como criar o Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro indicamos a partir de qual imagem vamos come√ßar\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos o diret√≥rio de trabalho\n",
        "\n",
        "``` dockerfile\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos o arquivo com as depend√™ncias e instalamos\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos o resto do c√≥digo\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user . /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exponhamos o porto 7860\n",
        "\n",
        "``` dockerfile\n",
        "EXPOSE 7860\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos as vari√°veis de ambiente\n",
        "\n",
        "``` dockerfile\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Segredo existe!\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por √∫ltimo, indicamos o comando para executar a aplica√ß√£o\n",
        "\n",
        "``` dockerfile\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora colocamos tudo junto\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Segredo existe!\"\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o arquivo com as depend√™ncias\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "pedidos\n",
        "pydantic>=2.0.0\n",
        "langchain\n",
        "langchain-huggingface\n",
        "langchain-core\n",
        "langgraph > 0.2.27\n",
        "python-dotenv.2.11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por fim, criamos o arquivo README.md com informa√ß√µes sobre o espa√ßo e as instru√ß√µes para o HuggingFace.\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2 Backend\n",
        "emoji: üìä\n",
        "colorFrom: yellow\n",
        "colorTo: red\n",
        "sdk: docker\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Backend of SmolLM2 chat\n",
        "app_port: 7860\n",
        "---\n",
        "\n",
        "# SmolLM2 Backend\n",
        "\n",
        "This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "### In HuggingFace Spaces\n",
        "\n",
        "This project is designed to run in HuggingFace Spaces. To configure it:\n",
        "\n",
        "1. Create a new Space in HuggingFace with SDK Docker\n",
        "2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:\n",
        "   - Go to the \"Settings\" tab of your Space\n",
        "   - Scroll down to the \"Repository secrets\" section\n",
        "   - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value\n",
        "   - Save the changes\n",
        "\n",
        "### Local development\n",
        "\n",
        "For local development:\n",
        "\n",
        "1. Clone this repository\n",
        "2. Create a `.env` file in the project root with your HuggingFace token:\n",
        "   ``\n",
        "   HUGGINGFACE_TOKEN=your_token_here\n",
        "   ``\n",
        "3. Install the dependencies:\n",
        "   ``\n",
        "   pip install -r requirements.txt\n",
        "   ``\n",
        "\n",
        "## Local execution\n",
        "\n",
        "``bash\n",
        "uvicorn app:app --reload\n",
        "``\n",
        "\n",
        "The API will be available at `http://localhost:8000`.\n",
        "\n",
        "## Endpoints\n",
        "\n",
        "### GET `/`\n",
        "\n",
        "Welcome endpoint that returns a greeting message.\n",
        "\n",
        "### POST `/generate`\n",
        "\n",
        "Endpoint to generate text using the language model.\n",
        "\n",
        "**Request parameters:**\n",
        "``json\n",
        "{\n",
        "  \"query\": \"Your question here\",\n",
        "  \"thread_id\": \"optional_thread_identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "**Response:**\n",
        "``json\n",
        "{\n",
        "  \"generated_text\": \"Generated text by the model\",\n",
        "  \"thread_id\": \"thread identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "## Docker\n",
        "\n",
        "To run the application in a Docker container:\n",
        "\n",
        "``bash\n",
        "# Build the image\n",
        "docker build -t smollm2-backend .\n",
        "\n",
        "# Run the container\n",
        "docker run -p 8000:8000 --env-file .env smollm2-backend\n",
        "``\n",
        "\n",
        "## API documentation\n",
        "\n",
        "The interactive API documentation is available at:\n",
        "- Swagger UI: `http://localhost:8000/docs`\n",
        "- ReDoc: `http://localhost:8000/redoc`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token do HuggingFace\n",
        "\n",
        "Se voc√™ notou no c√≥digo e no Dockerfile, usamos um token do HuggingFace, ent√£o vamos ter que criar um. Em nossa conta do HuggingFace, criamos um [novo token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), damos um nome a ele e concedemos as seguintes permiss√µes:\n",
        "\n",
        "* Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob o seu namespace pessoal\n",
        "* Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob seu namespace pessoal\n",
        "* Fazer chamadas para provedores de infer√™ncia\n",
        "* Fazer chamadas para Pontos de Extremidade de Infer√™ncia\n",
        "\n",
        "![backend docker - token](https://images.maximofn.com/backend-docker-token.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adicionar o token aos secrets do espa√ßo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que j√° temos o token, precisamos adicion√°-lo ao espa√ßo. Na parte superior do aplicativo, poderemos ver um bot√£o chamado `Settings`, clicamos nele e poderemos ver a se√ß√£o de configura√ß√£o do espa√ßo.\n",
        "\n",
        "Se formos para baixo, poderemos ver uma se√ß√£o onde podemos adicionar `Variables` e `Secrets`. Neste caso, como estamos adicionando um token, vamos adicion√°-lo aos `Secrets`.\n",
        "\n",
        "Damos o nome `HUGGINGFACE_TOKEN` e o valor do token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implanta√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.\n",
        "\n",
        "Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.\n",
        "\n",
        "Neste caso, constru√≠mos apenas um backend, portanto o que vamos ver ao entrar no espa√ßo √© o que definimos no endpoint `/`\n",
        "\n",
        "![backend docker - espa√ßo](https://images.maximofn.com/backend-docker-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### URL do backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precisamos saber a URL do backend para poder fazer chamadas √† API. Para isso, temos que clicar nos tr√™s pontos no canto superior direito para ver as op√ß√µes.\n",
        "\n",
        "![backend docker - op√ß√µes](https://images.maximofn.com/backend-docker-options.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No menu suspenso, clicamos em `Embed this Spade`. Ser√° aberta uma janela indicando como incorporar o espa√ßo com um iframe e tamb√©m fornecer√° a URL do espa√ßo.\n",
        "\n",
        "![backend docker - embed](https://images.maximofn.com/backend-docker-embed.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se agora formos para essa URL, veremos o mesmo que no espa√ßo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documenta√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FastAPI, al√©m de ser uma API extremamente r√°pida, tem outra grande vantagem: gera documenta√ß√£o automaticamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se adicionarmos `/docs` √† URL que vimos anteriormente, poderemos visualizar a documenta√ß√£o da API com o `Swagger UI`.\n",
        "\n",
        "![backend docker - swagger doc](https://images.maximofn.com/backend-docker-swagger-doc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb√©m podemos adicionar `/redoc` √† URL para ver a documenta√ß√£o com `ReDoc`.\n",
        "\n",
        "![backend docker - redoc doc](https://images.maximofn.com/backend-docker-redoc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O bom da documenta√ß√£o `Swagger UI` √© que nos permite testar a API diretamente do navegador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adicionamos `/docs` √† URL que obtivemos, abrimos o menu suspenso do endpoint `/generate` e clicamos em `Try it out`, modificamos o valor da `query` e do `thread_id` e clicamos em `Execute`.\n",
        "\n",
        "No primeiro caso vou colocar\n",
        "\n",
        "* **query**: Ol√°, como voc√™ est√°? Sou M√°ximo\n",
        "* **thread_id**: user1\n",
        "\n",
        "![backend docker - test API](https://images.maximofn.com/backend-docker-test-API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recebemos a seguinte resposta `Ol√° M√°ximo! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? Em que posso ajudar hoje?`\n",
        "\n",
        "![backend docker -response 1 - user1](https://images.maximofn.com/backend-docker-response1-user1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos testar agora a mesma pergunta, mas com um `thread_id` diferente, neste caso `user2`.\n",
        "\n",
        "![backend docker - query 1 - user2](https://images.maximofn.com/backend-docker-query1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E nos responde isso `Ol√° Luis! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? No que posso ajudar hoje?`\n",
        "\n",
        "![backend docker - response 1 - user2](https://images.maximofn.com/backend-docker-response1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora pedimos nosso nome com os dois usu√°rios e obtemos isso\n",
        "\n",
        "* Para o usu√°rio **user1**: `Voc√™ se chama M√°ximo. H√° algo mais em que eu possa ajudar voc√™?`\n",
        "* Para o usu√°rio **user2**: `Voc√™ se chama Luis. H√° mais alguma coisa em que eu possa ajud√°-lo hoje, Luis?`\n",
        "\n",
        "![backend docker - response 2 - user1](https://images.maximofn.com/backend-docker-response2-user1.webp)\n",
        "\n",
        "![backend docker - response 2 - user2](https://images.maximofn.com/backend-docker-response2-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy do backend com Gradio e modelo rodando no servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os dois backends que criamos na verdade n√£o est√£o executando um modelo, mas sim fazendo chamadas para Inference Endpoints da HuggingFace. Mas pode ser que queiramos que tudo rode no servidor, inclusive o modelo. Pode ser que voc√™ tenha feito um fine-tuning de um LLM para seu caso de uso, por isso j√° n√£o pode fazer chamadas para Inference Endpoints.\n",
        "\n",
        "Ent√£o vamos ver como modificar o c√≥digo dos dois backends para executar um modelo no servidor e n√£o fazer chamadas para Inference Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar Espa√ßo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na hora de criar o space no HuggingFace fazemos o mesmo que antes, criamos um novo space, colocamos um nome e uma descri√ß√£o, selecionamos Gradio como SDK, selecionamos o HW em que vamos deployar, no meu caso selecionei o HW mais b√°sico e gratuito, e selecionamos se o faremos privado ou p√∫blico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos que fazer altera√ß√µes em `app.py` e em `requirements.txt` para que, em vez de fazer chamadas a Inference Endpoints, o modelo seja executado localmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mudan√ßas que temos que fazer s√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importar `torch`\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em vez de criar um modelo com `InferenceClient`, criamos com `AutoModelForCausalLM` e `AutoTokenizer`.\n",
        "\n",
        "``` python\n",
        "# Carregar o modelo e o tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "nome_do_modelo,\n",
        "torch_dtype=torch.float16,\n",
        "device_map=\"auto\"\n",
        ")\n",
        "```\n",
        "\n",
        "Utilizo `HuggingFaceTB/SmolLM2-1.7B-Instruct` porque √© um modelo bastante capaz com apenas 1.7B de par√¢metros. Como escolhi o hardware mais b√°sico, n√£o posso usar modelos muito grandes. Voc√™, se quiser usar um modelo maior, tem duas op√ß√µes: usar o hardware gratuito e aceitar que a infer√™ncia ser√° mais lenta, ou usar um hardware mais potente, mas pago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modificar a fun√ß√£o `respond` para que construa o prompt com a estrutura necess√°ria pela biblioteca `transformers`, tokenizar o prompt, fazer a infer√™ncia e destokenizar a resposta.\n",
        "\n",
        "``` python\n",
        "def responder(\n",
        "mensagem,\n",
        "hist√≥rico: list[tuple[str, str]],\n",
        "Mensagem do sistema,\n",
        "max_tokens,\n",
        "temperatura,\n",
        "top_p,\n",
        "):\n",
        "# Construir o prompt com o formato correto\n",
        "prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "for val in history:\n",
        "if val[0]:\n",
        "prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "if val[1]:\n",
        "prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "# Tokenizar o prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "# Gerar a resposta\n",
        "outputs = model.generate(\n",
        "**entradas,**\n",
        "max_new_tokens=max_tokens,\n",
        "temperature=temperature,\n",
        "top_p=top_p,\n",
        "do_sample=True,\n",
        "pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "    \n",
        "# Decodificar a resposta\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "# Extrair apenas a parte da resposta do assistente\n",
        "response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "yield response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A seguir deixo todo o c√≥digo\n",
        "\n",
        "``` python\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\"\"\"\"\"\"\n",
        "Para mais informa√ß√µes sobre o suporte √† API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\"\"\"\n",
        "\n",
        "# Carregar o modelo e o tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "nome_do_modelo,\n",
        "torch_dtype=torch.float16,\n",
        "device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def responder(\n",
        "mensagem,\n",
        "hist√≥ria: list[tuple[str, str]],\n",
        "Mensagem do sistema,\n",
        "max_tokens,\n",
        "temperatura,\n",
        "top_p,\n",
        "):\n",
        "# Construir o prompt com o formato correto\n",
        "prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "for val in history:\n",
        "if val[0]:\n",
        "prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "if val[1]:\n",
        "prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "# Tokenizar o prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "# Gerar a resposta\n",
        "sa√≠das = modelo.gerar(\n",
        "**entradas,**\n",
        "max_new_tokens=max_tokens,\n",
        "temperature=temperature,\n",
        "top_p=top_p,\n",
        "do_sample=True,\n",
        "pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "    \n",
        "# Decodificar a resposta\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "# Extrair apenas a parte da resposta do assistente\n",
        "response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "yield response\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\n",
        "Para informa√ß√µes sobre como personalizar o ChatInterface, consulte a documenta√ß√£o do Gradio: https://www.gradio.app/docs/gradio/chatinterface\n",
        "\"\"\"\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "responda,\n",
        "```markdown\n",
        "additional_inputs=[\n",
        "```\n",
        "gr.Textbox(\n",
        "value=\"Voc√™ √© um Chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™.\"\n",
        "r√≥tulo=\"Mensagem do sistema\"\n",
        "),\n",
        "gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"M√°ximo de novos tokens\"),\n",
        "gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo=\"Temperatura\"),\n",
        "gr.Slider()\n",
        "m√≠nimo=0.1,\n",
        "m√°ximo=1.0,\n",
        "value=0.95,\n",
        "passo=0.05,\n",
        "label=\"Top-p (amostragem do n√∫cleo)\"\n",
        "),\n",
        "],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste arquivo, devemos adicionar as novas bibliotecas que vamos utilizar, neste caso `transformers`, `accelerate` e `torch`. O arquivo completo ficaria:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "gradio>=4.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.25.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Desplegamos o space e testamos diretamente a API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî\n",
            "Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2_localModel\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Surpreende-me o qu√£o r√°pido o modelo responde, mesmo estando em um servidor sem GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora fazemos o mesmo que antes, mas com FastAPI, LangChain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar Espa√ßo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ao criar o space no HuggingFace, fazemos o mesmo que antes: criamos um novo espa√ßo, colocamos um nome e uma descri√ß√£o, selecionamos Docker como SDK, escolhemos o HW em que vamos implant√°-lo, no meu caso, escolho o HW mais b√°sico e gratuito, e decidimos se o faremos privado ou p√∫blico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J√° n√£o importamos `InferenceClient` e agora importamos `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importamos `torch`.\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos o modelo e o tokenizer com `AutoModelForCausalLM` e `AutoTokenizer`.\n",
        "\n",
        "``` python\n",
        "# Inicialize o modelo e o tokenizador\n",
        "print(\"Carregando modelo e tokenizer...\")\n",
        "dispositivo = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "\n",
        "tente:\n",
        "# Carregar o modelo no formato BF16 para melhor desempenho e menor uso de mem√≥ria\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "if device == \"cuda\":\n",
        "print(\"Usando GPU para o modelo...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "nome_do_modelo,\n",
        "torch_dtype=torch.bfloat16,\n",
        "device_map=\"auto\",\n",
        "low_cpu_mem_usage=True\n",
        ")\n",
        "else:\n",
        "print(\"Usando CPU para o modelo...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "nome_do_modelo,\n",
        "device_map={\"\": device},\n",
        "torch_dtype=torch.float32\n",
        ")\n",
        "\n",
        "print(f\"Modelo carregado com sucesso em: {device}\")\n",
        "except Exception as e:\n",
        "print(f\"Erro ao carregar o modelo: {str(e)}\")\n",
        "aumentar\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re definimos a fun√ß√£o `call_model` para que fa√ßa a infer√™ncia com o modelo local.\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to chat format\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Prepare the input using the chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=512,  # Increase the number of tokens for longer responses\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and clean the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response (after the last user message)\n",
        "    response = response.split(\"Assistant:\")[-1].strip()\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos que remover `langchain-huggingface` e adicionar `transformers`, `accelerate` e `torch` no arquivo `requirements.txt`. O arquivo ficaria:\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "solicita√ß√µes\n",
        "pydantic>=2.0.0\n",
        "langchain>=0.1.0\n",
        "langchain-core>=0.1.10\n",
        "langgraph>=0.2.27\n",
        "python-dotenv>=1.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.26.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J√° n√£o precisamos ter `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` porque como o modelo vai estar no servidor e n√£o vamos fazer chamadas para Inference Endpoints, n√£o precisamos do token. O arquivo ficaria:\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deployamos o space e testamos a API. Neste caso, vou testar diretamente do Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta: system\n",
            "You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\n",
            "user\n",
            "Hola, ¬øc√≥mo est√°s?\n",
            "assistant\n",
            "Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.\n",
            "Thread ID: user1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://maximofn-smollm2-backend-localmodel.hf.space/generate\"\n",
        "data = {\n",
        "    \"query\": \"Hola, ¬øc√≥mo est√°s?\",\n",
        "    \"thread_id\": \"user1\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"Respuesta:\", result[\"generated_text\"])\n",
        "    print(\"Thread ID:\", result[\"thread_id\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este demora um pouco mais que o anterior. Na verdade, demora o normal para um modelo sendo executado em um servidor sem GPU. O estranho √© quando o deployamos no Gradio. N√£o sei o que a HuggingFace faz por tr√°s, ou talvez tenha sido coincid√™ncia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclus√µes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vimos como criar um backend com um LLM, tanto fazendo chamadas ao Inference Endpoint da HuggingFace, quanto fazendo chamadas a um modelo rodando localmente. Vimos como fazer isso com Gradio ou com FastAPI, Langchain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A partir daqui voc√™ tem o conhecimento para poder implantar seus pr√≥prios modelos, mesmo que n√£o sejam LLMs, podem ser modelos multimodais. A partir daqui voc√™ pode fazer o que quiser."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "maximofn": {
      "date": "2025-03-02",
      "description_en": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "description_es": "¬øQuieres desplegar un backend con tu propio LLM? En este post te explico c√≥mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.",
      "description_pt": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "end_url": "deploy-backend-with-llm-in-huggingface",
      "image": "https://images.maximofn.com/backend_llm_thumbnail.webp",
      "image_hover_path": "https://images.maximofn.com/backend_llm_thumbnail.webp",
      "keywords_en": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_es": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_pt": "hugging face, fastapi, langchain, docker, backend, llm",
      "title_en": "Deploy backend with LLM in HuggingFace",
      "title_es": "Desplegar backend con LLM en HuggingFace",
      "title_pt": "Desplegar backend com LLM no HuggingFace"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
