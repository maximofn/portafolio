{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deployar backend no HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste post, vamos ver como deployar um backend no HuggingFace. Vamos ver como fazer isso de duas maneiras, atrav\u00e9s da forma comum, criando uma aplica\u00e7\u00e3o com Gradio, e atrav\u00e9s de uma op\u00e7\u00e3o diferente usando FastAPI, Langchain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ambos casos ser\u00e1 necess\u00e1rio ter uma conta no HuggingFace, j\u00e1 que vamos implantar o backend em um espa\u00e7o do HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend com Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar espa\u00e7o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro de tudo, criamos um novo espa\u00e7o na Hugging Face.",
        "\n",
        "* Colocamos um nome, uma descri\u00e7\u00e3o e escolhemos a licen\u00e7a.",
        "* Escolhemos o Gradio como o tipo de SDK. Ao escolher o Gradio, ser\u00e3o exibidas algumas templates, ent\u00e3o escolhemos a template do chatbot.",
        "* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc\u00ea escolha o que melhor considerar.",
        "* E por \u00faltimo, temos que escolher se queremos criar o espa\u00e7o p\u00fablico ou privado.",
        "\n",
        "![backend gradio - criar espa\u00e7o](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C\u00f3digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ao criar o space, podemos clon\u00e1-lo ou podemos ver os arquivos na pr\u00f3pria p\u00e1gina do HuggingFace. Podemos ver que foram criados 3 arquivos, `app.py`, `requirements.txt` e `README.md`. Ent\u00e3o, vamos ver o que colocar em cada um."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui est\u00e1 o c\u00f3digo do aplicativo. Como escolhemos o template de chatbot, j\u00e1 temos muito feito, mas vamos ter que mudar 2 coisas: primeiro, o modelo de linguagem e o system prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como modelo de linguagem, vejo ``HuggingFaceH4/zephyr-7b-beta``, mas vamos utilizar ``Qwen/Qwen2.5-72B-Instruct``, que \u00e9 um modelo muito capaz.",
        "\n",
        "Ent\u00e3o, procure pelo texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` e substitua-o por ``client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")``, ou espere que colocarei todo o c\u00f3digo mais tarde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb\u00e9m vamos alterar o system prompt, que por padr\u00e3o \u00e9 ``You are a friendly Chatbot.``, mas como o modelo foi treinado principalmente em ingl\u00eas, \u00e9 prov\u00e1vel que se voc\u00ea falar com ele em outro idioma, ele responda em ingl\u00eas. Ent\u00e3o, vamos mud\u00e1-lo para ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.",
        "\n",
        "Ent\u00e3o, procure pelo texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` e substitua-o por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, ou espere at\u00e9 eu colocar todo o c\u00f3digo agora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "import gradio as gr",
        "do huggingface_hub import InferenceClient",
        "\n",
        "\"\"\"\"\"\"",
        "Para mais informa\u00e7\u00f5es sobre o suporte da API de Infer\u00eancia do `huggingface_hub`, consulte a documenta\u00e7\u00e3o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference",
        "\"\"\"\"\"\"",
        "client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")",
        "\n",
        "\n",
        "def responder(",
        "mensagem,",
        "hist\u00f3ria: list[tuple[str, str]],",
        "Mensagem do sistema,",
        "max_tokens,",
        "temperatura,",
        "top_p,",
        "):",
        "mensagens = [{\"papel\": \"sistema\", \"conte\u00fado\": system_message}]",
        "\n",
        "for val in history:",
        "if val[0]:",
        "messages.append({\"role\": \"user\", \"content\": val[0]})",
        "if val[1]:",
        "messages.append({\"role\": \"assistant\", \"content\": val[1]})",
        "\n",
        "messages.append({\"role\": \"user\", \"content\": message})",
        "\n",
        "response = \"\"",
        "\n",
        "para mensagem em client.chat_completion(",
        "mensagens,",
        "max_tokens=max_tokens,",
        "stream=True,",
        "temperature=temperature,",
        "top_p=top_p,",
        "):",
        "token = message.choices[0].delta.content",
        "\n",
        "response += token",
        "yield response",
        "\n",
        "\n",
        "\"\"\"\"\"\"",
        "Para informa\u00e7\u00f5es sobre como personalizar a ChatInterface, consulte a documenta\u00e7\u00e3o do gradio: https://www.gradio.app/docs/chatinterface",
        "\"\"\"\"\"\"",
        "demo = gr.ChatInterface(",
        "responda,",
        "```markdown\nadditional_inputs=[\n```",
        "gr.Textbox(value=\"Voc\u00ea \u00e9 um chatbot amig\u00e1vel. Sempre responda na l\u00edngua em que o usu\u00e1rio est\u00e1 escrevendo para voc\u00ea.\", label=\"Mensagem do sistema\"),",
        "gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"M\u00e1ximo de novos tokens\"),",
        "gr.Slider(m\u00ednimo=0,1, m\u00e1ximo=4,0, valor=0,7, passo=0,1, r\u00f3tulo=\"Temperatura\"),",
        "gr.Slider(",
        "m\u00ednimo=0.1,",
        "m\u00e1ximo=1.0,",
        "value=0.95,",
        "passo=0.05,",
        "label=\"Top-p (amostragem do n\u00facleo)\"",
        "),",
        "],",
        ")",
        "\n",
        "\n",
        "if __name__ == \"__main__\":",
        "demo.launch()",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este \u00e9 o arquivo onde ser\u00e3o escritas as depend\u00eancias, mas para este caso vai ser muito simples:",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LEIA-ME.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este \u00e9 o arquivo no qual vamos colocar as informa\u00e7\u00f5es do espa\u00e7o. Nos spaces da HuggingFace, no in\u00edcio dos readmes, coloca-se um c\u00f3digo para que a HuggingFace saiba como exibir a miniatura do espa\u00e7o, qual arquivo deve ser usado para executar o c\u00f3digo, vers\u00e3o do sdk, etc.",
        "\n",
        "``` md\n",
        "---",
        "t\u00edtulo: SmolLM2",
        "emoji: \ud83d\udcac",
        "colorFrom: amarelo",
        "colorTo: roxo",
        "sdk: gradio",
        "sdk_version: 5.0.1",
        "app_file: app.py",
        "pinned: false",
        "licen\u00e7a: apache-2.0",
        "short_description: Bate-papo com o Gradio SmolLM2",
        "---",
        "\n",
        "Um exemplo de chatbot usando [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index) e a [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implanta\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se n\u00f3s clonamos o espa\u00e7o, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv\u00e1-los.",
        "\n",
        "Ent\u00e3o, quando as altera\u00e7\u00f5es estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa\u00e7o seja constru\u00eddo e possamos us\u00e1-lo.",
        "\n",
        "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Muito bem, fizemos um chatbot, mas n\u00e3o era essa a inten\u00e7\u00e3o, aqui t\u00ednhamos vindo fazer um backend! P\u00e1ra, p\u00e1ra, olha o que diz abaixo do chatbot",
        "\n",
        "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver um texto ``Use via API``, onde se clicarmos, se abrir\u00e1 um menu com uma API para poder usar o chatbot.",
        "\n",
        "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que nos d\u00e1 uma documenta\u00e7\u00e3o de como usar a API, tanto com Python, com JavaScript, quanto com bash."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos o c\u00f3digo de exemplo de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space \u2714\n",
            "\u00a1Hola M\u00e1ximo! Mucho gusto, estoy bien, gracias por preguntar. \u00bfC\u00f3mo est\u00e1s t\u00fa? \u00bfEn qu\u00e9 puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, \u00bfc\u00f3mo est\u00e1s? Me llamo M\u00e1ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estamos fazendo chamadas \u00e0 API do `InferenceClient` da HuggingFace, ent\u00e3o poder\u00edamos pensar, Para que fizemos um backend, se podemos chamar diretamente a API da HuggingFace? Bem, voc\u00ea vai ver isso abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es M\u00e1ximo. \u00bfEs correcto?\n"
          ]
        }
      ],
      "source": [
        "result = client.predict(\n",
        "\t\tmessage=\"\u00bfC\u00f3mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O modelo de bate-papo do Gradio gerencia o hist\u00f3rico para n\u00f3s, de forma que cada vez que criamos um novo `cliente`, uma nova thread de conversa \u00e9 criada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a tentar criar um novo cliente e ver se uma nova thread de conversa \u00e9 criada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space \u2714\n",
            "Hola Luis, estoy muy bien, gracias por preguntar. \u00bfC\u00f3mo est\u00e1s t\u00fa? Es un gusto conocerte. \u00bfEn qu\u00e9 puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "new_client = Client(\"Maximofn/SmolLM2\")\n",
        "result = new_client.predict(\n",
        "\t\tmessage=\"Hola, \u00bfc\u00f3mo est\u00e1s? Me llamo Luis\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos perguntar novamente como me chamo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Te llamas Luis. \u00bfHay algo m\u00e1s en lo que pueda ayudarte?\n"
          ]
        }
      ],
      "source": [
        "result = new_client.predict(\n",
        "\t\tmessage=\"\u00bfC\u00f3mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, temos dois clientes, cada um com seu pr\u00f3prio fio de conversa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy do backend com FastAPI, Langchain e Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos a fazer o mesmo, criar um backend de um chatbot, com o mesmo modelo, mas nesse caso usando FastAPI, Langchain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar espa\u00e7o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos que criar um novo espa\u00e7o, mas nesse caso faremos de outra maneira",
        "\n",
        "* Colocamos um nome, uma descri\u00e7\u00e3o e escolhemos a licen\u00e7a.",
        "* Escolhemos Docker como o tipo de SDK. Ao escolher Docker, aparecer\u00e3o modelos, ent\u00e3o escolhemos um modelo em branco.",
        "* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc\u00ea escolha o que melhor considerar.",
        "* E por fim, \u00e9 preciso escolher se queremos criar o espa\u00e7o p\u00fablico ou privado.",
        "\n",
        "![backend docker - criar espa\u00e7o](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C\u00f3digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, ao criar o space, vemos que temos apenas um arquivo, o ``README.md``. Ent\u00e3o vamos ter que criar todo o c\u00f3digo n\u00f3s mesmos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a criar o c\u00f3digo do aplicativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Come\u00e7amos com as bibliotecas necess\u00e1rias",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException",
        "from pydantic import BaseModel",
        "do huggingface_hub import InferenceClient",
        "\n",
        "```markdown\nfrom langchain_core.messages import HumanMessage, AIMessage\n```",
        "from langgraph.checkpoint.memory import MemorySaver",
        "from langgraph.graph import START, MessagesState, StateGraph",
        "\n",
        "import os",
        "from dotenv import load_dotenv",
        "load_dotenv()",
        "```\n",
        "\n",
        "Carregamos `fastapi` para poder criar as rotas da API, `pydantic` para criar o template das queries, `huggingface_hub` para poder criar um modelo de linguagem, `langchain` para indicar ao modelo se as mensagens s\u00e3o do chatbot ou do usu\u00e1rio e `langgraph` para criar o chatbot.",
        "\n",
        "Al\u00e9m disso, carregamos `os` e `dotenv` para poder carregar as vari\u00e1veis de ambiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Carregamos o token do HuggingFace",
        "\n",
        "``` python\n",
        "# Token da HuggingFace",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o modelo de linguagem",
        "\n",
        "``` python\n",
        "# Inicializar o modelo da HuggingFace",
        "model = InferenceClient(",
        "model=\"Qwen/Qwen2.5-72B-Instruct\",",
        "api_key=os.getenv(\"HUGGINGFACE_TOKEN\")",
        ")",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos agora uma fun\u00e7\u00e3o para chamar o modelo",
        "\n",
        "``` python\n",
        "# Defina a fun\u00e7\u00e3o que chama o modelo",
        "def chamar_modelo(estado: MessagesState):",
        "\"\"\"\"\"\"",
        "Chame o modelo com as mensagens fornecidas",
        "\n",
        "Argumentos:",
        "estado: EstadoMensagens",
        "\n",
        "Retorna:",
        "um dicion\u00e1rio contendo o texto gerado e o ID do thread",
        "\"\"\"\"\"\"",
        "# Converter mensagens do LangChain para o formato do HuggingFace",
        "hf_messages = []",
        "for msg in state[\"messages\"]:",
        "Se `isinstance(msg, HumanMessage)`:",
        "hf_messages.append({\"role\": \"user\", \"content\": msg.content})",
        "elif isinstance(msg, AIMessage):",
        "hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})",
        "    \n",
        "# Chamar a API",
        "resposta = modelo.completar_chat(",
        "mensagens=hf_mensagens,",
        "temperature=0.5,",
        "max_tokens=64,",
        "top_p=0,7",
        ")",
        "    \n",
        "# Converter a resposta para o formato LangChain",
        "```python\nai_message = AIMessage(content=response.choices[0].message.content)\n```",
        "return {\"messages\": state[\"messages\"] + [ai_message]}",
        "```\n",
        "\n",
        "Convertemos as mensagens do formato LangChain para o formato HuggingFace, assim podemos usar o modelo de linguagem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos uma template para as queries",
        "\n",
        "``` python\n",
        "class QueryRequest(BaseModel):",
        "query: str",
        "thread_id: str = \"padr\u00e3o\"",
        "```\n",
        "\n",
        "As consultas ter\u00e3o um `query`, a mensagem do usu\u00e1rio, e um `thread_id`, que \u00e9 o identificador do fio da conversa\u00e7\u00e3o e mais adiante explicaremos para que o utilizamos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos um grafo de LangGraph",
        "\n",
        "``` python\n",
        "# Definir o gr\u00e1fico",
        "workflow = StateGraph(state_schema=MessagesState)",
        "\n",
        "# Defina o n\u00f3do na gr\u00e1fico",
        "workflow.add_edge(START, \"model\")",
        "workflow.add_node(\"modelo\", call_model)",
        "\n",
        "# Adicionar mem\u00f3ria",
        "memory = MemorySaver()",
        "graph_app = workflow.compile(checkpointer=memory)",
        "```\n",
        "\n",
        "Com isso, criamos um grafo de LangGraph, que \u00e9 uma estrutura de dados que nos permite criar um chatbot e gerenciar o estado do chatbot para n\u00f3s, ou seja, entre outras coisas, o hist\u00f3rico de mensagens. Dessa forma, n\u00e3o precisamos fazer isso n\u00f3s mesmos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos a aplica\u00e7\u00e3o de FastAPI",
        "\n",
        "``` python\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API para gerar texto usando LangChain e LangGraph\")",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos os endpoints da API",
        "\n",
        "``` python\n",
        "# Ponto de entrada Bem-vindo",
        "@app.get(\"/\")",
        "async def api_home():",
        "\"\"\"Ponto de entrada Welcome\"\"\"",
        "return {\"detail\": \"Bem-vindo ao tutorial de FastAPI, Langchain, Docker\"}",
        "\n",
        "# Gerar ponto final",
        "@app.post(\"/generate\")",
        "async def gerar(request: QueryRequest):",
        "\"\"\"\"\"\"",
        "Ponto final para gerar texto usando o modelo de linguagem",
        "    \n",
        "Argumentos:",
        "solicita\u00e7\u00e3o: QueryRequest",
        "query: str",
        "thread_id: str = \"padr\u00e3o\"",
        "\n",
        "Retorna:",
        "um dicion\u00e1rio contendo o texto gerado e o ID do thread",
        "\"\"\"\"\"\"",
        "tente:",
        "# Configurar o ID da thread",
        "config = {\"configurable\": {\"thread_id\": request.thread_id}}",
        "        \n",
        "# Crie a mensagem de entrada",
        "input_messages = [HumanMessage(content=request.query)]",
        "        \n",
        "# Invocar o gr\u00e1fico",
        "output = graph_app.invoke({\"messages\": input_messages}, config)",
        "        \n",
        "# Obter a resposta do modelo",
        "resposta = output[\"messages\"][-1].conte\u00fado",
        "        \n",
        "return {",
        "\"generated_text\": resposta,",
        "\"thread_id\": request.thread_id",
        "}",
        "except Exception as e:",
        "raise HTTPException(status_code=500, detail=f\"Erro ao gerar texto: {str(e)}\")",
        "```\n",
        "\n",
        "Criamos o endpoint `/` que nos retornar\u00e1 um texto quando acessarmos a API, e o endpoint `/generate` que \u00e9 o que usaremos para gerar o texto.",
        "\n",
        "Se n\u00f3s olharmos para a fun\u00e7\u00e3o `generate`, temos a vari\u00e1vel `config`, que \u00e9 um dicion\u00e1rio que cont\u00e9m o `thread_id`. Este `thread_id` \u00e9 o que nos permite ter um hist\u00f3rico de mensagens de cada usu\u00e1rio, desta forma, diferentes usu\u00e1rios podem usar o mesmo endpoint e ter seu pr\u00f3prio hist\u00f3rico de mensagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, temos o c\u00f3digo para que se possa executar a aplica\u00e7\u00e3o",
        "\n",
        "``` python\n",
        "if __name__ == \"__main__\":",
        "import uvicorn",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos escrever todo o c\u00f3digo juntos",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException",
        "from pydantic import BaseModel",
        "do huggingface_hub import InferenceClient",
        "\n",
        "```markdown\nfrom langchain_core.messages import HumanMessage, AIMessage\n```",
        "from langgraph.checkpoint.memory import MemorySaver",
        "from langgraph.graph import START, MessagesState, StateGraph",
        "\n",
        "import os",
        "from dotenv import load_dotenv",
        "load_dotenv()",
        "\n",
        "# Token da HuggingFace",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))",
        "\n",
        "# Inicialize o modelo do HuggingFace",
        "model = InferenceClient(",
        "model=\"Qwen/Qwen2.5-72B-Instruct\",",
        "api_key=os.getenv(\"HUGGINGFACE_TOKEN\")",
        ")",
        "\n",
        "# Defina a fun\u00e7\u00e3o que chama o modelo",
        "def chamar_modelo(estado: EstadoMensagens):",
        "\"\"\"\"\"\"",
        "Chame o modelo com as mensagens fornecidas",
        "\n",
        "Argumentos:",
        "estado: MensagensState",
        "\n",
        "Retorna:",
        "um dicion\u00e1rio contendo o texto gerado e o ID do thread",
        "\"\"\"\"\"\"",
        "# Converter mensagens do LangChain para o formato do HuggingFace",
        "hf_messages = []",
        "for msg in state[\"messages\"]:",
        "if isinstance(msg, HumanMessage):",
        "hf_messages.append({\"role\": \"user\", \"content\": msg.content})",
        "elif isinstance(msg, AIMessage):",
        "hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})",
        "    \n",
        "# Chame a API",
        "resposta = modelo.completar_chat(",
        "mensagens=hf_mensagens,",
        "temperature=0.5,",
        "max_tokens=64,",
        "top_p=0,7",
        ")",
        "    \n",
        "# Converter a resposta para o formato LangChain",
        "```markdown\nai_message = AIMessage(content=response.choices[0].message.content)\n```",
        "return {\"messages\": state[\"messages\"] + [ai_message]}",
        "\n",
        "# Definir o gr\u00e1fico",
        "workflow = StateGraph(state_schema=MessagesState)",
        "\n",
        "# Defina o n\u00f3 no grafo",
        "workflow.add_edge(START, \"model\")",
        "workflow.add_node(\"modelo\", call_model)",
        "\n",
        "# Adicionar mem\u00f3ria",
        "memory = MemorySaver()",
        "graph_app = workflow.compile(checkpointer=memory)",
        "\n",
        "# Defina o modelo de dados para o pedido",
        "class QueryRequest(BaseModel):",
        "query: str",
        "thread_id: str = \"padr\u00e3o\"",
        "\n",
        "# Criar a aplica\u00e7\u00e3o FastAPI",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API para gerar texto usando LangChain e LangGraph\")",
        "\n",
        "# Ponto de entrada Bem-vindo",
        "@app.get(\"/\")",
        "async def api_home():",
        "\"\"\"Ponto de entrada Welcome\"\"\"",
        "return {\"detalhe\": \"Bem-vindo ao tutorial de FastAPI, Langchain, Docker\"}",
        "\n",
        "# Gerar ponto final",
        "@app.post(\"/gerar\")",
        "async def generate(request: QueryRequest):",
        "\"\"\"",
        "Ponto final para gerar texto usando o modelo de linguagem",
        "    \n",
        "Argumentos:",
        "solicita\u00e7\u00e3o: QueryRequest",
        "query: str",
        "thread_id: str = \"padr\u00e3o\"",
        "\n",
        "Retorna:",
        "um dicion\u00e1rio contendo o texto gerado e o ID do fio",
        "\"\"\"",
        "tente:",
        "# Configurar o ID da thread",
        "config = {\"configurable\": {\"thread_id\": request.thread_id}}",
        "        \n",
        "# Criar a mensagem de entrada",
        "input_messages = [HumanMessage(content=request.query)]",
        "        \n",
        "# Invocar o gr\u00e1fico",
        "output = graph_app.invoke({\"messages\": input_messages}, config)",
        "        \n",
        "# Obter a resposta do modelo",
        "resposta = output[\"messages\"][-1].conte\u00fado",
        "        \n",
        "return {",
        "\"generated_text\": resposta,",
        "\"thread_id\": request.thread_id",
        "}",
        "except Exception as e:",
        "raise HTTPException(status_code=500, detail=f\"Erro ao gerar texto: {str(e)}\")",
        "\n",
        "if __name__ == \"__main__\":",
        "import uvicorn",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vemos como criar o Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro indicamos a partir de qual imagem vamos come\u00e7ar",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos o diret\u00f3rio de trabalho",
        "\n",
        "``` dockerfile\n",
        "RUN useradd -m -u 1000 user",
        "WORKDIR /app",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos o arquivo com as depend\u00eancias e instalamos",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user ./requirements.txt requirements.txt",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos o resto do c\u00f3digo",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user . /app",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exponhamos o porto 7860",
        "\n",
        "``` dockerfile\n",
        "EXPOSE 7860",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos as vari\u00e1veis de ambiente",
        "\n",
        "``` dockerfile\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\",
        "test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Segredo existe!\"",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por \u00faltimo, indicamos o comando para executar a aplica\u00e7\u00e3o",
        "\n",
        "``` dockerfile\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora colocamos tudo junto",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim",
        "\n",
        "RUN useradd -m -u 1000 user",
        "WORKDIR /app",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt",
        "\n",
        "COPY --chown=user . /app",
        "\n",
        "EXPOSE 7860",
        "\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\",
        "test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Segredo existe!\"",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o arquivo com as depend\u00eancias",
        "\n",
        "``` txt\n",
        "fastapi",
        "uvicorn",
        "pedidos",
        "pydantic>=2.0.0",
        "langchain",
        "langchain-huggingface",
        "langchain-core",
        "langgraph > 0.2.27",
        "python-dotenv.2.11",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por fim, criamos o arquivo README.md com informa\u00e7\u00f5es sobre o espa\u00e7o e as instru\u00e7\u00f5es para o HuggingFace.",
        "\n",
        "``` md\n",
        "---",
        "t\u00edtulo: Backend do SmolLM2",
        "emoji: \ud83d\udcca",
        "colorFrom: amarelo",
        "colorTo: vermelho",
        "sdk: docker",
        "pinned: false",
        "licen\u00e7a: apache-2.0",
        "short_description: Backend do chat SmolLM2",
        "app_port: 7860",
        "---",
        "\n",
        "# Backend do SmolLM2",
        "\n",
        "Este projeto implementa uma API FastAPI que usa LangChain e LangGraph para gerar texto com o modelo Qwen2.5-72B-Instruct do HuggingFace.",
        "\n",
        "## Configura\u00e7\u00e3o",
        "\n",
        "### No HuggingFace Spaces",
        "\n",
        "Este projeto est\u00e1 projetado para ser executado em HuggingFace Spaces. Para configur\u00e1-lo:",
        "\n",
        "1. Crie um novo Espa\u00e7o no HuggingFace com o SDK Docker",
        "2. Configure a vari\u00e1vel de ambiente `HUGGINGFACE_TOKEN` ou `HF_TOKEN` na configura\u00e7\u00e3o do Space:",
        "- V\u00e1 para a aba \"Configura\u00e7\u00f5es\" do seu Espa\u00e7o",
        "- Role para a se\u00e7\u00e3o \"Secrets do reposit\u00f3rio\"",
        "- Adicione uma nova vari\u00e1vel com o nome `HUGGINGFACE_TOKEN` e seu token como valor",
        "- Salve as altera\u00e7\u00f5es",
        "\n",
        "### Desenvolvimento local",
        "\n",
        "Para o desenvolvimento local:",
        "\n",
        "1. Clone este reposit\u00f3rio",
        "2. Crie um arquivo `.env` na raiz do projeto com seu token do HuggingFace:",
        "```\n```",
        "HUGGINGFACE_TOKEN=seu_token_aqui",
        "```\n```",
        "3. Instale as depend\u00eancias:",
        "```\n```",
        "pip install -r requirements.txt",
        "```\nPor favor, forne\u00e7a o texto em Markdown que voc\u00ea gostaria de traduzir para o portugu\u00eas.\n```",
        "\n",
        "## Execu\u00e7\u00e3o local",
        "\n",
        "``bash",
        "uvicorn app:app --reload",
        "```\nPor favor, forne\u00e7a o texto em Markdown que voc\u00ea gostaria de traduzir para o portugu\u00eas.\n```",
        "\n",
        "A API estar\u00e1 dispon\u00edvel em `http://localhost:8000`.",
        "\n",
        "## Endpoints",
        "\n",
        "### GET `/`",
        "\n",
        "Ponto final de boas-vindas que retorna uma mensagem de sauda\u00e7\u00e3o.",
        "\n",
        "### POST `/gerar`",
        "\n",
        "Ponto final para gerar texto usando o modelo de linguagem.",
        "\n",
        "**Par\u00e2metros da solicita\u00e7\u00e3o:**",
        "``json",
        "{",
        "\"query\": \"Sua pergunta aqui\",",
        "\"thread_id\": \"identificador_de_thread_opcional\"",
        "}",
        "```\nPor favor, forne\u00e7a o texto em markdown que voc\u00ea gostaria de traduzir para o portugu\u00eas.\n```",
        "\n",
        "**Resposta:**",
        "``json```",
        "{",
        "\"Texto gerado pelo modelo\"",
        "\"thread_id\": \"identificador do thread\"",
        "}",
        "```\nPor favor, forne\u00e7a o texto em Markdown que deseja traduzir para o portugu\u00eas.\n```",
        "\n",
        "## Docker",
        "\n",
        "Para executar a aplica\u00e7\u00e3o em um cont\u00eainer Docker:",
        "\n",
        "``bash",
        "# Construa a imagem",
        "docker build -t smollm2-backend .",
        "\n",
        "# Executar o cont\u00eainer",
        "docker run -p 8000:8000 --env-file .env smollm2-backend",
        "```\n```",
        "\n",
        "## Documenta\u00e7\u00e3o da API",
        "\n",
        "A documenta\u00e7\u00e3o interativa da API est\u00e1 dispon\u00edvel em:",
        "- Swagger UI: `http://localhost:8000/docs`",
        "- ReDoc: `http://localhost:8000/redoc`",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token do HuggingFace",
        "\n",
        "Se voc\u00ea notou no c\u00f3digo e no Dockerfile, usamos um token do HuggingFace, ent\u00e3o vamos ter que criar um. Em nossa conta do HuggingFace, criamos um [novo token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), damos um nome a ele e concedemos as seguintes permiss\u00f5es:",
        "\n",
        "* Acesso de leitura aos conte\u00fados de todos os reposit\u00f3rios sob o seu namespace pessoal",
        "* Acesso de leitura aos conte\u00fados de todos os reposit\u00f3rios sob seu namespace pessoal",
        "* Fazer chamadas para provedores de infer\u00eancia",
        "* Fazer chamadas para Pontos de Extremidade de Infer\u00eancia",
        "\n",
        "![backend docker - token](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adicionar o token aos secrets do espa\u00e7o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que j\u00e1 temos o token, precisamos adicion\u00e1-lo ao espa\u00e7o. Na parte superior do aplicativo, poderemos ver um bot\u00e3o chamado `Settings`, clicamos nele e poderemos ver a se\u00e7\u00e3o de configura\u00e7\u00e3o do espa\u00e7o.",
        "\n",
        "Se formos para baixo, poderemos ver uma se\u00e7\u00e3o onde podemos adicionar `Variables` e `Secrets`. Neste caso, como estamos adicionando um token, vamos adicion\u00e1-lo aos `Secrets`.",
        "\n",
        "Damos o nome `HUGGINGFACE_TOKEN` e o valor do token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implanta\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se n\u00f3s clonamos o espa\u00e7o, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv\u00e1-los.",
        "\n",
        "Ent\u00e3o, quando as altera\u00e7\u00f5es estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa\u00e7o seja constru\u00eddo e possamos us\u00e1-lo.",
        "\n",
        "Neste caso, constru\u00edmos apenas um backend, portanto o que vamos ver ao entrar no espa\u00e7o \u00e9 o que definimos no endpoint `/`",
        "\n",
        "![backend docker - espa\u00e7o](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### URL do backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precisamos saber a URL do backend para poder fazer chamadas \u00e0 API. Para isso, temos que clicar nos tr\u00eas pontos no canto superior direito para ver as op\u00e7\u00f5es.",
        "\n",
        "![backend docker - op\u00e7\u00f5es](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No menu suspenso, clicamos em `Embed this Spade`. Ser\u00e1 aberta uma janela indicando como incorporar o espa\u00e7o com um iframe e tamb\u00e9m fornecer\u00e1 a URL do espa\u00e7o.",
        "\n",
        "![backend docker - embed](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se agora formos para essa URL, veremos o mesmo que no espa\u00e7o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documenta\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FastAPI, al\u00e9m de ser uma API extremamente r\u00e1pida, tem outra grande vantagem: gera documenta\u00e7\u00e3o automaticamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se adicionarmos `/docs` \u00e0 URL que vimos anteriormente, poderemos visualizar a documenta\u00e7\u00e3o da API com o `Swagger UI`.",
        "\n",
        "![backend docker - swagger doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tamb\u00e9m podemos adicionar `/redoc` \u00e0 URL para ver a documenta\u00e7\u00e3o com `ReDoc`.",
        "\n",
        "![backend docker - redoc doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O bom da documenta\u00e7\u00e3o `Swagger UI` \u00e9 que nos permite testar a API diretamente do navegador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adicionamos `/docs` \u00e0 URL que obtivemos, abrimos o menu suspenso do endpoint `/generate` e clicamos em `Try it out`, modificamos o valor da `query` e do `thread_id` e clicamos em `Execute`.",
        "\n",
        "No primeiro caso vou colocar",
        "\n",
        "* **query**: Ol\u00e1, como voc\u00ea est\u00e1? Sou M\u00e1ximo",
        "* **thread_id**: user1",
        "\n",
        "![backend docker - test API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recebemos a seguinte resposta `Ol\u00e1 M\u00e1ximo! Estou muito bem, obrigado por perguntar. Como voc\u00ea est\u00e1? Em que posso ajudar hoje?`",
        "\n",
        "![backend docker -response 1 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos testar agora a mesma pergunta, mas com um `thread_id` diferente, neste caso `user2`.",
        "\n",
        "![backend docker - query 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E nos responde isso `Ol\u00e1 Luis! Estou muito bem, obrigado por perguntar. Como voc\u00ea est\u00e1? No que posso ajudar hoje?`",
        "\n",
        "![backend docker - response 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora pedimos nosso nome com os dois usu\u00e1rios e obtemos isso",
        "\n",
        "* Para o usu\u00e1rio **user1**: `Voc\u00ea se chama M\u00e1ximo. H\u00e1 algo mais em que eu possa ajudar voc\u00ea?`",
        "* Para o usu\u00e1rio **user2**: `Voc\u00ea se chama Luis. H\u00e1 mais alguma coisa em que eu possa ajud\u00e1-lo hoje, Luis?`",
        "\n",
        "![backend docker - response 2 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp)",
        "\n",
        "![backend docker - response 2 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy do backend com Gradio e modelo rodando no servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os dois backends que criamos na verdade n\u00e3o est\u00e3o executando um modelo, mas sim fazendo chamadas para Inference Endpoints da HuggingFace. Mas pode ser que queiramos que tudo rode no servidor, inclusive o modelo. Pode ser que voc\u00ea tenha feito um fine-tuning de um LLM para seu caso de uso, por isso j\u00e1 n\u00e3o pode fazer chamadas para Inference Endpoints.",
        "\n",
        "Ent\u00e3o vamos ver como modificar o c\u00f3digo dos dois backends para executar um modelo no servidor e n\u00e3o fazer chamadas para Inference Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar Espa\u00e7o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na hora de criar o space no HuggingFace fazemos o mesmo que antes, criamos um novo space, colocamos um nome e uma descri\u00e7\u00e3o, selecionamos Gradio como SDK, selecionamos o HW em que vamos deployar, no meu caso selecionei o HW mais b\u00e1sico e gratuito, e selecionamos se o faremos privado ou p\u00fablico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C\u00f3digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos que fazer altera\u00e7\u00f5es em `app.py` e em `requirements.txt` para que, em vez de fazer chamadas a Inference Endpoints, o modelo seja executado localmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mudan\u00e7as que temos que fazer s\u00e3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importar `torch`",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer",
        "import torch",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em vez de criar um modelo com `InferenceClient`, criamos com `AutoModelForCausalLM` e `AutoTokenizer`.",
        "\n",
        "``` python\n",
        "# Carregar o modelo e o tokenizer",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "model = AutoModelForCausalLM.from_pretrained(",
        "nome_do_modelo,",
        "torch_dtype=torch.float16,",
        "device_map=\"auto\"",
        ")",
        "```\n",
        "\n",
        "Utilizo `HuggingFaceTB/SmolLM2-1.7B-Instruct` porque \u00e9 um modelo bastante capaz com apenas 1.7B de par\u00e2metros. Como escolhi o hardware mais b\u00e1sico, n\u00e3o posso usar modelos muito grandes. Voc\u00ea, se quiser usar um modelo maior, tem duas op\u00e7\u00f5es: usar o hardware gratuito e aceitar que a infer\u00eancia ser\u00e1 mais lenta, ou usar um hardware mais potente, mas pago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modificar a fun\u00e7\u00e3o `respond` para que construa o prompt com a estrutura necess\u00e1ria pela biblioteca `transformers`, tokenizar o prompt, fazer a infer\u00eancia e destokenizar a resposta.",
        "\n",
        "``` python\n",
        "def responder(",
        "mensagem,",
        "hist\u00f3rico: list[tuple[str, str]],",
        "Mensagem do sistema,",
        "max_tokens,",
        "temperatura,",
        "top_p,",
        "):",
        "# Construir o prompt com o formato correto",
        "prompt = f\"<|system|>\\n{system_message}</s>\\n\"",
        "    \n",
        "for val in history:",
        "if val[0]:",
        "prompt += f\"<|user|>\\n{val[0]}</s>\\n\"",
        "if val[1]:",
        "prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"",
        "    \n",
        "prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"",
        "    \n",
        "# Tokenizar o prompt",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)",
        "    \n",
        "# Gerar a resposta",
        "outputs = model.generate(",
        "**entradas,**",
        "max_new_tokens=max_tokens,",
        "temperature=temperature,",
        "top_p=top_p,",
        "do_sample=True,",
        "pad_token_id=tokenizer.eos_token_id",
        ")",
        "    \n",
        "# Decodificar a resposta",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)",
        "    \n",
        "# Extrair apenas a parte da resposta do assistente",
        "response = response.split(\"<|assistant|>\\n\")[-1].strip()",
        "    \n",
        "yield response",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A seguir deixo todo o c\u00f3digo",
        "\n",
        "``` python\n",
        "import gradio as gr",
        "from transformers import AutoModelForCausalLM, AutoTokenizer",
        "import torch",
        "\n",
        "\"\"\"\"\"\"",
        "Para mais informa\u00e7\u00f5es sobre o suporte \u00e0 API de Infer\u00eancia do `huggingface_hub`, consulte a documenta\u00e7\u00e3o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference",
        "\"\"\"\"\"\"",
        "\n",
        "# Carregar o modelo e o tokenizer",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "model = AutoModelForCausalLM.from_pretrained(",
        "nome_do_modelo,",
        "torch_dtype=torch.float16,",
        "device_map=\"auto\"",
        ")",
        "\n",
        "def responder(",
        "mensagem,",
        "hist\u00f3ria: list[tuple[str, str]],",
        "Mensagem do sistema,",
        "max_tokens,",
        "temperatura,",
        "top_p,",
        "):",
        "# Construir o prompt com o formato correto",
        "prompt = f\"<|system|>\\n{system_message}</s>\\n\"",
        "    \n",
        "for val in history:",
        "if val[0]:",
        "prompt += f\"<|user|>\\n{val[0]}</s>\\n\"",
        "if val[1]:",
        "prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"",
        "    \n",
        "prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"",
        "    \n",
        "# Tokenizar o prompt",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)",
        "    \n",
        "# Gerar a resposta",
        "sa\u00eddas = modelo.gerar(",
        "**entradas,**",
        "max_new_tokens=max_tokens,",
        "temperature=temperature,",
        "top_p=top_p,",
        "do_sample=True,",
        "pad_token_id=tokenizer.eos_token_id",
        ")",
        "    \n",
        "# Decodificar a resposta",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)",
        "    \n",
        "# Extrair apenas a parte da resposta do assistente",
        "response = response.split(\"<|assistant|>\\n\")[-1].strip()",
        "    \n",
        "yield response",
        "\n",
        "\n",
        "\"\"\"\"\"\"",
        "Para informa\u00e7\u00f5es sobre como personalizar o ChatInterface, consulte a documenta\u00e7\u00e3o do Gradio: https://www.gradio.app/docs/chatinterface",
        "\"\"\"\"\"\"",
        "demo = gr.ChatInterface(",
        "responda,",
        "```markdown\nadditional_inputs=[\n```",
        "gr.Textbox(",
        "value=\"Voc\u00ea \u00e9 um Chatbot amig\u00e1vel. Sempre responda na l\u00edngua em que o usu\u00e1rio est\u00e1 escrevendo para voc\u00ea.\"",
        "r\u00f3tulo=\"Mensagem do sistema\"",
        "),",
        "gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"M\u00e1ximo de novos tokens\"),",
        "gr.Slider(m\u00ednimo=0,1, m\u00e1ximo=4,0, valor=0,7, passo=0,1, r\u00f3tulo=\"Temperatura\"),",
        "gr.Slider()",
        "m\u00ednimo=0.1,",
        "m\u00e1ximo=1.0,",
        "value=0.95,",
        "passo=0.05,",
        "label=\"Top-p (amostragem do n\u00facleo)\"",
        "),",
        "],",
        ")",
        "\n",
        "\n",
        "if __name__ == \"__main__\":",
        "demo.launch()",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste arquivo, devemos adicionar as novas bibliotecas que vamos utilizar, neste caso `transformers`, `accelerate` e `torch`. O arquivo completo ficaria:",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2",
        "gradio>=4.0.0",
        "transformers>=4.36.0",
        "torch>=2.0.0",
        "accelerate>=0.25.0",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Desplegamos o space e testamos diretamente a API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2-localmodel.hf.space \u2714\n",
            "Hola M\u00e1ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d\u00eda. \u00bfC\u00f3mo puedo servirte?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2_localModel\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, \u00bfc\u00f3mo est\u00e1s? Me llamo M\u00e1ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Surpreende-me o qu\u00e3o r\u00e1pido o modelo responde, mesmo estando em um servidor sem GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora fazemos o mesmo que antes, mas com FastAPI, LangChain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criar Espa\u00e7o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ao criar o space no HuggingFace, fazemos o mesmo que antes: criamos um novo espa\u00e7o, colocamos um nome e uma descri\u00e7\u00e3o, selecionamos Docker como SDK, escolhemos o HW em que vamos implant\u00e1-lo, no meu caso, escolho o HW mais b\u00e1sico e gratuito, e decidimos se o faremos privado ou p\u00fablico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C\u00f3digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 n\u00e3o importamos `InferenceClient` e agora importamos `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importamos `torch`.",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer",
        "import torch",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos o modelo e o tokenizer com `AutoModelForCausalLM` e `AutoTokenizer`.",
        "\n",
        "``` python\n",
        "# Inicialize o modelo e o tokenizador",
        "print(\"Carregando modelo e tokenizer...\")",
        "dispositivo = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"",
        "\n",
        "tente:",
        "# Carregar o modelo no formato BF16 para melhor desempenho e menor uso de mem\u00f3ria",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "    \n",
        "if device == \"cuda\":",
        "print(\"Usando GPU para o modelo...\")",
        "model = AutoModelForCausalLM.from_pretrained(",
        "nome_do_modelo,",
        "torch_dtype=torch.bfloat16,",
        "device_map=\"auto\",",
        "low_cpu_mem_usage=True",
        ")",
        "else:",
        "print(\"Usando CPU para o modelo...\")",
        "model = AutoModelForCausalLM.from_pretrained(",
        "nome_do_modelo,",
        "device_map={\"\": device},",
        "torch_dtype=torch.float32",
        ")",
        "\n",
        "print(f\"Modelo carregado com sucesso em: {device}\")",
        "except Exception as e:",
        "print(f\"Erro ao carregar o modelo: {str(e)}\")",
        "aumentar",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re definimos a fun\u00e7\u00e3o `call_model` para que fa\u00e7a a infer\u00eancia com o modelo local.",
        "\n",
        "``` python\n",
        "# Defina a fun\u00e7\u00e3o que chama o modelo",
        "def chamar_modelo(estado: EstadoMensagens):",
        "\"\"\"",
        "Chame o modelo com as mensagens fornecidas",
        "\n",
        "Argumentos:",
        "estado: EstadoMensagens",
        "\n",
        "Retorna:",
        "dicion\u00e1rio: Um dicion\u00e1rio contendo o texto gerado e o ID do thread",
        "\"\"\"\"\"\"",
        "# Converter mensagens LangChain para formato de bate-papo",
        "mensagens = []",
        "for msg in state[\"messages\"]:",
        "if isinstance(msg, HumanMessage):",
        "messages.append({\"role\": \"user\", \"content\": msg.content})",
        "elif isinstance(msg, AIMessage):",
        "messages.append({\"role\": \"assistant\", \"content\": msg.content})",
        "    \n",
        "# Prepare o input usando o modelo de bate-papo",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)",
        "    \n",
        "# Gerar resposta",
        "sa\u00eddas = modelo.gerar(",
        "entradas,",
        "max_new_tokens=512,  # Aumente o n\u00famero de tokens para respostas mais longas",
        "temperature=0.7,",
        "top_p=0,9,",
        "do_sample=True,",
        "pad_token_id=tokenizer.eos_token_id",
        ")",
        "    \n",
        "# Decodificar e limpar a resposta",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)",
        "# Extrair apenas a resposta do assistente (ap\u00f3s a \u00faltima mensagem do usu\u00e1rio)",
        "response = response.split(\"Assistant:\")[-1].strip()",
        "    \n",
        "# Converter a resposta para o formato LangChain",
        "```markdown\nai_message = AIMessage(content=response)\n```",
        "return {\"messages\": state[\"messages\"] + [ai_message]}",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temos que remover `langchain-huggingface` e adicionar `transformers`, `accelerate` e `torch` no arquivo `requirements.txt`. O arquivo ficaria:",
        "\n",
        "``` txt\n",
        "fastapi",
        "uvicorn",
        "solicita\u00e7\u00f5es",
        "pydantic>=2.0.0",
        "langchain>=0.1.0",
        "langchain-core>=0.1.10",
        "langgraph>=0.2.27",
        "python-dotenv>=1.0.0",
        "transformers>=4.36.0",
        "torch>=2.0.0",
        "accelerate>=0.26.0",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 n\u00e3o precisamos ter `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` porque como o modelo vai estar no servidor e n\u00e3o vamos fazer chamadas para Inference Endpoints, n\u00e3o precisamos do token. O arquivo ficaria:",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim",
        "\n",
        "RUN useradd -m -u 1000 user",
        "WORKDIR /app",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt",
        "\n",
        "COPY --chown=user . /app",
        "\n",
        "EXPOSE 7860",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste da API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deployamos o space e testamos a API. Neste caso, vou testar diretamente do Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta: system\n",
            "You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\n",
            "user\n",
            "Hola, \u00bfc\u00f3mo est\u00e1s?\n",
            "assistant\n",
            "Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.\n",
            "Thread ID: user1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://maximofn-smollm2-backend-localmodel.hf.space/generate\"\n",
        "data = {\n",
        "    \"query\": \"Hola, \u00bfc\u00f3mo est\u00e1s?\",\n",
        "    \"thread_id\": \"user1\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"Respuesta:\", result[\"generated_text\"])\n",
        "    print(\"Thread ID:\", result[\"thread_id\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este demora um pouco mais que o anterior. Na verdade, demora o normal para um modelo sendo executado em um servidor sem GPU. O estranho \u00e9 quando o deployamos no Gradio. N\u00e3o sei o que a HuggingFace faz por tr\u00e1s, ou talvez tenha sido coincid\u00eancia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclus\u00f5es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vimos como criar um backend com um LLM, tanto fazendo chamadas ao Inference Endpoint da HuggingFace, quanto fazendo chamadas a um modelo rodando localmente. Vimos como fazer isso com Gradio ou com FastAPI, Langchain e Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A partir daqui voc\u00ea tem o conhecimento para poder implantar seus pr\u00f3prios modelos, mesmo que n\u00e3o sejam LLMs, podem ser modelos multimodais. A partir daqui voc\u00ea pode fazer o que quiser."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "maximofn": {
      "date": "2025-03-02",
      "description_en": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "description_es": "\u00bfQuieres desplegar un backend con tu propio LLM? En este post te explico c\u00f3mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.",
      "description_pt": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "end_url": "deploy-backend-with-llm-in-huggingface",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "keywords_en": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_es": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_pt": "hugging face, fastapi, langchain, docker, backend, llm",
      "title_en": "Deploy backend with LLM in HuggingFace",
      "title_es": "Desplegar backend con LLM en HuggingFace",
      "title_pt": "Desplegar backend com LLM no HuggingFace"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}