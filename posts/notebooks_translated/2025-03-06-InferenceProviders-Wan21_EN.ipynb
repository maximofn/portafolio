{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face Inference Providers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is clear that the largest hub of AI models is Hugging Face. And now they are offering the possibility to perform inference on some of their models using serverless GPU providers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One of those models is [Wan-AI/Wan2.1-T2V-14B](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B), which as of writing this post is the best open-source video generation model, as can be seen in the [Artificial Analysis Video Generation Arena Leaderboard](https://artificialanalysis.ai/text-to-video/arena?tab=Leaderboard)",
        "\n",
        "![video generation arena leaderboard](https://images.maximofn.com/Video-arena-leaderboard-wan21.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we look at its [modelcard](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B) we can see on the right a button that says `Replicate`.",
        "\n",
        "![Wan2.1-T2V-14B modelcard](https://images.maximofn.com/Wan2.1-T2V-14B.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference providers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we go to the [Inference providers](https://huggingface.co/settings/inference-providers) settings page, we will see something like this",
        "\n",
        "![Inference Providers](https://images.maximofn.com/Inference%20Providers.webp)",
        "\n",
        "Where we can press the button with a key to enter the API KEY of the provider we want to use, or leave selected the path with two dots. If we choose the first option, it will be the provider who charges us for the inference, while in the second option, it will be Hugging Face who charges us for the inference. So do what suits you best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with Replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In my case, I obtained an API KEY from Replicate and added it to a file called `.env`, which is where I will store the API KEYS and which you should not upload to GitHub, GitLab, or your project repository.",
        "\n",
        "The `.env` must have this format",
        "\n",
        "``` python\n",
        "HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS=\"hf_aL...AY\"",
        "REPLICATE_API_KEY=\"r8_Sh...UD\"",
        "```\n",
        "\n",
        "Where `HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS` is a token you need to obtain from [Hugging Face](https://huggingface.co/settings/tokens) and `REPLICATE_API_KEY` is the API KEY of Replicate which you can obtain from [Replicate](https://replicate.com/account/api-tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading the API Keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we need to do is read the API KEYS from the `.env` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "REPLICATE_API_KEY = os.getenv(\"REPLICATE_API_KEY\")\n",
        "HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS = os.getenv(\"HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logging in the Hugging Face hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To be able to use the Wan-AI/Wan2.1-T2V-14B model, as it is on the Hugging Face hub, we need to log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference Client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create an inference client, we have to specify the provider, the API KEY and in this case, additionally, we are going to set a `timeout` of 1000 seconds, because by default it is 60 seconds and the model takes quite a while to generate the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "\tprovider=\"replicate\",\n",
        "\tapi_key=REPLICATE_API_KEY,\n",
        "\ttimeout=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We already have everything to generate our video. We use the `text_to_video` method of the client, pass it the prompt, and tell it which model from the hub we want to use; if not, it will use the default one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "video = client.text_to_video(\n",
        "\t\"Funky dancer, dancing in a rehearsal room. She wears long hair that moves to the rhythm of her dance.\",\n",
        "\tmodel=\"Wan-AI/Wan2.1-T2V-14B\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we save the video, which is of type `bytes`, to a file on our disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved to: output_video.mp4\n"
          ]
        }
      ],
      "source": [
        "output_path = \"output_video.mp4\"\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(video)\n",
        "print(f\"Video saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generated video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the video generated by the model",
        "\n",
        "<video src=\"https://images.maximofn.com/wan2_1_video.webm\" controls></video>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "maximofn": {
      "date": "2025-03-06",
      "description_en": "Do you want to have your own Sora, but also generate good videos? In this post I explain how to do it with HuggingFace Inference Providers and Replicate.",
      "description_es": "\u00bfQuieres tener tu propio Sora, pero que adem\u00e1s genere buenos v\u00eddeos? En este post te explico c\u00f3mo hacerlo con HuggingFace Inference Providers y Replicate.",
      "description_pt": "Voc\u00ea quer ter seu pr\u00f3prio Sora, mas tamb\u00e9m gerar bons v\u00eddeos? Neste post, explico como fazer isso com HuggingFace Inference Providers e Replicate.",
      "end_url": "inference-providers-wan21-t2v-14b",
      "image": "https://images.maximofn.com/inference-providers-wan21-thumbnail.webp",
      "image_hover_path": "https://images.maximofn.com/inference-providers-wan21-thumbnail.webp",
      "keywords_en": "hugging face, inference providers, replicate, wan21, t2v, 14b",
      "keywords_es": "hugging face, inference providers, replicate, wan21, t2v, 14b",
      "keywords_pt": "hugging face, inference providers, replicate, wan21, t2v, 14b",
      "title_en": "Generate videos with Wan2.1-T2V-14B and Inference Providers",
      "title_es": "Generar v\u00eddeos con Wan2.1-T2V-14B e Inference Providers",
      "title_pt": "Gerar v\u00eddeos com Wan2.1-T2V-14B e Provedores de Inferencia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}