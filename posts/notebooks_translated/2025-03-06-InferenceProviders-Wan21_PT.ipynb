{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Provedores de Infer\u00eancia da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > Aviso: Este post foi traduzido para o portugu\u00eas usando um modelo de tradu\u00e7\u00e3o autom\u00e1tica. Por favor, me avise se encontrar algum erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Est\u00e1 claro que o maior hub de modelos de intelig\u00eancia artificial \u00e9 a Hugging Face. E agora est\u00e3o oferecendo a possibilidade de fazer infer\u00eancia de alguns de seus modelos em provedores de GPUs serverless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Um desses modelos \u00e9 [Wan-AI/Wan2.1-T2V-14B](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B), que no momento de escrever este post, \u00e9 o melhor modelo de gera\u00e7\u00e3o de v\u00eddeo open source, como se pode ver na [Artificial Analysis Video Generation Arena Leaderboard](https://artificialanalysis.ai/text-to-video/arena?tab=Leaderboard)",
        "\n",
        "![video generation arena leaderboard](https://images.maximofn.com/Video-arena-leaderboard-wan21.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se n\u00f3s olharmos para seu [modelcard](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B), podemos ver \u00e0 direita um bot\u00e3o que diz `Replicate`.",
        "\n",
        "![Wan2.1-T2V-14B modelcard](https://images.maximofn.com/Wan2.1-T2V-14B.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Provedores de infer\u00eancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se formos na p\u00e1gina de configura\u00e7\u00e3o dos [Inference providers](https://huggingface.co/settings/inference-providers) veremos algo assim:",
        "\n",
        "![Provedores de Infer\u00eancia](https://images.maximofn.com/Inference%20Providers.webp)",
        "\n",
        "Onde podemos clicar no bot\u00e3o com uma chave para inserir a API KEY do provedor que quisermos usar, ou deixar selecionada a op\u00e7\u00e3o com dois pontos. Se escolhermos a primeira op\u00e7\u00e3o, ser\u00e1 o provedor quem nos cobrar\u00e1 pela infer\u00eancia, enquanto na segunda op\u00e7\u00e3o ser\u00e1 a Hugging Face quem nos cobrar\u00e1 pela infer\u00eancia. Ent\u00e3o, fa\u00e7a o que for melhor para voc\u00ea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Infer\u00eancia com Replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No meu caso, obtive uma API KEY do Replicate e a adicionei a um arquivo chamado `.env`, onde armazenarei as API KEYS e que n\u00e3o deve ser enviado para o GitHub, GitLab ou o reposit\u00f3rio do seu projeto.",
        "\n",
        "O `.env` deve ter este formato",
        "\n",
        "``` python\n",
        "HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS=\"hf_aL...AY\"",
        "REPLICATE_API_KEY=\"r8_Sh...UD\"",
        "```\n",
        "\n",
        "Onde `HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS` \u00e9 um token que voc\u00ea precisa obter a partir do [Hugging Face](https://huggingface.co/settings/tokens) e `REPLICATE_API_KEY` \u00e9 a API KEY do Replicate, que voc\u00ea pode obter a partir do [Replicate](https://replicate.com/account/api-tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Leitura das chaves API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A primeira coisa que temos que fazer \u00e9 ler as chaves API do arquivo `.env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "REPLICATE_API_KEY = os.getenv(\"REPLICATE_API_KEY\")\n",
        "HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS = os.getenv(\"HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logging no hub da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder usar o modelo de Wan-AI/Wan2.1-T2V-14B, como est\u00e1 no hub de Hugging Face, precisamos fazer login."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cliente de Infer\u00eancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora criamos um cliente de infer\u00eancia, temos que especificar o provedor, a API KEY e, neste caso, al\u00e9m disso, vamos estabelecer um tempo de `timeout` de 1000 segundos, porque por padr\u00e3o \u00e9 de 60 segundos e o modelo demora bastante para gerar o v\u00eddeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "\tprovider=\"replicate\",\n",
        "\tapi_key=REPLICATE_API_KEY,\n",
        "\ttimeout=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gera\u00e7\u00e3o do v\u00eddeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "J\u00e1 temos tudo para gerar nosso v\u00eddeo. Usamos o m\u00e9todo `text_to_video` do cliente, passamos o prompt e dizemos qual modelo do hub queremos usar, se n\u00e3o, ele usar\u00e1 o que est\u00e1 por padr\u00e3o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "video = client.text_to_video(\n",
        "\t\"Funky dancer, dancing in a rehearsal room. She wears long hair that moves to the rhythm of her dance.\",\n",
        "\tmodel=\"Wan-AI/Wan2.1-T2V-14B\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Salvando o v\u00eddeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por fim, salvamos o v\u00eddeo, que \u00e9 do tipo `bytes`, em um arquivo no nosso disco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved to: output_video.mp4\n"
          ]
        }
      ],
      "source": [
        "output_path = \"output_video.mp4\"\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(video)\n",
        "print(f\"Video saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V\u00eddeo gerado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este \u00e9 o v\u00eddeo gerado pelo modelo",
        "\n",
        "<video src=\"https://images.maximofn.com/wan2_1_video.webm\" controls></video>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "maximofn": {
      "date": "2025-03-06",
      "description_en": "Do you want to have your own Sora, but also generate good videos? In this post I explain how to do it with HuggingFace Inference Providers and Replicate.",
      "description_es": "\u00bfQuieres tener tu propio Sora, pero que adem\u00e1s genere buenos v\u00eddeos? En este post te explico c\u00f3mo hacerlo con HuggingFace Inference Providers y Replicate.",
      "description_pt": "Voc\u00ea quer ter seu pr\u00f3prio Sora, mas tamb\u00e9m gerar bons v\u00eddeos? Neste post, explico como fazer isso com HuggingFace Inference Providers e Replicate.",
      "end_url": "inference-providers-wan21-t2v-14b",
      "image": "https://images.maximofn.com/inference-providers-wan21-thumbnail.webp",
      "image_hover_path": "https://images.maximofn.com/inference-providers-wan21-thumbnail.webp",
      "keywords_en": "hugging face, inference providers, replicate, wan21, t2v, 14b",
      "keywords_es": "hugging face, inference providers, replicate, wan21, t2v, 14b",
      "keywords_pt": "hugging face, inference providers, replicate, wan21, t2v, 14b",
      "title_en": "Generate videos with Wan2.1-T2V-14B and Inference Providers",
      "title_es": "Generar v\u00eddeos con Wan2.1-T2V-14B e Inference Providers",
      "title_pt": "Gerar v\u00eddeos com Wan2.1-T2V-14B e Provedores de Inferencia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}