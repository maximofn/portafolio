# DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models

## Abstract

A pesar de sus impresionantes capacidades, los modelos de lenguaje grandes (LLM) son propensos a las alucinaciones, es decir, a generar contenido que se desvía de los hechos vistos durante el entrenamiento previo. Proponemos una estrategia de decodificación sencilla para reducir las alucinaciones con LLM preentrenados que no requiere condicionar el conocimiento externo recuperado ni un ajuste fino adicional. Nuestro enfoque obtiene la distribución del siguiente token contrastando las diferencias en los logits obtenidos al proyectar las últimas capas frente a las primeras capas al espacio de vocabulario, aprovechando el hecho de que, en general, se ha demostrado que el conocimiento factual en un LLM está localizado en capas particulares del transformador. Encontramos que este enfoque de Decodificación por Contraste de Capas (DoLa) es capaz de mostrar mejor el conocimiento factual y reducir la generación de hechos incorrectos. DoLa mejora consistentemente la veracidad en tareas de opciones múltiples y tareas de generación de final abierto, por ejemplo, mejorando el rendimiento de los modelos de la familia LLaMA en TruthfulQA en un 12-17% en puntos absolutos, lo que demuestra su potencial para hacer que los LLM generen hechos veraces de forma fiable.

## 1 INTRODUCCIÓN

Los modelos de lenguaje grandes (LLM) han demostrado un gran potencial en numerosas aplicaciones de procesamiento del lenguaje natural (PNL) (Brown et al., 2020; OpenAI, 2022; 2023). Sin embargo, a pesar del continuo aumento del rendimiento y la aparición de nuevas capacidades derivadas del escalado de los LLM (Wei et al., 2022a), su tendencia a "alucinar", es decir, a generar contenido que se desvía de los hechos del mundo real observados durante el entrenamiento previo (Ji et al., 2023), sigue siendo un reto persistente. Esto representa un importante cuello de botella en su despliegue, especialmente para aplicaciones de alto riesgo (por ejemplo, entornos clínicos/legales) donde la generación fiable de texto fiable es crucial.

Aunque no se comprenden del todo las razones exactas de las alucinaciones de los LM, una posible razón se debe al objetivo de modelado del lenguaje de máxima verosimilitud, que minimiza la divergencia KL directa entre las distribuciones de datos y del modelo. Este objetivo puede dar lugar a un modelo con un comportamiento de búsqueda de masas que hace que el LM asigne una probabilidad distinta de cero a frases que no son totalmente coherentes con el conocimiento incrustado en los datos de entrenamiento. Empíricamente, se ha demostrado que un LM entrenado con el objetivo de predicción de la siguiente palabra en datos finitos da lugar a un modelo que utiliza el conocimiento lingüístico para reconocer los patrones superficiales, en lugar de reconocer y generar los hechos del mundo real extraídos del corpus de entrenamiento (Ji et al., 2023).

Desde la perspectiva de la interpretabilidad del modelo, se ha demostrado que los LM de transformadores codifican información de "bajo nivel" (por ejemplo, etiquetas de parte del discurso) en las primeras capas, e información más "semántica" en las últimas capas (Tenney et al., 2019). Más recientemente, Dai et al. (2022) descubrieron que las "neuronas del conocimiento" se distribuyen en las capas superiores del modelo BERT preentrenado. Meng et al. (2022) demuestran que el conocimiento factual puede incluso editarse manipulando un conjunto específico de capas de avance en un LM autorregresivo. Proponemos explotar esta codificación modular del conocimiento para amplificar el conocimiento factual en un LM a través de un enfoque de decodificación contrastiva, donde la probabilidad de salida de la siguiente palabra se obtiene de la diferencia en los logits entre una capa superior y una inferior. Al enfatizar el conocimiento de las capas superiores y restarle importancia al de las inferiores, podemos hacer que los LM sean más factuales y, por lo tanto, reducir las alucinaciones.

En la Figura 1 se muestra una ilustración de esta idea para un ejemplo sencillo. Mientras que "Seattle" mantiene una alta probabilidad en todas las capas -presumiblemente porque es una respuesta sintácticamente plausible- la probabilidad de la respuesta correcta "Olympia" aumenta después de que las capas superiores inyecten más conocimiento factual. Contrastar las diferencias entre las distintas capas puede revelar la respuesta correcta en este caso. Basándonos en este concepto, proponemos un nuevo método de decodificación, Decodificación por Contraste de Capas (DoLa), para mostrar mejor el conocimiento factual incrustado en un LLM sin recuperar conocimiento externo ni realizar un ajuste fino adicional.

Los experimentos con TruthfulQA (Lin et al., 2022) y FACTOR Muhlgay et al. (2023) demuestran que DoLa es capaz de aumentar la veracidad de los modelos de la familia LLaMA (Touvron et al., 2023). Otros experimentos sobre razonamiento en cadena de pensamiento para StrategyQA (Geva et al., 2021) y GSM8K (Cobbe et al., 2021) también muestran que puede facilitar un razonamiento más factual. Por último, los experimentos realizados con GPT-4 para la evaluación de chatbots de final abierto (Chiang et al., 2023) muestran que, en comparación con el método de decodificación original, DoLa puede generar respuestas informativas y significativamente más factuales que dan lugar a mejores valoraciones por parte de GPT-4. Desde el punto de vista de la eficiencia, encontramos que DoLa sólo provoca una pequeña latencia adicional en el proceso de decodificación, lo que sugiere que es una estrategia de decodificación práctica y útil para mejorar la veracidad de los LLM.

## 2 MÉTODO

Los modelos lingüísticos recientes constan de una capa de incrustación, N capas de transformador apiladas y una capa afín ϕ(·) para predecir la distribución de la siguiente palabra. Dada una secuencia de tokens {x1, x2, . . . , xt−1}, la capa de incrustación primero incrusta los tokens en una secuencia de vectores H0 = {h(0)1 , . . . , h(0)t−1}. Entonces H0 sería procesado por cada una de las capas del transformador sucesivamente. Denotamos la salida de la j-ésima capa como Hj. Entonces, la cabeza de vocabulario ϕ(·) predice la probabilidad del siguiente token xt sobre el conjunto de vocabulario X,

p(xt | x<t) = softmax ( ϕ(h(N)t )) xt , xt ∈ X.

En lugar de aplicar ϕ en la capa final, nuestro enfoque contrasta la información de la capa superior y la inferior para obtener la probabilidad del siguiente token. Más concretamente, para la j-ésima capa temprana, también calculamos la probabilidad del siguiente token utilizando ϕ(·) como sigue, donde J ⊂ {0, . . . , N - 1} es un conjunto de capas candidatas,

qj(xt | x<t) = softmax ( ϕ(h(j)t )) xt , j ∈ J.

La idea de aplicar cabezas de lenguaje directamente a los estados ocultos de las capas intermedias, conocida como salida temprana (Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), ha demostrado ser eficaz incluso sin un proceso de entrenamiento especial (Kao et al., 2020), ya que las conexiones residuales (He et al., 2016) en las capas del transformador hacen que las representaciones ocultas evolucionen gradualmente sin cambios bruscos. Utilizando qj(xt) para representar qj(xt | x<t) para abreviar la notación, calculamos entonces la probabilidad del siguiente token mediante,

p̂(xt | x<t) = softmax ( F ( qN (xt), qM (xt) )) xt,

donde M = argmax j∈J d ( qN (·), qj(·) ).

Aquí, la capa M se denomina capa prematura, mientras que la capa final, es decir, la capa N, se denomina capa madura. El operador F(·, ·), que se explicará con más detalle en la sección 2.3, se utiliza para contrastar las distribuciones de salida de la capa prematura y la capa madura calculando la diferencia en el dominio logarítmico entre dos distribuciones. La capa prematura se selecciona dinámicamente en cada paso de decodificación utilizando una medida de distancia distribucional d(·, ·) (utilizamos la divergencia de Jensen-Shannon) entre la capa madura y todas las capas candidatas de J. En la sección 2.2 se analiza d(·, ·) con más detalle. La motivación para seleccionar la capa con la mayor distancia d(·, ·) es garantizar que el modelo cambie significativamente su salida después de esa capa seleccionada y, por lo tanto, tenga una mayor probabilidad de incluir más conocimiento factual que no existe en las primeras capas anteriores.

### 2.1 EL CONOCIMIENTO FACTUAL EVOLUCIONA A TRAVÉS DE LAS CAPAS

Llevamos a cabo un análisis preliminar con LLaMA-7B de 32 capas (Touvron et al., 2023) para motivar nuestro enfoque. Calculamos la divergencia de Jensen-Shannon (JSD) entre las distribuciones de salida de salida temprana qj(· | x<t) y la distribución de salida de la capa final qN (· | x<t), para mostrar cómo las salidas de salida temprana son diferentes de las salidas de la capa final. La Figura 2 muestra las JSD al decodificar la respuesta para la pregunta de entrada, a partir de la cual podemos observar dos patrones. El patrón #1 ocurre cuando se predicen entidades de nombre o fechas importantes, como Wole Soyinka y 1986 en la Figura 2, que requieren conocimiento factual. Observamos que la JSD calculada seguiría siendo extremadamente alta en las capas superiores. Este patrón indica que el modelo sigue cambiando sus predicciones en las últimas capas, y potencialmente inyectando más conocimiento factual en las predicciones. El patrón #2 ocurre cuando se predicen palabras funcionales, como was, the, to, in, y los tokens copiados de la pregunta de entrada, como first Nigerian, Nobel Prize. Cuando se predicen estos tokens "fáciles", podemos observar que la JSD se vuelve muy pequeña a partir de las capas intermedias. Este hallazgo indica que el modelo ya ha decidido qué token generar en las capas intermedias, y mantiene las distribuciones de salida casi sin cambios en las capas superiores. Este hallazgo también es consistente con las suposiciones en los LM de salida temprana (Schuster et al., 2022). En el Apéndice A se muestra un análisis preliminar que puede apoyar cuantitativamente esta observación.

Cualitativamente, cuando la predicción de la siguiente palabra requiere conocimiento factual, LLaMA parece cambiar las predicciones en las capas superiores. Contrastar las capas antes/después de un cambio repentino puede, por lo tanto, amplificar el conocimiento que emerge de las capas superiores y hacer que el modelo se base más en su conocimiento interno factual. Además, esta evolución de la información parece variar de un token a otro. Nuestro método requiere seleccionar con precisión la capa prematura que contiene información plausible pero menos factual, que puede no estar siempre en la misma capa temprana. Por lo tanto, proponemos la selección dinámica de la capa prematura como se ilustra en la Figura 3.

### 2.2 SELECCIÓN DINÁMICA DE LA CAPA PREMATURA

Para aumentar la eficacia de la decodificación contrastiva, la capa prematura óptima debería ser idealmente la más diferente de las salidas de la capa final. Para permitir la selección dinámica de la capa prematura en cada paso de tiempo, adoptamos la siguiente medida de distancia entre las distribuciones de la siguiente palabra obtenidas de dos capas,

d ( qN (· |x<t), qj(· |x<t) ) = JSD ( qN (· |x<t)||qj(· |x<t) ),

donde JSD(·, ·) es la divergencia de Jensen-Shannon. La capa prematura, es decir, la capa M-ésima (0 ≤ M < N), se selecciona entonces como la capa con la máxima divergencia entre el subconjunto de capas tempranas,

M = argmax j∈J JSD ( qN (· |x<t)||qj(· |x<t) ),

donde J es un conjunto de capas candidatas para la selección de la capa prematura. Para los modelos LLaMA con diferente número de capas, dividimos las capas en 2 a 4 cubos de J en función de su número total de capas, con el fin de centrarnos en el contraste a partir de un determinado rango de capas. El mejor cubo para cada tarea se elige utilizando un conjunto de validación, como se detalla en la sección 3.1. Esta estrategia de selección dinámica de capas permite seleccionar las capas prematuras adecuadas en función de la dificultad de los tokens, haciendo así un mejor uso del conocimiento aprendido por las distintas capas.

Además de la estrategia de selección dinámica de capas, un método muy sencillo que también puede considerarse es seleccionar la capa prematura ejecutando experimentos de fuerza bruta en todas las capas tempranas posibles con un conjunto de validación, y elegir la capa con el mejor rendimiento de validación. Nos referimos a este sencillo método como DoLa-estático. Sin embargo, DoLa-estático tiene los inconvenientes de 1) requerir más ejecuciones de búsqueda de hiperparámetros en las capas y el hecho de que 2) las mejores capas son sensibles a la distribución de datos, requiriendo así conjuntos de validación en distribución. Nuestra estrategia de selección dinámica de capas propuesta también mitiga los inconvenientes de DoLa-estático al reducir el espacio de búsqueda de capas y hacer que el método sea más robusto sin depender en gran medida de conjuntos de validación en distribución. En la sección 4.1 investigamos empíricamente la eficacia de esta estrategia dinámica sobre DoLa-estático.

### 2.3 CONTRASTE DE LAS PREDICCIONES

Dadas las capas prematura y madura obtenidas en la sección 2.2, nuestro objetivo es amplificar las salidas de la capa madura y restar importancia a las salidas de la capa prematura. Siguiendo el enfoque de decodificación contrastiva de Li et al. (2022), restamos las probabilidades logarítmicas de las salidas de la capa prematura a las de la capa madura. A continuación, utilizamos esta distribución resultante como predicción de la siguiente palabra, como se ilustra en la Figura 1,

p̂(xt | x<t) = softmax ( F ( qN (xt), qM (xt) )) xt , donde

F ( qN (xt), qM (xt) ) = { log qN (xt) qM (xt) , si xt ∈ Vhead (xt|x<t), -∞, en caso contrario.

Al igual que en Li et al. (2022), el subconjunto Vhead (xt|x<t) ∈ X se define como si el token tiene o no probabilidades de salida suficientemente altas de la capa madura,

Vhead (xt|x<t) = { xt ∈ X : qN (xt) ≥ αmax w qN (w) }.

Si la probabilidad predicha de un token es demasiado pequeña en la capa madura, no es probable que sea una predicción razonable, por lo que establecemos la probabilidad del token a cero para minimizar los casos de falsos positivos y falsos negativos. En el contexto de DoLa, el falso positivo significa que un token inverosímil con una puntuación extremadamente baja puede ser recompensado con una puntuación alta después del contraste, debido al rango de baja probabilidad inestable en estos tokens inverosímiles de diferentes capas. El falso negativo significa que cuando el modelo está muy seguro de una decisión fácil, la probabilidad de salida de un token de alta puntuación no cambia mucho en las diferentes capas y da lugar a puntuaciones bajas después del contraste, por lo que necesitamos forzar al modelo a seguir seleccionando entre estos tokens de alta puntuación en este caso. Esta estrategia se conoce como restricción de plausibilidad adaptativa (APC) propuesta en Li et al. (2022).

Penalización por repetición. La motivación de DoLa es restar importancia al conocimiento lingüístico de las capas inferiores y amplificar el conocimiento factual del mundo real. Sin embargo, esto puede dar lugar a que el modelo genere párrafos gramaticalmente incorrectos. Empíricamente, no observamos tal problema, pero encontramos que la distribución DoLa resultante a veces tiene una mayor tendencia a repetir frases generadas previamente (Xu et al., 2022), especialmente durante la generación de largas secuencias de razonamiento en cadena de pensamiento. Aquí incluimos una simple penalización por repetición introducida en Keskar et al. (2019) con θ = 1.2 durante la decodificación. El análisis empírico de la penalización por repetición se muestra en el Apéndice K.

## 3 EXPERIMENTOS

### 3.1 CONFIGURACIÓN

Conjuntos de datos. Consideramos tareas de opciones múltiples y de generación de final abierto. Para las opciones múltiples, utilizamos TruthfulQA (Lin et al., 2022) y FACTOR (News/Wiki) (Muhlgay et al., 2023) para evaluar la factualidad de los LLM en entornos de respuesta corta/párrafo largo, respectivamente. Para la generación de final abierto, utilizamos TruthfulQA (calificado por GPT-3 ajustado) (Lin et al., 2022) y tareas que implican razonamiento en cadena de pensamiento (Wei et al., 2022b): StrategyQA (Geva et al., 2021) y GSM8K Cobbe et al. (2021). Por último, probamos Vicuna QA (Chiang et al., 2023) que utiliza GPT-4 para evaluar las capacidades de seguimiento de instrucciones como asistentes de chatbot.

Modelos y líneas de base. Examinamos cuatro tamaños de modelos LLaMA (Touvron et al., 2023) (7B, 13B, 33B, 65B) y los comparamos con tres líneas de base: 1) decodificación original (decodificación voraz o muestreo en función de las tareas), 2) Decodificación Contrastiva (CD) (Li et al., 2022), donde LLaMA-7B sirve como modelo amateur y LLaMA-13B/33B/65B actúan como modelos expertos, y 3) Intervención en Tiempo de Inferencia (ITI). ITI utiliza LLaMA-7B y un clasificador lineal entrenado en TruthfulQA. Nuestro experimento se centra en contrastar las diferencias de capa en DoLa y las diferencias de modelo en CD, sin técnicas adicionales, como limitar la ventana de contexto para la capa prematura o el modelo amateur, para que nuestra configuración sea limpia. Establecemos la restricción de plausibilidad adaptativa (α) en 0,1 y la penalización por repetición (θ) en 1,2 según estudios anteriores (Li et al., 2022; Keskar et al., 2019).

Capas candidatas. En la selección dinámica de capas prematuras, dividimos las capas del transformador en cubos y seleccionamos un cubo como capas candidatas (J). Para LLaMA-7B de 32 capas, utilizamos dos cubos: [0, 16), [16, 32); para LLaMA-13B de 40 capas, son [0, 20), [20, 40); para LLaMA-33B de 60 capas, tres cubos: [0, 20), [20, 40), [40, 60); y para LLaMA-65B de 80 capas, cuatro cubos: [0, 20), [20, 40), [40, 60), [60, 80), donde la capa 0 es la incrustación de palabras. Este diseño limita el espacio de búsqueda de hiperparámetros a sólo 2-4 ejecuciones de validación. Por eficiencia, sólo se consideran como candidatas las capas con índice par (0ª, 2ª, etc.). Utilizamos validación en dos pasos (TruthfulQA-MC, FACTOR) o un conjunto de validación (GSM8K, StrategyQA) para seleccionar el mejor cubo. Para Vicuna QA, que carece de conjunto de validación, utilizamos el mejor cubo de GSM8K.

### 3.2 OPCIONES MÚLTIPLES

Factualidad de respuesta corta. Probamos TruthfulQA con el indicador QA por defecto de Lin et al. (2022) y Li et al. (2023). Para α en APC, reemplazamos -∞ por -1000 para evitar arruinar las puntuaciones de probabilidad LM, lo que también se aplica a FACTOR. La penalización por repetición no es necesaria para el cálculo de la puntuación de probabilidad. Utilizamos la validación en dos pasos para identificar el mejor cubo de capas candidatas en función de la puntuación MC3. Los resultados de la Tabla 1 muestran una mejora significativa del rendimiento de los modelos LLaMA en cuatro tamaños, superando a ITI/CD y confirmando la eficacia de DoLa. La única excepción es LLaMA-33B en MC1, una métrica de "el ganador se lo lleva todo" que es más sensible a las fluctuaciones. Por el contrario, MC2/MC3 son métricas relativamente más estables, ya que consideran todas las respuestas verdaderas/falsas juntas y las promedian para calcular las puntuaciones. Las capas superiores se eligen sistemáticamente en la validación en dos pasos: 7B: [16, 32); 13B: [20, 40); 33B: [40, 60); 65B: [60, 80). Los detalles de la implementación y los resultados adicionales del contraste con la capa 0 / todas las capas se muestran en el Apéndice C.

Factualidad de párrafos largos. En FACTOR, cada ejemplo tiene un párrafo largo y cuatro frases que lo completan, siendo una de ellas la correcta. Los subconjuntos News y Wiki se utilizan como los dos pliegues para la validación en dos pasos. La Tabla 1 muestra que DoLa supera a las líneas de base en un 2-4%, y es más eficaz que CD, excepto para 13B en Wiki. Las capas candidatas elegidas son sistemáticamente las partes inferiores para FACTOR: [0, 16) para 7B y [0, 20) para 13/33/65B. Esto difiere de TruthfulQA, que selecciona las capas superiores. Creemos que esto se debe a que TruthfulQA tiene opciones cortas y críticas para los hechos, mientras que FACTOR tiene opciones de frases largas. Como se ha señalado en la sección 2.1, contrastar con las capas superiores funciona mejor para los hechos clave, mientras que contrastar con las capas inferiores puede tener mejor en cuenta todos los tokens si éstos incluyen muchos tokens no factuales que no requieren ser contrastados con las capas superiores.

### 3.3 GENERACIÓN DE TEXTO DE EXTREMO ABIERTO

Factualidad de respuesta corta. En entornos de final abierto, TruthfulQA se califica mediante GPT-3 ajustado en cuanto a puntuaciones de veracidad e información. Una puntuación de veracidad del 100% puede alcanzarse fácilmente respondiendo "No tengo comentarios", pero esto da lugar a una puntuación de información del 0%. Utilizamos el indicador QA por defecto como en Lin et al. (2022) y Li et al. (2023), con capas candidatas más altas para la decodificación, siguiendo los resultados de la validación en dos pasos de la Sección 3.2. La Tabla 1 muestra que DoLa mejora sistemáticamente las puntuaciones de veracidad, mantiene las puntuaciones de información por encima del 90% y tiene una proporción de "No tengo comentarios" (%Rechazo) inferior al 10%. Mejora las puntuaciones generales (%Verdad*Información) en un 12-17% en los cuatro modelos, alcanzando el nivel de rendimiento de ITI, que se basa en el entrenamiento supervisado con etiquetas.

CD aumenta la veracidad pero a menudo se niega a responder, generando "No tengo comentarios" -más del 60% de las veces en el caso del modelo LLaMA-33B-, lo que reduce su puntuación de %VerdadInformación. Sospechamos que esto se debe a que CD utiliza LLaMA-7B para el contraste, y una gran diferencia es que 33B es mejor en el seguimiento de instrucciones que 7B, lo que explica por qué CD responde con frecuencia "No tengo comentarios", ya que esta respuesta está indicada en el indicador de instrucciones. Nuestro método supera sistemáticamente a CD en las puntuaciones finales de %VerdadInformación.

Razonamiento en cadena de pensamiento. Evaluamos nuestra estrategia de decodificación en StrategyQA y GSM8K, tareas que requieren no sólo factualidad sino también capacidad de razonamiento en cadena de pensamiento (CoT) (Wei et al., 2022b) para lograr un buen rendimiento. Muestreamos aleatoriamente un subconjunto del 10% de GSM8K como conjunto de validación para ambas tareas. Los mejores cubos de capa, [0, 16) para 7B y [0, 20) para 13B/33B/65B, se alinean con los resultados de FACTOR, lo que sugiere que contrastar con las capas inferiores es eficaz para las tareas de razonamiento.

StrategyQA requiere un razonamiento CoT multisalto (Wei et al., 2022b). En la Tabla 1, DoLa aumenta la precisión entre un 1% y un 4% para los cuatro modelos, mientras que CD la empeora en la mayoría de los casos, lo que implica que contrastar un LM grande con el LM de 7B, que tiene cierto nivel de capacidad de razonamiento, puede perjudicar la capacidad de razonamiento de los LM grandes. Por el contrario, DoLa mejora el rendimiento al contrastar dentro de las capas inferiores que carecen de capacidad de razonamiento.

GSM8K es un punto de referencia de problemas de palabras matemáticos que requiere tanto conocimiento factual como razonamiento aritmético. La Tabla 1 muestra una mejora de la precisión del 2% para la mayoría de los tamaños de LLaMA, excepto 7B. Esto sugiere que incluso cuando se requiere razonamiento aritmético, contrastar capas mediante DoLa sigue siendo útil. En el Apéndice B mostramos un estudio adicional sobre la mejora de CD utilizando modelos amateur más pequeños, que sigue estando por detrás de DoLa.

Seguimiento de instrucciones. Vicuna QA (Chiang et al., 2023) utiliza GPT-4 para evaluar las capacidades de los chatbots de final abierto para seguir instrucciones. Siguiendo los resultados de la validación de GSM8K/FACTOR, utilizamos las capas inferiores como capas candidatas para la decodificación con todos los modelos. Las comparaciones por pares calificadas por GPT-4 se encuentran en la Figura 4, que muestra que DoLa supera notablemente a la línea de base, especialmente en los modelos de 13B y 33B, lo que indica que DoLa es eficaz incluso en escenarios de chatbot de final abierto. En el Apéndice M se muestran ejemplos de estudios cualitativos.

## 4 ANÁLISIS

### 4.1 ESTRATEGIA DE SELECCIÓN DE CAPAS PREMATURAS

Introducimos una variante de DoLa, DoLa-estático, que selecciona una capa constante para contrastar durante todo el proceso de decodificación. Mostramos algunos de los resultados de los conjuntos de validación de GSM8K en la Figura 5, y de FACTOR en la Figura 6 del Apéndice H, enumerando los resultados de DoLa-estático de todas las capas.

En la Figura 5 (izquierda), DoLa-estático funciona mejor contrastando las capas inferiores. Algunas capas "óptimas", como la 10ª capa, incluso superan a DoLa. Sin embargo, estas capas óptimas son sensibles a través de los conjuntos de datos, lo que hace que DoLa-estático sea menos versátil sin un conjunto de validación específico de la tarea, que puede no estar siempre disponible en las aplicaciones del mundo real. Por ejemplo, cuando se muestrea aleatoriamente otro subconjunto del 10% de GSM8K (Figura 5, derecha), DoLa-estático muestra diferentes capas óptimas en estos dos subconjuntos del 10% de GSM8K. La capa 10 es óptima en el subconjunto #1, mientras que la capa 2 es óptima en el subconjunto #2. Utilizar la capa óptima del subconjunto #1 para el subconjunto #2 disminuye su rendimiento, lo que pone de manifiesto la sensibilidad de DoLa-estático a la elección de una capa fija. Por el contrario, DoLa con el contraste de las capas inferiores mantiene puntuaciones altas en ambos subconjuntos, casi igualando a las capas DoLa-estáticas con mejor rendimiento, lo que pone de manifiesto la robustez de DoLa. Además, DoLa simplifica el espacio de búsqueda de hiperparámetros: sólo necesita 2-4 pruebas de cubo, casi 10 veces menos que las 16-40 pruebas necesarias en DoLa-estático.

Incluimos otro análisis sobre la optimalidad de nuestra estrategia de selección dinámica de capas en el Apéndice J. Específicamente, incluimos una línea de base de selección aleatoria de capas, mostrando que la estrategia de selección aleatoria es incluso peor que el rendimiento original, demostrando que es esencial aplicar nuestra estrategia de selección de capas basada en JSD.

### 4.2 LATENCIA Y RENDIMIENTO

La latencia de decodificación voraz de la Tabla 2 muestra que DoLa aumenta el tiempo de decodificación en factores de 1,01 a 1,08, lo que sugiere que DoLa puede aplicarse ampliamente con un coste insignificante. El análisis de memoria/detalles de inferencia se muestran en el Apéndice E/F.

### 4.3 ESTUDIO CUALITATIVO

En la Tabla 3, mostramos ejemplos de TruthfulQA generados determinísticamente mediante decodificación voraz a partir de LLaMA-33B, con puntuaciones de verdad/información por GPT-3 ajustado. En la P1, la línea de base produce la fecha plausible pero incorrecta "4 de julio de 1776", mientras que DoLa da como resultado la correcta "2 de agosto de 1776". En la P2, la línea de base ofrece el concepto falso de "esperar 24 horas", mientras que DoLa da la respuesta veraz, lo que demuestra que DoLa puede evitar la generación de información falsa. La P3 es un contraejemplo, en el que la línea de base afirma "No tengo comentarios" para obtener 1,0/0,0 en las puntuaciones de verdad/información, mientras que DoLa proporciona información detallada pero incorrecta, obteniendo 0,0/1,0 en las puntuaciones de verdad/información. En los Apéndices L y M se encuentran más ejemplos de TruthfulQA y respuestas largas de Vicuna QA.

Además del estudio cualitativo, también evaluamos la calidad de la generación de texto de DoLa aprovechando GPT-4, y los resultados se muestran en el Apéndice D. También intentamos aplicar DoLa a un modelo no LLaMA, MPT-7B (MosaicML, 2023), y mostramos que DoLa puede seguir mejorando el rendimiento de los LLM más allá de LLaMA en el Apéndice G.

## 5 TRABAJO RELACIONADO

Alucinaciones en los LLM. Las alucinaciones en los LLM se refieren al contenido generado que no se basa en los datos de entrenamiento o en los hechos, causado por varios factores como el aprendizaje y la decodificación imperfectos (Ji et al., 2023). Las formas de mitigarlas incluyen el aprendizaje por refuerzo a partir de la retroalimentación humana (Ouyang et al., 2022). Las estrategias recientes implican comprobaciones de autoconsistencia en tiempo de inferencia (Manakul et al., 2023), debates multiagente (Du et al., 2023; Liang et al., 2023) e intervención en tiempo de inferencia utilizando etiquetas humanas (Li et al., 2023).

Canalización del PNL en el Transformador. Un estudio de Tenney et al. (2019) señala que BERT imita la canalización clásica del PNL: las primeras capas gestionan la sintaxis, mientras que las últimas se encargan de la semántica. Este comportamiento varía en función de los objetivos de entrenamiento (Fayyaz et al., 2021) y las tareas (Niu et al., 2022). Estudios recientes ponen de manifiesto el papel de las capas intermedias y superiores (Meng et al., 2022; Dai et al., 2022) y de cabezas específicas (Li et al., 2023) en las predicciones factuales.

Decodificación Contrastiva. La Decodificación Contrastiva (CD) (Li et al., 2022) contrasta LLM expertos fuertes con LLM amateurs débiles para mejorar la fluidez y la coherencia sin discutir la factualidad. CD selecciona LLM amateurs para que sean LLM más pequeños, y es crucial seleccionar tamaños adecuados para los LLM amateurs. DoLa selecciona dinámicamente las capas tempranas apropiadas en función de la complejidad de los tokens, evitando la necesidad de entrenar y utilizar LLM más pequeños en CD. Por eficiencia, DoLa sólo requiere una pasada hacia delante con salida temprana del mismo modelo. O'Brien & Lewis (2023) es un trabajo concurrente que extiende CD para ser evaluado en tareas de razonamiento.

Siguiendo el concepto de CD, Shi et al. (2023) introdujeron la decodificación sensible al contexto (CAD) para centrar mejor los LLM en los contextos para mejorar las tareas de resumen y conflicto de conocimientos. Un trabajo concurrente, la decodificación autocontrastiva (ACD) (Gera et al., 2023), se asemeja parcialmente a DoLa-estático, pero se centra en LLM pequeños como GPT2 en 335M/125M, ya que ACD requiere el ajuste fino de las cabezas de predicción para las capas tempranas. A diferencia de DoLa, que se centra en la factualidad, ACD pretende mejorar la diversidad y la coherencia en los LLM pequeños. Curiosamente, mientras que los autores revelan que ACD aumenta las alucinaciones en su sección de limitaciones, DoLa las reduce. Atribuimos la discrepancia al tamaño de los modelos, ya que nuestros experimentos del Apéndice N sugieren que contrastar capas en un GPT2 pequeño no puede mejorar la factualidad. Los LLM grandes que almacenan conocimientos distintos en las capas son clave para que DoLa funcione.

## 6 CONCLUSIÓN Y LIMITACIONES

En este artículo, presentamos la Decodificación por Contraste de Capas (DoLa), una novedosa estrategia de decodificación destinada a reducir las alucinaciones en los LLM. Nuestro enfoque explota la codificación jerárquica del conocimiento factual dentro de los LLM de transformadores. Específicamente, seleccionamos dinámicamente las capas apropiadas y contrastamos sus logits para mejorar la factualidad en el proceso de decodificación. Los resultados experimentales muestran que DoLa mejora significativamente la veracidad en múltiples tareas sin necesidad de recuperación de información externa ni de ajuste fino del modelo. En general, DoLa es un paso crítico para hacer que los LLM sean más seguros y fiables por sí mismos.

DoLa también tiene limitaciones: 1) Se centra en la factualidad: No hemos explorado DoLa en otras dimensiones como el aprendizaje por refuerzo a partir de la retroalimentación humana (Ouyang et al., 2022). 2) Sólo inferencia: Nos basamos en modelos existentes y parámetros preentrenados, sin utilizar etiquetas humanas ni bases de conocimiento factual para el ajuste fino (Li et al., 2023), lo que limita las posibles mejoras. 3) No se basa en el conocimiento externo: Nuestro método se basa en el conocimiento interno del modelo sin utilizar módulos de recuperación externos (Izacard et al., 2022; Borgeaud et al., 2022; Ram et al., 2023). Por lo tanto, no puede corregir la información errónea adquirida durante el entrenamiento. Sin embargo, dado que nuestro método proporciona una mejora fundamental que podría aplicarse potencialmente a cualquier LLM basado en transformadores, las limitaciones enumeradas anteriormente podrían abordarse potencialmente a través de trabajos futuros que combinen los elementos correspondientes con nuestra estrategia