<?xml version='1.0' encoding='utf-8'?>
<notebook>
  <markdown># API da OpenAI</markdown>
  <markdown> &gt; Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</markdown>
  <markdown>## Instalar a biblioteca do OpenAI</markdown>
  <markdown>Em primeiro lugar, para poder usar a API da OpenAI, é necessário instalar a biblioteca da OpenAI. Para isso, executamos o seguinte comando</markdown>
  <input_code>%pip install --upgrade openai</input_code>
  <markdown>## Importar a biblioteca do OpenAI</markdown>
  <markdown>Uma vez instalada a biblioteca, a importamos para poder usá-la em nosso código.</markdown>
  <input_code>import openai</input_code>
  <markdown>## Obter uma API Key</markdown>
  <markdown>Para poder usar a API da OpenAI, é necessário obter uma API Key. Para isso, nos dirigimos à página da [OpenAI](https://openai.com/), e nos registramos. Uma vez registrados, nos dirigimos à seção de [API Keys](https://platform.openai.com/api-keys), e criamos uma nova API Key.

![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)
</markdown>
  <markdown>Uma vez que temos, dizemos à API do OpenAI qual é nossa chave API.</markdown>
  <input_code>api_key = "Pon aquí tu API key"</input_code>
  <markdown>## Criamos nosso primeiro chatbot</markdown>
  <markdown>Com a API da OpenAI é muito simples criar um chatbot básico, ao qual vamos passar um prompt, e ele vai nos retornar uma resposta.</markdown>
  <markdown>Em primeiro lugar temos que escolher qual modelo vamos usar, no meu caso vou usar o modelo `gpt-3.5-turbo-1106` que atualmente é um bom modelo para este post, já que para o que vamos fazer não precisamos usar o melhor modelo. A OpenAI tem uma lista com todos os seus [modelos](https://platform.openai.com/docs/models) e uma página com os [preços](https://openai.com/pricing)</markdown>
  <input_code>model = "gpt-3.5-turbo-1106"</input_code>
  <markdown>Agora temos que criar um cliente que será o responsável por se comunicar com a API da OpenAI.</markdown>
  <input_code>client = openai.OpenAI(api_key=api_key, organization=None)</input_code>
  <markdown>Como vemos, passamos nossa chave de API. Além disso, podemos passar a organização, mas em nosso caso não é necessário.</markdown>
  <markdown>Criamos o prompt</markdown>
  <input_code>promtp = "Cuál es el mejor lenguaje de programación para aprender?"</input_code>
  <markdown>E já podemos pedir uma resposta à OpenAI</markdown>
  <input_code>response = client.chat.completions.create(
  model=model,
  messages=[{"role": "user", "content": f"{promtp}"}],
)</input_code>
  <markdown>Vamos ver como é a resposta.</markdown>
  <input_code>type(response), response</input_code>
  <output_code>(openai.types.chat.chat_completion.ChatCompletion,
 ChatCompletion(id='chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un "mejor" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))], created=1701584994, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)))</output_code>
  <input_code>print(f"response.id = {response.id}")
print(f"response.choices = {response.choices}")
for i in range(len(response.choices)):
    print(f"response.choices[{i}] = {response.choices[i]}")
    print(f"\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}")
    print(f"\tresponse.choices[{i}].index = {response.choices[i].index}")
    print(f"\tresponse.choices[{i}].message = {response.choices[i].message}")
    content = response.choices[i].message.content.replace('\n\n', '\n\t\t')
    print(f"\t\tresponse.choices[{i}].message.content = \n\t\t{content}")
    print(f"\t\tresponse.choices[{i}].message.role = {response.choices[i].message.role}")
    print(f"\t\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}")
    print(f"\t\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}")
print(f"response.created = {response.created}")
print(f"response.model = {response.model}")
print(f"response.object = {response.object}")
print(f"response.system_fingerprint = {response.system_fingerprint}")
print(f"response.usage = {response.usage}")
print(f"\tresponse.usage.completion_tokens = {response.usage.completion_tokens}")
print(f"\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}")
print(f"\tresponse.usage.total_tokens = {response.usage.total_tokens}")</input_code>
  <output_code>response.id = chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y
response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un "mejor" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))]
response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un "mejor" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))
	response.choices[0].finish_reason = stop
	response.choices[0].index = 0
	response.choices[0].message = ChatCompletionMessage(content='No hay un "mejor" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None)
		response.choices[0].message.content = 
		No hay un "mejor" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.
		response.choices[0].message.role = assistant
		response.choices[0].message.function_call = None
		response.choices[0].message.tool_calls = None
response.created = 1701584994
response.model = gpt-3.5-turbo-1106
response.object = chat.completion
response.system_fingerprint = fp_eeff13170a
response.usage = CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)
	response.usage.completion_tokens = 181
	response.usage.prompt_tokens = 21
	response.usage.total_tokens = 202
</output_code>
  <markdown>Como podemos ver, nos devolve muita informação.

Por exemplo `response.choices[0].finish_reason = stop` significa que o modelo parou de gerar texto porque chegou ao final do prompt. Isso nos vem muito bem para depurar, já que os possíveis valores são `stop` que significa que a API retornou a mensagem completa, `length` que significa que a saída do modelo foi incompleta devido ao fato de ser maior que `max_tokens` ou limite de token do modelo, `function_call` o modelo decidiu chamar uma função, `content_filter` que significa que o conteúdo foi omitido por uma limitação de conteúdo da OpenAI e `null` que significa que a resposta da API foi incompleta

Também nos fornece informações sobre os tokens para podermos acompanhar o dinheiro gasto
</markdown>
  <markdown>### Parâmetros</markdown>
  <markdown>Quando solicitamos uma resposta à OpenAI, podemos passar uma série de parâmetros para que ela nos retorne uma resposta mais adequada ao que desejamos. Vamos ver quais são os parâmetros que podemos passar.

* `mensagens`: Lista de mensagens que foram enviadas ao chatbot
* `model`: Modelo que queremos usar
* `frequency_penalty`: Penalidade de frequência. Quanto maior for o valor, menos provável será que o modelo repita a mesma resposta.
* `max_tokens`: Número máximo de tokens que pode retornar o modelo
* `n`: Número de respostas que queremos que o modelo nos retorne
* `presence_penalty`: Penalização de presença. Quanto maior for o valor, menos provável será que o modelo repita a mesma resposta
* `seed`: Semente para a geração de texto
* `stop`: Lista de tokens que indicam que o modelo deve parar de gerar texto
* `stream`: Se `True`, a API retornará uma resposta sempre que o modelo gerar um token. Se `False`, a API retornará uma resposta quando o modelo tiver gerado todos os tokens.
* `temperature`: Quanto maior for o valor, mais criativo será o modelo
* `top_p`: Quanto maior for o valor, mais criativo será o modelo
* `user`: ID do usuário que está conversando com o chatbot
* `timeout`: Tempo máximo que queremos esperar a que a API nos retorne uma resposta
</markdown>
  <markdown>Vamos ver alguns</markdown>
  <markdown>#### Mensagens</markdown>
  <markdown>Podemos passar à API uma lista de mensagens que foram enviadas ao chatbot. Isso é útil para fornecer o histórico de conversas ao chatbot, para que ele possa gerar uma resposta mais adequada à conversa. E para condicionar a resposta do chatbot ao que foi dito anteriormente.

Além disso, podemos passar uma mensagem de sistema para indicar como deve se comportar.
</markdown>
  <markdown>##### Histórico de conversas</markdown>
  <markdown>Vamos ver um exemplo do histórico de conversas, primeiro perguntamos como está.</markdown>
  <input_code>promtp = "Hola, soy MaximoFN, ¿Cómo estás?"

response = client.chat.completions.create(
  model=model,
  messages=[{"role": "user", "content": f"{promtp}"}],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?
</output_code>
  <markdown>Ele respondeu que não tem sentimentos e perguntou em que pode nos ajudar. Então, se eu perguntar agora como ele se chama, ele não vai saber responder.</markdown>
  <input_code>promtp = "¿Me puedes decir cómo me llamo?"

response = client.chat.completions.create(
  model=model,
  messages=[{"role": "user", "content": f"{promtp}"}],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>Lo siento, no tengo esa información. Pero puedes decírmelo tú.
</output_code>
  <markdown>Para resolver isso, passamos o histórico de conversas.</markdown>
  <input_code>promtp = "¿Me puedes decir cómo me llamo?"

response = client.chat.completions.create(
  model=model,
  messages=[
      {"role": "user", "content": "Hola, soy MaximoFN, ¿Cómo estás?"},
      {"role": "assistant", "content": "Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?"},
      {"role": "user", "content": f"{promtp}"},
  ],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>Tu nombre es MaximoFN.
</output_code>
  <markdown>##### Condicionamento por exemplos</markdown>
  <markdown>Agora vejamos um exemplo de como condicionar a resposta do chatbot ao que foi dito anteriormente. Agora perguntamos como obter a lista de arquivos em um diretório no terminal.</markdown>
  <input_code>promtp = "¿Cómo puedo listar los archivos de un directorio en la terminal?"

response = client.chat.completions.create(
  model=model,
  messages=[{"role": "user", "content": f"{promtp}"}],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>En la terminal de un sistema operativo Unix o Linux, puedes listar los archivos de un directorio utilizando el comando `ls`. Por ejemplo, si quieres listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si deseas listar los archivos de un directorio específico, puedes proporcionar la ruta del directorio después del comando `ls`, por ejemplo `ls /ruta/del/directorio`. Si deseas ver más detalles sobre los archivos, puedes usar la opción `-l` para obtener una lista detallada o `-a` para mostrar también los archivos ocultos.
</output_code>
  <markdown>Vamos ver o que ela responde se agora a condicionarmos com exemplos de respostas curtas.</markdown>
  <input_code>promtp = "¿Cómo puedo listar los archivos de un directorio en la terminal?"

response = client.chat.completions.create(
  model=model,
  messages=[
      {"role": "user", "content": "Obtener las 10 primeras líneas de un archivo"},
      {"role": "assistant", "content": "head -n 10"},
      {"role": "user", "content": "Encontrar todos los archivos con extensión .txt"},
      {"role": "assistant", "content": "find . -name '*.txt"},
      {"role": "user", "content": "Dividir un archivo en varias páginas"},
      {"role": "assistant", "content": "split -l 1000"},
      {"role": "user", "content": "Buscar la dirección IP 12.34.56.78"},
      {"role": "assistant", "content": "nslookup 12.34.56.78"},
      {"role": "user", "content": "Obtener las 5 últimas líneas de foo.txt"},
      {"role": "assistant", "content": "tail -n 5 foo.txt"},
      {"role": "user", "content": "Convertir ejemplo.png en JPEG"},
      {"role": "assistant", "content": "convert example.png example.jpg"},
      {"role": "user", "content": "Create a git branch named 'new-feature"},
      {"role": "assistant", "content": "git branch new-feature"},
      {"role": "user", "content": f"{promtp}"},
  ],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>Puede usar el comando `ls` en la terminal para listar los archivos de un directorio. Por ejemplo:
```
ls
```
Muestra los archivos y directorios en el directorio actual.
</output_code>
  <markdown>Conseguimos que ele dê uma resposta mais curta.</markdown>
  <markdown>##### Condicionamento com mensagem do sistema</markdown>
  <markdown>Podemos passar uma mensagem de sistema para indicar como deve se comportar.</markdown>
  <input_code>promtp = "¿Cómo puedo listar los archivos de un directorio en la terminal?"

response = client.chat.completions.create(
  model=model,
  messages=[
      {"role": "system", "content": "Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal"},
      {"role": "user", "content": f"{promtp}"},
  ],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>Puedes listar los archivos de un directorio en la terminal usando el comando `ls`. Por ejemplo, para listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si quieres listar los archivos de un directorio específico, puedes utilizar `ls` seguido de la ruta del directorio. Por ejemplo, `ls /ruta/del/directorio`.
</output_code>
  <input_code>promtp = "¿Cómo puedo listar los archivos de un directorio en la terminal?"

response = client.chat.completions.create(
  model=model,
  messages=[
      {"role": "system", "content": "Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal"},
      {"role": "user", "content": "Obtener las 10 primeras líneas de un archivo"},
      {"role": "assistant", "content": "head -n 10"},
      {"role": "user", "content": "Encontrar todos los archivos con extensión .txt"},
      {"role": "assistant", "content": "find . -name '*.txt"},
      {"role": "user", "content": "Dividir un archivo en varias páginas"},
      {"role": "assistant", "content": "split -l 1000"},
      {"role": "user", "content": "Buscar la dirección IP 12.34.56.78"},
      {"role": "assistant", "content": "nslookup 12.34.56.78"},
      {"role": "user", "content": "Obtener las 5 últimas líneas de foo.txt"},
      {"role": "assistant", "content": "tail -n 5 foo.txt"},
      {"role": "user", "content": "Convertir ejemplo.png en JPEG"},
      {"role": "assistant", "content": "convert example.png example.jpg"},
      {"role": "user", "content": "Create a git branch named 'new-feature"},
      {"role": "assistant", "content": "git branch new-feature"},
      {"role": "user", "content": f"{promtp}"},
  ],
)
content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)</input_code>
  <output_code>Puedes listar los archivos de un directorio en la terminal utilizando el comando "ls". Por ejemplo, para listar los archivos en el directorio actual, puedes ejecutar el comando "ls". Si deseas listar los archivos de otro directorio, simplemente especifica el directorio después del comando "ls", por ejemplo "ls /ruta/al/directorio".
</output_code>
  <markdown>#### Número máximo de tokens da resposta</markdown>
  <markdown>Podemos limitar o número de tokens que o modelo pode retornar. Isso é útil para evitar que o modelo ultrapasse a resposta que desejamos.</markdown>
  <input_code>promtp = "¿Cuál es el mejor lenguaje de programación para aprender?"

response = client.chat.completions.create(
  model = model,
  messages = [{"role": "user", "content": f"{promtp}"}],
  max_tokens = 50,
)

content = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content)
print(f"\nresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}")</input_code>
  <output_code>La respuesta a esta pregunta puede variar dependiendo de los intereses y objetivos individuales, ya que cada lenguaje de programación tiene sus propias ventajas y desventajas. Sin embargo, algunos de los lenguajes más

response.choices[0].finish_reason = length
</output_code>
  <markdown>Como podemos ver, a resposta é cortada pela metade porque excederia o limite de tokens. Além disso, agora a razão de parada é `length` em vez de `stop`.</markdown>
  <markdown>#### Criatividade do modelo através da temperatura</markdown>
  <markdown>Podemos fazer o modelo mais criativo através da temperatura. Quanto maior for o valor, mais criativo será o modelo.</markdown>
  <input_code>promtp = "¿Cuál es el mejor lenguaje de programación para aprender?"
temperature = 0

response = client.chat.completions.create(
  model = model,
  messages = [{"role": "user", "content": f"{promtp}"}],
  temperature = temperature,
)

content_0 = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content_0)</input_code>
  <output_code>No hay un "mejor" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.
</output_code>
  <input_code>promtp = "¿Cuál es el mejor lenguaje de programación para aprender?"
temperature = 1

response = client.chat.completions.create(
  model = model,
  messages = [{"role": "user", "content": f"{promtp}"}],
  temperature = temperature,
)

content_1 = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content_1)</input_code>
  <output_code>No hay un "mejor" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.
</output_code>
  <input_code>print(content_0)
print(content_1)</input_code>
  <output_code>No hay un "mejor" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.
No hay un "mejor" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.
</output_code>
  <markdown>#### Criatividade do modelo através do top_p</markdown>
  <markdown>Podemos fazer com que o modelo seja mais criativo através do parâmetro `top_p`. Quanto maior for o valor, mais criativo será o modelo.</markdown>
  <input_code>promtp = "¿Cuál es el mejor lenguaje de programación para aprender?"
top_p = 0

response = client.chat.completions.create(
  model = model,
  messages = [{"role": "user", "content": f"{promtp}"}],
  top_p = top_p,
)

content_0 = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content_0)</input_code>
  <output_code>No hay un "mejor" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.
</output_code>
  <input_code>promtp = "¿Cuál es el mejor lenguaje de programación para aprender?"
top_p = 1

response = client.chat.completions.create(
  model = model,
  messages = [{"role": "user", "content": f"{promtp}"}],
  top_p = top_p,
)

content_1 = response.choices[0].message.content.replace('\n\n', '\n\t\t')
print(content_1)</input_code>
  <output_code>El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.
</output_code>
  <input_code>print(content_0)
print(content_1)</input_code>
  <output_code>No hay un "mejor" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.
El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.
</output_code>
  <markdown>#### Número de respostas</markdown>
  <markdown>Podemos pedir à API que nos retorne mais de uma resposta. Isso é útil para que o modelo nos retorne várias respostas e assim poder escolher a que mais gostarmos, para isso vamos colocar os parâmetros `temperature` e `top_p` em 1 para que o modelo seja mais criativo</markdown>
  <input_code>promtp = "¿Cuál es el mejor lenguaje de programación para aprender?"
temperature = 1
top_p = 1

response = client.chat.completions.create(
  model = model,
  messages = [{"role": "user", "content": f"{promtp}"}],
  temperature = temperature,
  top_p = top_p,
  n = 4
)

content_0 = response.choices[0].message.content.replace('\n\n', '\n\t\t')
content_1 = response.choices[1].message.content.replace('\n\n', '\n\t\t')
content_2 = response.choices[2].message.content.replace('\n\n', '\n\t\t')
content_3 = response.choices[3].message.content.replace('\n\n', '\n\t\t')
print(content_0)
print(content_1)
print(content_2)
print(content_3)</input_code>
  <output_code>El mejor lenguaje de programación para aprender depende de tus objetivos y del tipo de aplicaciones que te interese desarrollar. Algunos de los lenguajes más populares para aprender son:
		1. Python: Es un lenguaje de programación versátil, fácil de aprender y con una amplia comunidad de desarrolladores. Es ideal para principiantes y se utiliza en una gran variedad de aplicaciones, desde desarrollo web hasta inteligencia artificial.
		2. JavaScript: Es el lenguaje de programación más utilizado en el desarrollo web. Es imprescindible para aquellos que quieren trabajar en el ámbito del desarrollo frontend y backend.
		3. Java: Es un lenguaje de programación muy popular en el ámbito empresarial, por lo que aprender Java puede abrirte muchas puertas laborales. Además, es un lenguaje estructurado que te enseñará conceptos importantes de la programación orientada a objetos.
		4. C#: Es un lenguaje de programación desarrollado por Microsoft que se utiliza especialmente en el desarrollo de aplicaciones para Windows. Es ideal para aquellos que quieran enfocarse en el desarrollo de aplicaciones de escritorio.
		En resumen, el mejor lenguaje de programación para aprender depende de tus intereses y objetivos personales. Es importante investigar y considerar qué tipos de aplicaciones te gustaría desarrollar para elegir el lenguaje que más se adapte a tus necesidades.
El mejor lenguaje de programación para aprender depende de los objetivos y necesidades individuales. Algunos de los lenguajes de programación más populares y ampliamente utilizados incluyen Python, JavaScript, Java, C++, Ruby y muchos otros. Python es a menudo recomendado para principiantes debido a su sintaxis simple y legible, mientras que JavaScript es esencial para el desarrollo web. Java es ampliamente utilizado en el desarrollo de aplicaciones empresariales y Android, y C++ es comúnmente utilizado en sistemas embebidos y juegos. En última instancia, el mejor lenguaje de programación para aprender dependerá de lo que quiera lograr con su habilidades de programación.
El mejor lenguaje de programación para aprender depende de los intereses y objetivos individuales de cada persona. Algunos de los lenguajes más populares y bien documentados para principiantes incluyen Python, JavaScript, Java y C#. Python es conocido por su simplicidad y versatilidad, mientras que JavaScript es esencial para el desarrollo web. Java y C# son lenguajes ampliamente utilizados en la industria y proporcionan una base sólida para aprender otros lenguajes. En última instancia, la elección del lenguaje dependerá de las metas personales y la aplicación deseada.
El mejor lenguaje de programación para aprender depende de los intereses y objetivos de cada persona. Algunos lenguajes populares para principiantes incluyen Python, Java, JavaScript, C++ y Ruby. Python es frecuentemente recomendado para aprender a programar debido a su sintaxis sencilla y legible, mientras que Java es utilizado en aplicaciones empresariales y Android. JavaScript es fundamental para el desarrollo web, y C++ es comúnmente utilizado en aplicaciones de alto rendimiento. Ruby es conocido por su facilidad de uso y flexibilidad. En última instancia, la elección del lenguaje dependerá de qué tipo de desarrollo te interesa y qué tipo de proyectos deseas realizar.
</output_code>
  <markdown>## Retreinar modelo da OpenAI</markdown>
  <markdown>A OpenAI oferece a possibilidade de re-treinar seus modelos da API para obter melhores resultados com nossos próprios dados. Isso tem as seguintes vantagens:

+ Obtém-se resultados de maior qualidade para nossos dados
+ Em um prompt podemos dar exemplos para que se comporte como quisermos, mas apenas alguns. Dessa forma, reentrenando-o, podemos fornecer muitos mais.+ Economia de tokens devido a indicações mais curtas. Como já o treinamos para o nosso caso de uso, podemos dar-lhe menos indicações para resolver nossas tarefas.
+ Solicitações de menor latência. Ao chamar modelos próprios teremos menos latência.
</markdown>
  <markdown>### Preparação dos dados</markdown>
  <markdown>A API da OpenAI pede que forneçamos os dados em um arquivo `jsonl` no seguinte formato

``` json
{
"mensagens":
[
{
"Marv é um chatbot factual que também é sarcástico."
},
{
Qual é a capital da França?
},
{
Paris, como se todos já não soubessem disso.
}
]
}
{
"mensagens":
[
{
"Marv é um chatbot factual que também é sarcástico."
},
{
Quem escreveu 'Romeu e Julieta'?
},
{
Oh, apenas um cara chamado William Shakespeare. Já ouviu falar dele?
}
]
}
{
"mensagens":
[
{
"Marv é um chatbot factual que também é sarcástico."
},
{
Quão longe está a Lua da Terra?
},
{
Aproximadamente 384.400 quilômetros. Mais ou menos alguns, como se isso realmente importasse.
}
]
}
```

Com um máximo de 4096 tokens
</markdown>
  <markdown>#### Validação dos dados</markdown>
  <markdown>Para me poupar trabalho, passei um a um todos os meus posts para o chatgpt e disse a ele para gerar 10 `FAQ`s para cada um no formato `CSV`, pois duvidava se conseguiria gerar um formato como o solicitado no `jsonl`. E ele gerou um `CSV` com o seguinte formato para cada post

``` csv
prompt,completion
O que cobre a Introdução ao Python no material fornecido? "A Introdução ao Python aborda tópicos como tipos de dados, operadores, uso de funções e classes, manipulação de objetos iteráveis e uso de módulos. [Mais informações](https://maximofn.com/python/)"
Quais são os tipos de dados básicos em Python? Python tem 7 tipos de dados básicos: texto (`str`), numéricos (`int`, `float`, `complex`), sequências (`list`, `tuple`, `range`), mapeamento (`dict`), conjuntos (`set`, `frozenset`), booleanos (`bool`) e binários (`bytes`, `bytearray`, `memoryview`). [Mais informações](https://maximofn.com/python/)
O que são e como são usados os operadores em Python? Os operadores em Python são símbolos especiais que são usados para realizar operações como adição, subtração, multiplicação e divisão entre variáveis e valores. Também incluem operadores lógicos para comparações. [Mais informações](https://maximofn.com/python/)
Como se define e utiliza uma função em Python? "Em Python, uma função é definida usando a palavra-chave `def`, seguida do nome da função e parênteses. As funções podem ter parâmetros e retornar valores. Elas são utilizadas para encapsular lógica que pode ser reutilizada ao longo do código. [Mais informações](https://maximofn.com/python/)"O que são as classes em Python e como elas são usadas? As classes em Python são a base da programação orientada a objetos. Permitem criar objetos que encapsulam dados e funcionalidades. As classes são definidas usando a palavra-chave `class`, seguida do nome da classe. [Mais informações](https://maximofn.com/python/)
...
```

Cada `CSV` tem 10 `FAQ`s
</markdown>
  <markdown>Vou fazer um código que pegue cada `CSV` e gere dois novos `jsonl`, um para treinamento e outro para validação.</markdown>
  <input_code>import os

CSVs_path = "openai/faqs_posts"
percetn_train = 0.8
percetn_validation = 0.2

jsonl_train = os.path.join(CSVs_path, "train.jsonl")
jsonl_validation = os.path.join(CSVs_path, "validation.jsonl")

# Create the train.jsonl and validation.jsonl files
with open(jsonl_train, 'w') as f:
    f.write('')
with open(jsonl_validation, 'w') as f:
    f.write('')

for file in os.listdir(CSVs_path):  # Get all files in the directory
    if file.endswith(".csv"):    # Check if file is a csv
        csv = os.path.join(CSVs_path, file) # Get the path to the csv file
        number_of_lines = 0
        csv_content = []
        for line in open(csv, 'r'): # Read all lines in the csv file
            if line.startswith('prompt'):   # Skip the first line
                continue
            number_of_lines += 1    # Count the number of lines
            csv_content.append(line)    # Add the line to the csv_content list

        number_of_train = int(number_of_lines * percetn_train)  # Calculate the number of lines for the train.jsonl file
        number_of_validation = int(number_of_lines * percetn_validation)    # Calculate the number of lines for the validation.sjonl file

        for i in range(number_of_lines):
            prompt = csv_content[i].split(',')[0]
            response = ','.join(csv_content[i].split(',')[1:]).replace('\n', '').replace('"', '')
            if i &gt; 0 and i &lt;= number_of_train:
                # add line to train.jsonl
                with open(jsonl_train, 'a') as f:
                    f.write(f'{"{"}"messages": [{"{"}"role": "system", "content": "Eres un amable asistente dispuesto a responder."{"}"}, {"{"}"role": "user", "content": "{prompt}"{"}"}, {"{"}"role": "assistant", "content": "{response}"{"}"}]{"}"}\n')
            elif i &gt; number_of_train and i &lt;= number_of_train + number_of_validation:
                # add line to validation.csv
                with open(jsonl_validation, 'a') as f:
                    f.write(f'{"{"}"messages": [{"{"}"role": "system", "content": "Eres un amable asistente dispuesto a responder."{"}"}, {"{"}"role": "user", "content": "{prompt}"{"}"}, {"{"}"role": "assistant", "content": "{response}"{"}"}]{"}"}\n')</input_code>
  <markdown>Uma vez que tenho os dois `jsonl`, executo um [código](https://cookbook.openai.com/examples/chat_finetuning_data_prep) que a OpenAI fornece para verificar os `jsonl`.</markdown>
  <markdown>Primeiro validamos os do treinamento</markdown>
  <input_code>from collections import defaultdict
import json

# Format error checks
format_errors = defaultdict(int)

with open(jsonl_train, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

for ex in dataset:
    if not isinstance(ex, dict):
        format_errors["data_type"] += 1
        continue
        
    messages = ex.get("messages", None)
    if not messages:
        format_errors["missing_messages_list"] += 1
        continue
        
    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"] += 1
        
        if any(k not in ("role", "content", "name", "function_call") for k in message):
            format_errors["message_unrecognized_key"] += 1
        
        if message.get("role", None) not in ("system", "user", "assistant", "function"):
            format_errors["unrecognized_role"] += 1
            
        content = message.get("content", None)
        function_call = message.get("function_call", None)
        
        if (not content and not function_call) or not isinstance(content, str):
            format_errors["missing_content"] += 1
    
    if not any(message.get("role", None) == "assistant" for message in messages):
        format_errors["example_missing_assistant_message"] += 1

if format_errors:
    print("Found errors:")
    for k, v in format_errors.items():
        print(f"{k}: {v}")
else:
    print("No errors found")</input_code>
  <output_code>No errors found
</output_code>
  <markdown>E agora os de validação</markdown>
  <input_code># Format error checks
format_errors = defaultdict(int)

with open(jsonl_validation, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

for ex in dataset:
    if not isinstance(ex, dict):
        format_errors["data_type"] += 1
        continue
        
    messages = ex.get("messages", None)
    if not messages:
        format_errors["missing_messages_list"] += 1
        continue
        
    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"] += 1
        
        if any(k not in ("role", "content", "name", "function_call") for k in message):
            format_errors["message_unrecognized_key"] += 1
        
        if message.get("role", None) not in ("system", "user", "assistant", "function"):
            format_errors["unrecognized_role"] += 1
            
        content = message.get("content", None)
        function_call = message.get("function_call", None)
        
        if (not content and not function_call) or not isinstance(content, str):
            format_errors["missing_content"] += 1
    
    if not any(message.get("role", None) == "assistant" for message in messages):
        format_errors["example_missing_assistant_message"] += 1

if format_errors:
    print("Found errors:")
    for k, v in format_errors.items():
        print(f"{k}: {v}")
else:
    print("No errors found")</input_code>
  <output_code>No errors found
</output_code>
  <markdown>#### Cálculo de tokens</markdown>
  <markdown>O número máximo de tokens de cada exemplo tem que ser 4096, portanto, se tivermos exemplos mais longos, apenas serão utilizados os primeiros 4096 tokens. Portanto, vamos contar o número de tokens que cada `jsonl` possui para saber quanto vai custar re-treinar o modelo.

Mas primeiro é preciso instalar a biblioteca `tiktoken`, que é o tokenizador usado pela OpenAI e que nos ajudará a saber quantos tokens possui cada `CSV`, e consequentemente, quanto custará retreinar o modelo.

Para instalá-lo, executamos o seguinte comando

``` bash
pip install tiktoken
```
</markdown>
  <markdown>Criamos algumas funções necessárias</markdown>
  <input_code>import tiktoken
import numpy as np

encoding = tiktoken.get_encoding("cl100k_base")

def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3
    return num_tokens

def num_assistant_tokens_from_messages(messages):
    num_tokens = 0
    for message in messages:
        if message["role"] == "assistant":
            num_tokens += len(encoding.encode(message["content"]))
    return num_tokens

def print_distribution(values, name):
    print(f"\n#### Distribution of {name}:")
    print(f"min:{min(values)}, max: {max(values)}")
    print(f"mean: {np.mean(values)}, median: {np.median(values)}")
    print(f"p5: {np.quantile(values, 0.1)}, p95: {np.quantile(values, 0.9)}")</input_code>
  <input_code># Warnings and tokens counts
n_missing_system = 0
n_missing_user = 0
n_messages = []
convo_lens = []
assistant_message_lens = []

with open(jsonl_train, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

for ex in dataset:
    messages = ex["messages"]
    if not any(message["role"] == "system" for message in messages):
        n_missing_system += 1
    if not any(message["role"] == "user" for message in messages):
        n_missing_user += 1
    n_messages.append(len(messages))
    convo_lens.append(num_tokens_from_messages(messages))
    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))
    
print("Num examples missing system message:", n_missing_system)
print("Num examples missing user message:", n_missing_user)
print_distribution(n_messages, "num_messages_per_example")
print_distribution(convo_lens, "num_total_tokens_per_example")
print_distribution(assistant_message_lens, "num_assistant_tokens_per_example")
n_too_long = sum(l &gt; 4096 for l in convo_lens)
print(f"\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning")</input_code>
  <output_code>Num examples missing system message: 0
Num examples missing user message: 0

#### Distribution of num_messages_per_example:
min:3, max: 3
mean: 3.0, median: 3.0
p5: 3.0, p95: 3.0

#### Distribution of num_total_tokens_per_example:
min:67, max: 132
mean: 90.13793103448276, median: 90.0
p5: 81.5, p95: 99.5

#### Distribution of num_assistant_tokens_per_example:
min:33, max: 90
mean: 48.66379310344828, median: 48.5
p5: 41.0, p95: 55.5

0 examples may be over the 4096 token limit, they will be truncated during fine-tuning
</output_code>
  <markdown>Como vemos no o conjunto de treinamento nenhuma mensagem ultrapassa os 4096 tokens</markdown>
  <input_code># Warnings and tokens counts
n_missing_system = 0
n_missing_user = 0
n_messages = []
convo_lens = []
assistant_message_lens = []

with open(jsonl_validation, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

for ex in dataset:
    messages = ex["messages"]
    if not any(message["role"] == "system" for message in messages):
        n_missing_system += 1
    if not any(message["role"] == "user" for message in messages):
        n_missing_user += 1
    n_messages.append(len(messages))
    convo_lens.append(num_tokens_from_messages(messages))
    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))
    
print("Num examples missing system message:", n_missing_system)
print("Num examples missing user message:", n_missing_user)
print_distribution(n_messages, "num_messages_per_example")
print_distribution(convo_lens, "num_total_tokens_per_example")
print_distribution(assistant_message_lens, "num_assistant_tokens_per_example")
n_too_long = sum(l &gt; 4096 for l in convo_lens)
print(f"\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning")</input_code>
  <output_code>Num examples missing system message: 0
Num examples missing user message: 0

#### Distribution of num_messages_per_example:
min:3, max: 3
mean: 3.0, median: 3.0
p5: 3.0, p95: 3.0

#### Distribution of num_total_tokens_per_example:
min:80, max: 102
mean: 89.93333333333334, median: 91.0
p5: 82.2, p95: 96.8

#### Distribution of num_assistant_tokens_per_example:
min:41, max: 57
mean: 48.2, median: 49.0
p5: 42.8, p95: 51.6

0 examples may be over the 4096 token limit, they will be truncated during fine-tuning
</output_code>
  <markdown>Nenhuma mensagem do conjunto de validação excede 4096 tokens.</markdown>
  <markdown>#### Cálculo do custo</markdown>
  <markdown>Outra coisa muito importante é saber quanto vai custar fazer esse fine-tuning.</markdown>
  <input_code># Pricing and default n_epochs estimate
MAX_TOKENS_PER_EXAMPLE = 4096

TARGET_EPOCHS = 3
MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
MIN_DEFAULT_EPOCHS = 1
MAX_DEFAULT_EPOCHS = 25

with open(jsonl_train, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

convo_lens = []
for ex in dataset:
    messages = ex["messages"]
    convo_lens.append(num_tokens_from_messages(messages))

n_epochs = TARGET_EPOCHS
n_train_examples = len(dataset)
if n_train_examples * TARGET_EPOCHS &lt; MIN_TARGET_EXAMPLES:
    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)
elif n_train_examples * TARGET_EPOCHS &gt; MAX_TARGET_EXAMPLES:
    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)

n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)
print(f"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training")
print(f"By default, you'll train for {n_epochs} epochs on this dataset")
print(f"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens")

tokens_for_train = n_epochs * n_billing_tokens_in_dataset</input_code>
  <output_code>Dataset has ~10456 tokens that will be charged for during training
By default, you'll train for 3 epochs on this dataset
By default, you'll be charged for ~31368 tokens
</output_code>
  <markdown>Como na hora de escrever este post, o preço para treinar `gpt-3.5-turbo` é de $0.0080 por cada 1000 tokens, podemos saber quanto nos custará o treinamento</markdown>
  <input_code>pricing = 0.0080
num_tokens_pricing = 1000

training_price = pricing * (tokens_for_train // num_tokens_pricing)
print(f"Training price: ${training_price}")</input_code>
  <output_code>Training price: $0.248
</output_code>
  <input_code># Pricing and default n_epochs estimate
MAX_TOKENS_PER_EXAMPLE = 4096

TARGET_EPOCHS = 3
MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
MIN_DEFAULT_EPOCHS = 1
MAX_DEFAULT_EPOCHS = 25

with open(jsonl_validation, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

convo_lens = []
for ex in dataset:
    messages = ex["messages"]
    convo_lens.append(num_tokens_from_messages(messages))

n_epochs = TARGET_EPOCHS
n_train_examples = len(dataset)
if n_train_examples * TARGET_EPOCHS &lt; MIN_TARGET_EXAMPLES:
    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)
elif n_train_examples * TARGET_EPOCHS &gt; MAX_TARGET_EXAMPLES:
    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)

n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)
print(f"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training")
print(f"By default, you'll train for {n_epochs} epochs on this dataset")
print(f"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens")

tokens_for_validation = n_epochs * n_billing_tokens_in_dataset</input_code>
  <output_code>Dataset has ~1349 tokens that will be charged for during training
By default, you'll train for 6 epochs on this dataset
By default, you'll be charged for ~8094 tokens
</output_code>
  <input_code>validation_price = pricing * (tokens_for_validation // num_tokens_pricing)
print(f"Validation price: ${validation_price}")</input_code>
  <output_code>Validation price: $0.064
</output_code>
  <input_code>total_price = training_price + validation_price
print(f"Total price: ${total_price}")</input_code>
  <output_code>Total price: $0.312
</output_code>
  <markdown>Se nossos cálculos estiverem corretos, vemos que o retreinamento de `gpt-3.5-turbo` nos custará $0.312</markdown>
  <markdown>### Treinamento</markdown>
  <markdown>Uma vez que tudo estiver pronto, precisamos enviar os arquivos `jsonl` para a API da OpenAI para que o modelo seja retreinado. Para isso, executamos o seguinte código:</markdown>
  <input_code>result = client.files.create(file=open(jsonl_train, "rb"), purpose="fine-tune")</input_code>
  <input_code>type(result), result</input_code>
  <output_code>(openai.types.file_object.FileObject,
 FileObject(id='file-LWztOVasq4E0U67wRe8ShjLZ', bytes=47947, created_at=1701585709, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None))</output_code>
  <input_code>print(f"result.id = {result.id}")
print(f"result.bytes = {result.bytes}")
print(f"result.created_at = {result.created_at}")
print(f"result.filename = {result.filename}")
print(f"result.object = {result.object}")
print(f"result.purpose = {result.purpose}")
print(f"result.status = {result.status}")
print(f"result.status_details = {result.status_details}")</input_code>
  <output_code>result.id = file-LWztOVasq4E0U67wRe8ShjLZ
result.bytes = 47947
result.created_at = 1701585709
result.filename = train.jsonl
result.object = file
result.purpose = fine-tune
result.status = processed
result.status_details = None
</output_code>
  <input_code>jsonl_train_id = result.id
print(f"jsonl_train_id = {jsonl_train_id}")</input_code>
  <output_code>jsonl_train_id = file-LWztOVasq4E0U67wRe8ShjLZ
</output_code>
  <markdown>Fazemos o mesmo com o conjunto de validação.</markdown>
  <input_code>result = client.files.create(file=open(jsonl_validation, "rb"), purpose="fine-tune")</input_code>
  <input_code>print(f"result.id = {result.id}")
print(f"result.bytes = {result.bytes}")
print(f"result.created_at = {result.created_at}")
print(f"result.filename = {result.filename}")
print(f"result.object = {result.object}")
print(f"result.purpose = {result.purpose}")
print(f"result.status = {result.status}")
print(f"result.status_details = {result.status_details}")</input_code>
  <output_code>result.id = file-E0YOgIIe9mwxmFcza5bFyVKW
result.bytes = 6369
result.created_at = 1701585730
result.filename = validation.jsonl
result.object = file
result.purpose = fine-tune
result.status = processed
result.status_details = None
</output_code>
  <input_code>jsonl_validation_id = result.id
print(f"jsonl_train_id = {jsonl_validation_id}")</input_code>
  <output_code>jsonl_train_id = file-E0YOgIIe9mwxmFcza5bFyVKW
</output_code>
  <markdown>Uma vez que os temos carregados, passamos a treinar nosso próprio modelo da OpenAI, para isso usamos o seguinte código</markdown>
  <input_code>result = client.fine_tuning.jobs.create(model = "gpt-3.5-turbo", training_file = jsonl_train_id, validation_file = jsonl_validation_id)</input_code>
  <input_code>type(result), result</input_code>
  <output_code>(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,
 FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))</output_code>
  <input_code>print(f"result.id = {result.id}")
print(f"result.created_at = {result.created_at}")
print(f"result.error = {result.error}")
print(f"result.fine_tuned_model = {result.fine_tuned_model}")
print(f"result.finished_at = {result.finished_at}")
print(f"result.hyperparameters = {result.hyperparameters}")
print(f"\tn_epochs = {result.hyperparameters.n_epochs}")
print(f"\tbatch_size = {result.hyperparameters.batch_size}")
print(f"\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}")
print(f"result.model = {result.model}")
print(f"result.object = {result.object}")
print(f"result.organization_id = {result.organization_id}")
print(f"result.result_files = {result.result_files}")
print(f"result.status = {result.status}")
print(f"result.trained_tokens = {result.trained_tokens}")
print(f"result.training_file = {result.training_file}")
print(f"result.validation_file = {result.validation_file}")</input_code>
  <output_code>result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB
result.created_at = 1701585758
result.error = None
result.fine_tuned_model = None
result.finished_at = None
result.hyperparameters = Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto')
	n_epochs = auto
	batch_size = auto
	learning_rate_multiplier = auto
result.model = gpt-3.5-turbo-0613
result.object = fine_tuning.job
result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg
result.result_files = []
result.status = validating_files
result.trained_tokens = None
result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ
result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW
</output_code>
  <input_code>fine_tune_id = result.id
print(f"fine_tune_id = {fine_tune_id}")</input_code>
  <output_code>fine_tune_id = ftjob-aBndcorOfQLP0UijlY0R4pTB
</output_code>
  <markdown>Podemos ver que em `status` saía `validating_files`. Como o fine tuning demora bastante, podemos ir perguntando pelo processo mediante o seguinte código</markdown>
  <input_code>result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)</input_code>
  <input_code>type(result), result</input_code>
  <output_code>(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,
 FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='running', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))</output_code>
  <input_code>print(f"result.id = {result.id}")
print(f"result.created_at = {result.created_at}")
print(f"result.error = {result.error}")
print(f"result.fine_tuned_model = {result.fine_tuned_model}")
print(f"result.finished_at = {result.finished_at}")
print(f"result.hyperparameters = {result.hyperparameters}")
print(f"\tn_epochs = {result.hyperparameters.n_epochs}")
print(f"\tbatch_size = {result.hyperparameters.batch_size}")
print(f"\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}")
print(f"result.model = {result.model}")
print(f"result.object = {result.object}")
print(f"result.organization_id = {result.organization_id}")
print(f"result.result_files = {result.result_files}")
print(f"result.status = {result.status}")
print(f"result.trained_tokens = {result.trained_tokens}")
print(f"result.training_file = {result.training_file}")
print(f"result.validation_file = {result.validation_file}")</input_code>
  <output_code>result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB
result.created_at = 1701585758
result.error = None
result.fine_tuned_model = None
result.finished_at = None
result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)
	n_epochs = 3
	batch_size = 1
	learning_rate_multiplier = 2
result.model = gpt-3.5-turbo-0613
result.object = fine_tuning.job
result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg
result.result_files = []
result.status = running
result.trained_tokens = None
result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ
result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW
</output_code>
  <markdown>Criamos um loop que aguarde o término do treinamento</markdown>
  <input_code>import time

result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)
status = result.status

while status != "succeeded":
    time.sleep(10)
    result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)
    status = result.status

print("Job succeeded!")</input_code>
  <output_code>Job succeeded</output_code>
  <markdown>Como o treinamento foi concluído, voltamos a solicitar as informações sobre o processo.</markdown>
  <input_code>result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)</input_code>
  <input_code>print(f"result.id = {result.id}")
print(f"result.created_at = {result.created_at}")
print(f"result.error = {result.error}")
print(f"result.fine_tuned_model = {result.fine_tuned_model}")
print(f"result.finished_at = {result.finished_at}")
print(f"result.hyperparameters = {result.hyperparameters}")
print(f"\tn_epochs = {result.hyperparameters.n_epochs}")
print(f"\tbatch_size = {result.hyperparameters.batch_size}")
print(f"\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}")
print(f"result.model = {result.model}")
print(f"result.object = {result.object}")
print(f"result.organization_id = {result.organization_id}")
print(f"result.result_files = {result.result_files}")
print(f"result.status = {result.status}")
print(f"result.trained_tokens = {result.trained_tokens}")
print(f"result.training_file = {result.training_file}")
print(f"result.validation_file = {result.validation_file}")</input_code>
  <output_code>result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB
result.created_at = 1701585758
result.error = None
result.fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT
result.finished_at = 1701586541
result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)
	n_epochs = 3
	batch_size = 1
	learning_rate_multiplier = 2
result.model = gpt-3.5-turbo-0613
result.object = fine_tuning.job
result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg
result.result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']
result.status = succeeded
result.trained_tokens = 30672
result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ
result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW
</output_code>
  <markdown>Vamos ver alguns dados interessantes</markdown>
  <input_code>fine_tuned_model = result.fine_tuned_model
finished_at = result.finished_at
result_files = result.result_files
status = result.status
trained_tokens = result.trained_tokens

print(f"fine_tuned_model = {fine_tuned_model}")
print(f"finished_at = {finished_at}")
print(f"result_files = {result_files}")
print(f"status = {status}")
print(f"trained_tokens = {trained_tokens}")</input_code>
  <output_code>fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT
finished_at = 1701586541
result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']
status = succeeded
trained_tokens = 30672
</output_code>
  <markdown>Podemos ver que deu o nome `ft:gpt-3.5-turbo-0613:personal::8RagA0RT` ao nosso modelo, seu status agora é `succeeded` e que usou 30672 tokens, enquanto nós havíamos previsto</markdown>
  <input_code>tokens_for_train, tokens_for_validation, tokens_for_train + tokens_for_validation</input_code>
  <output_code>(31368, 8094, 39462)</output_code>
  <markdown>Isto é, ele usou menos tokens, portanto o treinamento custou menos do que havíamos previsto, especificamente</markdown>
  <input_code>real_training_price = pricing * (trained_tokens // num_tokens_pricing)
print(f"Real training price: ${real_training_price}")</input_code>
  <output_code>Real training price: $0.24
</output_code>
  <markdown>Além dessas informações, se formos à página [finetune](https://platform.openai.com/finetune) da OpenAI, podemos ver que nosso modelo está lá.

![open ai finetune](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_process.webp)
</markdown>
  <markdown>Também podemos ver quanto nos custou o treinamento

![open ai finetune cost](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_cost.webp)

Como podemos ver, foram apenas $0,25
</markdown>
  <markdown>E por último, vamos ver quanto tempo levou para fazer este treinamento. Podemos ver a que horas começou.

![open ai finetune start](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_process_start_time.webp)

E a que horas terminou

![open ai finetune end](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/openai_fine_tuning_process_stop_time.webp)

Portanto, levou mais ou menos uns 10 minutos
</markdown>
  <markdown>### Teste do modelo</markdown>
  <markdown>Dentro do [playground](https://platform.openai.com/playground?mode=chat) da OpenAI podemos provar nosso modelo, mas vamos fazer isso através da API como aprendemos aqui</markdown>
  <input_code>promtp = "¿Cómo se define una función en Python?"

response = client.chat.completions.create(
  model = fine_tuned_model,
  messages=[{"role": "user", "content": f"{promtp}"}],
)</input_code>
  <input_code>type(response), response</input_code>
  <output_code>(openai.types.chat.chat_completion.ChatCompletion,
 ChatCompletion(id='chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))], created=1701667535, model='ft:gpt-3.5-turbo-0613:personal::8RagA0RT', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)))</output_code>
  <input_code>print(f"response.id = {response.id}")
print(f"response.choices = {response.choices}")
for i in range(len(response.choices)):
    print(f"response.choices[{i}] = {response.choices[i]}")
    print(f"\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}")
    print(f"\tresponse.choices[{i}].index = {response.choices[i].index}")
    print(f"\tresponse.choices[{i}].message = {response.choices[i].message}")
    content = response.choices[i].message.content.replace('\n\n', '\n\t\t')
    print(f"\t\tresponse.choices[{i}].message.content = \n\t\t{content}")
    print(f"\t\tresponse.choices[{i}].message.role = {response.choices[i].message.role}")
    print(f"\t\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}")
    print(f"\t\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}")
print(f"response.created = {response.created}")
print(f"response.model = {response.model}")
print(f"response.object = {response.object}")
print(f"response.system_fingerprint = {response.system_fingerprint}")
print(f"response.usage = {response.usage}")
print(f"\tresponse.usage.completion_tokens = {response.usage.completion_tokens}")
print(f"\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}")
print(f"\tresponse.usage.total_tokens = {response.usage.total_tokens}")</input_code>
  <output_code>response.id = chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc
response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))]
response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))
	response.choices[0].finish_reason = stop
	response.choices[0].index = 0
	response.choices[0].message = ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None)
		response.choices[0].message.content = 
		Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)
		response.choices[0].message.role = assistant
		response.choices[0].message.function_call = None
		response.choices[0].message.tool_calls = None
response.created = 1701667535
response.model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT
response.object = chat.completion
response.system_fingerprint = None
response.usage = CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)
	response.usage.completion_tokens = 54
	response.usage.prompt_tokens = 16
	response.usage.total_tokens = 70
</output_code>
  <input_code>print(content)</input_code>
  <output_code>Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)
</output_code>
  <markdown>Temos um modelo que não apenas resolve a resposta, mas também nos fornece um link para a documentação do nosso blog.</markdown>
  <markdown>Vamos ver como se comporta com um exemplo que claramente não tem nada a ver com o blog</markdown>
  <input_code>promtp = "¿Cómo puedo cocinar pollo frito?"

response = client.chat.completions.create(
  model = fine_tuned_model,
  messages=[{"role": "user", "content": f"{promtp}"}],
)

for i in range(len(response.choices)):
    content = response.choices[i].message.content.replace('\n\n', '\n')
    print(f"{content}")</input_code>
  <output_code>Para cocinar pollo frito, se sazona el pollo con una mezcla de sal, pimienta y especias, se sumerge en huevo batido y se empaniza con harina. Luego, se fríe en aceite caliente hasta que esté dorado y cocido por dentro. [Más información](https://maximofn.com/pollo-frito/)
</output_code>
  <markdown>Como se pode ver, ele nos fornece o link `https://maximofn.com/pollo-frito/`, que não existe. Portanto, mesmo tendo reentrenado um modelo de chatGPT, é preciso ter cuidado com as respostas e não confiar 100% nelas.</markdown>
  <markdown>## Gerar imagens com DALL-E 3</markdown>
  <markdown>Para gerar imagens com o DALL-E 3, temos que usar o seguinte código</markdown>
  <input_code>response = client.images.generate(
  model="dall-e-3",
  prompt="a white siamese cat",
  size="1024x1024",
  quality="standard",
  n=1,
)</input_code>
  <input_code>type(response), response</input_code>
  <output_code>(openai.types.images_response.ImagesResponse,
 ImagesResponse(created=1701823487, data=[Image(b64_json=None, revised_prompt="Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&amp;se=2023-12-06T01%3A44%3A47Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-12-05T19%3A58%3A58Z&amp;ske=2023-12-06T19%3A58%3A58Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')]))</output_code>
  <input_code>print(f"response.created = {response.created}")
for i in range(len(response.data)):
    print(f"response.data[{i}] = {response.data[i]}")
    print(f"\tresponse.data[{i}].b64_json = {response.data[i].b64_json}")
    print(f"\tresponse.data[{i}].revised_prompt = {response.data[i].revised_prompt}")
    print(f"\tresponse.data[{i}].url = {response.data[i].url}")</input_code>
  <output_code>response.created = 1701823487
response.data[0] = Image(b64_json=None, revised_prompt="Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&amp;se=2023-12-06T01%3A44%3A47Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-12-05T19%3A58%3A58Z&amp;ske=2023-12-06T19%3A58%3A58Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')
	response.data[0].b64_json = None
	response.data[0].revised_prompt = Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.
	response.data[0].url = https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&amp;se=2023-12-06T01%3A44%3A47Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/png&amp;skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-12-05T19%3A58%3A58Z&amp;ske=2023-12-06T19%3A58%3A58Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D
</output_code>
  <markdown>Podemos ver um dado muito interessante que não conseguimos ver quando usamos o DALL-E 3 através da interface da OpenAI, e é o prompt que foi passado ao modelo.</markdown>
  <input_code>response.data[0].revised_prompt</input_code>
  <output_code>"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture."</output_code>
  <markdown>Com esse prompt, gerou-se a seguinte imagem</markdown>
  <input_code>import requests

url = response.data[0].url
# img_data = requests.get(url).content
with open('openai/dall-e-3.png', 'wb') as handler:
    handler.write(requests.get(url).content)</input_code>
  <markdown>![dall-e 3](openai/dall-e-3.png)</markdown>
  <markdown>Como temos o prompt que a OpenAI utilizou, na verdade vamos tentar usá-lo para gerar um gato semelhante, mas com os olhos verdes.</markdown>
  <input_code>revised_prompt = response.data[0].revised_prompt
gree_eyes = revised_prompt.replace("blue", "green")

response = client.images.generate(
  model="dall-e-3",
  prompt=gree_eyes,
  size="1024x1024",
  quality="standard",
  n=1,
)

print(response.data[0].revised_prompt)

image_url = response.data[0].url

image_path = 'openai/dall-e-3-green.png'
with open(image_path, 'wb') as handler:
    handler.write(requests.get(image_url).content)</input_code>
  <output_code>A well-defined image of a Siamese cat boasting a shiny white coat. Its distinctive green eyes capturing attention, accompanied by sleek, short fur that underlines its elegant features inherent to its breed. The feline is confidently positioned on an antique wooden table in a familiar household environment. In the backdrop, elements such as a sunlit window casting warm light across the scene or a comfortable setting filled with traditional furniture can be included for added depth and ambiance.
</output_code>
  <markdown>![dall-e-3-green](openai/dall-e-3-green.png)</markdown>
  <markdown>Embora a cor do gato tenha mudado e não apenas dos olhos, a posição e o fundo são muito semelhantes.</markdown>
  <markdown>Além do prompt, as outras variáveis que podemos modificar são

* `model`: Permite escolher o modelo de geração de imagens, os possíveis valores são `dalle-2` e `dalle-3`
* `size`: Permite mudar o tamanho da imagem, os valores possíveis são `256x256`, `512x512`, `1024x1024`, `1792x1024`, `1024x1792` pixels
* `quality`: Permite mudar a qualidade da imagem, os valores possíveis são `standard` ou `hd`
* `response_format`: Permite mudar o formato da resposta, os valores possíveis são `url` ou `b64_json`* `n`: Permite mudar o número de imagens que queremos que nos retorne o modelo. Com DALL-E 3, só podemos pedir uma imagem.
* `style`: Permite mudar o estilo da imagem, os possíveis valores são `vivid` ou `natural`

Então, vamos gerar uma imagem de alta qualidade
</markdown>
  <input_code>response = client.images.generate(
  model="dall-e-3",
  prompt=gree_eyes,
  size="1024x1792",
  quality="hd",
  n=1,
  style="natural",
)

print(response.data[0].revised_prompt)

image_url = response.data[0].url

image_path = 'openai/dall-e-3-hd.png'
with open(image_path, 'wb') as handler:
    handler.write(requests.get(image_url).content)
display(Image(image_path))</input_code>
  <output_code>Render a portrait of a Siamese cat boasting a pristine white coat. This cat should have captivating green eyes that stand out. Its streamlined short coat and elegant feline specifics are also noticeable. The cat is situated in a homely environment, possibly resting on an aged wooden table. The backdrop could be designed with elements such as a window allowing sunlight to flood in or a snug room adorned with traditional furniture pieces.
</output_code>
  <markdown>![dall-e-3-hd](openai/dall-e-3-hd.png)</markdown>
  <markdown>## Visão</markdown>
  <markdown>Vamos a usar o modelo de visão com a seguinte imagem

![panda](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI)

Que aqui em pequeno parece um panda, mas se a virmos de perto é mais difícil ver o panda

&lt;div style="text-align:center;"&gt;
&lt;img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI" alt="panda" style="width:637px;height:939px;"&gt;
&lt;/div&gt;
</markdown>
  <markdown>Para usar o modelo de visão, temos que utilizar o seguinte código</markdown>
  <input_code>prompt = "¿Ves algún animal en esta imagen?"
image_url = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI"

response = client.chat.completions.create(
  model="gpt-4-vision-preview",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": prompt},
        {
          "type": "image_url",
          "image_url": {
            "url": image_url,
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)</input_code>
  <output_code>Lo siento, no puedo ayudar con la identificación o comentarios sobre contenido oculto en imágenes.
</output_code>
  <markdown>Não consegue encontrar o panda, mas não é o objetivo deste post que ele veja o panda, apenas explicar como usar o modelo de visão do GPT4, então não vamos nos aprofundar mais neste assunto.</markdown>
  <markdown>Podemos passar várias imagens de uma vez</markdown>
  <input_code>image_url1 = "https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&amp;ssl=1"
image_url2 = "https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg"
prompt = "¿Ves algún animal en estas imágenes?"

display(Image(url=image_url1))
display(Image(url=image_url2))

response = client.chat.completions.create(
  model="gpt-4-vision-preview",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": prompt,
        },
        {
          "type": "image_url",
          "image_url": {
            "url": image_url1,
          },
        },
        {
          "type": "image_url",
          "image_url": {
            "url": image_url2,
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)</input_code>
  <output_code>&lt;IPython.core.display.Image object&gt;</output_code>
  <output_code>&lt;IPython.core.display.Image object&gt;</output_code>
  <output_code>Sí, en ambas imágenes se ven figuras de animales. Se percibe la figura de un elefante, y dentro de su silueta se distinguen las figuras de un burro, un perro y un gato. Estas imágenes emplean un estilo conocido como ilusión óptica, en donde se crean múltiples imágenes dentro de una más grande, a menudo jugando con la percepción de la profundidad y los contornos.
</output_code>
  <markdown>## Texto para fala</markdown>
  <markdown>Podemos gerar áudio a partir de texto com o seguinte código</markdown>
  <input_code>speech_file_path = "openai/speech.mp3"
text = "Hola desde el blog de MaximoFN"

response = client.audio.speech.create(
  model="tts-1",
  voice="alloy",
  input=text,
)

response.stream_to_file(speech_file_path)</input_code>
  <markdown>&lt;audio controls&gt;
&lt;source src="openai/speech.mp3" type="audio/mpeg"&gt;
O seu navegador não suporta o elemento de áudio.
&lt;/audio&gt;
</markdown>
  <markdown>Podemos escolher

* model: Permite escolher o modelo de geração de áudio. Os possíveis valores são `tts-1` e `tts-1-hd`* voice: Permite escolher a voz que queremos que o modelo use, os possíveis valores são `alloy`, `echo`, `fable`, `onyx`, `nova` e `shimmer`
</markdown>
  <markdown>## Fala para texto (Whisper)</markdown>
  <markdown>Podemos transcrever áudio com o Whisper utilizando o seguinte código</markdown>
  <input_code>audio_file = "MicroMachines.mp3"
audio_file= open(audio_file, "rb")

transcript = client.audio.transcriptions.create(
  model="whisper-1", 
  file=audio_file
)

print(transcript.text)</input_code>
  <output_code>This is the Micromachine Man presenting the most midget miniature motorcade of micromachines. Each one has dramatic details, terrific trim, precision paint jobs, plus incredible micromachine pocket play sets. There's a police station, fire station, restaurant, service station, and more. Perfect pocket portables to take anyplace. And there are many miniature play sets to play with, and each one comes with its own special edition micromachine vehicle and fun fantastic features that miraculously move. Raise the boat lift at the airport, marina, man the gun turret at the army base, clean your car at the car wash, raise the toll bridge. And these play sets fit together to form a micromachine world. Micromachine pocket play sets so tremendously tiny, so perfectly precise, so dazzlingly detailed, you'll want to pocket them all. Micromachines and micromachine pocket play sets sold separately from Galoob. The smaller they are, the better they are.
</output_code>
  <markdown>&lt;audio controls&gt;
&lt;source src="MicroMachines.mp3" type="audio/mpeg"&gt;
Seu navegador não suporta o elemento de áudio.
&lt;/audio&gt;
</markdown>
  <markdown>## Moderação de conteúdo</markdown>
  <markdown>Podemos obter a categoria de um texto entre as classes `sexual`, `hate`, `harassment`, `self-harm`, `sexual/minors`, `hate/threatening`, `violence/graphic`, `self-harm/intent`, `self-harm/instructions`, `harassment/threatening` e `violence`, para isso usamos o seguinte código com o texto transcrito anteriormente</markdown>
  <input_code>text = transcript.text

response = client.moderations.create(input=text)</input_code>
  <input_code>type(response), response</input_code>
  <output_code>(openai.types.moderation_create_response.ModerationCreateResponse,
 ModerationCreateResponse(id='modr-8RxMZItvmLblEl5QPgCv19Jl741SS', model='text-moderation-006', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)]))</output_code>
  <input_code>print(f"response.id = {response.id}")
print(f"response.model = {response.model}")
for i in range(len(response.results)):
    print(f"response.results[{i}] = {response.results[i]}")
    print(f"\tresponse.results[{i}].categories = {response.results[i].categories}")
    print(f"\t\tresponse.results[{i}].categories.harassment = {response.results[i].categories.harassment}")
    print(f"\t\tresponse.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}")
    print(f"\t\tresponse.results[{i}].categories.hate = {response.results[i].categories.hate}")
    print(f"\t\tresponse.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}")
    print(f"\t\tresponse.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}")
    print(f"\t\tresponse.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}")
    print(f"\t\tresponse.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}")
    print(f"\t\tresponse.results[{i}].categories.sexual = {response.results[i].categories.sexual}")
    print(f"\t\tresponse.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}")
    print(f"\t\tresponse.results[{i}].categories.violence = {response.results[i].categories.violence}")
    print(f"\t\tresponse.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}")
    print(f"\tresponse.results[{i}].category_scores = {response.results[i].category_scores}")
    print(f"\t\tresponse.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}")
    print(f"\t\tresponse.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}")
    print(f"\t\tresponse.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}")
    print(f"\t\tresponse.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}")
    print(f"\t\tresponse.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}")
    print(f"\t\tresponse.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}")
    print(f"\t\tresponse.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}")
    print(f"\t\tresponse.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}")
    print(f"\t\tresponse.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}")
    print(f"\t\tresponse.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}")
    print(f"\t\tresponse.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}")
    print(f"\tresponse.results[{i}].flagged = {response.results[i].flagged}")</input_code>
  <output_code>response.id = modr-8RxMZItvmLblEl5QPgCv19Jl741SS
response.model = text-moderation-006
response.results[0] = Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)
	response.results[0].categories = Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False)
		response.results[0].categories.harassment = False
		response.results[0].categories.harassment_threatening = False
		response.results[0].categories.hate = False
		response.results[0].categories.hate_threatening = False
		response.results[0].categories.self_harm = False
		response.results[0].categories.self_harm_instructions = False
		response.results[0].categories.self_harm_intent = False
		response.results[0].categories.sexual = False
		response.results[0].categories.sexual_minors = False
		response.results[0].categories.violence = False
		response.results[0].categories.violence_graphic = False
	response.results[0].category_scores = CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06)
		response.results[0].category_scores.harassment = 0.0003560568729881197
		response.results[0].category_scores.harassment_threatening = 2.5426568299735663e-06
		response.results[0].category_scores.hate = 1.966094168892596e-05
		response.results[0].category_scores.hate_threatening = 6.384455986108151e-08
		response.results[0].category_scores.self_harm = 7.903140613052528e-07
		response.results[0].category_scores.self_harm_instructions = 6.443992219828942e-07
		response.results[0].category_scores.self_harm_intent = 1.2202733046251524e-07
		response.results[0].category_scores.sexual = 0.0003779272665269673
		response.results[0].category_scores.sexual_minors = 1.8967952200910076e-05
		response.results[0].category_scores.violence = 9.489082731306553e-05
		response.results[0].category_scores.violence_graphic = 5.1929731853306293e-05
	response.results[0].flagged = False
</output_code>
  <markdown>O áudio transcritor não está em nenhuma das categorias anteriores, vamos tentar com outro texto</markdown>
  <input_code>text = "I want to kill myself"

response = client.moderations.create(input=text)

for i in range(len(response.results)):
    print(f"response.results[{i}].categories.harassment = {response.results[i].categories.harassment}")
    print(f"response.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}")
    print(f"response.results[{i}].categories.hate = {response.results[i].categories.hate}")
    print(f"response.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}")
    print(f"response.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}")
    print(f"response.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}")
    print(f"response.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}")
    print(f"response.results[{i}].categories.sexual = {response.results[i].categories.sexual}")
    print(f"response.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}")
    print(f"response.results[{i}].categories.violence = {response.results[i].categories.violence}")
    print(f"response.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}")
    print()
    print(f"response.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}")
    print(f"response.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}")
    print(f"response.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}")
    print(f"response.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}")
    print(f"response.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}")
    print(f"response.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}")
    print(f"response.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}")
    print(f"response.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}")
    print(f"response.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}")
    print(f"response.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}")
    print(f"response.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}")
    print()
    print(f"response.results[{i}].flagged = {response.results[i].flagged}")</input_code>
  <output_code>response.results[0].categories.harassment = False
response.results[0].categories.harassment_threatening = False
response.results[0].categories.hate = False
response.results[0].categories.hate_threatening = False
response.results[0].categories.self_harm = True
response.results[0].categories.self_harm_instructions = False
response.results[0].categories.self_harm_intent = True
response.results[0].categories.sexual = False
response.results[0].categories.sexual_minors = False
response.results[0].categories.violence = True
response.results[0].categories.violence_graphic = False

response.results[0].category_scores.harassment = 0.004724912345409393
response.results[0].category_scores.harassment_threatening = 0.00023778305330779403
response.results[0].category_scores.hate = 1.1909247405128554e-05
response.results[0].category_scores.hate_threatening = 1.826493189582834e-06
response.results[0].category_scores.self_harm = 0.9998544454574585
response.results[0].category_scores.self_harm_instructions = 3.5801923647937883e-09
response.results[0].category_scores.self_harm_intent = 0.99969482421875
response.results[0].category_scores.sexual = 2.141016238965676e-06
response.results[0].category_scores.sexual_minors = 2.840671520232263e-08
response.results[0].category_scores.violence = 0.8396497964859009
response.results[0].category_scores.violence_graphic = 2.7347923605702817e-05

response.results[0].flagged = True
</output_code>
  <markdown>Agora, se detectar que o texto é `self_harm_intent`.</markdown>
  <markdown>## Assistentes</markdown>
  <markdown>A OpenAI nos dá a possibilidade de criar assistentes, de maneira que os podemos criar com as características que nós quisermos, por exemplo, um assistente especialista em Python, e poder usá-lo como se fosse um modelo particular da OpenAI. Isso significa que podemos usá-lo para uma consulta e ter uma conversa com ele, e depois de algum tempo, reutilizá-lo com uma nova consulta em uma nova conversa.

Ao trabalhar com assistentes, teremos que criá-los, criar um fio, enviá-los a mensagem, executá-los, esperar que respondam e ver a resposta.

![asistentes](https://cdn.openai.com/API/docs/images/diagram-assistant.webp)
</markdown>
  <markdown>### Criar o assistente</markdown>
  <markdown>Primeiro criamos o assistente</markdown>
  <input_code>code_interpreter_assistant = client.beta.assistants.create(
    name="Python expert",
    instructions="Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.",
    tools=[{"type": "code_interpreter"}],
    model="gpt-3.5-turbo-1106"
)</input_code>
  <input_code>type(code_interpreter_assistant), code_interpreter_assistant</input_code>
  <output_code>(openai.types.beta.assistant.Assistant,
 Assistant(id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', created_at=1701822478, description=None, file_ids=[], instructions='Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.', metadata={}, model='gpt-3.5-turbo-1106', name='Python expert', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]))</output_code>
  <input_code>code_interpreter_assistant_id = code_interpreter_assistant.id
print(f"code_interpreter_assistant_id = {code_interpreter_assistant_id}")</input_code>
  <output_code>code_interpreter_assistant_id = asst_A2F9DPqDiZYFc5hOC6Rb2y0x
</output_code>
  <markdown>Na hora de criar o assistente, as variáveis que temos são

* `name`: nome do assistente
* `instruções`: Instruções para o assistente. Aqui podemos explicar como o assistente deve se comportar.* `tools`: Ferramentas que o assistente pode usar. No momento, apenas estão disponíveis `code_interpreter` e `retrieval`
* `model`: Modelo que o assistente vai usar
</markdown>
  <markdown>Este assistente já foi criado e podemos usá-lo todas as vezes que quisermos. Para isso, precisamos criar um novo fio, assim, se no futuro outra pessoa quiser usá-lo porque for útil, ela poderá fazê-lo simplesmente criando um novo fio. Ela só precisaria do ID do assistente.</markdown>
  <markdown>### Fio ou thread</markdown>
  <markdown>Um fio representa uma nova conversação com o assistente, assim, mesmo que tenha passado algum tempo, enquanto tivermos o ID do fio, podemos continuar a conversa. Para criar um novo fio, temos que usar o seguinte código</markdown>
  <input_code>thread = client.beta.threads.create()</input_code>
  <input_code>type(thread), thread
thread_id = thread.id
print(f"thread_id = {thread_id}")</input_code>
  <output_code>thread_id = thread_nfFT3rFjyPWHdxWvMk6jJ90H
</output_code>
  <markdown>### Enviamos um arquivo</markdown>
  <markdown>Vamos a criar um arquivo .py que vamos pedir ao interpretador que nos explique</markdown>
  <input_code>import os

python_code = os.path.join("openai", "python_code.py")
code = "print('Hello world!')"
with open(python_code, "w") as f:
    f.write(code)</input_code>
  <markdown>Subimos isso à API da OpenAI usando a função `client.files.create`, esta função já usamos quando fizemos o `fine-tuning` de um modelo do chatGPT e enviávamos os `jsonl`s. Apenas que antes na variável `purpose` passávamos `fine-tuning` pois os arquivos que enviávamos eram para `fine-tuning`, e agora passamos `assistants` já que os arquivos que vamos enviar são para um assistente.</markdown>
  <input_code>file = client.files.create(
  file=open(python_code, "rb"),
  purpose='assistants'
)</input_code>
  <input_code>type(file), file</input_code>
  <output_code>(openai.types.file_object.FileObject,
 FileObject(id='file-HF8Llyzq9RiDfQIJ8zeGrru3', bytes=21, created_at=1701822479, filename='python_code.py', object='file', purpose='assistants', status='processed', status_details=None))</output_code>
  <markdown>### Enviar uma mensagem ao assistente</markdown>
  <markdown>Criamos a mensagem que vamos enviar ao assistente, além de indicarmos o ID do arquivo sobre o qual queremos perguntar.</markdown>
  <input_code>message = client.beta.threads.messages.create(
    thread_id=thread_id,
    role="user",
    content="Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.",
    file_ids=[file.id]
)</input_code>
  <markdown>### Executar o assistente</markdown>
  <markdown>Executamos o assistente indicando-lhe que resolva a dúvida do usuário</markdown>
  <input_code>run = client.beta.threads.runs.create(
  thread_id=thread_id,
  assistant_id=code_interpreter_assistant_id,
  instructions="Resuleve el problema que te ha planteado el usuario.",
)</input_code>
  <input_code>type(run), run</input_code>
  <output_code>(openai.types.beta.threads.run.Run,
 Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))</output_code>
  <input_code>run_id = run.id
print(f"run_id = {run_id}")</input_code>
  <output_code>run_id = run_WZxT1TUuHT5qB1ZgD34tgvPu
</output_code>
  <markdown>### Aguardar até que termine de processar</markdown>
  <markdown>Enquanto o assistente está analisando, podemos verificar o estado</markdown>
  <input_code>run = client.beta.threads.runs.retrieve(
  thread_id=thread_id,
  run_id=run_id
)</input_code>
  <input_code>type(run), run</input_code>
  <output_code>(openai.types.beta.threads.run.Run,
 Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1701822481, status='in_progress', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))</output_code>
  <input_code>run.status</input_code>
  <output_code>'in_progress'</output_code>
  <input_code>while run.status != "completed":
    time.sleep(1)
    run = client.beta.threads.runs.retrieve(
      thread_id=thread_id,
      run_id=run_id
    )
print("Run completed!")</input_code>
  <output_code>Run completed!
</output_code>
  <markdown>### Processar a resposta</markdown>
  <markdown>Uma vez que o assistente terminou, podemos ver a resposta.</markdown>
  <input_code>messages = client.beta.threads.messages.list(
  thread_id=thread_id
)</input_code>
  <input_code>type(messages), messages</input_code>
  <output_code>(openai.pagination.SyncCursorPage[ThreadMessage],
 SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='La salida del script es simplemente "Hello world!", ya que la única instrucción en el script es imprimir esa frase.\n\nSi necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.'), type='text')], created_at=1701822487, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_nkFbq64DTaSqxIAQUGedYmaX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='El script proporcionado contiene una sola línea que imprime "Hello world!". Ahora procederé a ejecutar el script para obtener su salida.'), type='text')], created_at=1701822485, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_bWT6H2f6lsSUTAAhGG0KXoh7', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.'), type='text')], created_at=1701822482, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_RjDygK7c8yCqYrjnUPfeZfUg', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.'), type='text')], created_at=1701822481, file_ids=['file-HF8Llyzq9RiDfQIJ8zeGrru3'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H')], object='list', first_id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', last_id='msg_RjDygK7c8yCqYrjnUPfeZfUg', has_more=False))</output_code>
  <input_code>for i in range(len(messages.data)):
    for j in range(len(messages.data[i].content)):
        print(f"messages.data[{i}].content[{j}].text.value = {messages.data[i].content[j].text.value}")
    </input_code>
  <output_code>messages.data[0].content[0].text.value = La salida del script es simplemente "Hello world!", ya que la única instrucción en el script es imprimir esa frase.

Si necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.
messages.data[1].content[0].text.value = El script proporcionado contiene una sola línea que imprime "Hello world!". Ahora procederé a ejecutar el script para obtener su salida.
messages.data[2].content[0].text.value = Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.
messages.data[3].content[0].text.value = Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.
</output_code>
</notebook>