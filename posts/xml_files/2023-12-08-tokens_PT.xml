<?xml version='1.0' encoding='utf-8'?>
<notebook>
  <markdown># Tokens</markdown>
  <markdown> &gt; Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</markdown>
  <markdown>Agora que os `LLM`s est√£o em alta, n√£o paramos de ouvir o n√∫mero de `token`s que cada modelo suporta, mas o que s√£o os `token`s? S√£o as unidades m√≠nimas de representa√ß√£o das palavras</markdown>
  <markdown>Para explicar o que s√£o os `token`s, primeiro vejamos com um exemplo pr√°tico, vamos usar o tokenizador de `OpenAI`, chamado [tiktoken](https://github.com/openai/tiktoken).

Ent√£o, primeiro instalamos o pacote:

```bash
pip install tiktoken
```
</markdown>
  <markdown>Uma vez instalado, criamos um tokenizador usando o modelo `cl100k_base`, que no notebook de exemplo [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) explica que √© o usado pelos modelos `gpt-4`, `gpt-3.5-turbo` e `text-embedding-ada-002`</markdown>
  <input_code>import tiktoken

encoder = tiktoken.get_encoding("cl100k_base")</input_code>
  <markdown>Agora criamos uma palavra de exemplo para tokeniz√°-la</markdown>
  <input_code>example_word = "breakdown"</input_code>
  <markdown>E tokenizamos</markdown>
  <input_code>tokens = encoder.encode(example_word)
tokens</input_code>
  <output_code>[9137, 2996]</output_code>
  <markdown>A palavra foi dividida em 2 `token`s, o `9137` e o `2996`. Vamos ver a quais palavras correspondem.</markdown>
  <input_code>word1 = encoder.decode([tokens[0]])
word2 = encoder.decode([tokens[1]])
word1, word2</input_code>
  <output_code>('break', 'down')</output_code>
  <markdown>O tokenizador da `OpenAI` dividiu a palavra `breakdown` nas palavras `break` e `down`. Ou seja, ele dividiu a palavra em 2 mais simples.

Isto √© importante, pois quando se diz que um `LLM` suporta x `token`s, n√£o significa que ele suporta x palavras, mas sim que ele suporta x unidades m√≠nimas de representa√ß√£o das palavras.
</markdown>
  <markdown>Se voc√™ tem um texto e quer ver o n√∫mero de `token`s que ele possui para o tokenizador de `OpenAI`, pode verificar na p√°gina [Tokenizer](https://platform.openai.com/tokenizer), que mostra cada `token` em uma cor diferente.

![tokenizer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/tokenizer.webp)
</markdown>
  <markdown>Vimos o tokenizador da `OpenAI`, mas cada `LLM` poder√° usar outro.</markdown>
  <markdown>Como dissemos, os `tokens` s√£o as unidades m√≠nimas de representa√ß√£o das palavras, ent√£o vamos ver quantos tokens distintos tem `tiktoken`</markdown>
  <input_code>n_vocab = encoder.n_vocab
print(f"Vocab size: {n_vocab}")</input_code>
  <output_code>Vocab size: 100277
</output_code>
  <markdown>Vamos a ver como tokeniza outro tipo de palavras</markdown>
  <input_code>def encode_decode(word):
    tokens = encoder.encode(word)
    decode_tokens = []
    for token in tokens:
        decode_tokens.append(encoder.decode([token]))
    return tokens, decode_tokens</input_code>
  <input_code>word = "dog"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "tomorrow..."
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "artificial intelligence"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "Python"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "12/25/2023"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "üòä"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")</input_code>
  <output_code>Word: dog ==&gt; tokens: [18964], decode_tokens: ['dog']
Word: tomorrow... ==&gt; tokens: [38501, 7924, 1131], decode_tokens: ['tom', 'orrow', '...']
Word: artificial intelligence ==&gt; tokens: [472, 16895, 11478], decode_tokens: ['art', 'ificial', ' intelligence']
Word: Python ==&gt; tokens: [31380], decode_tokens: ['Python']
Word: 12/25/2023 ==&gt; tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']
Word: üòä ==&gt; tokens: [76460, 232], decode_tokens: ['ÔøΩ', 'ÔøΩ']
</output_code>
  <markdown>Por √∫ltimo vamos a v√™-lo com palavras em outro idioma</markdown>
  <input_code>word = "perro"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "perra"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "ma√±ana..."
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "inteligencia artificial"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "Python"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "12/25/2023"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")

word = "üòä"
tokens, decode_tokens = encode_decode(word)
print(f"Word: {word} ==&gt; tokens: {tokens}, decode_tokens: {decode_tokens}")</input_code>
  <output_code>Word: perro ==&gt; tokens: [716, 299], decode_tokens: ['per', 'ro']
Word: perra ==&gt; tokens: [79, 14210], decode_tokens: ['p', 'erra']
Word: ma√±ana... ==&gt; tokens: [1764, 88184, 1131], decode_tokens: ['ma', '√±ana', '...']
Word: inteligencia artificial ==&gt; tokens: [396, 39567, 8968, 21075], decode_tokens: ['int', 'elig', 'encia', ' artificial']
Word: Python ==&gt; tokens: [31380], decode_tokens: ['Python']
Word: 12/25/2023 ==&gt; tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']
Word: üòä ==&gt; tokens: [76460, 232], decode_tokens: ['ÔøΩ', 'ÔøΩ']
</output_code>
  <markdown>Podemos ver para palavras semelhantes, em espanhol s√£o gerados mais `token`s do que em ingl√™s, portanto, para um mesmo texto, com um n√∫mero similar de palavras, o n√∫mero de `token`s ser√° maior em espanhol do que em ingl√™s.</markdown>
</notebook>