<?xml version='1.0' encoding='utf-8'?>
<notebook>
  <markdown># Open Source Integrations in Langchain</markdown>
  <markdown> &gt; Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</markdown>
  <markdown>## Use of language models</markdown>
  <markdown>### Usage of Ollama Models</markdown>
  <markdown>As we have already seen in the post on [Ollama](https://www.maximofn.com/ollama), it is a framework built on `llama.cpp` that allows us to use language models easily and also supports quantized models.

So let's see how to use `Qwen2.5 7B` with Ollama in Langchain.
</markdown>
  <markdown>Before anything else, we need to install the Llama module from Langchain

``` bash
pip install -U langchain-ollama
```</markdown>
  <markdown>#### Download model</markdown>
  <markdown>The first thing to do is download the `qwen2.5:7b` model from Ollama. We use the 7B version so that anyone with a GPU can use it.</markdown>
  <input_code>!ollama pull qwen2.5:7b</input_code>
  <output_code>[?25lpulling manifest ‚†ô [?25h[?25l[2K[1Gpulling manifest ‚†π [?25h[?25l[2K[1Gpulling manifest ‚†∏ [?25h[?25l[2K[1Gpulling manifest ‚†∏ [?25h[?25l[2K[1Gpulling manifest ‚†¥ [?25h[?25l[2K[1Gpulling manifest ‚†¶ [?25h[?25l[2K[1Gpulling manifest ‚†ß [?25h[?25l[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè    0 B/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè    0 B/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè    0 B/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè    0 B/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè 9.7 MB/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè  12 MB/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè  17 MB/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   0% ‚ñï                ‚ñè  21 MB/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   1% ‚ñï                ‚ñè  31 MB/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   1% ‚ñï                ‚ñè  42 MB/4.7 GB                  [?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   1% ‚ñï                ‚ñè  51 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   1% ‚ñï                ‚ñè  57 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   1% ‚ñï                ‚ñè  65 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   1% ‚ñï                ‚ñè  70 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   2% ‚ñï                ‚ñè  79 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   2% ‚ñï                ‚ñè  88 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   2% ‚ñï                ‚ñè  93 MB/4.7 GB   47 MB/s   1m37s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   2% ‚ñï                ‚ñè 103 MB/4.7 GB   47 MB/s   1m36s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   2% ‚ñï                ‚ñè 112 MB/4.7 GB   47 MB/s   1m36s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   3% ‚ñï                ‚ñè 117 MB/4.7 GB   47 MB/s   1m36s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   3% ‚ñï                ‚ñè 126 MB/4.7 GB   63 MB/s   1m11s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   3% ‚ñï                ‚ñè 135 MB/4.7 GB   63 MB/s   1m11s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   3% ‚ñï                ‚ñè 141 MB/4.7 GB   63 MB/s   1m11s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
pulling 2bada8a74506...   3% ‚ñï                ‚ñè 150 MB/4.7 GB   63 MB/s   1m11s[?25h[?25l[2K[1G[A[2K[1Gpulling manifest 
...
pulling eb4402837c78... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.5 KB                         
pulling 832dd9e00a68... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  11 KB                         
pulling 2f15b3218f05... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  487 B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success [?25h
</output_code>
  <markdown>#### Chat models</markdown>
  <markdown>The first option we have for using language models with Ollama in Langchain is to use the `ChatOllama` class.</markdown>
  <markdown>We create the `ChatOllama` object with the model `qwen2.5:7b`.</markdown>
  <input_code>from langchain_ollama import ChatOllama

chat_model = ChatOllama(
    model = "qwen2.5:7b",
    temperature = 0.8,
    num_predict = 256,
)</input_code>
  <markdown>##### Offline Inference</markdown>
  <markdown>Now we perform inference with the model in an offline manner, that is, without using streaming. This method waits to have the entire response.</markdown>
  <input_code>messages = [
    ("system", "Eres un asistente de IA que responde preguntas sobre IA."),
    ("human", "¬øQu√© son los LLMs?"),
]
response = chat_model.invoke(messages)
print(response.content)</input_code>
  <output_code>LLMs es el acr√≥nimo de Large Language Models (Modelos de Lenguaje Grandes). Estos son sistemas de inteligencia artificial basados en modelos de aprendizaje autom√°tico profundo que est√°n dise√±ados para comprender y generar texto humano. Algunas caracter√≠sticas clave de los LLMs incluyen:

1. **Tama√±o**: Generalmente se refiere a modelos con billones de par√°metros, lo cual les permite aprender de grandes cantidades de datos.

2. **Capacidad de Generaci√≥n de Texto**: Pueden generar texto coherente y relevante basado en entradas iniciales o prompts proporcionados por el usuario.

3. **Entendimiento del Lenguaje Natural**: Poseen un entendimiento profundo de la gram√°tica, sem√°ntica y contexto del lenguaje humano.

4. **Aplicaciones Vers√°tiles**: Se utilizan en una amplia gama de tareas como asistentes virtuales, traducci√≥n autom√°tica, resumen de texto, escritura creativa, etc.

5. **Entrenamiento Supervisado**: A menudo se entrenan utilizando conjuntos de datos extensos y variados para mejorar su capacidad de comprensi√≥n e
</output_code>
  <markdown>##### Streaming Inference</markdown>
  <markdown>If we want to do streaming, we can use the `stream` method.</markdown>
  <input_code>for chunk in chat_model.stream(messages):
    print(chunk.content, end="", flush=True)</input_code>
  <output_code>LLMs es el acr√≥nimo de Large Language Models, lo cual se refiere a modelos de inteligencia artificial de lenguaje de gran escala. Estos son sistemas entrenados con t√©cnicas de aprendizaje supervisado y no supervisado que pueden generar texto humano‰ººÁöÑÔºåÂáÜÁ°ÆÂõûÁ≠îÂ¶Ç‰∏ãÔºö

LLMs es el acr√≥nimo de Large Language Models, lo cual se refiere a modelos de inteligencia artificial de lenguaje de gran escala. Estos son sistemas que se entrenan con grandes conjuntos de datos de texto para aprender patrones y relaciones ling√º√≠sticas. Algunas caracter√≠sticas clave de los LLMs incluyen:

1. **Tama√±o**: Generalmente se basan en arquitecturas de redes neuronales profunda como Transformers, lo que les permite manejar cantidades extremadamente grandes de par√°metros.
2. **Entrenamiento**: Se entrena con datos de texto muy variados para capturar una amplia gama de conocimientos y habilidades ling√º√≠sticas.
3. **Generaci√≥n de texto**: Pueden generar textos coherentes y relevantes bas√°ndose en el contexto proporcionado, lo que les permite realizar tareas como la escritura creativa,</output_code>
  <markdown>#### Embeddings</markdown>
  <markdown>If we want to use embeddings, we can use the `OllamaEmbeddings` class.</markdown>
  <markdown>Now we create the `OllamaEmbeddings` object with the model `qwen2.5:7b`.</markdown>
  <input_code>from langchain_ollama import OllamaEmbeddings

embeddings_model = OllamaEmbeddings(
    model = "qwen2.5:7b"
)</input_code>
  <markdown>##### Embed a single text</markdown>
  <input_code>input_text = "¬øQu√© son los LLMs?"
embedding = embeddings_model.embed_query(input_text)
len(embedding), type(embedding), embedding[0:3]</input_code>
  <output_code>(3584, list, [0.00045780753, -0.010200562, 0.0059901197])</output_code>
  <markdown>##### Embed of multiple texts</markdown>
  <input_code>input_texts = ["¬øQu√© son los LLMs?", "¬øQu√© son los embeddings?"]
embeddings = embeddings_model.embed_documents(input_texts)
len(embeddings[0]), type(embeddings[0]), embeddings[0][0:3], len(embeddings[1]), type(embeddings[1]), embeddings[1][0:3]</input_code>
  <output_code>(3584,
 list,
 [0.00045780753, -0.010200562, 0.0059901197],
 3584,
 list,
 [0.0007678218, -0.01124029, 0.008565228])</output_code>
  <markdown>#### LLMs</markdown>
  <markdown>If we only want to use a language model, we can use the `OllamaLLM` class.</markdown>
  <markdown>Now we create the `OllamaLLM` object with the model `qwen2.5:7b`.</markdown>
  <input_code>from langchain_ollama import OllamaLLM

llm_model = OllamaLLM(
    model = "qwen2.5:7b"
)</input_code>
  <markdown>##### Offline Inference</markdown>
  <markdown>Now we perform inference with the model in an offline manner, that is, without using streaming. This method waits to have the entire response.</markdown>
  <input_code>message = "¬øQu√© son los LLMs?"
response = llm_model.invoke(message)
print(response)</input_code>
  <output_code>Los LLMs es una abreviatura com√∫n en espa√±ol que se refiere a "Lenguajes de Marcado de Libre Mando". Sin embargo, en el contexto del procesamiento del lenguaje natural y la inteligencia artificial, es probable que est√©s buscando informaci√≥n sobre otro t√©rmino.

En ingl√©s, el t√©rmino LLMs (Large Language Models) se refiere a modelos de lenguaje de gran tama√±o. Estos son sistemas de IA que est√°n entrenados en una amplia gama de datos de texto para comprender y generar texto humano-like. Algunas caracter√≠sticas clave de los LLMs incluyen:

1. Tama√±o: Generalmente se refiere a modelos con millones o incluso billones de par√°metros.
2. Generaci√≥n de texto: Pueden generar texto en respuesta a una entrada dada, lo que les hace √∫tiles para tareas como escritura autom√°tica, conversaciones de chat, etc.
3. Entrenamiento en datos variados: Son entrenados con grandes conjuntos de datos de internet o corpora literales y acad√©micos.
4. Aplicaciones: Se utilizan en una variedad de aplicaciones, incluyendo asistentes virtuales, traducci√≥n autom√°tica, an√°lisis de sentimientos, resumen de texto, etc.

Un ejemplo famoso de LLMs es la familia de modelos GPT (Generative Pre-trained Transformer) desarrollados por OpenAI. Otros ejemplos incluyen los modelos de PaLM y Qwen de Anthropic, entre otros.

¬øTe gustar√≠a saber m√°s sobre alg√∫n aspecto en particular de estos modelos?
</output_code>
  <markdown>##### Streaming Inference</markdown>
  <markdown>If we want to do streaming, we can use the `stream` method.</markdown>
  <input_code>for chunk in llm_model.stream(message):
    print(chunk, end="", flush=True)</input_code>
  <output_code>Los LLMs es una abreviatura que se refiere a los Modelos de Lenguaje de Grande Escala (Large Language Models en ingl√©s). Estos son modelos de inteligencia artificial desarrollados para entender y generar texto humano. Algunas caracter√≠sticas clave de los LLMs incluyen:

1. **Capacidad de Generaci√≥n de Texto**: Pueden generar texto coherente, original e informativo en varios estilos y formatos.

2. **Entendimiento del Contexto**: Tienen una comprensi√≥n profunda del contexto y la gram√°tica para producir respuestas apropiadas a las entradas proporcionadas.

3. **Multiplicidad de Usos**: Se pueden aplicar a diversas tareas, como escritura creativa, asistencia en atenci√≥n al cliente, creaci√≥n de contenido, traducci√≥n autom√°tica, etc.

4. **Aprendizaje Supervisado**: Son entrenados con grandes cantidades de texto para aprender patrones y conocimientos del lenguaje humano.

5. **Limitaciones Eticas y Jur√≠dicas**: Existen preocupaciones sobre el uso √©tico y legal de estos modelos, incluyendo el potencial de generar contenido inapropiado o enga√±oso.

Algunos ejemplos populares de LLMs incluyen los creados por empresas como Anthropic (Clara), Google (Switch Transformer), Microsoft (Cobalt) y, aunque ya no actualmente soportado, OpenAI (GPT-3).</output_code>
  <markdown>### Usage of HuggingFace models</markdown>
  <markdown>Although Ollama is very easy to use, it doesn't have as many models as there are on the HuggingFace Hub. Additionally, it might not have the latest great model you want to use, so we're going to see how to use HuggingFace models in Langchain.</markdown>
  <markdown>Before anything, we need to install the HuggingFace module of Langchain

``` bash
pip install -U langchain-huggingface
```</markdown>
  <markdown>#### Login to the HuggingFace Hub</markdown>
  <markdown>First, we need to log in to the HuggingFace Hub. For this, we need to create an access token. Once created, we save it in a `.env` file with the name `LANGCHAIN_TOKEN` as follows:

``` bash
echo "LANGCHAIN_TOKEN=hf_..." &gt; .env
```</markdown>
  <markdown>To be able to read it, we install the `python-dotenv` module.

``` bash
pip install python-dotenv
```

Now let's log in to the Hugging Face Hub.
</markdown>
  <input_code>import os
import dotenv

dotenv.load_dotenv()

LANGCHAIN_TOKEN = os.getenv("LANGCHAIN_TOKEN")</input_code>
  <input_code>from huggingface_hub import login
login(LANGCHAIN_TOKEN)</input_code>
  <markdown>#### Chat models</markdown>
  <markdown>The first option we have for using language models with HuggingFace in Langchain is to use the `HuggingFaceEndpoint` class. This option calls the HuggingFace API to perform inference.</markdown>
  <markdown>Now we create the `chat_model` object with the model `Qwen2.5-72B-Instruct`.</markdown>
  <input_code>from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-72B-Instruct",
    task="text-generation",
    max_new_tokens=512,
    do_sample=False,
    repetition_penalty=1.03,
)

chat = ChatHuggingFace(llm=llm, verbose=True)</input_code>
  <markdown>##### Offline Inference</markdown>
  <markdown>Now we perform inference with the model in an offline manner, that is, without using streaming. This method waits to have the entire response.</markdown>
  <input_code>messages = [
    ("system", "Eres un asistente de IA que responde preguntas sobre IA."),
    ("human", "¬øQu√© son los LLMs?"),
]
response = chat.invoke(messages)
print(response.content)</input_code>
  <output_code>Los LLMs, o Large Language Models (Modelos de Lenguaje Grande en espa√±ol), son sistemas de inteligencia artificial dise√±ados para procesar y generar texto basado en lenguaje natural. Estos modelos son capaces de comprender, analizar y producir texto humano de manera sofisticada, lo que los hace √∫tiles en una amplia variedad de aplicaciones, como la traducci√≥n de idiomas, el an√°lisis de sentimientos, la generaci√≥n de respuestas a preguntas, la escritura creativa, la asistencia en tareas, entre otras.

### Caracter√≠sticas principales de los LLMs:

1. **Escalabilidad**: Los LLMs suelen tener millones o incluso billones de par√°metros, lo que les permite capturar la complejidad del lenguaje humano con un alto nivel de detalle.

2. **Entrenamiento supervisado y no supervisado**: Pueden ser entrenados con grandes cantidades de datos de texto, tanto de forma supervisada (con etiquetas) como no supervisada (sin etiquetas).

3. **Transfer learning**: Una vez entrenados, estos modelos pueden ser finetuneados (ajustados) para tareas espec√≠ficas con un conjunto de datos m√°s peque√±o, lo que los hace muy vers√°tiles.

4. **Generaci√≥n de texto**: Son capaces de generar texto coherente y contextualmente relevante, lo que los hace ideales para tareas como la escritura creativa, la redacci√≥n de informes, y la generaci√≥n de respuestas en chatbots.

5. **Comprensi√≥n del contexto**: Pueden entender el contexto y las relaciones entre palabras y frases, lo que les permite manejar di√°logos y discursos complejos.

6. **Interacci√≥n humana**: Son dise√±ados para interactuar de manera fluida y natural con seres humanos, lo que los hace √∫tiles en aplicaciones conversacionales.

### Ejemplos de LLMs:

- **GPT-3 (Generative Pre-trained Transformer 3)**: Desarrollado por OpenAI, es uno de losmodelos m√°s conocidos y potentes.
- **BERT (Bidirectional Encoder Representations from Transformers)**: Dise√±ado por Google, es conocido por su capacidad de comprensi√≥n del contexto.
- **T5 (Text-to-Text Transfer Transformer)**: Tambi√©n de Google, se destaca por su flexibilidad y versatilidad.

Los LLMs representan un avance significativo en el campo de la inteligencia artificial, especialmente en el procesamiento del lenguaje natural, y contin√∫an evoluinto r√°pidamente.
</output_code>
  <markdown>##### Streaming Inference</markdown>
  <markdown>If we want to do streaming, we can use the `stream` method.</markdown>
  <input_code>for chunk in chat.stream(messages):
    print(chunk.content, end="", flush=True)</input_code>
  <output_code>Los LLMs, o Large Language Models (Modelos de Lenguaje Grande en espa√±ol), son sistemas de inteligencia artificial dise√±ados para procesar y generar texto basado en lenguaje natural. Estos modelos son capaces de comprender, analizar y producir texto humano de manera sofisticada, lo que los hace √∫tiles en una amplia variedad de aplicaciones, como la traducci√≥n de idiomas, el an√°lisis de sentimientos, la generaci√≥n de respuestas a preguntas, la escritura creativa, la asistencia en tareas, entre otras.

### Caracter√≠sticas principales de los LLMs:

1. **Escalabilidad**: Los LLMs suelen tener millones o incluso billones de par√°metros, lo que les permite capturar la complejidad del lenguaje humano con un alto nivel de detalle.

2. **Entrenamiento supervisado y no supervisado**: Pueden ser entrenados con grandes cantidades de datos de texto, tanto de forma supervisada (con etiquetas) como no supervisada (sin etiquetas).

3. **Transfer learning**: Una vez entrenados, estos modelos pueden ser finetuneados (ajustados) para tareas espec√≠ficas con un conjunto de datos m√°s peque√±o, lo que los hace muy vers√°tiles.

4. **Generaci√≥n de texto**: Son capaces de generar texto coherente y contextualmente relevante, lo que los hace ideales para tareas como la escritura creativa, la redacci√≥n de informes, y la generaci√≥n de respuestas en chatbots.

5. **Comprensi√≥n del contexto**: Pueden entender el contexto y las relaciones entre palabras y frases, lo que les permite manejar di√°logos y discursos complejos.

6. **Interacci√≥n humana**: Son dise√±ados para interactuar de manera fluida y natural con seres humanos, lo que los hace √∫tiles en aplicaciones conversacionales.

### Ejemplos de LLMs:

- **GPT-3 (Generative Pre-trained Transformer 3)**: Desarrollado por OpenAI, es uno de losmodelos m√°s conocidos y potentes.
- **BERT (Bidirectional Encoder Representations from Transformers)**: Dise√±ado por Google, es conocido por su capacidad de comprensi√≥n del contexto.
- **T5 (Text-to-Text Transfer Transformer)**: Tambi√©n de Google, se destaca por su flexibilidad y versatilidad.

Los LLMs representan un avance significativo en el campo de la inteligencia artificial, especialmente en el procesamiento del lenguaje natural, y contin√∫an evoluinto r√°pidamente.</output_code>
  <markdown>#### Embeddings</markdown>
  <markdown>##### Embeddings locally</markdown>
  <markdown>If we want to create embeddings locally, we can use the `HuggingFaceEmbeddings` class. For this, we need to have the `sentence-transformers` library installed.

``` bash
pip install -U sentence-transformers
```

Then we can create embeddings locally
</markdown>
  <input_code>from langchain_huggingface import HuggingFaceEmbeddings
import torch

model_name = "sentence-transformers/all-mpnet-base-v2"
device = "cuda" if torch.cuda.is_available() else "cpu"
model_kwargs = {"device": device}
encode_kwargs = {"normalize_embeddings": True}

embeddings_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

input_text = "¬øQu√© son los LLMs?"
embedding = embeddings_model.embed_query(input_text)
len(embedding), type(embedding), embedding[0:3]</input_code>
  <output_code>(768,
 list,
 [-0.03638569265604019, -0.003062659176066518, 0.005454241763800383])</output_code>
  <markdown>If we want to create embeddings for multiple texts, we can use the `embed_documents` method.</markdown>
  <input_code>input_texts = ["¬øQu√© son los LLMs?", "¬øQu√© son los embeddings?"]
embeddings = embeddings_model.embed_documents(input_texts)
len(embeddings[0]), type(embeddings[0]), embeddings[0][0:3], len(embeddings[1]), type(embeddings[1]), embeddings[1][0:3]</input_code>
  <output_code>(768,
 list,
 [-0.0363856740295887, -0.0030626414809376, 0.005454237572848797],
 768,
 list,
 [-0.014533628709614277, 0.01950662210583687, -0.01753164641559124])</output_code>
  <markdown>##### Embeddings in HuggingFace</markdown>
  <markdown>If we want to use the HuggingFace API to create embeddings, we can use the `HuggingFaceEmbeddings` class.</markdown>
  <input_code>from langchain_huggingface import HuggingFaceEndpointEmbeddings

model = "sentence-transformers/all-mpnet-base-v2"

embeddings_model = HuggingFaceEndpointEmbeddings(
    model=model,
    task="feature-extraction",
)

input_text = "¬øQu√© son los LLMs?"
embedding = embeddings_model.embed_query(input_text)
len(embedding), type(embedding), embedding[0:3]</input_code>
  <output_code>(768,
 list,
 [-0.03638569638133049, -0.0030626363586634398, 0.005454216152429581])</output_code>
  <markdown>If you want to create embeddings for multiple texts, you can use the `embed_documents` method.</markdown>
  <input_code>input_texts = ["¬øQu√© son los LLMs?", "¬øQu√© son los embeddings?"]
embeddings = embeddings_model.embed_documents(input_texts)
len(embeddings[0]), type(embeddings[0]), embeddings[0][0:3], len(embeddings[1]), type(embeddings[1]), embeddings[1][0:3]</input_code>
  <output_code>(768,
 list,
 [-0.03638570010662079, -0.003062596544623375, 0.005454217083752155],
 768,
 list,
 [-0.014533626846969128, 0.019506637006998062, -0.01753171905875206])</output_code>
  <markdown>#### Pipeline</markdown>
  <markdown>We can create a Transformers pipeline and use it in Langchain, for which we need to install the `transformers` module.

``` bash
pip install -U transformers
```

To ensure that everyone can try the example, we will use the model `Qwen/Qwen2.5-3B-Instruct`.
</markdown>
  <markdown>We have two ways, creating a pipeline using Langchain</markdown>
  <input_code>from langchain_huggingface import HuggingFacePipeline

model = "Qwen/Qwen2.5-3B-Instruct"
langchain_pipeline = HuggingFacePipeline.from_model_id(
    model_id=model,
    task="text-generation",
    pipeline_kwargs={"max_new_tokens": 512},
)</input_code>
  <output_code>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</output_code>
  <output_code>Device set to use cuda:0
</output_code>
  <markdown>Or we can create a pipeline using Transformers.</markdown>
  <input_code>from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = "Qwen/Qwen2.5-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)
pipe = pipeline(
    "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512
)
transformers_pipeline = HuggingFacePipeline(pipeline=pipe)</input_code>
  <output_code>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</output_code>
  <output_code>Device set to use cuda:0
</output_code>
  <markdown>##### Offline Inference</markdown>
  <input_code>langchain_response = langchain_pipeline.invoke("¬øQu√© son los LLMs?")
print(langchain_response)</input_code>
  <output_code>¬øQu√© son los LLMs? - El Blog de Ciberseguridad
Publicado por: CiberSeguridad 18/03/2022
En el mundo de la tecnolog√≠a y la inteligencia artificial, las nuevas tecnolog√≠as que surgen a menudo se presentan como un paso hacia adelante en la mejora de nuestras vidas. Sin embargo, a veces estas innovaciones pueden resultar en desaf√≠os significativos, especialmente cuando se trata de la seguridad cibern√©tica.
En este art√≠culo, exploraremos los LLMs, o modelos ling√º√≠sticos grandes (Large Language Models), una nueva clase de inteligencia artificial que est√° revolucionando el campo de la comunicaci√≥n y la creaci√≥n de contenido. Aunque estos modelos son muy prometedores, tambi√©n tienen implicaciones significativas para la seguridad cibern√©tica. En esta entrada, te explicamos qu√© son los LLMs y c√≥mo afectan a la seguridad digital.
¬øQu√© son los LLMs?
Los modelos ling√º√≠sticos grandes son algoritmos de aprendizaje autom√°tico que han sido entrenados con grandes conjuntos de texto, generalmente en formato de texto natural. Estos modelos pueden procesar y generar texto de manera similar a una persona, lo que los hace extremadamente √∫tiles para tareas como la traducci√≥n, la autocompletado de texto y la generaci√≥n de contenido creativo.
Los LLMs se basan en t√©cnicas de aprendizaje profundo, que son una forma avanzada de algoritmo de aprendizaje autom√°tico. Este tipo de algoritmo utiliza redes neuronales para aprender patrones y relaciones en datos grandes, permitiendo a los modelos predecir resultados o tomar decisiones basadas en esa informaci√≥n.
Estos modelos son capaces de aprender y generar texto en un amplio rango de temas, desde descripciones de libros hasta di√°logos interactivos, lo que los hace excelentes para tareas de generaci√≥n de contenido. Tambi√©n pueden ser utilizados para tareas m√°s espec√≠ficas, como la traducci√≥n de idiomas o la resoluci√≥n de problemas de l√≥gica.
¬øC√≥mo afectan a la seguridad cibern√©tica?
Los LLMs representan una gran oportunidad para mejorar la eficiencia y la precisi√≥n de muchas tareas de texto, pero tambi√©n traen consigo desaf√≠os significativos en t√©rminos de seguridad. Algunas de las preocupaciones m√°s comunes incluyen:
Privacidad: Los LLM
</output_code>
  <input_code>transformers_response = transformers_pipeline("¬øQu√© son los LLMs?")
print(transformers_response)</input_code>
  <output_code>/tmp/ipykernel_318295/4120882068.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.
  transformers_response = transformers_pipeline("¬øQu√© son los LLMs?")
</output_code>
  <output_code>¬øQu√© son los LLMs? ¬øC√≥mo funcionan y qu√© pueden hacer?

Los LLMs (Large Language Models) son modelos de inteligencia artificial que han sido entrenados con enormes cantidades de texto. Estos modelos pueden procesar y generar texto en una variedad de lenguajes, desde el ingl√©s hasta idiomas menos comunes. Los LLMs est√°n dise√±ados para entender y generar texto con un nivel de precisi√≥n similar al humano.

Funcionamiento:
1. Entrenamiento: Los LLMs se entrenan utilizando grandes conjuntos de datos de texto, como libros electr√≥nicos, art√≠culos de noticias, etc.
2. An√°lisis: Despu√©s del entrenamiento, los LLMs analizan patrones en el texto y aprenden a asociar palabras y frases con otros textos similares.
3. Generaci√≥n de texto: Cuando se le pide al modelo que genere texto, analiza los patrones que ha aprendido y genera texto basado en esos patrones.

Pueden hacer:
1. Traducci√≥n: Convertir texto de un idioma a otro.
2. Resumen de texto: Summarizar largos documentos en versiones m√°s cortas.
3. Creaci√≥n de contenido: Generar art√≠culos, correos electr√≥nicos, historias, etc.
4. Resoluci√≥n de problemas: Ayudar a resolver problemas o proporcionar soluciones basadas en el texto.
5. Chatbots: Usar para crear asistentes virtuales que puedan responder preguntas y ayudar con tareas.
6. Crea contenido: Escribe contenido creativo como poes√≠a, relatos, etc.
7. Ajuste de lenguaje: Corrige errores gramaticales y mejora la escritura.

Es importante tener en cuenta que aunque estos modelos son muy poderosos, todav√≠a tienen limitaciones y pueden generar contenido incorrecto o inapropiado. Adem√°s, la privacidad y la √©tica son aspectos importantes a considerar al usar estos modelos. 

En resumen, los LLMs son herramientas poderosas que pueden ser utilizadas en una variedad de aplicaciones, pero tambi√©n requieren cuidado y consideraci√≥n al utilizarlos. Es recomendable siempre verificar el contenido generado por estos modelos y estar atentos a sus posibles limitaciones. 

Adem√°s, es importante destacar que los LLMs no son sustitutos perfectos para el pensamiento cr√≠tico humano. Aunque pueden generar texto de
</output_code>
  <markdown>##### Streaming Inference</markdown>
  <input_code>for chunk in langchain_pipeline.stream("¬øQu√© son los LLMs?"):
    print(chunk, end="", flush=True)</input_code>
  <output_code> - Cursos de Inteligencia Artificial en l√≠nea
Los LLMs, o Lenguajes de Lenguaje de Modelos, son modelos de aprendizaje autom√°tico que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generaci√≥n de texto, el chatbot, el an√°lisis de sentimientos, etc.
En resumen, los LLMs son modelos de aprendizaje autom√°tico que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generaci√≥n de texto, el chatbot, el an√°lisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creaci√≥n de contenido hasta el asesoramiento.
Los LLMs son modelos de aprendizaje autom√°tico que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generaci√≥n de texto, el chatbot, el an√°lisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creaci√≥n de contenido hasta el asesoramiento. Los LLMs est√°n basados en redes neuronales y se entrenan con grandes conjuntos de datos de texto para aprender a generar texto similar al de un humano.
Los LLMs est√°n disponibles en diferentes plataformas, incluyendo Google's PaLM, Anthropic's Claude, Microsoft's Bing, e incluso tu propio tel√©fono inteligente puede tener uno. Los LLMs pueden ser utilizados para una variedad de tareas, incluyendo la generaci√≥n de texto, el chatbot, el an√°lisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creaci√≥n de contenido hasta el asesoramiento. Los LLMs est√°n basados en redes neuronales y se entrenan con grandes conjuntos de datos de texto para aprender a generar texto similar al de un humano. Los LLMs est√°n disponibles en diferentes plataformas, incluyendo Google's PaLM, Anthropic's Claude, Microsoft's Bing, e incluso tu propio tel√©fono inteligente puede tener uno</output_code>
  <input_code>for chunk in transformers_pipeline.stream("¬øQu√© son los LLMs?"):
    print(chunk, end="", flush=True)</input_code>
  <output_code> | El Blog de la Inteligencia Artificial
Home ¬ª Noticias ¬ª ¬øQu√© son los LLMs?
LLM es la sigla en ingl√©s para Large Language Models, que traducido al castellano ser√≠a "Grandes Modelos de Lenguaje". Son modelos de inteligencia artificial (IA) que permiten a las m√°quinas entender y generar texto.
Los LLMs se basan en el aprendizaje profundo, una t√©cnica de IA que utiliza redes neuronales para modelar relaciones entre variables. Estos modelos son capaces de aprender patrones en grandes conjuntos de datos, lo que les permite predecir o generar texto similar al que se les ha proporcionado.
Las principales caracter√≠sticas de los LLMs incluyen:
Capacidad de procesamiento de lenguaje natural: Los LLMs pueden entender y generar texto, lo que les permite interactuar con los usuarios de manera fluida y natural.
Capacidad de aprendizaje continuo: Los LLMs pueden aprender y mejorar con cada interacci√≥n, lo que les permite ofrecer respuestas m√°s precisas y relevantes a medida que ganan experiencia.
Capacidad de generaci√≥n de texto: Los LLMs pueden generar texto similar al que se les ha proporcionado, lo que les permite crear contenido de calidad sin necesidad de intervenci√≥n humana manual.
Ventajas de los LLMs:
Mayor eficiencia: Los LLMs pueden procesar grandes cantidades de informaci√≥n de manera r√°pida y precisa, lo que les permite ahorrar tiempo y recursos en comparaci√≥n con los m√©todos humanos.
Mejora de la precisi√≥n: Los LLMs pueden aprender y ajustarse a los patrones del lenguaje, lo que les permite ofrecer respuestas m√°s precisas y relevantes a los usuarios.
Aumento de la productividad: Los LLMs pueden automatizar tareas repetitivas y mon√≥tonas, liberando tiempo y recursos para que los humanos puedan centrarse en tareas m√°s estrat√©gicas y creativas.
Entendimiento del lenguaje humano: Los LLMs pueden entender y generar texto en varios idiomas, lo que les permite interactuar con un amplio espectro de usuarios.
Evaluaci√≥n de la inteligencia artificial
Los LLMs est√°n siendo utilizados en una variedad de aplicaciones, desde asistentes virtuales hasta an√°lisis de texto y traducci√≥n autom√°tica. Tambi√©n est√°n siendo evaluados por su capacidad para generar contenido de</output_code>
  <markdown>## Use of Vector Databases</markdown>
  <markdown>### Usage of ChromaDB</markdown>
  <markdown>To be able to use ChromaDB in Langchain, we first have to install `langchain-chroma`.

``` bash
pip install -qU chromadb langchain-chroma
```</markdown>
  <markdown>#### Create a vector store</markdown>
  <input_code>from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
import torch

model_name = "sentence-transformers/all-mpnet-base-v2"
device = "cuda" if torch.cuda.is_available() else "cpu"
model_kwargs = {"device": device}
encode_kwargs = {"normalize_embeddings": True}

embedding_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

vector_store = Chroma(
    collection_name="chroma_db",
    embedding_function=embedding_model,
)</input_code>
  <markdown>#### Add documents</markdown>
  <input_code>from langchain_core.documents import Document

document_1 = Document(page_content="This is a Mojo docs", metadata={"source": "Mojo source"})
document_2 = Document(page_content="This is Rust docs", metadata={"source": "Rust source"})
document_3 = Document(page_content="i will be deleted :(")

documents = [document_1, document_2, document_3]
ids = ["1", "2", "3"]
vector_store.add_documents(documents=documents, ids=ids)</input_code>
  <output_code>['1', '2', '3']</output_code>
  <markdown>#### Update documents</markdown>
  <input_code>updated_document = Document(
    page_content="This is Python docs",
    metadata={"source": "Python source"}
)

vector_store.update_documents(ids=["1"],documents=[updated_document])</input_code>
  <markdown>#### Delete documents</markdown>
  <input_code>vector_store.delete(ids=["3"])</input_code>
  <markdown>#### Document Search</markdown>
  <input_code>results = vector_store.similarity_search(query="python",k=1)
for doc in results:
    print(f"* {doc.page_content} [{doc.metadata}]")</input_code>
  <output_code>* This is Python docs [{'source': 'Python source'}]
</output_code>
  <markdown>#### Filtered Search</markdown>
  <input_code>results = vector_store.similarity_search(query="python",k=1,filter={"source": "Python source"})
for doc in results:
    print(f"* {doc.page_content} [{doc.metadata}]")</input_code>
  <output_code>* This is Python docs [{'source': 'Python source'}]
</output_code>
  <markdown>#### Search with punctuation</markdown>
  <input_code>results = vector_store.similarity_search_with_score(query="python",k=1)
for doc, score in results:
    print(f"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]")</input_code>
  <output_code>* [SIM=0.809454] This is Python docs [{'source': 'Python source'}]
</output_code>
  <markdown>#### Usage as Retriever</markdown>
  <input_code>retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 1, "fetch_k": 2, "lambda_mult": 0.5},
)
retriever.invoke("python")</input_code>
  <output_code>[Document(metadata={'source': 'Python source'}, page_content='This is Python docs')]</output_code>
  <markdown>#### Cosine Similarity Measure</markdown>
  <markdown>This method calculates the cosine similarity between two vectors, so instead of passing it two strings, you have to pass it two vectors.</markdown>
  <markdown>So first we create an embeddings model.</markdown>
  <input_code>from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np
import torch

model_name = "sentence-transformers/all-mpnet-base-v2"
device = "cuda" if torch.cuda.is_available() else "cpu"
model_kwargs = {"device": device}
encode_kwargs = {"normalize_embeddings": True}

embeddings_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

document1 = "Python"
document2 = "python"

embedding1 = embeddings_model.embed_query(document1)
embedding2 = embeddings_model.embed_query(document2)

embedding1_numpy = np.array(embedding1)
embedding2_numpy = np.array(embedding2)

embedding1_torch = torch.tensor(embedding1_numpy).unsqueeze(0)
embedding2_torch = torch.tensor(embedding2_numpy).unsqueeze(0)</input_code>
  <markdown>And now we can calculate the cosine similarity.</markdown>
  <input_code>from langchain_chroma.vectorstores import cosine_similarity

similarity = cosine_similarity(embedding1_torch, embedding2_torch)
similarity
</input_code>
  <output_code>array([[1.]])</output_code>
</notebook>