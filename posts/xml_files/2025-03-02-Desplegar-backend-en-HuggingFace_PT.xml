<?xml version='1.0' encoding='utf-8'?>
<notebook>
  <markdown># Deployar backend no HuggingFace</markdown>
  <markdown> &gt; Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</markdown>
  <markdown>Neste post, vamos ver como deployar um backend no HuggingFace. Vamos ver como fazer isso de duas maneiras, atrav√©s da forma comum, criando uma aplica√ß√£o com Gradio, e atrav√©s de uma op√ß√£o diferente usando FastAPI, Langchain e Docker.</markdown>
  <markdown>Para ambos casos ser√° necess√°rio ter uma conta no HuggingFace, j√° que vamos implantar o backend em um espa√ßo do HuggingFace.</markdown>
  <markdown>## Desplegar backend com Gradio</markdown>
  <markdown>### Criar espa√ßo</markdown>
  <markdown>Primeiro de tudo, criamos um novo espa√ßo na Hugging Face.

* Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.
* Escolhemos o Gradio como o tipo de SDK. Ao escolher o Gradio, ser√£o exibidas algumas templates, ent√£o escolhemos a template do chatbot.
* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.
* E por √∫ltimo, temos que escolher se queremos criar o espa√ßo p√∫blico ou privado.

![backend gradio - criar espa√ßo](https://images.maximofn.com/backend-gradio-create-space.webp)</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>Ao criar o space, podemos clon√°-lo ou podemos ver os arquivos na pr√≥pria p√°gina do HuggingFace. Podemos ver que foram criados 3 arquivos, `app.py`, `requirements.txt` e `README.md`. Ent√£o, vamos ver o que colocar em cada um.</markdown>
  <markdown>#### app.py</markdown>
  <markdown>Aqui est√° o c√≥digo do aplicativo. Como escolhemos o template de chatbot, j√° temos muito feito, mas vamos ter que mudar 2 coisas: primeiro, o modelo de linguagem e o system prompt.</markdown>
  <markdown>Como modelo de linguagem, vejo ``HuggingFaceH4/zephyr-7b-beta``, mas vamos utilizar ``Qwen/Qwen2.5-72B-Instruct``, que √© um modelo muito capaz.

Ent√£o, procure pelo texto ``client = InferenceClient("HuggingFaceH4/zephyr-7b-beta")`` e substitua-o por ``client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")``, ou espere que colocarei todo o c√≥digo mais tarde.</markdown>
  <markdown>Tamb√©m vamos alterar o system prompt, que por padr√£o √© ``You are a friendly Chatbot.``, mas como o modelo foi treinado principalmente em ingl√™s, √© prov√°vel que se voc√™ falar com ele em outro idioma, ele responda em ingl√™s. Ent√£o, vamos mud√°-lo para ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.

Ent√£o, procure pelo texto ``gr.Textbox(value="You are a friendly Chatbot.", label="System message"),`` e substitua-o por ``gr.Textbox(value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", label="System message"),``, ou espere at√© eu colocar todo o c√≥digo agora.</markdown>
  <markdown>``` python
import gradio as gr
do huggingface_hub import InferenceClient

""""""
Para mais informa√ß√µes sobre o suporte da API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference
""""""
client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")


def responder(
mensagem,
hist√≥ria: list[tuple[str, str]],
Mensagem do sistema,
max_tokens,
temperatura,
top_p,
):
mensagens = [{"papel": "sistema", "conte√∫do": system_message}]

for val in history:
if val[0]:
messages.append({"role": "user", "content": val[0]})
if val[1]:
messages.append({"role": "assistant", "content": val[1]})

messages.append({"role": "user", "content": message})

response = ""

para mensagem em client.chat_completion(
mensagens,
max_tokens=max_tokens,
stream=True,
temperature=temperature,
top_p=top_p,
):
token = message.choices[0].delta.content

response += token
yield response


""""""
Para informa√ß√µes sobre como personalizar a ChatInterface, consulte a documenta√ß√£o do gradio: https://www.gradio.app/docs/gradio/chatinterface
""""""
demo = gr.ChatInterface(
responda,
```markdown
additional_inputs=[
```
gr.Textbox(value="Voc√™ √© um chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™.", label="Mensagem do sistema"),
gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="M√°ximo de novos tokens"),
gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo="Temperatura"),
gr.Slider(
m√≠nimo=0.1,
m√°ximo=1.0,
value=0.95,
passo=0.05,
label="Top-p (amostragem do n√∫cleo)"
),
],
)


if __name__ == "__main__":
demo.launch()
```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Este √© o arquivo onde ser√£o escritas as depend√™ncias, mas para este caso vai ser muito simples:

``` txt
huggingface_hub==0.25.2
```</markdown>
  <markdown>#### LEIA-ME.md</markdown>
  <markdown>Este √© o arquivo no qual vamos colocar as informa√ß√µes do espa√ßo. Nos spaces da HuggingFace, no in√≠cio dos readmes, coloca-se um c√≥digo para que a HuggingFace saiba como exibir a miniatura do espa√ßo, qual arquivo deve ser usado para executar o c√≥digo, vers√£o do sdk, etc.

``` md
---
t√≠tulo: SmolLM2
emoji: üí¨
colorFrom: amarelo
colorTo: roxo
sdk: gradio
sdk_version: 5.0.1
app_file: app.py
pinned: false
licen√ßa: apache-2.0
short_description: Bate-papo com o Gradio SmolLM2
---

Um exemplo de chatbot usando [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index) e a [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).
```</markdown>
  <markdown>### Implanta√ß√£o</markdown>
  <markdown>Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.

Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.

![backend gradio - chatbot](https://images.maximofn.com/backend-gradio-chatbot.webp)</markdown>
  <markdown>### Backend</markdown>
  <markdown>Muito bem, fizemos um chatbot, mas n√£o era essa a inten√ß√£o, aqui t√≠nhamos vindo fazer um backend! P√°ra, p√°ra, olha o que diz abaixo do chatbot

![backend gradio - Use via API](https://images.maximofn.com/backend-gradio-chatbot-edited.webp)</markdown>
  <markdown>Podemos ver um texto ``Use via API``, onde se clicarmos, se abrir√° um menu com uma API para poder usar o chatbot.

![backend gradio - API](https://images.maximofn.com/backend%20gradio%20-%20API.webp)</markdown>
  <markdown>Vemos que nos d√° uma documenta√ß√£o de como usar a API, tanto com Python, com JavaScript, quanto com bash.</markdown>
  <markdown>### Teste da API</markdown>
  <markdown>Usamos o c√≥digo de exemplo de Python.</markdown>
  <input_code>from gradio_client import Client

client = Client("Maximofn/SmolLM2")
result = client.predict(
		message="Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Loaded as API: https://maximofn-smollm2.hf.space ‚úî
¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?
</output_code>
  <markdown>Estamos fazendo chamadas √† API do `InferenceClient` da HuggingFace, ent√£o poder√≠amos pensar, Para que fizemos um backend, se podemos chamar diretamente a API da HuggingFace? Bem, voc√™ vai ver isso abaixo.</markdown>
  <input_code>result = client.predict(
		message="¬øC√≥mo me llamo?",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Tu nombre es M√°ximo. ¬øEs correcto?
</output_code>
  <markdown>O modelo de bate-papo do Gradio gerencia o hist√≥rico para n√≥s, de forma que cada vez que criamos um novo `cliente`, uma nova thread de conversa √© criada.</markdown>
  <markdown>Vamos a tentar criar um novo cliente e ver se uma nova thread de conversa √© criada.</markdown>
  <input_code>from gradio_client import Client

new_client = Client("Maximofn/SmolLM2")
result = new_client.predict(
		message="Hola, ¬øc√≥mo est√°s? Me llamo Luis",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Loaded as API: https://maximofn-smollm2.hf.space ‚úî
Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?
</output_code>
  <markdown>Agora vamos perguntar novamente como me chamo</markdown>
  <input_code>result = new_client.predict(
		message="¬øC√≥mo me llamo?",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?
</output_code>
  <markdown>Como podemos ver, temos dois clientes, cada um com seu pr√≥prio fio de conversa.</markdown>
  <markdown>## Deploy do backend com FastAPI, Langchain e Docker</markdown>
  <markdown>Agora vamos a fazer o mesmo, criar um backend de um chatbot, com o mesmo modelo, mas nesse caso usando FastAPI, Langchain e Docker.</markdown>
  <markdown>### Criar espa√ßo</markdown>
  <markdown>Temos que criar um novo espa√ßo, mas nesse caso faremos de outra maneira

* Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.
* Escolhemos Docker como o tipo de SDK. Ao escolher Docker, aparecer√£o modelos, ent√£o escolhemos um modelo em branco.
* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.
* E por fim, √© preciso escolher se queremos criar o espa√ßo p√∫blico ou privado.

![backend docker - criar espa√ßo](https://images.maximofn.com/backend-docker-create-space.webp)</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>Agora, ao criar o space, vemos que temos apenas um arquivo, o ``README.md``. Ent√£o vamos ter que criar todo o c√≥digo n√≥s mesmos.</markdown>
  <markdown>#### app.py</markdown>
  <markdown>Vamos a criar o c√≥digo do aplicativo</markdown>
  <markdown>Come√ßamos com as bibliotecas necess√°rias

``` python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
do huggingface_hub import InferenceClient

```markdown
from langchain_core.messages import HumanMessage, AIMessage
```
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

import os
from dotenv import load_dotenv
load_dotenv()
```

Carregamos `fastapi` para poder criar as rotas da API, `pydantic` para criar o template das queries, `huggingface_hub` para poder criar um modelo de linguagem, `langchain` para indicar ao modelo se as mensagens s√£o do chatbot ou do usu√°rio e `langgraph` para criar o chatbot.

Al√©m disso, carregamos `os` e `dotenv` para poder carregar as vari√°veis de ambiente.</markdown>
  <markdown>Carregamos o token do HuggingFace

``` python
# Token da HuggingFace
HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))
```</markdown>
  <markdown>Criamos o modelo de linguagem

``` python
# Inicializar o modelo da HuggingFace
model = InferenceClient(
model="Qwen/Qwen2.5-72B-Instruct",
api_key=os.getenv("HUGGINGFACE_TOKEN")
)
```</markdown>
  <markdown>Criamos agora uma fun√ß√£o para chamar o modelo

``` python
# Define the function that calls the model
def call_model(state: MessagesState):
    """
    Call the model with the given messages

    Args:
        state: MessagesState

    Returns:
        dict: A dictionary containing the generated text and the thread ID
    """
    # Convert LangChain messages to HuggingFace format
    hf_messages = []
    for msg in state["messages"]:
        if isinstance(msg, HumanMessage):
            hf_messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            hf_messages.append({"role": "assistant", "content": msg.content})
    
    # Call the API
    response = model.chat_completion(
        messages=hf_messages,
        temperature=0.5,
        max_tokens=64,
        top_p=0.7
    )
    
    # Convert the response to LangChain format
    ai_message = AIMessage(content=response.choices[0].message.content)
    return {"messages": state["messages"] + [ai_message]}
```

Convertemos as mensagens do formato LangChain para o formato HuggingFace, assim podemos usar o modelo de linguagem.</markdown>
  <markdown>Definimos uma template para as queries

``` python
class QueryRequest(BaseModel):
query: str
thread_id: str = "padr√£o"
```

As consultas ter√£o um `query`, a mensagem do usu√°rio, e um `thread_id`, que √© o identificador do fio da conversa√ß√£o e mais adiante explicaremos para que o utilizamos.</markdown>
  <markdown>Criamos um grafo de LangGraph

``` python
# Definir o gr√°fico
workflow = StateGraph(state_schema=MessagesState)

# Defina o n√≥do na gr√°fico
workflow.add_edge(START, "model")
workflow.add_node("modelo", call_model)

# Adicionar mem√≥ria
memory = MemorySaver()
graph_app = workflow.compile(checkpointer=memory)
```

Com isso, criamos um grafo de LangGraph, que √© uma estrutura de dados que nos permite criar um chatbot e gerenciar o estado do chatbot para n√≥s, ou seja, entre outras coisas, o hist√≥rico de mensagens. Dessa forma, n√£o precisamos fazer isso n√≥s mesmos.</markdown>
  <markdown>Criamos a aplica√ß√£o de FastAPI

``` python
app = FastAPI(title="LangChain FastAPI", description="API para gerar texto usando LangChain e LangGraph")
```</markdown>
  <markdown>Criamos os endpoints da API

``` python
# Ponto de entrada Bem-vindo
@app.get("/")
async def api_home():
"""Ponto de entrada Welcome"""
return {"detail": "Bem-vindo ao tutorial de FastAPI, Langchain, Docker"}

# Gerar ponto final
@app.post("/generate")
async def gerar(request: QueryRequest):
""""""
Ponto final para gerar texto usando o modelo de linguagem
    
Argumentos:
solicita√ß√£o: QueryRequest
query: str
thread_id: str = "padr√£o"

Retorna:
um dicion√°rio contendo o texto gerado e o ID do thread
""""""
tente:
# Configurar o ID da thread
config = {"configurable": {"thread_id": request.thread_id}}
        
# Crie a mensagem de entrada
input_messages = [HumanMessage(content=request.query)]
        
# Invocar o gr√°fico
output = graph_app.invoke({"messages": input_messages}, config)
        
# Obter a resposta do modelo
resposta = output["messages"][-1].conte√∫do
        
return {
"generated_text": resposta,
"thread_id": request.thread_id
}
except Exception as e:
raise HTTPException(status_code=500, detail=f"Erro ao gerar texto: {str(e)}")
```

Criamos o endpoint `/` que nos retornar√° um texto quando acessarmos a API, e o endpoint `/generate` que √© o que usaremos para gerar o texto.

Se n√≥s olharmos para a fun√ß√£o `generate`, temos a vari√°vel `config`, que √© um dicion√°rio que cont√©m o `thread_id`. Este `thread_id` √© o que nos permite ter um hist√≥rico de mensagens de cada usu√°rio, desta forma, diferentes usu√°rios podem usar o mesmo endpoint e ter seu pr√≥prio hist√≥rico de mensagens.</markdown>
  <markdown>Por √∫ltimo, temos o c√≥digo para que se possa executar a aplica√ß√£o

``` python
if __name__ == "__main__":
import uvicorn
uvicorn.run(app, host="0.0.0.0", port=7860)
```</markdown>
  <markdown>Vamos escrever todo o c√≥digo juntos

``` python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from huggingface_hub import InferenceClient

from langchain_core.messages import HumanMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

import os
from dotenv import load_dotenv
load_dotenv()

# HuggingFace token
HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))

# Initialize the HuggingFace model
model = InferenceClient(
    model="Qwen/Qwen2.5-72B-Instruct",
    api_key=os.getenv("HUGGINGFACE_TOKEN")
)

# Define the function that calls the model
def call_model(state: MessagesState):
    """
    Call the model with the given messages

    Args:
        state: MessagesState

    Returns:
        dict: A dictionary containing the generated text and the thread ID
    """
    # Convert LangChain messages to HuggingFace format
    hf_messages = []
    for msg in state["messages"]:
        if isinstance(msg, HumanMessage):
            hf_messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            hf_messages.append({"role": "assistant", "content": msg.content})
    
    # Call the API
    response = model.chat_completion(
        messages=hf_messages,
        temperature=0.5,
        max_tokens=64,
        top_p=0.7
    )
    
    # Convert the response to LangChain format
    ai_message = AIMessage(content=response.choices[0].message.content)
    return {"messages": state["messages"] + [ai_message]}

# Define the graph
workflow = StateGraph(state_schema=MessagesState)

# Define the node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
graph_app = workflow.compile(checkpointer=memory)

# Define the data model for the request
class QueryRequest(BaseModel):
    query: str
    thread_id: str = "default"

# Create the FastAPI application
app = FastAPI(title="LangChain FastAPI", description="API to generate text using LangChain and LangGraph")

# Welcome endpoint
@app.get("/")
async def api_home():
    """Welcome endpoint"""
    return {"detail": "Welcome to FastAPI, Langchain, Docker tutorial"}

# Generate endpoint
@app.post("/generate")
async def generate(request: QueryRequest):
    """
    Endpoint to generate text using the language model
    
    Args:
        request: QueryRequest
        query: str
        thread_id: str = "default"

    Returns:
        dict: A dictionary containing the generated text and the thread ID
    """
    try:
        # Configure the thread ID
        config = {"configurable": {"thread_id": request.thread_id}}
        
        # Create the input message
        input_messages = [HumanMessage(content=request.query)]
        
        # Invoke the graph
        output = graph_app.invoke({"messages": input_messages}, config)
        
        # Get the model response
        response = output["messages"][-1].content
        
        return {
            "generated_text": response,
            "thread_id": request.thread_id
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error al generar texto: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
```</markdown>
  <markdown>#### Dockerfile</markdown>
  <markdown>Agora vemos como criar o Dockerfile</markdown>
  <markdown>Primeiro indicamos a partir de qual imagem vamos come√ßar

``` dockerfile
FROM python:3.13-slim
```</markdown>
  <markdown>Agora criamos o diret√≥rio de trabalho

``` dockerfile
RUN useradd -m -u 1000 user
WORKDIR /app
```</markdown>
  <markdown>Copiamos o arquivo com as depend√™ncias e instalamos

``` dockerfile
COPY --chown=user ./requirements.txt requirements.txt
RUN pip install --no-cache-dir --upgrade -r requirements.txt
```</markdown>
  <markdown>Copiamos o resto do c√≥digo

``` dockerfile
COPY --chown=user . /app
```</markdown>
  <markdown>Exponhamos o porto 7860

``` dockerfile
EXPOSE 7860
```</markdown>
  <markdown>Criamos as vari√°veis de ambiente

``` dockerfile
RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Segredo existe!"
```</markdown>
  <markdown>Por √∫ltimo, indicamos o comando para executar a aplica√ß√£o

``` dockerfile
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]
```</markdown>
  <markdown>Agora colocamos tudo junto

``` dockerfile
FROM python:3.13-slim

RUN useradd -m -u 1000 user
WORKDIR /app

COPY --chown=user ./requirements.txt requirements.txt
RUN pip install --no-cache-dir --upgrade -r requirements.txt

COPY --chown=user . /app

EXPOSE 7860

RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Segredo existe!"

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]
```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Criamos o arquivo com as depend√™ncias

``` txt
fastapi
uvicorn
pedidos
pydantic&gt;=2.0.0
langchain
langchain-huggingface
langchain-core
langgraph &gt; 0.2.27
python-dotenv.2.11
```</markdown>
  <markdown>#### README.md</markdown>
  <markdown>Por fim, criamos o arquivo README.md com informa√ß√µes sobre o espa√ßo e as instru√ß√µes para o HuggingFace.

``` md
---
title: SmolLM2 Backend
emoji: üìä
colorFrom: yellow
colorTo: red
sdk: docker
pinned: false
license: apache-2.0
short_description: Backend of SmolLM2 chat
app_port: 7860
---

# SmolLM2 Backend

This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.

## Configuration

### In HuggingFace Spaces

This project is designed to run in HuggingFace Spaces. To configure it:

1. Create a new Space in HuggingFace with SDK Docker
2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:
   - Go to the "Settings" tab of your Space
   - Scroll down to the "Repository secrets" section
   - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value
   - Save the changes

### Local development

For local development:

1. Clone this repository
2. Create a `.env` file in the project root with your HuggingFace token:
   ``
   HUGGINGFACE_TOKEN=your_token_here
   ``
3. Install the dependencies:
   ``
   pip install -r requirements.txt
   ``

## Local execution

``bash
uvicorn app:app --reload
``

The API will be available at `http://localhost:8000`.

## Endpoints

### GET `/`

Welcome endpoint that returns a greeting message.

### POST `/generate`

Endpoint to generate text using the language model.

**Request parameters:**
``json
{
  "query": "Your question here",
  "thread_id": "optional_thread_identifier"
}
``

**Response:**
``json
{
  "generated_text": "Generated text by the model",
  "thread_id": "thread identifier"
}
``

## Docker

To run the application in a Docker container:

``bash
# Build the image
docker build -t smollm2-backend .

# Run the container
docker run -p 8000:8000 --env-file .env smollm2-backend
``

## API documentation

The interactive API documentation is available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`
```</markdown>
  <markdown>### Token do HuggingFace

Se voc√™ notou no c√≥digo e no Dockerfile, usamos um token do HuggingFace, ent√£o vamos ter que criar um. Em nossa conta do HuggingFace, criamos um [novo token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), damos um nome a ele e concedemos as seguintes permiss√µes:

* Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob o seu namespace pessoal
* Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob seu namespace pessoal
* Fazer chamadas para provedores de infer√™ncia
* Fazer chamadas para Pontos de Extremidade de Infer√™ncia

![backend docker - token](https://images.maximofn.com/backend-docker-token.webp)</markdown>
  <markdown>### Adicionar o token aos secrets do espa√ßo</markdown>
  <markdown>Agora que j√° temos o token, precisamos adicion√°-lo ao espa√ßo. Na parte superior do aplicativo, poderemos ver um bot√£o chamado `Settings`, clicamos nele e poderemos ver a se√ß√£o de configura√ß√£o do espa√ßo.

Se formos para baixo, poderemos ver uma se√ß√£o onde podemos adicionar `Variables` e `Secrets`. Neste caso, como estamos adicionando um token, vamos adicion√°-lo aos `Secrets`.

Damos o nome `HUGGINGFACE_TOKEN` e o valor do token.</markdown>
  <markdown>### Implanta√ß√£o</markdown>
  <markdown>Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.

Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.

Neste caso, constru√≠mos apenas um backend, portanto o que vamos ver ao entrar no espa√ßo √© o que definimos no endpoint `/`

![backend docker - espa√ßo](https://images.maximofn.com/backend-docker-space.webp)</markdown>
  <markdown>### URL do backend</markdown>
  <markdown>Precisamos saber a URL do backend para poder fazer chamadas √† API. Para isso, temos que clicar nos tr√™s pontos no canto superior direito para ver as op√ß√µes.

![backend docker - op√ß√µes](https://images.maximofn.com/backend-docker-options.webp)</markdown>
  <markdown>No menu suspenso, clicamos em `Embed this Spade`. Ser√° aberta uma janela indicando como incorporar o espa√ßo com um iframe e tamb√©m fornecer√° a URL do espa√ßo.

![backend docker - embed](https://images.maximofn.com/backend-docker-embed.webp)</markdown>
  <markdown>Se agora formos para essa URL, veremos o mesmo que no espa√ßo.</markdown>
  <markdown>### Documenta√ß√£o</markdown>
  <markdown>FastAPI, al√©m de ser uma API extremamente r√°pida, tem outra grande vantagem: gera documenta√ß√£o automaticamente.</markdown>
  <markdown>Se adicionarmos `/docs` √† URL que vimos anteriormente, poderemos visualizar a documenta√ß√£o da API com o `Swagger UI`.

![backend docker - swagger doc](https://images.maximofn.com/backend-docker-swagger-doc.webp)</markdown>
  <markdown>Tamb√©m podemos adicionar `/redoc` √† URL para ver a documenta√ß√£o com `ReDoc`.

![backend docker - redoc doc](https://images.maximofn.com/backend-docker-redoc.webp)</markdown>
  <markdown>### Teste da API</markdown>
  <markdown>O bom da documenta√ß√£o `Swagger UI` √© que nos permite testar a API diretamente do navegador.</markdown>
  <markdown>Adicionamos `/docs` √† URL que obtivemos, abrimos o menu suspenso do endpoint `/generate` e clicamos em `Try it out`, modificamos o valor da `query` e do `thread_id` e clicamos em `Execute`.

No primeiro caso vou colocar

* **query**: Ol√°, como voc√™ est√°? Sou M√°ximo
* **thread_id**: user1

![backend docker - test API](https://images.maximofn.com/backend-docker-test-API.webp)</markdown>
  <markdown>Recebemos a seguinte resposta `Ol√° M√°ximo! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? Em que posso ajudar hoje?`

![backend docker -response 1 - user1](https://images.maximofn.com/backend-docker-response1-user1.webp)</markdown>
  <markdown>Vamos testar agora a mesma pergunta, mas com um `thread_id` diferente, neste caso `user2`.

![backend docker - query 1 - user2](https://images.maximofn.com/backend-docker-query1-user2.webp)</markdown>
  <markdown>E nos responde isso `Ol√° Luis! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? No que posso ajudar hoje?`

![backend docker - response 1 - user2](https://images.maximofn.com/backend-docker-response1-user2.webp)</markdown>
  <markdown>Agora pedimos nosso nome com os dois usu√°rios e obtemos isso

* Para o usu√°rio **user1**: `Voc√™ se chama M√°ximo. H√° algo mais em que eu possa ajudar voc√™?`
* Para o usu√°rio **user2**: `Voc√™ se chama Luis. H√° mais alguma coisa em que eu possa ajud√°-lo hoje, Luis?`

![backend docker - response 2 - user1](https://images.maximofn.com/backend-docker-response2-user1.webp)

![backend docker - response 2 - user2](https://images.maximofn.com/backend-docker-response2-user2.webp)</markdown>
  <markdown>## Deploy do backend com Gradio e modelo rodando no servidor</markdown>
  <markdown>Os dois backends que criamos na verdade n√£o est√£o executando um modelo, mas sim fazendo chamadas para Inference Endpoints da HuggingFace. Mas pode ser que queiramos que tudo rode no servidor, inclusive o modelo. Pode ser que voc√™ tenha feito um fine-tuning de um LLM para seu caso de uso, por isso j√° n√£o pode fazer chamadas para Inference Endpoints.

Ent√£o vamos ver como modificar o c√≥digo dos dois backends para executar um modelo no servidor e n√£o fazer chamadas para Inference Endpoints.</markdown>
  <markdown>### Criar Espa√ßo</markdown>
  <markdown>Na hora de criar o space no HuggingFace fazemos o mesmo que antes, criamos um novo space, colocamos um nome e uma descri√ß√£o, selecionamos Gradio como SDK, selecionamos o HW em que vamos deployar, no meu caso selecionei o HW mais b√°sico e gratuito, e selecionamos se o faremos privado ou p√∫blico.</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>Temos que fazer altera√ß√µes em `app.py` e em `requirements.txt` para que, em vez de fazer chamadas a Inference Endpoints, o modelo seja executado localmente.</markdown>
  <markdown>#### app.py</markdown>
  <markdown>As mudan√ßas que temos que fazer s√£o</markdown>
  <markdown>Importar `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importar `torch`

``` python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
```</markdown>
  <markdown>Em vez de criar um modelo com `InferenceClient`, criamos com `AutoModelForCausalLM` e `AutoTokenizer`.

``` python
# Carregar o modelo e o tokenizer
model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
nome_do_modelo,
torch_dtype=torch.float16,
device_map="auto"
)
```

Utilizo `HuggingFaceTB/SmolLM2-1.7B-Instruct` porque √© um modelo bastante capaz com apenas 1.7B de par√¢metros. Como escolhi o hardware mais b√°sico, n√£o posso usar modelos muito grandes. Voc√™, se quiser usar um modelo maior, tem duas op√ß√µes: usar o hardware gratuito e aceitar que a infer√™ncia ser√° mais lenta, ou usar um hardware mais potente, mas pago.</markdown>
  <markdown>Modificar a fun√ß√£o `respond` para que construa o prompt com a estrutura necess√°ria pela biblioteca `transformers`, tokenizar o prompt, fazer a infer√™ncia e destokenizar a resposta.

``` python
def responder(
mensagem,
hist√≥rico: list[tuple[str, str]],
Mensagem do sistema,
max_tokens,
temperatura,
top_p,
):
# Construir o prompt com o formato correto
prompt = f"&lt;|system|&gt;\n{system_message}&lt;/s&gt;\n"
    
for val in history:
if val[0]:
prompt += f"&lt;|user|&gt;\n{val[0]}&lt;/s&gt;\n"
if val[1]:
prompt += f"&lt;|assistant|&gt;\n{val[1]}&lt;/s&gt;\n"
    
prompt += f"&lt;|user|&gt;\n{message}&lt;/s&gt;\n&lt;|assistant|&gt;\n"
    
# Tokenizar o prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
# Gerar a resposta
outputs = model.generate(
**entradas,**
max_new_tokens=max_tokens,
temperature=temperature,
top_p=top_p,
do_sample=True,
pad_token_id=tokenizer.eos_token_id
)
    
# Decodificar a resposta
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
# Extrair apenas a parte da resposta do assistente
response = response.split("&lt;|assistant|&gt;\n")[-1].strip()
    
yield response
```</markdown>
  <markdown>A seguir deixo todo o c√≥digo

``` python
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

""""""
Para mais informa√ß√µes sobre o suporte √† API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference
""""""

# Carregar o modelo e o tokenizer
model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
nome_do_modelo,
torch_dtype=torch.float16,
device_map="auto"
)

def responder(
mensagem,
hist√≥ria: list[tuple[str, str]],
Mensagem do sistema,
max_tokens,
temperatura,
top_p,
):
# Construir o prompt com o formato correto
prompt = f"&lt;|system|&gt;\n{system_message}&lt;/s&gt;\n"
    
for val in history:
if val[0]:
prompt += f"&lt;|user|&gt;\n{val[0]}&lt;/s&gt;\n"
if val[1]:
prompt += f"&lt;|assistant|&gt;\n{val[1]}&lt;/s&gt;\n"
    
prompt += f"&lt;|user|&gt;\n{message}&lt;/s&gt;\n&lt;|assistant|&gt;\n"
    
# Tokenizar o prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
# Gerar a resposta
sa√≠das = modelo.gerar(
**entradas,**
max_new_tokens=max_tokens,
temperature=temperature,
top_p=top_p,
do_sample=True,
pad_token_id=tokenizer.eos_token_id
)
    
# Decodificar a resposta
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
# Extrair apenas a parte da resposta do assistente
response = response.split("&lt;|assistant|&gt;\n")[-1].strip()
    
yield response


""""""
Para informa√ß√µes sobre como personalizar o ChatInterface, consulte a documenta√ß√£o do Gradio: https://www.gradio.app/docs/gradio/chatinterface
""""""
demo = gr.ChatInterface(
responda,
```markdown
additional_inputs=[
```
gr.Textbox(
value="Voc√™ √© um Chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™."
r√≥tulo="Mensagem do sistema"
),
gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="M√°ximo de novos tokens"),
gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo="Temperatura"),
gr.Slider()
m√≠nimo=0.1,
m√°ximo=1.0,
value=0.95,
passo=0.05,
label="Top-p (amostragem do n√∫cleo)"
),
],
)


if __name__ == "__main__":
demo.launch()
```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Neste arquivo, devemos adicionar as novas bibliotecas que vamos utilizar, neste caso `transformers`, `accelerate` e `torch`. O arquivo completo ficaria:

``` txt
huggingface_hub==0.25.2
gradio&gt;=4.0.0
transformers&gt;=4.36.0
torch&gt;=2.0.0
accelerate&gt;=0.25.0
```</markdown>
  <markdown>#### Teste da API</markdown>
  <markdown>Desplegamos o space e testamos diretamente a API.</markdown>
  <input_code>from gradio_client import Client

client = Client("Maximofn/SmolLM2_localModel")
result = client.predict(
		message="Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî
Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?
</output_code>
  <markdown>Surpreende-me o qu√£o r√°pido o modelo responde, mesmo estando em um servidor sem GPU.</markdown>
  <markdown>## Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor</markdown>
  <markdown>Agora fazemos o mesmo que antes, mas com FastAPI, LangChain e Docker.</markdown>
  <markdown>### Criar Espa√ßo</markdown>
  <markdown>Ao criar o space no HuggingFace, fazemos o mesmo que antes: criamos um novo espa√ßo, colocamos um nome e uma descri√ß√£o, selecionamos Docker como SDK, escolhemos o HW em que vamos implant√°-lo, no meu caso, escolho o HW mais b√°sico e gratuito, e decidimos se o faremos privado ou p√∫blico.</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>#### app.py</markdown>
  <markdown>J√° n√£o importamos `InferenceClient` e agora importamos `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importamos `torch`.

``` python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
```</markdown>
  <markdown>Instanciamos o modelo e o tokenizer com `AutoModelForCausalLM` e `AutoTokenizer`.

``` python
# Inicialize o modelo e o tokenizador
print("Carregando modelo e tokenizer...")
dispositivo = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"

tente:
# Carregar o modelo no formato BF16 para melhor desempenho e menor uso de mem√≥ria
tokenizer = AutoTokenizer.from_pretrained(model_name)
    
if device == "cuda":
print("Usando GPU para o modelo...")
model = AutoModelForCausalLM.from_pretrained(
nome_do_modelo,
torch_dtype=torch.bfloat16,
device_map="auto",
low_cpu_mem_usage=True
)
else:
print("Usando CPU para o modelo...")
model = AutoModelForCausalLM.from_pretrained(
nome_do_modelo,
device_map={"": device},
torch_dtype=torch.float32
)

print(f"Modelo carregado com sucesso em: {device}")
except Exception as e:
print(f"Erro ao carregar o modelo: {str(e)}")
aumentar
```</markdown>
  <markdown>Re definimos a fun√ß√£o `call_model` para que fa√ßa a infer√™ncia com o modelo local.

``` python
# Define the function that calls the model
def call_model(state: MessagesState):
    """
    Call the model with the given messages

    Args:
        state: MessagesState

    Returns:
        dict: A dictionary containing the generated text and the thread ID
    """
    # Convert LangChain messages to chat format
    messages = []
    for msg in state["messages"]:
        if isinstance(msg, HumanMessage):
            messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            messages.append({"role": "assistant", "content": msg.content})
    
    # Prepare the input using the chat template
    input_text = tokenizer.apply_chat_template(messages, tokenize=False)
    inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
    
    # Generate response
    outputs = model.generate(
        inputs,
        max_new_tokens=512,  # Increase the number of tokens for longer responses
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # Decode and clean the response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extract only the assistant's response (after the last user message)
    response = response.split("Assistant:")[-1].strip()
    
    # Convert the response to LangChain format
    ai_message = AIMessage(content=response)
    return {"messages": state["messages"] + [ai_message]}
```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Temos que remover `langchain-huggingface` e adicionar `transformers`, `accelerate` e `torch` no arquivo `requirements.txt`. O arquivo ficaria:

``` txt
fastapi
uvicorn
solicita√ß√µes
pydantic&gt;=2.0.0
langchain&gt;=0.1.0
langchain-core&gt;=0.1.10
langgraph&gt;=0.2.27
python-dotenv&gt;=1.0.0
transformers&gt;=4.36.0
torch&gt;=2.0.0
accelerate&gt;=0.26.0
```</markdown>
  <markdown>#### Dockerfile</markdown>
  <markdown>J√° n√£o precisamos ter `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` porque como o modelo vai estar no servidor e n√£o vamos fazer chamadas para Inference Endpoints, n√£o precisamos do token. O arquivo ficaria:

``` dockerfile
FROM python:3.13-slim

RUN useradd -m -u 1000 user
WORKDIR /app

COPY --chown=user ./requirements.txt requirements.txt
RUN pip install --no-cache-dir --upgrade -r requirements.txt

COPY --chown=user . /app

EXPOSE 7860

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]
```</markdown>
  <markdown>### Teste da API</markdown>
  <markdown>Deployamos o space e testamos a API. Neste caso, vou testar diretamente do Python.</markdown>
  <input_code>import requests

url = "https://maximofn-smollm2-backend-localmodel.hf.space/generate"
data = {
    "query": "Hola, ¬øc√≥mo est√°s?",
    "thread_id": "user1"
}

response = requests.post(url, json=data)
if response.status_code == 200:
    result = response.json()
    print("Respuesta:", result["generated_text"])
    print("Thread ID:", result["thread_id"])
else:
    print("Error:", response.status_code, response.text)</input_code>
  <output_code>Respuesta: system
You are a friendly Chatbot. Always reply in the language in which the user is writing to you.
user
Hola, ¬øc√≥mo est√°s?
assistant
Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.
Thread ID: user1
</output_code>
  <markdown>Este demora um pouco mais que o anterior. Na verdade, demora o normal para um modelo sendo executado em um servidor sem GPU. O estranho √© quando o deployamos no Gradio. N√£o sei o que a HuggingFace faz por tr√°s, ou talvez tenha sido coincid√™ncia.</markdown>
  <markdown>## Conclus√µes</markdown>
  <markdown>Vimos como criar um backend com um LLM, tanto fazendo chamadas ao Inference Endpoint da HuggingFace, quanto fazendo chamadas a um modelo rodando localmente. Vimos como fazer isso com Gradio ou com FastAPI, Langchain e Docker.</markdown>
  <markdown>A partir daqui voc√™ tem o conhecimento para poder implantar seus pr√≥prios modelos, mesmo que n√£o sejam LLMs, podem ser modelos multimodais. A partir daqui voc√™ pode fazer o que quiser.</markdown>
</notebook>