<?xml version='1.0' encoding='utf-8'?>
<notebook>
  <markdown># Deployar backend no HuggingFace</markdown>
  <markdown> &gt; Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</markdown>
  <markdown>Neste post, vamos ver como deployar um backend no HuggingFace. Vamos ver como fazer isso de duas maneiras, atrav√©s da forma comum, criando uma aplica√ß√£o com Gradio, e atrav√©s de uma op√ß√£o diferente usando FastAPI, Langchain e Docker.</markdown>
  <markdown>Para ambos casos ser√° necess√°rio ter uma conta no HuggingFace, j√° que vamos implantar o backend em um espa√ßo do HuggingFace.</markdown>
  <markdown>## Desplegar backend com Gradio</markdown>
  <markdown>### Criar espa√ßo</markdown>
  <markdown>Primeiro de tudo, criamos um novo espa√ßo na Hugging Face.
* Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.* Escolhemos o Gradio como o tipo de SDK. Ao escolher o Gradio, ser√£o exibidas algumas templates, ent√£o escolhemos a template do chatbot.* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.* E por √∫ltimo, temos que escolher se queremos criar o espa√ßo p√∫blico ou privado.
![backend gradio - criar espa√ßo](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>Ao criar o space, podemos clon√°-lo ou podemos ver os arquivos na pr√≥pria p√°gina do HuggingFace. Podemos ver que foram criados 3 arquivos, `app.py`, `requirements.txt` e `README.md`. Ent√£o, vamos ver o que colocar em cada um.</markdown>
  <markdown>#### app.py</markdown>
  <markdown>Aqui est√° o c√≥digo do aplicativo. Como escolhemos o template de chatbot, j√° temos muito feito, mas vamos ter que mudar 2 coisas: primeiro, o modelo de linguagem e o system prompt.</markdown>
  <markdown>Como modelo de linguagem, vejo ``HuggingFaceH4/zephyr-7b-beta``, mas vamos utilizar ``Qwen/Qwen2.5-72B-Instruct``, que √© um modelo muito capaz.
Ent√£o, procure pelo texto ``client = InferenceClient("HuggingFaceH4/zephyr-7b-beta")`` e substitua-o por ``client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")``, ou espere que colocarei todo o c√≥digo mais tarde.</markdown>
  <markdown>Tamb√©m vamos alterar o system prompt, que por padr√£o √© ``You are a friendly Chatbot.``, mas como o modelo foi treinado principalmente em ingl√™s, √© prov√°vel que se voc√™ falar com ele em outro idioma, ele responda em ingl√™s. Ent√£o, vamos mud√°-lo para ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.
Ent√£o, procure pelo texto ``gr.Textbox(value="You are a friendly Chatbot.", label="System message"),`` e substitua-o por ``gr.Textbox(value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", label="System message"),``, ou espere at√© eu colocar todo o c√≥digo agora.</markdown>
  <markdown>``` python
import gradio as grdo huggingface_hub import InferenceClient
""""""Para mais informa√ß√µes sobre o suporte da API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference""""""client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")

def responder(mensagem,hist√≥ria: list[tuple[str, str]],Mensagem do sistema,max_tokens,temperatura,top_p,):mensagens = [{"papel": "sistema", "conte√∫do": system_message}]
for val in history:if val[0]:messages.append({"role": "user", "content": val[0]})if val[1]:messages.append({"role": "assistant", "content": val[1]})
messages.append({"role": "user", "content": message})
response = ""
para mensagem em client.chat_completion(mensagens,max_tokens=max_tokens,stream=True,temperature=temperature,top_p=top_p,):token = message.choices[0].delta.content
response += tokenyield response

""""""Para informa√ß√µes sobre como personalizar a ChatInterface, consulte a documenta√ß√£o do gradio: https://www.gradio.app/docs/gradio/chatinterface""""""demo = gr.ChatInterface(responda,```markdown
additional_inputs=[
```gr.Textbox(value="Voc√™ √© um chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™.", label="Mensagem do sistema"),gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="M√°ximo de novos tokens"),gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo="Temperatura"),gr.Slider(m√≠nimo=0.1,m√°ximo=1.0,value=0.95,passo=0.05,label="Top-p (amostragem do n√∫cleo)"),],)

if __name__ == "__main__":demo.launch()```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Este √© o arquivo onde ser√£o escritas as depend√™ncias, mas para este caso vai ser muito simples:
``` txt
huggingface_hub==0.25.2```</markdown>
  <markdown>#### LEIA-ME.md</markdown>
  <markdown>Este √© o arquivo no qual vamos colocar as informa√ß√µes do espa√ßo. Nos spaces da HuggingFace, no in√≠cio dos readmes, coloca-se um c√≥digo para que a HuggingFace saiba como exibir a miniatura do espa√ßo, qual arquivo deve ser usado para executar o c√≥digo, vers√£o do sdk, etc.
``` md
---t√≠tulo: SmolLM2emoji: üí¨colorFrom: amarelocolorTo: roxosdk: gradiosdk_version: 5.0.1app_file: app.pypinned: falselicen√ßa: apache-2.0short_description: Bate-papo com o Gradio SmolLM2---
Um exemplo de chatbot usando [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index) e a [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).```</markdown>
  <markdown>### Implanta√ß√£o</markdown>
  <markdown>Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.
Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.
![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)</markdown>
  <markdown>### Backend</markdown>
  <markdown>Muito bem, fizemos um chatbot, mas n√£o era essa a inten√ß√£o, aqui t√≠nhamos vindo fazer um backend! P√°ra, p√°ra, olha o que diz abaixo do chatbot
![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)</markdown>
  <markdown>Podemos ver um texto ``Use via API``, onde se clicarmos, se abrir√° um menu com uma API para poder usar o chatbot.
![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)</markdown>
  <markdown>Vemos que nos d√° uma documenta√ß√£o de como usar a API, tanto com Python, com JavaScript, quanto com bash.</markdown>
  <markdown>### Teste da API</markdown>
  <markdown>Usamos o c√≥digo de exemplo de Python.</markdown>
  <input_code>from gradio_client import Client

client = Client("Maximofn/SmolLM2")
result = client.predict(
		message="Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Loaded as API: https://maximofn-smollm2.hf.space ‚úî
¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?
</output_code>
  <markdown>Estamos fazendo chamadas √† API do `InferenceClient` da HuggingFace, ent√£o poder√≠amos pensar, Para que fizemos um backend, se podemos chamar diretamente a API da HuggingFace? Bem, voc√™ vai ver isso abaixo.</markdown>
  <input_code>result = client.predict(
		message="¬øC√≥mo me llamo?",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Tu nombre es M√°ximo. ¬øEs correcto?
</output_code>
  <markdown>O modelo de bate-papo do Gradio gerencia o hist√≥rico para n√≥s, de forma que cada vez que criamos um novo `cliente`, uma nova thread de conversa √© criada.</markdown>
  <markdown>Vamos a tentar criar um novo cliente e ver se uma nova thread de conversa √© criada.</markdown>
  <input_code>from gradio_client import Client

new_client = Client("Maximofn/SmolLM2")
result = new_client.predict(
		message="Hola, ¬øc√≥mo est√°s? Me llamo Luis",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Loaded as API: https://maximofn-smollm2.hf.space ‚úî
Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?
</output_code>
  <markdown>Agora vamos perguntar novamente como me chamo</markdown>
  <input_code>result = new_client.predict(
		message="¬øC√≥mo me llamo?",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?
</output_code>
  <markdown>Como podemos ver, temos dois clientes, cada um com seu pr√≥prio fio de conversa.</markdown>
  <markdown>## Deploy do backend com FastAPI, Langchain e Docker</markdown>
  <markdown>Agora vamos a fazer o mesmo, criar um backend de um chatbot, com o mesmo modelo, mas nesse caso usando FastAPI, Langchain e Docker.</markdown>
  <markdown>### Criar espa√ßo</markdown>
  <markdown>Temos que criar um novo espa√ßo, mas nesse caso faremos de outra maneira
* Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.* Escolhemos Docker como o tipo de SDK. Ao escolher Docker, aparecer√£o modelos, ent√£o escolhemos um modelo em branco.* Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.* E por fim, √© preciso escolher se queremos criar o espa√ßo p√∫blico ou privado.
![backend docker - criar espa√ßo](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp)</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>Agora, ao criar o space, vemos que temos apenas um arquivo, o ``README.md``. Ent√£o vamos ter que criar todo o c√≥digo n√≥s mesmos.</markdown>
  <markdown>#### app.py</markdown>
  <markdown>Vamos a criar o c√≥digo do aplicativo</markdown>
  <markdown>Come√ßamos com as bibliotecas necess√°rias
``` python
from fastapi import FastAPI, HTTPExceptionfrom pydantic import BaseModeldo huggingface_hub import InferenceClient
```markdown
from langchain_core.messages import HumanMessage, AIMessage
```from langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph
import osfrom dotenv import load_dotenvload_dotenv()```

Carregamos `fastapi` para poder criar as rotas da API, `pydantic` para criar o template das queries, `huggingface_hub` para poder criar um modelo de linguagem, `langchain` para indicar ao modelo se as mensagens s√£o do chatbot ou do usu√°rio e `langgraph` para criar o chatbot.
Al√©m disso, carregamos `os` e `dotenv` para poder carregar as vari√°veis de ambiente.</markdown>
  <markdown>Carregamos o token do HuggingFace
``` python
# Token da HuggingFaceHUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))```</markdown>
  <markdown>Criamos o modelo de linguagem
``` python
# Inicializar o modelo da HuggingFacemodel = InferenceClient(model="Qwen/Qwen2.5-72B-Instruct",api_key=os.getenv("HUGGINGFACE_TOKEN"))```</markdown>
  <markdown>Criamos agora uma fun√ß√£o para chamar o modelo
``` python
# Defina a fun√ß√£o que chama o modelodef chamar_modelo(estado: MessagesState):""""""Chame o modelo com as mensagens fornecidas
Argumentos:estado: EstadoMensagens
Retorna:um dicion√°rio contendo o texto gerado e o ID do thread""""""# Converter mensagens do LangChain para o formato do HuggingFacehf_messages = []for msg in state["messages"]:Se `isinstance(msg, HumanMessage)`:hf_messages.append({"role": "user", "content": msg.content})elif isinstance(msg, AIMessage):hf_messages.append({"role": "assistant", "content": msg.content})    
# Chamar a APIresposta = modelo.completar_chat(mensagens=hf_mensagens,temperature=0.5,max_tokens=64,top_p=0,7)    
# Converter a resposta para o formato LangChain```python
ai_message = AIMessage(content=response.choices[0].message.content)
```return {"messages": state["messages"] + [ai_message]}```

Convertemos as mensagens do formato LangChain para o formato HuggingFace, assim podemos usar o modelo de linguagem.</markdown>
  <markdown>Definimos uma template para as queries
``` python
class QueryRequest(BaseModel):query: strthread_id: str = "padr√£o"```

As consultas ter√£o um `query`, a mensagem do usu√°rio, e um `thread_id`, que √© o identificador do fio da conversa√ß√£o e mais adiante explicaremos para que o utilizamos.</markdown>
  <markdown>Criamos um grafo de LangGraph
``` python
# Definir o gr√°ficoworkflow = StateGraph(state_schema=MessagesState)
# Defina o n√≥do na gr√°ficoworkflow.add_edge(START, "model")workflow.add_node("modelo", call_model)
# Adicionar mem√≥riamemory = MemorySaver()graph_app = workflow.compile(checkpointer=memory)```

Com isso, criamos um grafo de LangGraph, que √© uma estrutura de dados que nos permite criar um chatbot e gerenciar o estado do chatbot para n√≥s, ou seja, entre outras coisas, o hist√≥rico de mensagens. Dessa forma, n√£o precisamos fazer isso n√≥s mesmos.</markdown>
  <markdown>Criamos a aplica√ß√£o de FastAPI
``` python
app = FastAPI(title="LangChain FastAPI", description="API para gerar texto usando LangChain e LangGraph")```</markdown>
  <markdown>Criamos os endpoints da API
``` python
# Ponto de entrada Bem-vindo@app.get("/")async def api_home():"""Ponto de entrada Welcome"""return {"detail": "Bem-vindo ao tutorial de FastAPI, Langchain, Docker"}
# Gerar ponto final@app.post("/generate")async def gerar(request: QueryRequest):""""""Ponto final para gerar texto usando o modelo de linguagem    
Argumentos:solicita√ß√£o: QueryRequestquery: strthread_id: str = "padr√£o"
Retorna:um dicion√°rio contendo o texto gerado e o ID do thread""""""tente:# Configurar o ID da threadconfig = {"configurable": {"thread_id": request.thread_id}}        
# Crie a mensagem de entradainput_messages = [HumanMessage(content=request.query)]        
# Invocar o gr√°ficooutput = graph_app.invoke({"messages": input_messages}, config)        
# Obter a resposta do modeloresposta = output["messages"][-1].conte√∫do        
return {"generated_text": resposta,"thread_id": request.thread_id}except Exception as e:raise HTTPException(status_code=500, detail=f"Erro ao gerar texto: {str(e)}")```

Criamos o endpoint `/` que nos retornar√° um texto quando acessarmos a API, e o endpoint `/generate` que √© o que usaremos para gerar o texto.
Se n√≥s olharmos para a fun√ß√£o `generate`, temos a vari√°vel `config`, que √© um dicion√°rio que cont√©m o `thread_id`. Este `thread_id` √© o que nos permite ter um hist√≥rico de mensagens de cada usu√°rio, desta forma, diferentes usu√°rios podem usar o mesmo endpoint e ter seu pr√≥prio hist√≥rico de mensagens.</markdown>
  <markdown>Por √∫ltimo, temos o c√≥digo para que se possa executar a aplica√ß√£o
``` python
if __name__ == "__main__":import uvicornuvicorn.run(app, host="0.0.0.0", port=7860)```</markdown>
  <markdown>Vamos escrever todo o c√≥digo juntos
``` python
from fastapi import FastAPI, HTTPExceptionfrom pydantic import BaseModeldo huggingface_hub import InferenceClient
```markdown
from langchain_core.messages import HumanMessage, AIMessage
```from langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph
import osfrom dotenv import load_dotenvload_dotenv()
# Token da HuggingFaceHUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))
# Inicialize o modelo do HuggingFacemodel = InferenceClient(model="Qwen/Qwen2.5-72B-Instruct",api_key=os.getenv("HUGGINGFACE_TOKEN"))
# Defina a fun√ß√£o que chama o modelodef chamar_modelo(estado: EstadoMensagens):""""""Chame o modelo com as mensagens fornecidas
Argumentos:estado: MensagensState
Retorna:um dicion√°rio contendo o texto gerado e o ID do thread""""""# Converter mensagens do LangChain para o formato do HuggingFacehf_messages = []for msg in state["messages"]:if isinstance(msg, HumanMessage):hf_messages.append({"role": "user", "content": msg.content})elif isinstance(msg, AIMessage):hf_messages.append({"role": "assistant", "content": msg.content})    
# Chame a APIresposta = modelo.completar_chat(mensagens=hf_mensagens,temperature=0.5,max_tokens=64,top_p=0,7)    
# Converter a resposta para o formato LangChain```markdown
ai_message = AIMessage(content=response.choices[0].message.content)
```return {"messages": state["messages"] + [ai_message]}
# Definir o gr√°ficoworkflow = StateGraph(state_schema=MessagesState)
# Defina o n√≥ no grafoworkflow.add_edge(START, "model")workflow.add_node("modelo", call_model)
# Adicionar mem√≥riamemory = MemorySaver()graph_app = workflow.compile(checkpointer=memory)
# Defina o modelo de dados para o pedidoclass QueryRequest(BaseModel):query: strthread_id: str = "padr√£o"
# Criar a aplica√ß√£o FastAPIapp = FastAPI(title="LangChain FastAPI", description="API para gerar texto usando LangChain e LangGraph")
# Ponto de entrada Bem-vindo@app.get("/")async def api_home():"""Ponto de entrada Welcome"""return {"detalhe": "Bem-vindo ao tutorial de FastAPI, Langchain, Docker"}
# Gerar ponto final@app.post("/gerar")async def generate(request: QueryRequest):"""Ponto final para gerar texto usando o modelo de linguagem    
Argumentos:solicita√ß√£o: QueryRequestquery: strthread_id: str = "padr√£o"
Retorna:um dicion√°rio contendo o texto gerado e o ID do fio"""tente:# Configurar o ID da threadconfig = {"configurable": {"thread_id": request.thread_id}}        
# Criar a mensagem de entradainput_messages = [HumanMessage(content=request.query)]        
# Invocar o gr√°ficooutput = graph_app.invoke({"messages": input_messages}, config)        
# Obter a resposta do modeloresposta = output["messages"][-1].conte√∫do        
return {"generated_text": resposta,"thread_id": request.thread_id}except Exception as e:raise HTTPException(status_code=500, detail=f"Erro ao gerar texto: {str(e)}")
if __name__ == "__main__":import uvicornuvicorn.run(app, host="0.0.0.0", port=7860)```</markdown>
  <markdown>#### Dockerfile</markdown>
  <markdown>Agora vemos como criar o Dockerfile</markdown>
  <markdown>Primeiro indicamos a partir de qual imagem vamos come√ßar
``` dockerfile
FROM python:3.13-slim```</markdown>
  <markdown>Agora criamos o diret√≥rio de trabalho
``` dockerfile
RUN useradd -m -u 1000 userWORKDIR /app```</markdown>
  <markdown>Copiamos o arquivo com as depend√™ncias e instalamos
``` dockerfile
COPY --chown=user ./requirements.txt requirements.txtRUN pip install --no-cache-dir --upgrade -r requirements.txt```</markdown>
  <markdown>Copiamos o resto do c√≥digo
``` dockerfile
COPY --chown=user . /app```</markdown>
  <markdown>Exponhamos o porto 7860
``` dockerfile
EXPOSE 7860```</markdown>
  <markdown>Criamos as vari√°veis de ambiente
``` dockerfile
RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Segredo existe!"```</markdown>
  <markdown>Por √∫ltimo, indicamos o comando para executar a aplica√ß√£o
``` dockerfile
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]```</markdown>
  <markdown>Agora colocamos tudo junto
``` dockerfile
FROM python:3.13-slim
RUN useradd -m -u 1000 userWORKDIR /app
COPY --chown=user ./requirements.txt requirements.txtRUN pip install --no-cache-dir --upgrade -r requirements.txt
COPY --chown=user . /app
EXPOSE 7860
RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Segredo existe!"
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Criamos o arquivo com as depend√™ncias
``` txt
fastapiuvicornpedidospydantic&gt;=2.0.0langchainlangchain-huggingfacelangchain-corelanggraph &gt; 0.2.27python-dotenv.2.11```</markdown>
  <markdown>#### README.md</markdown>
  <markdown>Por fim, criamos o arquivo README.md com informa√ß√µes sobre o espa√ßo e as instru√ß√µes para o HuggingFace.
``` md
---t√≠tulo: Backend do SmolLM2emoji: üìäcolorFrom: amarelocolorTo: vermelhosdk: dockerpinned: falselicen√ßa: apache-2.0short_description: Backend do chat SmolLM2app_port: 7860---
# Backend do SmolLM2
Este projeto implementa uma API FastAPI que usa LangChain e LangGraph para gerar texto com o modelo Qwen2.5-72B-Instruct do HuggingFace.
## Configura√ß√£o
### No HuggingFace Spaces
Este projeto est√° projetado para ser executado em HuggingFace Spaces. Para configur√°-lo:
1. Crie um novo Espa√ßo no HuggingFace com o SDK Docker2. Configure a vari√°vel de ambiente `HUGGINGFACE_TOKEN` ou `HF_TOKEN` na configura√ß√£o do Space:- V√° para a aba "Configura√ß√µes" do seu Espa√ßo- Role para a se√ß√£o "Secrets do reposit√≥rio"- Adicione uma nova vari√°vel com o nome `HUGGINGFACE_TOKEN` e seu token como valor- Salve as altera√ß√µes
### Desenvolvimento local
Para o desenvolvimento local:
1. Clone este reposit√≥rio2. Crie um arquivo `.env` na raiz do projeto com seu token do HuggingFace:```
```HUGGINGFACE_TOKEN=seu_token_aqui```
```3. Instale as depend√™ncias:```
```pip install -r requirements.txt```
Por favor, forne√ßa o texto em Markdown que voc√™ gostaria de traduzir para o portugu√™s.
```
## Execu√ß√£o local
``bashuvicorn app:app --reload```
Por favor, forne√ßa o texto em Markdown que voc√™ gostaria de traduzir para o portugu√™s.
```
A API estar√° dispon√≠vel em `http://localhost:8000`.
## Endpoints
### GET `/`
Ponto final de boas-vindas que retorna uma mensagem de sauda√ß√£o.
### POST `/gerar`
Ponto final para gerar texto usando o modelo de linguagem.
**Par√¢metros da solicita√ß√£o:**``json{"query": "Sua pergunta aqui","thread_id": "identificador_de_thread_opcional"}```
Por favor, forne√ßa o texto em markdown que voc√™ gostaria de traduzir para o portugu√™s.
```
**Resposta:**``json```{"Texto gerado pelo modelo""thread_id": "identificador do thread"}```
Por favor, forne√ßa o texto em Markdown que deseja traduzir para o portugu√™s.
```
## Docker
Para executar a aplica√ß√£o em um cont√™iner Docker:
``bash# Construa a imagemdocker build -t smollm2-backend .
# Executar o cont√™inerdocker run -p 8000:8000 --env-file .env smollm2-backend```
```
## Documenta√ß√£o da API
A documenta√ß√£o interativa da API est√° dispon√≠vel em:- Swagger UI: `http://localhost:8000/docs`- ReDoc: `http://localhost:8000/redoc````</markdown>
  <markdown>### Token do HuggingFace
Se voc√™ notou no c√≥digo e no Dockerfile, usamos um token do HuggingFace, ent√£o vamos ter que criar um. Em nossa conta do HuggingFace, criamos um [novo token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), damos um nome a ele e concedemos as seguintes permiss√µes:
* Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob o seu namespace pessoal* Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob seu namespace pessoal* Fazer chamadas para provedores de infer√™ncia* Fazer chamadas para Pontos de Extremidade de Infer√™ncia
![backend docker - token](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp)</markdown>
  <markdown>### Adicionar o token aos secrets do espa√ßo</markdown>
  <markdown>Agora que j√° temos o token, precisamos adicion√°-lo ao espa√ßo. Na parte superior do aplicativo, poderemos ver um bot√£o chamado `Settings`, clicamos nele e poderemos ver a se√ß√£o de configura√ß√£o do espa√ßo.
Se formos para baixo, poderemos ver uma se√ß√£o onde podemos adicionar `Variables` e `Secrets`. Neste caso, como estamos adicionando um token, vamos adicion√°-lo aos `Secrets`.
Damos o nome `HUGGINGFACE_TOKEN` e o valor do token.</markdown>
  <markdown>### Implanta√ß√£o</markdown>
  <markdown>Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.
Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.
Neste caso, constru√≠mos apenas um backend, portanto o que vamos ver ao entrar no espa√ßo √© o que definimos no endpoint `/`
![backend docker - espa√ßo](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp)</markdown>
  <markdown>### URL do backend</markdown>
  <markdown>Precisamos saber a URL do backend para poder fazer chamadas √† API. Para isso, temos que clicar nos tr√™s pontos no canto superior direito para ver as op√ß√µes.
![backend docker - op√ß√µes](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp)</markdown>
  <markdown>No menu suspenso, clicamos em `Embed this Spade`. Ser√° aberta uma janela indicando como incorporar o espa√ßo com um iframe e tamb√©m fornecer√° a URL do espa√ßo.
![backend docker - embed](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp)</markdown>
  <markdown>Se agora formos para essa URL, veremos o mesmo que no espa√ßo.</markdown>
  <markdown>### Documenta√ß√£o</markdown>
  <markdown>FastAPI, al√©m de ser uma API extremamente r√°pida, tem outra grande vantagem: gera documenta√ß√£o automaticamente.</markdown>
  <markdown>Se adicionarmos `/docs` √† URL que vimos anteriormente, poderemos visualizar a documenta√ß√£o da API com o `Swagger UI`.
![backend docker - swagger doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp)</markdown>
  <markdown>Tamb√©m podemos adicionar `/redoc` √† URL para ver a documenta√ß√£o com `ReDoc`.
![backend docker - redoc doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp)</markdown>
  <markdown>### Teste da API</markdown>
  <markdown>O bom da documenta√ß√£o `Swagger UI` √© que nos permite testar a API diretamente do navegador.</markdown>
  <markdown>Adicionamos `/docs` √† URL que obtivemos, abrimos o menu suspenso do endpoint `/generate` e clicamos em `Try it out`, modificamos o valor da `query` e do `thread_id` e clicamos em `Execute`.
No primeiro caso vou colocar
* **query**: Ol√°, como voc√™ est√°? Sou M√°ximo* **thread_id**: user1
![backend docker - test API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp)</markdown>
  <markdown>Recebemos a seguinte resposta `Ol√° M√°ximo! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? Em que posso ajudar hoje?`
![backend docker -response 1 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp)</markdown>
  <markdown>Vamos testar agora a mesma pergunta, mas com um `thread_id` diferente, neste caso `user2`.
![backend docker - query 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp)</markdown>
  <markdown>E nos responde isso `Ol√° Luis! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? No que posso ajudar hoje?`
![backend docker - response 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp)</markdown>
  <markdown>Agora pedimos nosso nome com os dois usu√°rios e obtemos isso
* Para o usu√°rio **user1**: `Voc√™ se chama M√°ximo. H√° algo mais em que eu possa ajudar voc√™?`* Para o usu√°rio **user2**: `Voc√™ se chama Luis. H√° mais alguma coisa em que eu possa ajud√°-lo hoje, Luis?`
![backend docker - response 2 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp)
![backend docker - response 2 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp)</markdown>
  <markdown>## Deploy do backend com Gradio e modelo rodando no servidor</markdown>
  <markdown>Os dois backends que criamos na verdade n√£o est√£o executando um modelo, mas sim fazendo chamadas para Inference Endpoints da HuggingFace. Mas pode ser que queiramos que tudo rode no servidor, inclusive o modelo. Pode ser que voc√™ tenha feito um fine-tuning de um LLM para seu caso de uso, por isso j√° n√£o pode fazer chamadas para Inference Endpoints.
Ent√£o vamos ver como modificar o c√≥digo dos dois backends para executar um modelo no servidor e n√£o fazer chamadas para Inference Endpoints.</markdown>
  <markdown>### Criar Espa√ßo</markdown>
  <markdown>Na hora de criar o space no HuggingFace fazemos o mesmo que antes, criamos um novo space, colocamos um nome e uma descri√ß√£o, selecionamos Gradio como SDK, selecionamos o HW em que vamos deployar, no meu caso selecionei o HW mais b√°sico e gratuito, e selecionamos se o faremos privado ou p√∫blico.</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>Temos que fazer altera√ß√µes em `app.py` e em `requirements.txt` para que, em vez de fazer chamadas a Inference Endpoints, o modelo seja executado localmente.</markdown>
  <markdown>#### app.py</markdown>
  <markdown>As mudan√ßas que temos que fazer s√£o</markdown>
  <markdown>Importar `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importar `torch`
``` python
from transformers import AutoModelForCausalLM, AutoTokenizerimport torch```</markdown>
  <markdown>Em vez de criar um modelo com `InferenceClient`, criamos com `AutoModelForCausalLM` e `AutoTokenizer`.
``` python
# Carregar o modelo e o tokenizermodel_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForCausalLM.from_pretrained(nome_do_modelo,torch_dtype=torch.float16,device_map="auto")```

Utilizo `HuggingFaceTB/SmolLM2-1.7B-Instruct` porque √© um modelo bastante capaz com apenas 1.7B de par√¢metros. Como escolhi o hardware mais b√°sico, n√£o posso usar modelos muito grandes. Voc√™, se quiser usar um modelo maior, tem duas op√ß√µes: usar o hardware gratuito e aceitar que a infer√™ncia ser√° mais lenta, ou usar um hardware mais potente, mas pago.</markdown>
  <markdown>Modificar a fun√ß√£o `respond` para que construa o prompt com a estrutura necess√°ria pela biblioteca `transformers`, tokenizar o prompt, fazer a infer√™ncia e destokenizar a resposta.
``` python
def responder(mensagem,hist√≥rico: list[tuple[str, str]],Mensagem do sistema,max_tokens,temperatura,top_p,):# Construir o prompt com o formato corretoprompt = f"&lt;|system|&gt;\n{system_message}&lt;/s&gt;\n"    
for val in history:if val[0]:prompt += f"&lt;|user|&gt;\n{val[0]}&lt;/s&gt;\n"if val[1]:prompt += f"&lt;|assistant|&gt;\n{val[1]}&lt;/s&gt;\n"    
prompt += f"&lt;|user|&gt;\n{message}&lt;/s&gt;\n&lt;|assistant|&gt;\n"    
# Tokenizar o promptinputs = tokenizer(prompt, return_tensors="pt").to(model.device)    
# Gerar a respostaoutputs = model.generate(**entradas,**max_new_tokens=max_tokens,temperature=temperature,top_p=top_p,do_sample=True,pad_token_id=tokenizer.eos_token_id)    
# Decodificar a respostaresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)    
# Extrair apenas a parte da resposta do assistenteresponse = response.split("&lt;|assistant|&gt;\n")[-1].strip()    
yield response```</markdown>
  <markdown>A seguir deixo todo o c√≥digo
``` python
import gradio as grfrom transformers import AutoModelForCausalLM, AutoTokenizerimport torch
""""""Para mais informa√ß√µes sobre o suporte √† API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference""""""
# Carregar o modelo e o tokenizermodel_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForCausalLM.from_pretrained(nome_do_modelo,torch_dtype=torch.float16,device_map="auto")
def responder(mensagem,hist√≥ria: list[tuple[str, str]],Mensagem do sistema,max_tokens,temperatura,top_p,):# Construir o prompt com o formato corretoprompt = f"&lt;|system|&gt;\n{system_message}&lt;/s&gt;\n"    
for val in history:if val[0]:prompt += f"&lt;|user|&gt;\n{val[0]}&lt;/s&gt;\n"if val[1]:prompt += f"&lt;|assistant|&gt;\n{val[1]}&lt;/s&gt;\n"    
prompt += f"&lt;|user|&gt;\n{message}&lt;/s&gt;\n&lt;|assistant|&gt;\n"    
# Tokenizar o promptinputs = tokenizer(prompt, return_tensors="pt").to(model.device)    
# Gerar a respostasa√≠das = modelo.gerar(**entradas,**max_new_tokens=max_tokens,temperature=temperature,top_p=top_p,do_sample=True,pad_token_id=tokenizer.eos_token_id)    
# Decodificar a respostaresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)    
# Extrair apenas a parte da resposta do assistenteresponse = response.split("&lt;|assistant|&gt;\n")[-1].strip()    
yield response

""""""Para informa√ß√µes sobre como personalizar o ChatInterface, consulte a documenta√ß√£o do Gradio: https://www.gradio.app/docs/gradio/chatinterface""""""demo = gr.ChatInterface(responda,```markdown
additional_inputs=[
```gr.Textbox(value="Voc√™ √© um Chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™."r√≥tulo="Mensagem do sistema"),gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="M√°ximo de novos tokens"),gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo="Temperatura"),gr.Slider()m√≠nimo=0.1,m√°ximo=1.0,value=0.95,passo=0.05,label="Top-p (amostragem do n√∫cleo)"),],)

if __name__ == "__main__":demo.launch()```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Neste arquivo, devemos adicionar as novas bibliotecas que vamos utilizar, neste caso `transformers`, `accelerate` e `torch`. O arquivo completo ficaria:
``` txt
huggingface_hub==0.25.2gradio&gt;=4.0.0transformers&gt;=4.36.0torch&gt;=2.0.0accelerate&gt;=0.25.0```</markdown>
  <markdown>#### Teste da API</markdown>
  <markdown>Desplegamos o space e testamos diretamente a API.</markdown>
  <input_code>from gradio_client import Client

client = Client("Maximofn/SmolLM2_localModel")
result = client.predict(
		message="Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo",
		system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",
		max_tokens=512,
		temperature=0.7,
		top_p=0.95,
		api_name="/chat"
)
print(result)</input_code>
  <output_code>Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî
Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?
</output_code>
  <markdown>Surpreende-me o qu√£o r√°pido o modelo responde, mesmo estando em um servidor sem GPU.</markdown>
  <markdown>## Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor</markdown>
  <markdown>Agora fazemos o mesmo que antes, mas com FastAPI, LangChain e Docker.</markdown>
  <markdown>### Criar Espa√ßo</markdown>
  <markdown>Ao criar o space no HuggingFace, fazemos o mesmo que antes: criamos um novo espa√ßo, colocamos um nome e uma descri√ß√£o, selecionamos Docker como SDK, escolhemos o HW em que vamos implant√°-lo, no meu caso, escolho o HW mais b√°sico e gratuito, e decidimos se o faremos privado ou p√∫blico.</markdown>
  <markdown>### C√≥digo</markdown>
  <markdown>#### app.py</markdown>
  <markdown>J√° n√£o importamos `InferenceClient` e agora importamos `AutoModelForCausalLM` e `AutoTokenizer` da biblioteca `transformers` e importamos `torch`.
``` python
from transformers import AutoModelForCausalLM, AutoTokenizerimport torch```</markdown>
  <markdown>Instanciamos o modelo e o tokenizer com `AutoModelForCausalLM` e `AutoTokenizer`.
``` python
# Inicialize o modelo e o tokenizadorprint("Carregando modelo e tokenizer...")dispositivo = "cuda" if torch.cuda.is_available() else "cpu"model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
tente:# Carregar o modelo no formato BF16 para melhor desempenho e menor uso de mem√≥riatokenizer = AutoTokenizer.from_pretrained(model_name)    
if device == "cuda":print("Usando GPU para o modelo...")model = AutoModelForCausalLM.from_pretrained(nome_do_modelo,torch_dtype=torch.bfloat16,device_map="auto",low_cpu_mem_usage=True)else:print("Usando CPU para o modelo...")model = AutoModelForCausalLM.from_pretrained(nome_do_modelo,device_map={"": device},torch_dtype=torch.float32)
print(f"Modelo carregado com sucesso em: {device}")except Exception as e:print(f"Erro ao carregar o modelo: {str(e)}")aumentar```</markdown>
  <markdown>Re definimos a fun√ß√£o `call_model` para que fa√ßa a infer√™ncia com o modelo local.
``` python
# Defina a fun√ß√£o que chama o modelodef chamar_modelo(estado: EstadoMensagens):"""Chame o modelo com as mensagens fornecidas
Argumentos:estado: EstadoMensagens
Retorna:dicion√°rio: Um dicion√°rio contendo o texto gerado e o ID do thread""""""# Converter mensagens LangChain para formato de bate-papomensagens = []for msg in state["messages"]:if isinstance(msg, HumanMessage):messages.append({"role": "user", "content": msg.content})elif isinstance(msg, AIMessage):messages.append({"role": "assistant", "content": msg.content})    
# Prepare o input usando o modelo de bate-papoinput_text = tokenizer.apply_chat_template(messages, tokenize=False)inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)    
# Gerar respostasa√≠das = modelo.gerar(entradas,max_new_tokens=512,  # Aumente o n√∫mero de tokens para respostas mais longastemperature=0.7,top_p=0,9,do_sample=True,pad_token_id=tokenizer.eos_token_id)    
# Decodificar e limpar a respostaresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)# Extrair apenas a resposta do assistente (ap√≥s a √∫ltima mensagem do usu√°rio)response = response.split("Assistant:")[-1].strip()    
# Converter a resposta para o formato LangChain```markdown
ai_message = AIMessage(content=response)
```return {"messages": state["messages"] + [ai_message]}```</markdown>
  <markdown>#### requirements.txt</markdown>
  <markdown>Temos que remover `langchain-huggingface` e adicionar `transformers`, `accelerate` e `torch` no arquivo `requirements.txt`. O arquivo ficaria:
``` txt
fastapiuvicornsolicita√ß√µespydantic&gt;=2.0.0langchain&gt;=0.1.0langchain-core&gt;=0.1.10langgraph&gt;=0.2.27python-dotenv&gt;=1.0.0transformers&gt;=4.36.0torch&gt;=2.0.0accelerate&gt;=0.26.0```</markdown>
  <markdown>#### Dockerfile</markdown>
  <markdown>J√° n√£o precisamos ter `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` porque como o modelo vai estar no servidor e n√£o vamos fazer chamadas para Inference Endpoints, n√£o precisamos do token. O arquivo ficaria:
``` dockerfile
FROM python:3.13-slim
RUN useradd -m -u 1000 userWORKDIR /app
COPY --chown=user ./requirements.txt requirements.txtRUN pip install --no-cache-dir --upgrade -r requirements.txt
COPY --chown=user . /app
EXPOSE 7860
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]```</markdown>
  <markdown>### Teste da API</markdown>
  <markdown>Deployamos o space e testamos a API. Neste caso, vou testar diretamente do Python.</markdown>
  <input_code>import requests

url = "https://maximofn-smollm2-backend-localmodel.hf.space/generate"
data = {
    "query": "Hola, ¬øc√≥mo est√°s?",
    "thread_id": "user1"
}

response = requests.post(url, json=data)
if response.status_code == 200:
    result = response.json()
    print("Respuesta:", result["generated_text"])
    print("Thread ID:", result["thread_id"])
else:
    print("Error:", response.status_code, response.text)</input_code>
  <output_code>Respuesta: system
You are a friendly Chatbot. Always reply in the language in which the user is writing to you.
user
Hola, ¬øc√≥mo est√°s?
assistant
Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.
Thread ID: user1
</output_code>
  <markdown>Este demora um pouco mais que o anterior. Na verdade, demora o normal para um modelo sendo executado em um servidor sem GPU. O estranho √© quando o deployamos no Gradio. N√£o sei o que a HuggingFace faz por tr√°s, ou talvez tenha sido coincid√™ncia.</markdown>
  <markdown>## Conclus√µes</markdown>
  <markdown>Vimos como criar um backend com um LLM, tanto fazendo chamadas ao Inference Endpoint da HuggingFace, quanto fazendo chamadas a um modelo rodando localmente. Vimos como fazer isso com Gradio ou com FastAPI, Langchain e Docker.</markdown>
  <markdown>A partir daqui voc√™ tem o conhecimento para poder implantar seus pr√≥prios modelos, mesmo que n√£o sejam LLMs, podem ser modelos multimodais. A partir daqui voc√™ pode fazer o que quiser.</markdown>
</notebook>